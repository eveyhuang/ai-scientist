{
    "call": "This funding organization is dedicated to catalyzing multidisciplinary scientific teams to synthesize publicly available data to address fundamental questions related to emergence phenomena in the molecular and cellular biosciences. It supports community-scale synthesis and integration of existing molecular and cellular data to: Answer novel questions Solve long-standing puzzles in the molecular and cellular sciences Develop innovative research and analytical strategies Tap diverse new talent Train the future data-savvy workforce A community-scale synthesis project has three defining characteristics. First, it uses only existing publicly available data and often integrates distinct datasets together. It does not generate any new experimental data. Second, the project is beyond the capabilities of most individual research labs and instead requires collaboration between two or more labs. Finally, such a project requires diverse scientific perspectives and expertise to come together, often resulting in novel, deeper, and broader insights. You research should focus on well-defined scientific questions or methodological developments that advance molecular and cellular sciences through synthesis research to acquire insights from a collaborative, transdisciplinary approach. This RFP invites proposals for Working Groups that will: Address Compelling Scientific Questions: Tackle novel and significant research questions in molecular and cellular biology through data synthesis. Stimulate Cross-Disciplinary Collaboration: Bring together researchers from diverse fields. Utilize Publicly Available Data: Leverage existing data sources to develop innovative research and analytical strategies. Require NCEMS Support and Resources: The proposed synthesis project must demonstrate a clear need for NCEMS support that goes beyond the capabilities of a single lab or existing collaboration. Promote Collaborative Partnerships: Assemble teams that reflect a range of scientific expertise, geographic locations, career stages, and institutional research levels. Adhere to Open Science Principles: Commit to making findings, data, and analysis workflows publicly available in accordance with community-created policies on open, team, and reproducible science. Train the Next Generation: Provide training opportunities to trainees (graduate students and postdocs) through collaborative research efforts within the working group.",


    "proposals": [
        {
            "proposal_id": "1",
            "proposal_title": "CYTOKINETIC BOTTLENECKS OF HEAT WAVES",
            "authors": ["Andrei Smertenko", "Carolyn Rasmussen", "Dawn Nagel", "Georgia Drakakaki", "Stephen Ficklin"],
            "authors_departments": ["Biological Chemistry", "Botany and Plant Sciences", "Plant Sciences", "Horticulture"],
            "abstract": "The lack of motility makes plants highly vulnerable to environmental anomalies such as periods of extreme high temperature known as heat waves. Plants can search for better habitats by exploiting seed mobility, which relies on completing the life cycle and producing seeds. Formation of reproductive organs and seed development requires the production of cells through cell division. The last stage of cell division, cytokinesis, encompasses many key cellular processes, including vesicle trafficking, cell wall biosynthesis, cytoskeletal dynamics, and signaling. While cytokinesis is highly sensitive to heat stress in plants adapted to temperate climates, cytokinesis is more resilient in species adapted to tropical zones. However, how heat stress affects cytokinesis and the mechanisms of cytokinetic robustness across different plant species remains unknown. This project aims to compare changes in cytokinetic components under heat stress in susceptible and hardy species to characterize differential responses of genetic circuits responsible for specific cellular events during cytokinesis. The long term goal is to harness this data for identifying the bottlenecks of cytokinetic robustness. The experimental approach here will be systematic analysis of publicly available data from single-cell and whole-organ omics data collected under control and high-temperature conditions including transcriptomics, translatomics, and proteomics. Predicting cytokinetic bottlenecks will inform future experimental design for their functional characterization leading to better understanding of cellular adaptation mechanisms to heat stress. This knowledge may provide insight into evolutionary strategies of plant adaptation to different climates and improve predictions for heat resiliency at the level of individual plants and populations.",
            "full_draft": "Introduction 4.1. Problem overview. 4.1.1. Cytokinesis is critical for plant life. The majority of planetary plant biomass derives from cytokinesis. Plant cytokinesis occurs via the formation of a specialized membrane compartment, the cell plate, between daughter cells. The cell plate gradually matures into a cell wall after cytokinesis is complete. One of the key processes accompanying this transition is replacing callose, the main polysaccharide in the cell plate lumen, with cellulose. A plant-specific polarized secretory module, the phragmoplast, is responsible for constructing the cell plate. The phragmoplast consists of microtubules and actin cytoskeleton, vesicles, and associated proteins. As the phragmoplast diameter is commonly smaller than the cell width, completing the cell plate construction requires phragmoplast expansion. The expansion is driven by microtubules and requires coordination between microtubule dynamics, membrane and cell-wall-component trafficking, exocytosis, vesicle fusion, and endocytosis. Phragmoplast expansion encompasses four steps: establishment of microtubule overlap, vesicle recruitment, vesicle fusion and remodeling, and microtubule depolymerization. Each step requires a dedicated genetic network comprising structural proteins, enzymes, and signaling components. 4.1.2. Cytokinesis is sensitive to heat stress. Exposing tobacco tissue culture cells to 38°C for 2 h causes cytokinetic arrest and elongation of phragmoplast microtubules while cells remain viable; subsequently, microtubule organization does not recover even by 2 hours at normal temperature, indicating that the arrest is long-term. In meiotic Arabidopsis cells treated at 36–38°C, chromosome segregation is disrupted and microtubule density in the phragmoplast is reduced. In the flowering sea grass Cymodocea nodosa, treatment at 38°C for 2 h depolymerizes some phragmoplast microtubules but does not block cytokinesis. The graded heat stress responses among species will allow us to identify both common and distinct resilience mechanisms of cytokinesis. 4.1.3. Outcomes of prior studies 4.1.3.1. Cytokinesis is differentially sensitive to heat stress within the plant lineage. Arabidopsis plants are killed at about 35°C whereas maize plants are killed by temperatures at least 10 degrees higher. Consequently, maize cytokinesis could be resilient to heat stress. We compared the impact of heat stress on cytokinesis in these species. On the basis of published data 38°C was used to induce stress. Exposure of 6-day-old Arabidopsis seedlings to 38°C for 1 hour caused cell plate tilting and disorganization, as indicated by the cytokinetic vesicle marker RABA2a. Cell plate assembly failed resulting in formation of cell wall stubs and phragmoplasts were aberrantly long. We examined microtubule stability in the elongated phragmoplasts using fluorescence recovery after photobleaching and found very slow recovery of the fluorescent signal. As the signal recovery requires microtubule depolymerization, this outcome suggests that heat stress suppressed depolymerization of phragmoplast microtubules. Oppositely, in 3-week-old maize seedlings the phragmoplast morphology appeared unaltered and no cell wall stubs were detected even after exposure to 42°C or 45°C. Thus, cytokinesis is heat-resilient in maize. This outcome is consistent with the report that the 2021 heat wave caused yield losses to all dryland farming crops except maize. Therefore, comparing the impact of 38°C on cytokinetic components in Arabidopsis and tobacco with those in maize will likely inform on mechanisms of cytokinetic robustness. 4.1.3.2. Impact of heat stress on transcription and translation of cytokinetic genes. While analyzing the impact of heat stress on circadian clock-regulated genes, we discovered changes in both the abundance of mRNAs and in the ribosome-binding fraction encoding key cytokinetic genes. For example, the phragmoplast-localized kinesin HINKEL was reduced after heat treatment on both transcriptome and translatome levels. AIR9 transcripts were not significantly affected by heat treatment. Heat stress resulted in lower transcription and translation of multiple cytokinetic genes with the exception of the midzone localized formin homology protein AtFH8 whose translation was specifically upregulated. AtFH8 might contribute to the stabilization of microtubule plus-ends at the phragmoplast midzone. Up-regulation of this protein could be a mechanism to sustain phragmoplast organization under heat stress. Therefore, comparison of transcriptome and translatome data captures the complexity of cytokinetic gene regulation in responses to stresses. 4.1.4. Conceptual framework. According to the theory of biological robustness, genetic networks for individual traits contain specific robustness alleles instead of a master robustness allele acting across multiple networks. Consequently, known robustness alleles for other traits are unlikely to contribute to heat resiliency of cytokinesis. Instead, cytokinetic robustness must be achieved by temporally controlling and genetically buffering distinct cytokinetic processes in a modular fashion. When a genetic network experiences an interruption, both the central hubs and less connected nodes could either limit or maintain resiliency. Consequently, identifying cytokinetic bottlenecks requires generating cytokinetic protein networks and comparing their behaviour under normal and stress conditions. Construction of cytokinetic networks is hindered by limited information about their components. Despite genetic screens for cytokinetic mutants having already identified multiple players, many cytokinetic genes remain unidentified either due to genetic redundancy or lethality. Furthermore, despite biophysical modeling indicating that callose provides the spreading force for membrane remodeling during cell plate expansion, there are numerous knowledge gaps about other carbohydrates and corresponding enzymes during cytokinesis. 4.2. Goals. The long-term goal of our research is to determine the rules and limits of cytokinetic robustness under heat stress and harness this knowledge for improving plant resiliency. Although it has been shown that high temperatures perturb membrane assembly and cytoskeletal dynamics during cytokinesis the underlying molecular mechanisms of such changes remain unknown. The objective of this project is to exploit available data to predict bottlenecks of cytokinesis. A bottleneck is defined as a gene whose product becomes dysfunctional or less functional in response to heat stress. The rationale for this project is based on the following elements: heat stress induces transcriptional changes of genes encoding cytokinetic proteins; plants adapted to hotter environments likely evolved mechanisms to buffer the effects of heat stress on cell division; analysis of available data can inform on differences of cytokinetic responses in heat resilient and heat tolerant species. We plan to achieve the main objective through three specific aims below. 5. Proposed Activities. 5.1. Aim 1. Construct cytokinetic networks using single-cell omics data. Our knowledge about cytokinetic components is limited to several dozens of cytokinesis-defective mutants, a few protein-protein interaction maps, and information about mitotic transcripts. Many proteins required for cytokinesis remain unknown. This aim will take a holistic approach to identify the cytokinetic gene networks using single-cell transcriptomics data from plant species including Arabidopsis thaliana, Zea mays, Oryza sativa, Sorghum bicolor, and Nicotiana tabacum to identify conserved cytokinetic transcripts as well as species-specific cytokinetic transcripts. Raw data will be reprocessed following a standardized pipeline for all omics datasets to allow for effective comparative analysis and interpretation of the data. For single-cell transcriptomics, the analysis for count normalization, dimensionality reduction, clustering visualization, trajectory analysis, and differential gene expression analysis may vary depending on the lab. Therefore, data reprocessing will account for variation and enable statistically significant perturbations to be compared. Then the following pipeline will be used. First, a list of mitotic-related genes including paralogs will be compiled for Arabidopsis from published data. Second, a standardized pipeline such as an orthology inference method will identify orthologous genes from maize, sorghum, rice, and tobacco. Third, the single-cell transcriptome data will be either analyzed or reanalyzed to identify co-expressed transcripts. Fourth, transcripts that are only expressed in cells expressing known mitotic-specific transcripts will be selected as benchmark transcripts to identify coexpressing transcripts. Then, transcripts that appear in fewer than half of the datasets per organism will be more closely assessed for removal from the list. The expected outcome would be a table of Arabidopsis mitosis and cytokinesis proteins and their homologs in other plant species. 5.2. Aim 2. Compare cytokinetic responses to heat stress in heat susceptible and resilient species. Our preliminary experiments demonstrate that cytokinesis is heat-sensitive in tobacco and Arabidopsis thaliana, and heat-tolerant in Zea mays. This aim will compare omics data of Zea mays and other heat-tolerant species with those of heat-sensitive species. As in Aim 1, the raw data will be reprocessed. Analysis will focus on identifying responses that are different between tolerant and susceptible species for each type of omics analyses. We will also identify the transcription factors and cis-regulatory components of these genes that act specifically in heat stress response using available datasets. Completing this aim will inform on which pathways are affected by heat stress in heat tolerant and susceptible species. 5.3. Aim 3. Predict cytokinetic bottlenecks through integration of available datasets. The activities in this aim will construct multi-layered cytokinetic-specific networks and compare their changes under heat stress conditions in susceptible and tolerant species. The first step will be single-layer analysis within a single type of omics data. Analyses for each layer will include differential analysis, correlation network analysis, time series clustering, and association networks. The second stage will be construction of a multiplex network that combines single-layer networks from different omics datasets into a multilayer model. Connections, or edges, between the layers indicate inter-layer relationships. Analysis of networks is expected to generate candidate molecular features with potential influence on differences in cytokinetic responses. These comparisons are predicted to identify cytokinetic bottlenecks under heat stress. We will write a manuscript on this topic and submit it towards the end of year 2. 5.4. Potential problems could arise from the datasets being generated using plants grown under different conditions as well as due to the differences in how the normalization and further analysis were performed. To address this problem we will reanalyze all datasets as outlined in Specific Aim 1, and also including a large number of datasets will help to average the noise caused by variable growth conditions. 6. Outcomes. We anticipate this project will predict cytokinetic bottlenecks under heat stress. This information will be used to formulate the theory of cytokinetic responses to heat stress. The theory will aim to answer the following questions. Which changes to cytokinetic processes in response to heat stress measured either at the transcript or the protein level are common to all species or species-specific? Are there specific subsets of cytokinesis genes or paralogs that have a differential response to heat stress? Do plants eliminate proteins or transcripts or stabilize them to adapt to heat stress? Would these changes promote or block cytokinesis? We will publish this theory as a review article. The theory will inform on the experiments that will lead to the identification of novel genetic, biochemical, and cellular markers of plant heat stress response. Such markers would enable predicting the performance of individual species and stability of entire ecosystems under elevated temperatures, identifying resilient genotypes among the available germplasm for urban ecosystems and afforestation projects, breeding crops with higher yield in hotter environments, developing new approaches for engineering heat-resilient landscaping plant species to contribute to sustainable habitats for humans and animals. 7. Rationale for a working group approach. Our team possesses expertise in cell biology of plant cytokinesis, membrane trafficking, heat stress impact on plant physiology, and protein biochemistry. We collectively have a deep mechanistic understanding of cytokinesis. Further, we collected preliminary data showing that heat stress does not break cytokinetic processes in susceptible plant species, but prevents cytokinetic progression through the arrest of cellular dynamics. The next logical step is to identify cytokinetic pathways affected by the heat stress, and compare them in tolerant and susceptible species using a bioinformatics approach. We want to team up with a synthesis center for the next important stage in understanding the impact of heat stress on cytokinesis. 8. Rationale for support. We hope to gain access to professional expertise in bioinformatics and complex data analysis. Publicly available single-cell and whole-tissue heat stress omics data offers rich material for predicting cytokinetic bottlenecks. Collaboration will lower the barriers of analyzing publicly available omics data for the investigators. Our cell biological data suggests that there will be differentially regulated cytokinetic pathways in heat tolerant and susceptible organisms and we can predict them using bioinformatics analyses. There is currently no alternative funding mechanism to support the transition from an initial concept to an experimental analysis. This project will contribute to developing the theory of cytokinetic robustness and help us to design experiments to test it. 9. Requested resources. To successfully perform the work described in the aims above, a staff scientist with bioinformatics expertise is requested for 4 months per year and an assistant staff scientist for 6 months per year for 2 years. Computational resources for storage and data processing are also needed to successfully implement and achieve the above aims. To accommodate the download and storage of the publicly available datasets listed in the data sources table and support the analyses we estimate and are requesting 50 terabytes of storage and 25,000 CPU hours. ",
            "proposal_status": "Accepted",
            "ranking": 1
        }, 
        {
            "proposal_id": "2",
            "proposal_title": "Elucidating genome structure-function relationships in human brain cells",
            "authors": ["Bin Zhang", "Longzhi Tan", "Tamar Schlick", "Dave Thirumalai", "Justin Whalley"],
            "authors_departments": ["Chemistry", "Neurobiology", "Mathematics", "Computer Science", "Bioinformatics"],
            "abstract": "Single-cell analysis has profoundly enhanced our understanding of brain tissue by facilitating the examination of gene expression and epigenetic features. However, investigating these modalities independently offers a fragmented view of gene regulation and its relationship to transcriptional activity. To overcome this limitation, we propose developing a multimodal generative AI model that integrates single-cell genomics datasets. This framework seeks to illuminate the interplay between three-dimensional genomic architecture and gene expression, uncovering the physical principles underlying the emergence of cell-type-specific gene expression programs, despite the inherent stochasticity of gene regulation at the single-cell level. Applied to datasets from Alzheimer's patients, this approach has the potential to identify epigenetic drivers of disease progression, thereby contributing to a deeper understanding of neurodegenerative processes.",
            "full_draft": "Introduction and Goals The human brain is a highly complex biological system that contains a wide array of distinct cell types, each characterized by specific functions and properties. Understanding this intricate neural architecture requires an investigation into the molecular mechanisms that regulate the brain's cellular diversity, development, and function. A key aspect of this research is the classification of all cell types and the elucidation of their origins from a shared genetic framework. All brain cells share the same genome, yet their identities and functions are shaped by their epigenomes, which consist of covalent modifications to histones and DNA. Chromatin, the structure formed by DNA wrapping around histone proteins, regulates gene expression in eukaryotic cells through a complex network of chemical modifications. These modifications determine the transcriptional activity of the genome within each cell, thus defining their distinct characteristics and roles. To fully characterize a cell, it is essential to identify specific chemical modifications in DNA regions, assess DNA accessibility, analyze chromatin spatial organization, and measure RNA transcription. Single-cell analysis provides a powerful framework for uncovering details and variability often obscured in population-averaged studies. By profiling RNA expression, chromatin accessibility, chromatin organization, and DNA methylation, such analyses have advanced our understanding of the cellular diversity within human brain tissue, shedding light on the intricate epigenetic elements that may constitute regulatory networks. Nonetheless, examining individual modalities at the single-cell level offers an incomplete perspective on the complexity of gene regulatory networks and is insufficient for inferring causal relationships between these networks and gene expression. We propose methodologies to integrate single-cell genomics data for constructing multimodal datasets. With these datasets, we will develop a multimodal generative AI model to capture the distribution across three genomic domains: chromatin accessibility, three-dimensional chromatin organization, and gene expression. This model will define genomic states, offering molecular insights into neuron characteristics, and facilitate a mechanistic understanding of the structure-function relationships within the genome. Further applying the model to single-cell data collected from Alzheimer’s patients will provide insight into molecular determinants that drive disease progression. Proposed Activities Develop a multimodal model for human brain cells. A primary objective of the working group is to develop a multimodal model that integrates chromatin accessibility and three-dimensional chromatin organization with gene expression. This initiative is inspired by recent advances in deep learning techniques, particularly in AI models capable of synthesizing text and image data to provide comprehensive contextual insights. These models have shown considerable effectiveness across various domains, including multimodal data analysis and predictive modeling. The development of this model will enhance the understanding of chromatin organization’s functional role in regulating gene expression under normal conditions and its potential contribution to aberrant gene expression during disease progression. Model Design The proposed model is based on the transformer architecture linking DNA sequences and chromatin accessibility to gene expression. The design builds upon the framework of EPCOT introduced by Zhang et al., which generates transferable embeddings for DNA sequence and chromatin accessibility data that can be fine-tuned for diverse downstream tasks. Notably, this framework demonstrated that these embeddings enable reliable predictions of gene expression from bulk RNA-seq data. Our approach introduces two key advances: a focus on single-cell RNA-seq prediction and the incorporation of structural information from single-cell Hi-C data to improve predictive accuracy. The three-dimensional chromatin organization captured by single-cell Hi-C is represented as a contact matrix and processed using the Vision Transformer architecture, originally designed for image data but successfully applied to Hi-C data. The Vision Transformer encoder transforms the three-dimensional chromatin data into numerical embeddings, which are integrated into the expression prediction module through cross-attention layers. Specifically, the three-dimensional organization is represented by a contact matrix centered on the target gene. This matrix encapsulates the structural context surrounding the gene, including chromatin loops, topologically associating domains, and compartments. The spatial range of the proposed model substantially exceeds that of existing approaches, enabling it to capture a diverse array of gene regulatory mechanisms. Model Parameterization and Validation Parameterizing this model requires co-assay data obtained from experiments that concurrently measure chromatin organization and gene expression. Due to limited availability of such datasets, we propose adopting a pretraining and fine-tuning approach to enhance model transferability. The encoder for single-cell Hi-C will be pretrained using data sources that provide chromatin organization and gene expression information at single-cell resolution for various human brain tissues. These datasets will be consolidated into a consensus list of cell types, facilitating the learning of cell-type-specific chromatin organization features that underpin the corresponding gene expression programs. Beyond generating embeddings for gene expression prediction, the proposed model, termed single-cell chromatin organization embedding (scCOE), is expected to serve as a robust tool for extracting interpretable features of chromatin organization. The pretraining strategy is inspired by the CLIP framework developed by OpenAI that achieves co-embedding of text and image data, thereby linking two distinct modalities. Specifically, we will utilize contrastive learning to align embeddings for chromatin organization with embeddings for gene expression derived from scGPT. scGPT, a large language model trained on single-cell RNA-seq data from millions of cells, has demonstrated significant utility in tasks including cell type annotation, perturbation response prediction, and gene network inference. By aligning chromatin organization embeddings with those from scGPT, our approach aims to uncover structurally relevant features linked to gene function. During this phase, gene expression embeddings will be precomputed and held constant, while only the parameters of the vision transformer encoding chromatin organization will be optimized. A key advantage of using contrastive learning lies in its ability to optimize the model using single-cell data derived from separate experiments. Although chromatin organization and gene expression are not measured in the same cells, categorization into a consensus set of cell types allows optimization. Within this framework, embeddings are optimized so that chromatin organization and gene expression from the same cell type share higher similarity than those from different cell types. Following pretraining, the complete model, named single-cell gene expression prediction (scGEP), will be fine-tuned using additional data. Co-assay data for human brain cells are unavailable; nevertheless, assuming that different cell types share similar gene regulatory mechanisms, we anticipate that the model fine-tuned with available cell types will be transferable to brain cells. Chromatin accessibility will be assessed at the bulk level via ATAC-seq. The chromatin organization and DNA sequence plus ATAC-seq encoders will be adopted from the pretrained EPCOT model, with their parameters frozen. Training will focus on predicting single-cell gene expression based on single-cell chromatin organization contextualized by bulk chromatin accessibility. Tensor Decomposition After preparing the data for the AI model, we can enhance analysis by performing tensor decomposition, a technique successfully applied in projects like the COVID-19 Multi-Omics Blood Atlas and Genomic Advances in Sepsis. This supplementary analysis offers two key benefits: first, dimensionality reduction and outlier detection; second, handling missing data through imputation. By decomposing multimodal data into components, we can reduce its dimensionality and identify or remove outliers that could adversely affect analysis. The tensor decomposition method can accommodate missing data, making it ideal for samples lacking certain modalities. By reconstructing data from decomposed tensors, we can impute missing values, expanding and enriching the dataset for further analysis. Potential Caveats Data will be divided into training, validation, and testing subsets to assess model performance. Limited co-assay data may impact model transferability. If performance is unsatisfactory, additional training will be carried out using co-assay data from mouse cells, which, while not as deep as human datasets, may still serve for pretraining. Hypothesis Variation in genome organization drives gene expression changes Despite extensive research, the mechanisms governing gene expression remain poorly understood. Prior studies have yielded conflicting conclusions regarding chromatin organization’s significance in gene regulation, often due to limited gene coverage or reliance on population-averaged data. The availability of genome-wide datasets encompassing both expression and three-dimensional organization from the same cell provides an opportunity for systematic exploration of chromatin’s role in gene regulation. The computational tool scCOE offers a framework to associate changes in chromatin organization with variations in gene expression. Traditional analyses emphasize structural motifs such as loops, domains, and compartments, but their functional relevance remains uncertain. scCOE generates embeddings aligned with gene expression, capturing functionally significant structural characteristics. Investigating the variation of these embeddings across brain cell types can provide insight into how chromatin organization contributes to neuronal diversity from the same DNA sequence. This approach also enables examination of how heterogeneity in gene expression within a cell type correlates with variations in chromatin architecture. While scCOE offers qualitative insights into chromatin organization’s influence, scGEP quantitatively assesses whether chromatin organization suffices for accurate expression prediction. The multimodal model’s performance will be compared to models using only DNA sequence, chromatin accessibility, or chromatin organization. We hypothesize that a model based solely on DNA sequence will perform moderately well but lack precision, while adding chromatin organization as a contextual feature will substantially improve predictions of cell type-specific expression patterns. Hypothesis Functionally relevant embedding of genome organization offers insight into Alzheimer’s disease Genome organization is hierarchical, encoding vast amounts of information. Interpreting changes within this organization, particularly between normal and diseased cells, poses challenges. Profiling the chromatin conformation of Alzheimer’s disease patients and controls has revealed variability, but identifying functionally relevant variations remains difficult. To address this, we propose two complementary analyses to interrogate genome organization alterations associated with disease. We will first use scCOE to derive embeddings of three-dimensional chromatin organization at the single-cell level. These embeddings, optimized for predicting gene expression, are designed to capture structural features linked to regulatory mechanisms. Dimensionality reduction visualization will be used to compare cells from Alzheimer’s patients and controls. Observing significant structural differences may reveal disease-relevant architectural changes. Downstream modeling will aim to uncover molecular determinants—potentially epigenetic or nucleosome-related—that underlie structural disruptions. Structural analysis of folding mechanisms involving looping motifs and nucleosome interactions can elucidate biophysical rules that dictate genome folding and its relationship with gene expression. Next, scGEP will be used to predict gene expression changes to directly compare patients and controls. Elevated expression of specific genes in microglia has been implicated in pathology. By using scGEP to predict alterations in gene expression, we aim to determine whether such changes result from modifications in chromatin organization. Together, these approaches may provide mechanistic insight into causal relationships between chromatin architecture and expression, identifying potential therapeutic strategies to restore normal gene regulation. Outcomes Technological advancement Two computational tools, scCOE and scGEP, will be produced. scCOE will facilitate analysis and interpretation of single-cell genome organization data as such data become increasingly available. scGEP will enable integration of single-cell multimodal data to better define cell types and characterize cellular diversity. Scientific advancement The proposed research will transform understanding of brain function, potentially identifying molecular markers for diseases such as Alzheimer’s. It will provide insight into the connection between DNA sequence, three-dimensional genome architecture, and gene expression, addressing fundamental questions regarding genome organization’s role in regulation. Publications Two manuscripts are expected, one focusing on technological innovation in multimodal modeling of gene regulatory mechanisms and another focusing on biological insights into genome organization in normal brain and diseased cells. Rationale for a working group approach The proposed approach is interdisciplinary, requiring bioinformatics expertise to organize genomics datasets, deep learning skills to build mechanistic models, and understanding of brain biology to interpret AI findings. The project’s complexity necessitates collaboration across multiple research groups. Rationale for NCEMS support The NCEMS team will play a key role in collecting single-cell genomics data and integrating assays into a unified database. The center’s computational resources will also facilitate deep learning model training. Requested resources Model training will require substantial GPU and CPU computing resources and data storage capacity. We request partial support of a staff scientist with expertise in single-cell genomics and deep learning.",
            "proposal_status": "Accepted",
            "ranking": 7
        },
        {
            "proposal_id": "3",
            "proposal_title": "MaiTool - LLM-powered bioinformatics tools for microbiome analysis",
            "authors": ["Daniel Huson", "Laura Weyrich", "Maik Pietzner"],
            "authors_departments": ["Algorithms in Bioinformatics", "Anthropology and Bioethics", "Health Data Modeling"],
            "abstract": "Large language models (LLMs) like ChatGPT and BioGPT have revolutionized how researchers engage with scientific literature, offering capabilities such as extracting insights, summarizing methodologies, and identifying relevant publications. These models have applications in microbiome research, where they can assist in identifying datasets in repositories like MGnify and MG-RAST. However, LLMs lack the ability to directly analyze datasets, necessitating integration with bioinformatics tools to enable robust computational analysis. This project aims to develop MaiTool, a graphical interface that combines LLM capabilities with bioinformatics workflows for microbiome and ancient DNA research. MaiTool will enable users to explore publications, access public datasets via APIs, and perform dynamic analyses, integrating quality control measures to enhance reliability. Existing GPT systems (e.g., BioGPT, BioMedLM, and GeneGPT) will be evaluated and fine-tuned for microbiome applications. We will assess repositories such as HMP, IMG/M, and SRA for their suitability and develop APIs for seamless data access and integration. By addressing queries such as studies involving specific taxa, MaiTool will allow LLMs to retrieve literature while bioinformatics pipelines analyze and visualize dataset profiles. This approach aims to advance interdisciplinary research workflows and establish a new standard for AI-driven bioinformatics.",
            "full_draft": "Introduction and Goals The rise of large language models (LLMs) like ChatGPT and BioGPT has revolutionized how researchers engage with scientific publications. These models excel at understanding complex queries, enabling intelligent data retrieval tailored to project-specific needs. For instance, they can extract insights from publications, summarize methodologies, and, in the area of microbiome research, say, identify relevant datasets in repositories like MGnify and MG-RAST. However, the direct analysis of datasets remains beyond the core capabilities of such systems. Integrating LLMs with traditional bioinformatics tools offers a promising solution, bridging the gap between natural language understanding and robust computational analysis. This integration could enable on-the-fly analyses of microbiome data while incorporating essential components like quality control to flag potential issues with datasets early in the pipeline. By combining LLMs’ contextual understanding with the computational power of bioinformatics tools, researchers could streamline workflows, ensuring efficient access to both published findings and actionable data. This project aims to develop a graphical interface “MaiTool” that facilitates the exploration of microbiome-related publications, provides seamless access to publicly available data, integrates quality control measures, and enables dynamic lightweight analyses. Such a system will empower researchers to make informed decisions efficiently, fostering innovation and collaboration in microbiome research and beyond. While the primary focus is on microbiome science, we also plan to explore extensions to computational challenges in analyzing ancient DNA sequences, including the microbiome analysis of ancient DNA samples. Proposed Activities: Data Collection, Curation, and Analysis Framework We propose to evaluate various GPT systems (BioGPT, BioMedLM and GeneGPT) to determine their effectiveness in assisting researchers with engaging and extracting insights from the scientific literature. Building on this foundation, we will identify 5–10 key use cases where the integration of bioinformatics tools—focused on quality assurance, data visualization, and lightweight computational analysis—can enhance the relevance and utility of the results. This approach aims to create a synergistic system that combines the contextual understanding of GPT models with the analytical power of bioinformatics to deliver targeted, actionable insights for microbiome research. In addition to evaluating existing GPT systems for their applicability in microbiome research, we will explore the feasibility of enhancing their performance through two approaches: either setting up our own large language model (LLM) tailored to specific microbiome research needs or fine-tuning existing open-source models. This exploration will help determine the optimal balance between leveraging pre-trained models and creating customized solutions to support advanced analysis and research workflows. In microbiome research, several public repositories provide access to microbiome data. Key repositories include MGnify, MG-RAST, the Human Microbiome Project (HMP), the Microbiome Atlas of Mothers and Infants (MAMI), the Integrated Microbial Genomes and Microbiomes system (IMG/M) and the Sequence Read Archive (SRA) from NCBI (further please see Data Sources Table), which is the most comprehensive resource for raw sequencing data. We will evaluate each of these repositories to assess their suitability as sources of analysis results, focusing on their data accessibility, quality, and relevance to specific research needs. Additionally, we plan to develop software tools that leverage the APIs provided by these repositories to facilitate seamless access, integration, and utilization of their datasets within our system. This will enable efficient retrieval and analysis of microbiome data, supporting advanced research workflows. As an example, consider a user interested in determining whether there are studies that include a specific microbiome species of interest, such as the keystone gut species Christensenella minuta or oral species Methanobrevibacter oralis. Using the proposed system, a GPT model would first identify relevant publications that mention this species, extracting key information such as study descriptions, methodologies, and findings and data accessions from the literature. Simultaneously, using the reported accessions, a bioinformatics pipeline could access taxonomic profiles for the associated studies by accessing public microbiome datasets (along with metadata) through APIs. The pipeline may perform additional analyses, such as generating visualizations or conducting statistical analyses to explore patterns and relationships involving C. minuta. In addition, the GPT can provide a pipeline that the user can run themselves. Preliminary Data, Tools and Analytical Approaches We will conduct a systematic investigation of several GPT models, including BioGPT, BioMedLM, and GeneGPT, to evaluate their suitability for applications in microbiome analysis. For each model, we will assess its capabilities and explore the potential for fine-tuning to enhance support for data analysis tasks. Simultaneously, we will examine key microbiome data resources, including MG-RAST, HMP, IMG/M, MAMI, and datasets hosted at NCBI. This evaluation will focus on methods for extracting data, identifying analyses that can be performed dynamically, and establishing effective communication with the APIs provided by each resource. A third focus of our activities will be determining the best approach for implementing the interactive exploration tool MaiTool. MaiTool will serve as the primary interface for users, connecting seamlessly to an API for tasks such as LLM usage, data retrieval, and lightweight microbiome analysis. We intend to develop a standalone application using Java, JavaFX and Gluon, which would provide a highly responsive and feature-rich user experience, running on all platforms. The development of MaiTool will include a dedicated phase for comprehensive testing and usability studies to ensure the system meets the needs of researchers effectively. Outcomes This project will deliver several key outcomes: 1. Enhanced GPT for Microbiome and Ancient DNA Analysis: We will identify or fine-tune a GPT model specifically tailored to support research in microbiome and ancient DNA analysis. 2. Unified Microbiome Data API: A unified API will be developed to facilitate seamless access to microbiome data from multiple resources, streamlining data retrieval and integration. 3. Lightweight Bioinformatics Analysis Framework: We will create a bioinformatics framework to perform lightweight analyses on data obtained through the API, ensuring efficient and accessible computational workflows. (While a GPT may provide the description of a pipeline, the user will have to run it themselves, if they are interested in starting from scratch or raw data .) 4. Integrated AI-Bioinformatics System: A GPT system will be integrated with the developed tools to enable interactive access and analysis of microbiome data, bridging AI capabilities with bioinformatics functionality. 5. MaiTool Software for Analysis: We will develop a user-friendly software tool called MaiTool, designed to support both microbiome and ancient DNA analysis. MaiTool will provide an interactive interface for researchers, connecting seamlessly to APIs and integrating some analysis capabilities. 6. Insights into AI-Bioinformatics Integration: This project will provide valuable insights into the challenges and opportunities of integrating AI-driven systems with traditional bioinformatics software and algorithms, setting a foundation for future advancements in computational biology. These outcomes aim to advance microbiome research by enhancing data accessibility, analysis capabilities, and the synergy between AI and bioinformatics. Rationale for a working group approach This project focuses on integrating large language models (LLMs) with traditional data retrieval and bioinformatics analysis in the fields of microbiome and ancient DNA research. Achieving this goal requires expertise spanning multiple disciplines. • Daniel Huson specializes in developing algorithms and software for metagenomics and ancient DNA analysis, bringing essential computational and methodological expertise to the project. • Laura Weyrich contributes her extensive experience in the analysis of ancient DNA and ancient microbiomes, providing critical domain knowledge and practical insights into data interpretation and validation. • Maik Pietzner has a substantial track record in molecular epidemiology, spearheading the computational integration of multimodal ‘omics data sets in population-based studies with a particular focus on molecular quantitative trait loci. Together, this collaboration combines cutting-edge AI technology with expertise in microbiome and ancient DNA research, ensuring a robust and innovative approach to tackling these complex scientific challenges. Rationale for NCEMS support This project will require substantial computational resources to support the exploration of LLMs and microbiome resources. These resources will also be critical for hosting the GPT model used by MaiTool. Robust computational capacity is essential for running experiments, managing data workflows, and ensuring the scalability and responsiveness of the system. In addition, staff support will be vital for setting up computational workflows and implementing a REST API, ensuring seamless integration between components and efficient operation of the system. To foster innovation and collaboration, the project will also require support for short-term visitations between the labs of Working Group members. These visits will facilitate brainstorming, knowledge exchange, and the development of strategies to address the project’s interdisciplinary challenges. Requested resources The project requires 10–20 TB of storage for the enhancement and optimization of LLMs. Computational resources include 200–500 CPU cores and 10–30 GPUs, each with at least 100 GB of memory, to facilitate model development and optimization. RAM demands may increase up to 1–3 TB when models are combined. This will require approximately 200,000 CPU hours and 25000 GPU hours. We estimate that running the final model on a NCEMS host for users will require about 100GB of runtime memory, based on a estimated model size of 10 billion parameters. A staff scientist (up to 33% FTE) with expertise in LLMs will assist in model optimization, complementing the efforts of our existing experts. In addition, they can help ensure optimal utilization of NCEMS resources, including computational infrastructure, project management, and team science training. Proposed Timeline Year 1 Months 1–3: Conduct initial exploration of GPT models to assess suitability for microbiome and ancient DNA analysis. Begin initial exploration of microbiome resources (MG-RAST, HMP, IMG/M, MAMI, SRA) to evaluate data accessibility and API capabilities. Months 4–9: Design the framework for integrating LLMs with microbiome resources, including methods for extracting sample accessions and structuring data workflows. Months 6–12: Develop and implement APIs for seamless access to various microbiome resources, ensuring compatibility and efficient data retrieval. Months 10–12: Refine the accuracy of LLM-driven dataset selection. Improve presentation of analysis results, focusing on clarity and relevance for microbiome researchers. Year 2 Months 1–6: Develop and deploy a REST API to provide access to an LLM, supporting data access and bioinformatics analysis. Months 4–6: Build and implement the front-end interface for MaiTool, providing an alpha release for initial feedback. Months 7–9: Conduct comprehensive testing of the integrated system. Finalize documentation and address feedback to prepare for a beta release. Months 10–12: Publish results, describing the system’s design, capabilities, and applications. Release the fully functional system to the public.",
            "proposal_status": "Rejected",
            "ranking": 10
        },
        {
            "proposal_id": "4",
            "proposal_title": "Synthesizing gene expression data to create an atlas of bacterial cell states",
            "authors": ["Jeffrey Barrick", "Jeremy Schmit"],
            "authors_departments": ["Molecular Biosciences", "Physics" ],
            "abstract": "Bacterial cells have evolved to adapt their molecular composition to maximize growth and survival. Our current knowledge of bacterial cell states is fragmented. Researchers often only analyze data they generate themselves—usually for just one strain under a limited number of conditions—because it is difficult to harmonize omics datasets collected with different technologies in different labs. Our NCEMS working group will synthesize bacterial gene expression data using machine learning to create a first-of-its-kind atlas of bacterial cell states. To manageably scale data collection and analysis, we will build from examining model Escherichia coli strains in lab conditions, to adding wild E. coli strains in natural environments such as the human gut, to including diverse bacterial species with additional cell states. We expect to identify distinct foci corresponding to bacterial growth states and lifestyles (e.g., replicating, starving, motile, biofilm, spore, viable but nonculturable) by mapping the organization and occupancy of cell state space with dimensional reduction approaches. We will examine to what extent the gene expression signatures of different cell states and the relationships among them are conserved during bacterial evolution. Finally, we will use this framework to create a classifier that predicts the cell state composition of a bacterial population from metatranscriptomic data. By following open-science practices, we will make our pipelines and datasets reusable and maintainable by the research community. Our project may reveal new bacterial cell states and ways that we can non-genetically reprogram bacteria to prevent them from adopting lifestyles that damage the environment and cause disease.",
            "full_draft": "Introduction and Goals Bacterial cells modulate gene expression to adapt their physiology to changes in their environment, for example transitioning from actively replicating to quiescent states in response to nutrient exhaustion. Much like cells within multicellular organisms, clonal bacterial populations can diversify and differentiate into cell types with distinct phenotypes such as biofilm or planktonic growth, motile or sessile forms, and specialized nitrogen-fixing cells. However, it remains unclear to what extent bacterial cells have evolved to occupy a small number of discrete gene expression states that they abruptly transition between, or if they exist along a continuum of states that gradually shift in composition. Less is known about how these gene expression states are related and conserved across species during evolution, and it is possible that some bacterial cell states remain undiscovered because existing data are limited to specific laboratory conditions and strains. The goal of this project is to test key hypotheses about the organization of bacterial gene expression space and to synthesize this information into a unified atlas of bacterial cell states. Specifically, we aim to determine whether bacterial cells occupy and transition between a limited number of distinct states, whether key principles such as the growth–quiescence axis are conserved across species, whether specialized lifestyles such as biofilm formation evolved from subfunctionalization of core states, and whether a comprehensive atlas of these states can enable prediction of population composition from transcriptomic data. We will synthesize existing bacterial gene expression datasets collected using microarray and RNA-seq technologies, beginning with model Escherichia coli strains, then expanding to wild isolates and ultimately to diverse bacterial species, to examine the organization and occupancy of bacterial gene expression state space. Existing Datasets Thousands of bacterial gene expression datasets have been generated and deposited in databases such as OKgeneExpDB, E. coli Expression 2, Ecomics, PRECISE, COLUMBOS, and iModulonDB. These repositories encompass tens of thousands of gene expression profiles representing variations in environmental conditions, genetic backgrounds, and experimental perturbations. Although many datasets include curated metadata and quality control, most existing resources are not fully open or community-maintained, limiting reuse and integration. Prior Analyses Machine learning approaches have previously been applied to predict bacterial growth rates and conditions from gene expression profiles and to identify modules of co-regulated genes. However, differences in technologies, protocols, and experimental designs introduce systematic variation that can confound these analyses. Robust harmonization of datasets and advanced quality control are necessary to enable meaningful synthesis and discovery. Project Significance Previous studies have not focused on systematically identifying bacterial cell states, which we define as distinct growth or lifestyle conditions characterized by coordinated shifts in gene expression across many genes. For example, aerobic planktonic growth represents one state, distinct from anaerobic growth, starvation, or biofilm formation. While single-cell RNA-seq could eventually define these states with precision, technical challenges currently limit its applicability to bacteria. Instead, we will integrate existing bulk gene expression datasets, reasoning that many of them capture nearly pure cell states or mixtures of a few dominant ones. Using dimensional reduction and clustering methods, we aim to infer an atlas of bacterial cell states and the relationships among them. Proposed Activities Develop the open-source inGEST pipeline for harmonizing bacterial gene expression datasets Our analysis requires assembling and processing large numbers of high-quality gene expression datasets. We will process tens of thousands of RNA-seq and microarray datasets from public repositories in a uniform way, applying standardized filtering and normalization while identifying and correcting for technical variation. The Integrative Gene Expression Synthesis Tool (InGEST) will include conventional preprocessing steps for both microarray and RNA-seq data as well as advanced quality control functions that detect anomalies such as DNA contamination or inconsistent replicates. Using the scale of the aggregated data, we will train algorithms to identify and adjust for systematic biases, such as those introduced by differing read lengths or protocols. InGEST will be released as an open, community-maintainable software package with full documentation, tutorials, and containerized deployment options to ensure accessibility and long-term sustainability. Apply dimensional reduction methods to create the Atlas of Bacterial Cell States To delineate distinct cell states and understand their relationships, we will apply unsupervised learning and dimensional reduction techniques commonly used in eukaryotic single-cell RNA-seq analysis. These approaches can reveal structure in high-dimensional data and help identify clusters corresponding to biological states. Our initial focus will be on curated Escherichia coli datasets, where we expect to identify primary attractors corresponding to exponential growth and stationary phase, forming a foundational growth–quiescence axis. Additional variation will likely correspond to sub-states such as stress responses or biofilm-associated growth. As InGEST matures, we will extend the analysis to non-laboratory E. coli strains and to natural environments such as the gut, identifying subfunctionalized states that build on the conserved axes. To account for variation in gene content between strains, we will use ortholog mapping tools such as Orthofinder to establish a consistent feature space across datasets. Finally, we will expand the analysis to include more than twenty bacterial species with sufficient datasets, encompassing model organisms and pathogens that exhibit additional states such as sporulation or quorum sensing. This broader atlas will allow us to identify conserved and specialized expression patterns and explore evolutionary relationships among cell states. Create a classifier that predicts the distribution of cell states in bacterial populations Using the atlas as a foundation, we will train classifiers to estimate the proportions of cells occupying different states within bulk transcriptomic or metatranscriptomic samples. Initially, we will train and validate these models on synthetic mixtures generated from pure-state datasets and later test them on available single-cell bacterial transcriptomes. Ultimately, we will apply these classifiers to environmental and host-associated microbial communities to predict functional composition and state distribution, providing a new lens for interpreting community-level gene expression. Outcomes This project will generate conceptual, methodological, and practical outcomes. Scientifically, it will yield the first comprehensive Atlas of Bacterial Cell States, revealing how gene expression landscapes are structured and conserved across species and environments. Methodologically, it will produce three key open-source products: the InGEST pipeline for harmonizing bacterial gene expression data, the Atlas of Bacterial Cell States as a community resource with visualization tools, and the Bacterial Cell Population Classifier that predicts cell state composition from expression data. Collectively, these resources will enable new analyses of bacterial physiology, evolution, and ecology. By integrating and harmonizing unprecedented amounts of data, we expect to uncover organizing principles of bacterial behavior and potentially identify novel cell states. Future work can extend this framework to other microbial domains and data types, including proteomics and metabolomics, enabling multi-omics integration of microbial states. Rationale for NCEMS support This project exemplifies mesoscale data synthesis: it leverages machine learning to integrate and analyze large volumes of bacterial gene expression data. Achieving this requires reproducible, scalable pipelines and community infrastructure that individual groups cannot maintain alone. NCEMS support is essential for implementing best practices in data management and sustainability, ensuring that the resulting resources are open, reusable, and extensible. In addition, NCEMS computational expertise will help in developing and applying advanced analytical methods for visualizing and quantifying relationships between bacterial cell states. Requested resources The primary computational demands involve downloading, processing, and analyzing thousands of RNA-seq datasets. Each dataset ranges from a few hundred megabytes to several gigabytes and can be processed within minutes on modern CPUs. Microarray data require negligible storage and compute time. Dimensional reduction and machine learning analyses can initially be run on modest hardware but may benefit from scalable computing for cross-species analyses. The final resource will consist of metadata, gene expression matrices, and annotated reference genomes, small enough to be hosted publicly and versioned for community access. Proposed Timeline The project will span two years. In the first year, we will focus on Escherichia coli, developing and validating the InGEST pipeline, harmonizing thousands of datasets, and identifying major cell states and marker genes. The second year will expand to additional bacterial species, refine the atlas to include conserved and specialized states, and develop the classifier for predicting state composition in mixed samples. Publications describing both the pipeline and the atlas will disseminate the findings and ensure long-term visibility and reuse.",
            "proposal_status": "Accepted",
            "ranking": 1
        },

        {
            "proposal_id": "5",
            "proposal_title": "Energetic Origins of Connectivity within Protein Interaction Networks",
            "authors": ["Jonathan Schlebach", "Shahid Mukhtar", "Adrian Serohijos" ],
            "authors_departments": ["Chemsitry", "Genetics and Biochemistry", "Biophysics"],
            "abstract": "At least 10% of cellular proteins are intrinsically unstable and fold with marginal efficiency. In the cell, these proteins are preferentially engaged by chaperones and quality control proteins that refold or degrade them. Similarly, intrinsically disordered proteins (IDPs) preferentially interact with specific partners that they require for folding and function. Such proteins must exhibit bias in the types of protein-protein interactions they form within the cell. We therefore hypothesize that the emergent scaling properties of protein interaction networks arise, in part, from the promiscuous interactions formed by unstable proteins and the obligate interactions formed by IDPs. To test this hypothesis, we will analyze existing large-scale interactome datasets to explore the context of unstable and disordered proteins within interaction networks. By extending this approach to evaluate how proteins with various conformational properties are distributed across several unicellular and multicellular networks, we aim to uncover generalizable insights into how protein biophysics shapes network architecture. Additionally, by mapping the centrality of proteins involved in various supramolecular organization regimes, we will identify topological niches associated with the occurrence of liquid-liquid phase separation and quinary structure. Finally, to determine the extent to which these biases constrain network evolution, we will utilize genetic algorithms to explore how changes in the stability of the proteome restrict the topologies that are accessible through evolution. Together, these investigations will provide insights into the role of protein stability and binding efficiency in the evolution of PPI networks and how this relates to the molecular mechanisms of evolution.",
            "full_draft":"Introduction and Goals Over the past 20 years, emergent evidence has suggested that the architecture, scaling, and topologies of protein-protein interaction networks are similar across prokaryotes and eukaryotes. One apparent property of these networks is a scale-free topology in which interactions among proteins follow a power law distribution. While true scaling may be obscured by experimental biases, it is clear that most proteins form only a few discrete interactions, whereas a small number of hub proteins engage in numerous promiscuous interactions. It has been proposed that selection pressures have refined resilient network topologies in which key metabolic pivots can be regulated through a handful of connecting nodes. To ensure robustness, these high-degree hubs often have functionally redundant paralogs that minimize the fitness impact of deleterious mutations. In biological systems, “party hubs” are large proteins that form continuous interactions within functional complexes and exhibit co-expression, while “date hubs” are smaller proteins that form transient interactions with different partners and bridge network modules. Intrinsically disordered regions are a common feature of eukaryotic hub proteins, and disordered proteins in both hub types can introduce new specificities that differentiate single- and multiple-interface hubs. The position of a node within a network strongly influences its function. This can be described by its betweenness centrality, which measures how often a node lies on the shortest paths between pairs of other nodes. Nodes with high betweenness act as bottlenecks and need not have high degree. Centrality descriptors can generally be categorized into neighborhood-based, path-based, and iterative refinement-based types. The magnitude of connections ultimately depends on the number of complexes a protein forms, which is determined by its biophysical properties and cellular abundance. Integrating multiscale biophysical and cellular network descriptors may reveal how intrinsic protein properties give rise to the topology of interaction networks and their biological roles. Numerous interactome datasets are available for constructing such networks, including for yeast and higher organisms like Arabidopsis and humans. These data have already provided insight into how pathogens target hubs and bottlenecks. Despite this progress, it remains unclear how cells achieve and maintain preferred network architectures given that their nodes—proteins—continuously evolve new interactions. While the evolutionary dynamics of interaction networks are poorly understood, much is known about evolutionary constraints on protein structure and function. Most mutations destabilize protein structure and reduce stability, meaning individual proteins can only tolerate certain combinations of stabilizing and destabilizing mutations before selection removes them. Based on this, we propose to develop a physical framework that describes how the properties of protein interaction networks emerge from the physical properties of their constituent proteins. For example, increases in abundance could compensate for low stability or weak binding, while weakened binding may promote promiscuous interactions. Unstable proteins might also form new chaperone interactions that facilitate transitions between clusters. Purifying selection could be weaker for low-degree or redundant high-degree proteins. We suspect that networks contain niches capable of accommodating variations in protein stability in ways that preserve overall topology. Such models could clarify how changes in protein stability, binding, and abundance contribute to network robustness. Our investigations will address how modular protein clusters maintain connectivity despite genetic drift, whether chaperone hubs act as anchors for topology, whether unstable clients engaging multiple chaperones move between clusters, and whether intrinsically disordered proteins become anchored within specific modules. We will repurpose interactome data to model networks across organisms and evaluate the topological context of proteins with divergent biophysical properties to reveal how protein physics gives rise to biological network organization. Goal Determine how protein stability biases interaction networks. Over time, most proteins traverse low-stability regimes that predispose them to chaperone binding. Shared chaperone hubs can create indirect proteostatic couplings that affect solubility and function. However, the influence of these client-chaperone interactions on network topology is unclear. We hypothesize that unstable proteins exhibit enhanced connectivity due to their association with the proteostasis network. To test this, we will construct a high-resolution yeast network using quality interactome data and combine it with computational protein stability estimates to assess the connectivity of unstable proteins. These analyses will be validated using proteolysis-based stability data, proteomic half-life measurements, and abundance data. Finally, we will repeat these analyses in organelle-specific subnetworks to test whether such patterns differ by compartment. Goal Survey interactions formed by unstable proteins and intrinsically disordered proteins within divergent networks. Higher organisms have evolved complex proteostasis networks that support structurally diverse proteomes and increase overall buffering capacity. It is unknown whether changes in proteostasis composition alter the interactions of unstable or disordered proteins or whether these proteins remain bound to specific chaperones across long evolutionary timescales. We hypothesize that unstable proteins form promiscuous chaperone interactions and more interactions overall than disordered proteins. To test this, we will extend the mapping approach to multiple organisms, characterize the topological positions of unstable and disordered proteins, and compare chaperone associations among homologous clients across species. By comparing metrics across stability and disorder classes, we will determine whether unstable proteins and their ancestors show preferential attachment to chaperone hubs that maintains their positions and preserves network scaling. Goal Determine how marginally stable and unstable proteins constrain network topology. Many proteins develop a dependence on specific chaperones as part of their assembly pathways. We hypothesize that persistent chaperone interactions restrain the topology of interaction networks. To test this, we will use genetic algorithms to simulate evolutionary trajectories of networks that integrate stability, binding efficiency, and abundance with tunable constraints on chaperone hub number and interaction persistence. These simulations will reveal whether degeneracy among client-chaperone interactions emerges across evolutionary timescales and how proteostasis interactions constrain network architecture. Proposed Activities Generation of network models The Mukhtar group will curate interaction data and use it to build models for yeast, E. coli, and C. elegans based on in vivo and high-throughput datasets. For each protein, we will compute multiple centrality measures—including degree, betweenness, eigenvector, closeness, load, PageRank, and weighted k-shell decomposition—using Python and Cytoscape. Node prioritization algorithms will identify influential proteins through composite metrics, and clustering will be performed with MCODE and Cytoscape visualization. We will also integrate single-cell and condition-specific data from GEO to parameterize context-dependent networks with the aid of contextual AI models. Functional enrichment analyses using GO, WikiPathways, and related databases will contextualize biological meaning. Generation of biophysical descriptors To determine whether proteins with particular structural properties cluster in networks, we will compare each protein’s centrality to predicted stability and disorder features. Using disorder prediction resources, we will distinguish structured from disordered proteins and estimate absolute stability for structured proteins through generative modeling and simulation-based validation. These stability descriptors will categorize proteins as stable, unstable, or disordered. Enrichment analyses will then test whether high-centrality proteins are overrepresented in these categories, and thresholds will be varied to assess robustness. Additional node annotations such as chaperones or phase-separating proteins will be incorporated to identify biochemical phenomena correlated with centrality. Data from proteolysis, turnover, and expression experiments will be integrated to evaluate associations between network position and proteostasis dynamics. Generation of evolutionary models The Serohijos and Shakhnovich groups have developed integrative models of protein evolution combining biophysics, cellular properties, and population genetics. These models, expanded to proteome scale, will incorporate chaperone and disorder effects. The model includes a fitness function linking native folding probability to free energy and chaperone efficiency, a description of random mutation effects, and an evolutionary algorithm implementing selection. The fitness of complexes will depend on protein stabilities, binding affinities, and abundances, satisfying the law of mass action across the proteome. Random mutations will alter folding and binding based on established datasets, and their selective advantages will be evaluated by fitness comparisons and fixation probabilities. This framework will allow exploration of how mutations affecting stability or binding influence network connectivity and evolution. Outcomes This work will clarify how variations in protein stability and disorder shape the architecture of protein interaction networks. We expect that unstable proteins will display higher degree than stable ones and cluster around chaperone hubs, while disordered proteins dependent on specific partners will maintain their positions across networks. Simulations will demonstrate how these interactions impose constraints on network evolution. Collectively, these results will establish a biophysical foundation for understanding how protein properties give rise to network connectivity and robustness. Working Group Rationale The project merges computational biophysics, systems biology, and evolutionary modeling. The Schlebach lab contributes expertise in protein biophysics and structural informatics; the Mukhtar lab contributes network biology and graph analysis capabilities; and the Serohijos lab contributes evolutionary modeling. The Shakhnovich lab provides complementary theoretical expertise linking these approaches to metabolic organization. This collaboration will enable the integration of data, methods, and trainees across disciplines. Rationale for NCEMS Support The investigations represent a new direction requiring computational infrastructure and collaborative coordination. NCEMS support will facilitate data integration, provide computing resources, and enable administrative and logistical support for efficient collaboration. Requested Resources The project requests a staff scientist and assistant, cyberinfrastructure including storage and computing hours, travel and publication support, and resources for personnel exchange. Proposed Timeline Major milestones include construction of interaction networks for multiple species, computation of centrality metrics, generation of protein stability estimates, annotation of chaperones and disordered proteins, enrichment analyses, and development of genetic algorithms for evolutionary simulation.",   
            "proposal_status": "Accepted",
            "ranking": 10
        },

        {
            "proposal_id": "6",
            "proposal_title": "Elucidating emergent structures in cellular RNA-protein interaction networks",
            "authors": ["Kalli Kappel", "Steven Boeynaems" ],
            "authors_departments": ["Chemistry", "Bio Chemistry", "Medicine"],
            "abstract": "Messenger RNA molecules are constantly bound by proteins that regulate all aspects of their function, including their processing, localization, translation, and degradation1. These mRNA-protein complexes (mRNPs) can also interact with each other and assemble into higher-order structures that play critical roles in cellular organization and response to environmental stimuli1-3. Despite their importance, we lack a comprehensive map of these cellular RNA-protein interaction networks at both the nano and micro scales. Years of efforts by researchers across the globe have yielded a large, but disjointed, collection of datasets that map different aspects of RNA-protein interactions. Currently, we lack tools to integrate these datasets, preventing their synthesis into a coherent and interpretable interaction map. Here, we propose to develop tools to combine disparate data types to build transcriptome-wide maps of mRNPs, and their higher-order interactions within cells. We will then apply these tools to gain insights into long-standing biological mysteries, including the impacts of synonymous mutations and cell-type specific effects of widely expressed RNAs.",
            "full_draft":"Introduction and Goals RNA is Life’s prime messenger molecule of genetic information. While for all organisms—minus some viral exceptions—DNA provides long-term storage, RNA is its direct output, relaying this genetic info into molecular actions. Messenger RNA instructs the ribosomes to fabricate the plethora of proteins that carry out most cellular and biochemical functions. There are also non-coding RNAs that serve mostly structural, enzymatic or regulatory functions. For example, long non-coding RNAs like NEAT1 RNA recruit specific sticky proteins to form biochemically distinct subcellular compartments called paraspeckles—a type of biomolecular condensate. Ribosomal RNA is a prime example of a catalytic RNA or ribozyme, and small RNAs like micro RNAs can regulate the abundance of other RNA species. While it is clear that RNAs carry out essential functions in cells, they do not act alone. It is well understood that RNAs are never naked in the cellular environment. Cells harbor hundreds of RNA-binding proteins that can decorate RNA molecules depending on their specific nucleotide sequence and secondary structure or simply based on the inherent negative charge of the RNA backbone. So, for every RNA molecule, from the moment it emerges from the polymerase transcription bubble, there will be a complex, spatially and temporally dynamic set of protein interaction partners that dictate its processing and structural remodeling, its transport through the cell, its biochemical function and eventual decay. Given that most of these RNA-protein complexes are considered to form their own single entity, they each present as a wildly complex and dynamic microcosm of different proteins interacting with a single RNA. Moreover, all of these proteins can have specific or non-specific interactions with each other—based on their biochemical composition, structure and disorder—that affect each other’s behavior and the eventual mRNP composition. Given such complexity and the breadth of potential RNA-RNA, RNA-protein and protein-protein interactions, it is highly unlikely that the sum of each of these individual components is sufficient to explain the outcome. On the contrary, new unexpected behaviors are bound to emerge from this complexity. We argue that mRNPs are one of the best examples to study emergent behavior in an essential cellular process for life on this planet. The goal of this project is to create a roadmap to understand the composition of the cell’s many thousands of RNPs, focusing specifically on mRNA-protein complexes and their higher order interactions, by integrating available experimental RNA-RNA, RNA-protein and protein-protein interaction data using recent state-of-the-art computational tools for predicting protein-protein and protein-RNA interactions. We postulate that this will allow us to come up with a predictive framework to understand the emergent behaviors that arise from mRNP composition. We will validate the predictive power of our framework through the lens of human sequence variation dictating RNA abundance and splicing, the robustness or sensitivity of splicing to RNA-binding protein loss, and the composition of higher-order biomolecular structures that form when individual mRNPs form environmentally-triggered condensates. Proposed Activities Integrate publicly available datasets to build a transcriptome-wide map of mRNPs. In cells, mRNA molecules are constantly bound by proteins, which regulate nearly every aspect of their processing, localization, translation, and degradation. As such, there have been numerous efforts to systematically map mRNA-protein interactions in cells, resulting in a long list of large-scale and transcriptome-wide datasets. These datasets have been generated through many different experimental techniques, each measuring a different aspect of these interactions, making it nontrivial to compare and combine them. Without the ability to integrate these measurements, we lack a transcriptome-wide mRNP map—we have a fragmented and incomplete understanding of the composition and architecture of mRNPs, despite an abundance of data. We hypothesize that a transcriptome-wide mRNP map would help mechanistically explain several long-standing biological mysteries, including how synonymous mutations and mutations in non-coding regions of RNA molecules impact cellular function, and why mutations in RNA molecules that are widely expressed have highly cell-type specific effects. Here, to address this challenge, we will develop a method to integrate disparate datasets, which report on multiple aspects of RNA-protein interactions, to build complete mRNP maps; apply this method to build a transcriptome-wide mRNP map for K562 cells; and use this map to predict and develop mechanistic hypotheses for RNA fate upon genetic perturbation. Develop a method to integrate disparate datasets to build complete mRNP maps. Our goal is to develop a computational method that takes a set of disparate RNA-protein interaction datasets as input and outputs an mRNP map consisting of mRNA molecules and bound proteins including specific nucleotides at which they are bound. We will first collect all publicly available datasets that report on different aspects of RNA-protein interactions in cells. To integrate the datasets, we will develop a set of linked features that connect each pair of data modalities. A linked feature is a shared characteristic or metric that is either directly measured by both datasets or can be reliably inferred from their respective measurements. These linked features will then serve as anchors for data normalization and integration. We will test multiple linked features and normalization approaches and assess their accuracy using the β-actin mRNP, which has been extensively characterized at high resolution. By linking all data modalities to transcriptome-wide mapping of protein-binding sites, this will result in a transcriptome-wide mRNP map. Build a transcriptome-wide mRNP map for K562 cells. We will extend the tool developed above to identify additional proteins that may be part of each mRNP. We will identify protein interaction partners for each protein member of each mRNP by utilizing data from existing protein-protein interaction databases and by predicting protein-protein interactions with AlphaFold for structured domains and with FINCHES for interactions between intrinsically disordered regions. Input to this tool will be a list of mRNP components and a parameter specifying the number of layers of connections to predict. We will apply this tool using data collected for K562 cells to build a complete mRNP map for this cell type. We will also assess the signal-to-noise for each dataset by comparing to corresponding high-resolution measurements for the extensively characterized β-actin mRNP, so we can assign confidence scores to each interaction within the mRNP map. Predict RNA fate upon genetic perturbation. To test the predictive power of our framework regarding regulation of RNAs upon perturbation, we will follow two approaches. First, we will determine whether our framework, which takes into account the different proteins and regulatory RNAs that bind a transcript, can explain the effect of a point mutation in the sequence on its eventual abundance or splicing. For example, a mutation may affect an RNA-binding protein or miRNA binding site. Second, we will ask whether our framework reproduces effects seen in RNA-binding protein knockout data. We will then use our model to make mechanistic predictions as to why mutations in RNA molecules that are widely expressed have highly cell-type specific effects. Map out interaction networks of mRNPs. This aim seeks to identify the network of interactions between mRNPs by addressing the following questions: What is the mesoscale interaction network that a given mRNP belongs to? Can we predict what subcellular compartments, such as biomolecular condensates, that a given mRNP would localize to? Can we predict the compositional variability of condensates such as stress granules based on mRNP networks? To this end, our team will pioneer new approaches to map out higher-order interaction networks of mRNPs within cells. We hypothesize that the physicochemical characteristics of mRNPs encode their spatiotemporal properties within living cells. Deploy our mRNP map tool to identify emergent structures of mRNPs. Our goal is to predict higher-order interaction networks of mRNPs. Using the mRNP map tool generated previously, we will determine the network of proteins associated with each mRNP. Protein interaction networks will be analyzed using clustering or community detection algorithms to group proteins into mesoscale networks based on shared physical properties, such as interaction motifs or binding affinities. Our identified clusters will be validated using mass spectrometry crosslinking data and proximity labeling datasets. Finally, we will compare the identified clusters to known protein compositions of individual condensates to assess their alignment with specific biomolecular compartments. Predict mRNP partitioning into biomolecular condensates. Leveraging databases such as CD-Code and RNA Granule Database, we will curate a subset of proteins and RNAs known to localize to stress granules and P bodies, gathering their physicochemical properties, including sequence composition, charge distribution, hydrophobicity, structural disorder, RNA-binding domains, and low-complexity domains. In parallel, we will use the tool developed earlier to identify mRNPs associated with both types of condensates. Based on the curated data, we will design model condensates that incorporate averaged physiological features representative of stress granules and P bodies. Using the Mpipi model, we will perform residue-resolution molecular dynamics simulations to measure the partitioning coefficients of approximately 2,500 mRNPs across both model condensates. This data will then be used to train a neural network capable of predicting the partition coefficient of any given mRNP into either stress granules or P bodies. The model will be validated against existing transcriptome datasets that profile RNAs enriched in stress granules and P bodies under specific conditions. Predict stress granule composition across cell types. Stress granules are some of the most widely studied biomolecular condensates with many datasets on their protein and RNA composition being readily available. Based on the RNA and protein abundance of specific cells, we will use our framework to predict the variance and similarity in stress granule composition. Addressing this question will help us uncover the molecular rules that dictate condensate formation and composition in both a cell-specific and non-specific manner. Outcomes Our work will result in open-source tools for integrating disparate data sources to map RNA-protein interactions at the nano-scale and the meso-scale, as well as specific applications of these tools to explain emergent structures and functional data. We will produce open-source tools for predicting transcriptome-wide mRNP maps from disparate datasets reporting on different aspects of RNA-protein interactions; a transcriptome-wide mRNP map/database for K562 cells containing detailed molecular annotations; an assessment of the predictive power of our mRNP map to explain the functional consequences of genetic perturbations; a database of microscale clusters composed of mRNPs and their interaction partners; an open-source tool to predict the relative localization of any mRNP to stress granules and P bodies; and a model that predicts cell type specific stress granule composition. Rationale for working group approach This project requires several distinct areas of expertise that go far beyond the skill sets of any individual lab. In addition to general subject-matter expertise in RNA-protein interactions, which all participants have, the work will require expertise in machine learning, statistics, molecular biophysics, and bioinformatics to build tools that integrate disparate data sources and create predictive models; molecular simulations to augment existing experimental data and ground our models in biophysics; and deep biological expertise in both RNA-protein interactions and condensates to guide development of useful tools and apply them to discover new biology. Additionally, each of our aims has a large data science component that would benefit from NCEMS Staff Scientist Support. Rationale for NCEMS support This work aligns closely with NCEMS goals: we will develop and apply tools that utilize only publicly available data; it requires close collaboration between three labs with distinct yet complementary skills; and by combining our diverse perspectives, we will address long-standing mysteries including how synonymous and non-coding RNA mutations impact cellular function and why widely expressed RNAs have cell-type specific effects. Additionally, this project will require NCEMS support for collaboration logistics, virtual leadership and team science training, centralized computing and storage through CyVerse, and staff scientist assistance for data science. Requested resources Our project requires 60 TB of data storage to store all datasets and simulation results, 1.35 million CPU hours for simulations, access to approximately 8 GPUs for machine learning, an NCEMS Staff Scientist at 33% FTE for 2 years with expertise in bioinformatics, machine learning, and statistics, an NCEMS Staff Scientist Assistant at 50% FTE for 2 years with expertise in RNP biology and data science, $5,000 for publication costs, and travel funds for in-person kickoff meetings and events. Proposed timeline We expect that this project can be completed with 2 years of NCEMS support. We plan for intermediate outputs, including publications describing tools and databases from the early aims and biological findings from later aims. The project will engage undergraduate teams across participating labs, with staff scientists providing mentorship and coordination. Regular meetings and collaborative writing will ensure integration across all components of the project.",
            "proposal_status": "Rejected",
            "ranking": 4
        },

        {
            "proposal_id": "7",
            "proposal_title": "Oceans of Disorder: Elucidating the Role of Disordered Proteins in Cellular Adaptation",
            "authors": ["Keren Lasker", "Jerelle A. Joseph", "Alex Holehouse"],
            "authors_departments": ["Biology", "Chemistry", "Molecular Biophysics"],
            "abstract": "Intrinsically disordered proteins and protein regions (collectively IDRs) are key regulators of environmental sensing, owing to their ability to undergo conformational changes in response to physicochemical fluctuations. Despite evidence from individual studies, a comprehensive understanding of IDR-driven adaptations across diverse environments remains elusive. The deep sea, characterized by extreme and variable conditions such as temperature, pressure, and salinity, offers a unique opportunity to address this knowledge gap. Over the past decade, large-scale ocean sampling expeditions using shotgun metagenomics have revolutionized our understanding of marine biodiversity, providing access to the genetic diversity of organisms, from microbes to fish, thriving in these extreme habitats. These datasets offer an unprecedented opportunity to explore how IDRs have evolved as molecular sensors to support survival across varied environmental conditions. Here, we will, for the same time, use these metagenomic datasets to systematically characterize IDRs as environmental sensors. Our multidisciplinary team, with expertise in metagenomics, protein biophysics, cell biology, and deep learning, will integrate bioinformatics, simulations, machine learning, and theoretical approaches to uncover sequence-level and functional adaptations of IDRs across marine environments. The outcomes of this research will produce a comprehensive map of IDR adaptations in marine systems, revealing their emergent properties as molecular sensors and actuators. In addition to advancing fundamental insights into stress adaptation and evolutionary biology, this work will inform biotechnological innovations, including engineering robust systems for extreme environments.",
            "full_draft":"Introduction & goals IDRs are emerging as critical components in environmental sensing due to their unique ability to adopt dynamic conformations in response to external stimuli. Unlike structured proteins, IDRs lack a fixed three-dimensional structure, allowing them to function as versatile molecular sensors capable of adapting to physicochemical changes. While several studies have highlighted the role of IDRs in stress adaptation, a systematic understanding of how IDRs enable organisms to thrive under extreme and fluctuating conditions remains a major knowledge gap. The marine environment serves as a unique natural laboratory for investigating how life adapts to variable conditions, including temperature, salinity, and pressure. The Tara Oceans project, launched in 2009, has been instrumental in advancing this understanding. Over four years, using standardized protocols, Tara Oceans systematically collected over 30,000 samples from more than 200 ocean stations, including depths reaching 2,000 meters. These samples encompass various environmental conditions and yielded a vast metagenomic dataset with millions of novel sequences from viruses, prokaryotes, and eukaryotes. Analyses of this dataset have revealed temperature as a key driver of microbial community structure and uncovered previously unknown gene families. Leveraging Tara's dataset, we will, for the first time, focus on the biophysical properties of individual proteins to uncover how they enable organisms to thrive in distinct marine environments. To achieve this, we are developing a multidisciplinary approach that integrates metagenomics, biophysical simulations, and machine learning with environmental metadata. Our project is organized around three goals: catalog and annotate IDRs by identifying them in marine protein sequences and correlating their biophysical properties with environmental parameters, simulate IDR dynamics by performing biophysical simulations of IDR dynamics under varying environmental conditions to extract key structural features, and elucidate IDR adaptation mechanisms by linking sequence-level features to functional biophysical properties and environmental stimuli. The outcomes of this work will address critical gaps in our understanding of IDRs functionality across diverse environments and provide a foundation for broader applications. Identifying universal principles of IDR adaptation will reveal protein plasticity mechanisms and enable the engineering of adaptable systems. Proposed activities Activity 1: Building a catalog of IDRs in marine proteins across environmental parameters We will build a comprehensive catalog of IDRs in marine proteins, correlating their biophysical properties with key environmental parameters, including temperature, salinity, and ocean depth. This work leverages the Ocean Gene Atlas v2.0, a publicly available database derived from the Tara Oceans expedition, which provides extensive marine protein data. This expedition standardized sampling methodologies, collecting 12,543 samples across diverse marine ecosystems, encompassing 472 unique sampling events. These samples span a broad range of environmental conditions, including temperatures from 0 to 30 °C, ocean depths from 0 to 1000 m, and salinities between 30 to 40 PSU. Linking marine proteins to environmental and genomic parameters. We will curate the OGA2 database, which contains approximately 45 million prokaryotic and 10 million eukaryotic protein sequences, to connect protein sequences with their environmental and genomic contexts. OGA2 includes protein abundance data and environmental metadata for each sampling location. We hypothesize that protein orthologs span diverse environments, allowing us to associate protein features with specific environmental conditions. Expanding on this analysis, we will systematically annotate each protein with the range of environmental conditions where it is detected. Additionally, we will link each protein to metagenome-associated genomes in the OceanDNA database to incorporate genomic metadata such as genome size and GC content. Annotating and structurally characterizing marine proteins. We will predict the structure of each protein using ESMFold and AlphaFold3. Domains will be identified using an integrative approach combining UniProt annotations, Chainsaw (a convolutional neural network for detection of structural domains), and Metapredict (a deep learning-based consensus predictor of IDRs). For ordered domains, we will extract detailed structural features, including secondary structures, dimensions, surface chemistry, and functional annotations where available. For disordered regions, we will extract sequence-derived features using CIDER. Labeling IDRs based on biophysical properties. IDRs will be labeled according to their intrinsic biophysical properties using ALBATROSS, a deep learning-based approach for predicting IDR ensembles under physiological conditions. ALBATROSS enables assignment of properties such as radius of gyration, end-to-end distance, asphericity, Flory scaling exponent, and prefactor. Since ALBATROSS was trained for physiological conditions, its reliability may decrease under extreme conditions. To address this, we will incorporate additional biophysical labels derived from molecular dynamics simulations, as outlined in Activity 2. Activity 2: Coarse-grained simulations to decipher IDR properties under different environmental conditions The functional versatility of IDRs derives from their 3D conformations, which cannot be directly inferred from their sequences. These conformations are expected to adapt dynamically to environmental changes, potentially encoding mechanisms for adaptation. To investigate these hypotheses, we will study IDR conformations using coarse-grained simulations under a range of environmental conditions. Specifically, we will simulate IDRs across gradients of temperature and salinity to capture their biophysical properties and structural responses. These simulations will provide detailed 3D conformational data for IDRs in marine proteins, offering insights into their adaptive mechanisms. Our approach leverages residue-level coarse-grained models that enable the efficient exploration of long timescales and larger molecular systems. To analyze the resulting high-dimensional data, we will integrate advanced deep learning techniques, including graph neural networks and variational autoencoders, to extract structural features and annotate IDR properties. Simulating IDRs under different environmental conditions. Building on insights from Activity 1, we will group homologous IDRs and select representative IDRs per protein homolog to create a diverse yet focused dataset for analysis. Coarse-grained one-bead-per-residue molecular dynamics simulations will be conducted for these representative IDRs. To enable extended timescale simulations and comprehensive trajectory sampling, we will employ chemically specific, residue-level coarse-grained models to simulate IDRs under specific environmental conditions: salt concentration gradients using the Mpipi-Recharge model and temperature gradients using a custom in-house model developed by our team. Pressure effects will not be simulated due to the absence of a validated coarse-grained model. Simulations will be performed using the open-source LAMMPS software. We will run each IDR at three temperatures and three salt concentrations, totaling eight simulated ensembles, excluding the case near physiological conditions, which will be annotated in Activity 1. Annotating biophysical properties that arise in simulations under a range of environmental conditions. We will employ deep learning-based methods to extract critical information from high-dimensional simulation data, focusing on uncovering conformational representations and structural propensities of IDRs. By leveraging advanced clustering and neural network techniques, we will learn nonlinear representations that disentangle key variation factors, capture invariant structural features, and enable the transfer of insights across diverse IDR populations. Specifically, we will integrate variational autoencoders, graph neural networks, and generative adversarial networks to analyze molecular dynamics trajectories under diverse environmental conditions. This approach will result in an ensemble decoder that outputs multi-vector conformational matrices, highlighting key features such as global conformations, pairwise contacts, and secondary structures, which collectively annotate IDR biophysical responses. Activity 3: Mapping the mechanisms of IDR adaptation and Engineering Environmentally Resilient IDRs We will uncover the molecular principles governing the adaptation of IDRs to diverse environmental conditions. By identifying chemical features of IDRs that correlate with environmental variables, we will elucidate universal properties that can be fine-tuned to enable IDR adaptability. Comparative analyses of IDRs from orthologous proteins across different environmental contexts will reveal chemical adaptations required to maintain essential protein functions. These insights will culminate in the development of a framework for designing IDRs with enhanced adaptability and functionality tailored to specific conditions. Identification of essential molecular features for IDR adaptation. To understand how IDRs adapt to environmental conditions, we will employ machine learning approaches to distill high-dimensional sequence and biophysical data into a low-dimensional latent space. This space will encapsulate critical features, such as compositional biases and environmentally dependent biophysical properties. Key steps include data distillation, where supervised or semi-supervised machine learning models encode sequence and property data into a compact latent representation; feature mapping, where environmental parameters are overlaid onto the latent space to identify how IDRs modulate their features to adapt; and validation, where comparative analysis with conventional dimensionality reduction techniques ensures robustness while enabling mutational and functional predictions that extend beyond conventional methods. Predicting evolutionary trajectories and designing novel IDR sequences. Building on these insights, we will model evolutionary trajectories and design IDRs optimized for specific biophysical and environmental properties. Adaptation trajectory prediction will use generative modeling frameworks to simulate IDR adaptation under progressive environmental conditions, allowing us to predict how sequence features evolve to optimize functionality. We will validate predicted trajectories by comparing them to adaptation trajectories in our dataset and cross-referencing experimental data. Adaptive rule discovery will identify how variations in sequence features influence adaptation, enabling formulation of design rules for engineering environmentally resilient IDRs. Using the latent space as a generative tool, we will design novel IDR sequences with desired biophysical properties. These sequences will be tested computationally and experimentally through collaborations with biophysics experts. This framework will enable creation of IDPs tailored for synthetic biology and environmental resilience. Expected outcomes This research will provide transformative insights into the adaptive mechanisms of IDRs in marine proteins, elucidating their role in survival across environmental gradients such as depth, salinity, and temperature. By cataloging IDRs alongside their biophysical properties and environmental metadata, we will establish the first comprehensive database linking IDR biophysical features to environmental contexts. This database will serve as a foundational resource for understanding the evolutionary adaptations of IDRs. Through large-scale simulations and machine learning models, we expect to uncover universal chemical features that govern IDR adaptability. These findings will address critical knowledge gaps and provide a foundation for engineering environmentally adaptable proteins. The resulting tools and frameworks will have broad applicability, extending beyond marine systems to other biological contexts, and paving the way for advances in synthetic biology, biotechnology, and evolutionary studies. We anticipate multiple publishable outcomes from this work, including a database of marine IDRs cataloging sequence features and environmental metadata, large-scale simulations and biophysical analyses revealing key adaptive features, a machine learning-based predictor of IDR adaptation pathways, and a computational tool for the de novo design of IDRs tailored to specific environmental niches. Rationale for the working group collaborative approach The scope and interdisciplinary demands of this project are beyond what a single lab can address alone. Understanding the interactions between IDR sequence, structure, and function across diverse environments requires expertise in multiple disciplines, including metagenomics, biophysics, and machine learning. By combining these skill sets, our working group provides a unique platform to tackle the project's challenges effectively. Metagenomics and bioinformatics experts will curate and analyze large datasets of marine proteins, annotate IDRs with environmental and genomic parameters, and uncover adaptation trends. This work establishes a foundation for identifying patterns of IDR adaptation across ecosystems. Biophysics and modeling experts will utilize advanced simulation tools developed by members of the working group for modeling IDR behavior under diverse environmental conditions, offering unparalleled insights into conformational dynamics and adaptation mechanisms. Machine learning experts will apply advanced algorithms to analyze high-dimensional molecular dynamics data, extracting structural and functional insights and enabling predictive modeling and design of novel, environmentally adaptable IDRs. By combining these areas of expertise, the working group fosters innovation and ensures the creation of broadly applicable tools and frameworks. Rationale for NCEMS support This project requires NCEMS support and resources because its scope and complexity extend beyond the capabilities of a single lab or existing collaborations. NCEMS's infrastructure and expertise make it an ideal partner to address the following critical needs: simulation management for approximately 440,000 molecular dynamics trajectories; database development to catalog IDRs and integrate biophysical properties with environmental metadata; collaborative infrastructure to support interdisciplinary synergy; and AI expertise to extract connections between IDR sequences, environmental factors, and adaptation mechanisms. Requested resources We request a 33% FTE Staff Scientist to help design and build the IDR catalog, develop data analysis strategies, and identify relationships between IDR behaviors and environmental conditions; a 50% FTE Staff Scientist Assistant to manage and curate molecular dynamics simulations and support infrastructure; approximately 26 million CPU hours for large-scale simulations; and 52 TB of data storage. Proposed Timeline The project spans 24 months. The first 6 months will focus on designing and building the IDR catalog. Months 5–16 will involve running and curating simulations, processing approximately 440,000 trajectories, and publishing preliminary findings. Year 2 will leverage results from simulations to develop predictive models and design de novo IDRs, culminating in final publications and public release of the tools and datasets.",
            "proposal_status": "Accepted",
            "ranking": 8
        },
        
        {
            "proposal_id": "8",
            "proposal_title": "Multimodel single-cell frameworks for cell lines with extensive multi-omics data",
            "authors": ["Elizabeth Brunk", "Hyebin Song", "Ferhat Ay", "Vasant Honavar", "William Noble"],
            "authors_departments": ["Chemistry", "Statistics", "Computational Biology", "Data Sciences", "Genome Sciences"],
            "abstract": "Large-scale consortia like the Dependency Map and Cancer Cell Line Encyclopedia have generated extensive multi-omics data for over 2000 primary cell lines, including bulk measurements of DNA, chromatin accessibility, RNA, and proteins. Recently, multi-omics single-cell sequencing and imaging data labeling RNA and proteins for these cell lines have become increasingly available. However, these datasets remain fragmented, lacking integration into a unified framework to model and quantify DNA, RNA, and protein within the same cell. While previous efforts have created multimodal benchmark datasets, these often include diverse samples from multiple donors and heterogeneous cell types. In contrast, our project focuses on single model systems, such as commonly used cell lines, which already feature extensive single-cell, bulk sequencing, and imaging data. By eliminating donor and cell type variability, our approach enables robust analyses and precise alignment across modalities, including RNA, protein, and chromatin accessibility. This focus on homogenous cell types within a consistent biological context facilitates mechanistic studies of regulatory interactions and perturbation responses, minimizing confounding variability. We aim to develop multimodal frameworks for 7 cell lines and computational pipelines to analyze relationships across DNA, RNA, and protein levels. By incorporating additional multi-omics data, including bulk and single-cell perturbation datasets (e.g., loss-of-function screens and Perturb-seq), we will create a comprehensive benchmarking resource. With expertise in machine learning, network biology, and genomics, and NCEMS support for data wrangling, our team will deliver tools to study regulatory mechanisms and cellular resource allocation. These efforts will foster fundamental discoveries, community-driven resource development, and innovation in multi-omics research.",
            "full_draft":"",
            "proposal_status": "Accepted",
            "ranking": 9
        },
        {
            "proposal_id": "9",
            "proposal_title": "From image data and biophysical simulations to principles of mesoscale organization through integrated transfer learning",
            "authors": ["Mary Mirvis", "Adriana Dawes"],
            "authors_departments": ["Biochemistry and Biophysics", "Mathematical Biology"],
            "abstract": "Mesoscale cellular organization emerges from complex interactions among chemical, structural, and physical components. Understanding how mesoscale structures influence local and collective cellular properties remains a significant challenge, often beyond the scope of individual experimental studies. While vast image datasets for cellular components exist, their full potential for deeper secondary analysis and synthesis remains untapped. We propose to synthesize morphological and physico-chemical measurements from cell image, in vitro, and in silico data to unravel the principles of mesoscale cellular organization. Our approach will develop a pipeline using transfer learning and mutual prediction, trained on complete datasets (all components of interest present) and applied to incomplete datasets (missing one or more components). This methodology will uncover relationships between cellular components and enable holistic systems-level predictions. To demonstrate the generalizability and biological relevance of this approach, we will test it on three increasingly complex mesoscale systems centered on the nucleus: (1) nucleus-nucleolus, (2) nucleus-centrosome-microtubules, and (3) nucleus-mitochondria-mitochondrial condensates. These test cases will showcase how our method integrates and predicts complex systems relationships, advancing our understanding of mesoscale cellular organization.",
            "full_draft":"Background Understanding mesoscale cellular organization requires investigating the interplay of physical forces, spatial constraints, and interactions between cellular components. Advances in morphological profiling, spatial and organellar proteomics, and generative whole-cell modeling have laid the foundation for mapping organizational patterns on scales ranging from molecules to organelles. However, achieving a holistic view of what drives mesoscale organization necessitates integrating these components across each other and with biophysical principles. This proposal shifts focus from traditional gene/protein-centric views to explore how physical principles and geometric constraints give rise to cellular architecture. By combining existing cell imaging data, in vitro reconstitution of mesoscale structures, and published outcomes of molecular simulations, we aim to develop a framework to predict how cellular architectures emerge from the localization, morphology, and interactions of components. This integrative approach will uncover emergent behaviors and previously unidentified features in high-dimensional datasets. Quantitative comparisons across high-dimensional datasets are challenging due to differences in data space. Traditional methods like clustering require data to reside in a single space, but advances in metric geometry, particularly Gromov-Wasserstein distances, enable comparisons across distinct spaces. Recent applications of GW distances have demonstrated their utility in aligning single-cell data and comparing biological networks and time-series data. Here, we propose leveraging GW distances to develop transfer learning algorithms to integrate biological datasets of mesoscale structures. This approach addresses critical gaps in cross-space learning, enabling rigorous integrative analyses of multimodal data. Driving Questions How much of mesoscale cell organization arises from intrinsic physical properties of structures versus their interactions? What role do mesoscale interactions play in determining the morphological and physico-chemical properties of cytoskeletal, organelle, and condensate components? Can a transfer learning approach infer morphological and physical properties of mesoscale structures from joint and incomplete datasets to address these questions and beyond? Proposed Activities Generalizable computational analysis and modeling approach Our approach integrates multicomponent image data to predict unseen properties in incomplete datasets. We extract high-dimensional morphological feature sets describing component size, shape, number, position, and inter-component interactions. Dynamic metrics, such as positional and shape changes, are included for live-cell data. Dimensional reduction generates interpretable feature spaces, which can be tuned for exploratory or hypothesis-driven studies, including granular dissection of cross-component interdependencies and contextual comparisons such as drug treatments, disease states, and cell types. The feature space is sampled along statistical dimensions to reveal feature covariance and specific feature evolutions. Using GW distance analysis to go beyond by-eye interpretations, we quantitatively describe feature covariance and predictive relationships between components. This two-step approach integrates and learns from data across cellular contexts: Step 1: Integrate data across cells. Multicomponent images are aligned via GW distances, generating pairwise similarity matrices for features of interest. This enables comparisons across distinct data spaces, leveraging complex features such as organelle shape. Step 2: Infer feature relationships. Deep learning on alignment matrices reveals predictive relationships between features. Generative adversarial networks under GW distance and transfer learning algorithms facilitate inference, validated using open-source datasets. To tackle the complexity of cellular data, we will incorporate data from minimal systems to compare intrinsic properties in isolated components versus cellular contexts and test whether morphology predicts physical or chemical properties. These data reveal the intrinsic properties of the component in isolation, providing a baseline for comparison with the cellular context. Morphological and physical feature spaces extracted from in vitro and in silico data will be compared with cell image features. New simulations will be developed to ask whether inter-component interactions could close the gap between minimal system and cellular feature distributions. If not, this could indicate that other interactors or environmental factors need to be incorporated. Translating images, which are high-dimensional representations of reality, into physical models requires a combination of approaches. First, we analyze images to extract the underlying physics using low-dimensional models. Second, GW distance analysis can form the basis for standard granular agent-based models. We aim to develop physical models alongside molecular and cellular descriptions to identify mechanisms governing complex processes. Application of workflow to test cases We will refine our workflow by applying it to three mesoscale interaction systems: Nucleus and Nucleolus. Integration of image and minimal system data will explore universal principles governing nucleolar morphology, dynamics, and chromatin interactions. In vitro and simulation data for nucleoli revealing biophysical properties such as surface tension and fluidity will be incorporated. Nucleus, centrosome, and microtubules. This system spans membrane-bound, condensate, and cytoskeletal components. Using multicomponent imaging datasets, we will reconstruct three-part feature relationships from two-component feature relationships and validate findings with in vitro data for microtubules and centrosome. The visibility of all three components in tubulin staining patterns will be leveraged to predict missing components from single-component images. Mitochondria, mitochondrial nucleoids, and nucleus. We examine how morphological properties of mitochondrial nucleoids, which behave as biomolecular condensates, are related to mitochondrial and nuclear structure and function. High-throughput imaging of fibroblasts from healthy and HGPS patients, where mitochondrial nucleoid morphology is altered, will reveal links between structural changes and cellular aging and enable prediction of mitochondrial nucleoid properties in other images. Integration and Synthesis The nucleus serves as an anchor point across test cases due to its extensive biological characterization and rich representation across multicomponent and minimal system data types. After initial workflow implementation, findings will be integrated into a larger model incorporating all components across test cases with the nucleus at the nexus, revealing new mesoscale interactions. Limitations in dataset availability, findability, and quality will likely necessitate further data scouting, particularly for in vitro and in silico data. Systematic literature synthesis methods adapted from the interdisciplinary community will provide complementary insights, building on novel applications for fundamental cell biology currently under development by Mary Mirvis. This combined approach will give insight into variability and reproducibility of findings, supporting robust statistical analyses and modeling efforts. Outcomes The project will yield key outcomes aimed at advancing our understanding of mesoscale cellular organization. These include: a generalizable analytical framework, a computational pipeline leveraging GW distances and transfer learning to analyze multicomponent imaging datasets; predictive models validated for inferring unseen properties from incomplete datasets; integrated datasets combining cell image, in vitro, and in silico sources; and new biophysical insights into mesoscale structure-function relationships spanning sub-nucleus dynamics, organelle interactions, and cytoskeletal organization. Broader Impacts By enabling the integration of diverse datasets, the project will facilitate systems-level analyses of cellular organization and dynamic processes. The generalizable methods will benefit disciplines ranging from biophysics to computational biology. Additionally, by providing new tools and datasets, the work will empower researchers to explore previously intractable mesoscale and multiscale cellular organization questions. Rationale for a Working Group Approach The proposed work requires expertise in computational image analysis, statistical and mathematical modeling including machine learning and simulations, biological and biophysical insight into specific test cases, and deep subject matter expertise for meaningful interpretation of results. No single lab possesses all of these qualities, but our group includes field leaders in all relevant fields. A multi-lab collaboration also expands access to additional complete and well-documented datasets, enhancing the robustness and impact of our analyses. Rationale for NCEMS Support Our proposal builds on several open-source data and methods resources and aims to produce an open-source, highly generalizable data integration workflow to benefit the scientific community, strongly in line with NCEMS’ mission. We will require IT support from NCEMS due to the computational demands of GW distance analyses, which are computationally expensive. Additionally, high-resolution image data necessitates substantial data storage resources, another area where NCEMS can provide critical support. Furthermore, this project involves extensive data and knowledge synthesis, including the integration of image datasets and feature data from dynamic and static cell images, in vitro studies, and simulations, as well as comprehensively synthesized published findings. NCEMS support can enhance these efforts by facilitating systematic curation and pooling of smaller datasets, assembling a large, diverse corpus. These efforts address gaps in publicly available resources, enabling broader applications of the pipeline. Requested Resources We expect to use TensorFlow and PyTorch to train deep learning models and the POT Python library to compute GW distances. For each instance of model training, we expect approximately two hours of training time per GPU, with total time depending on the need to train additional models. Initially, we estimate requiring at least ten thousand GPU hours, with this demand scaling proportionally as additional datasets are integrated into the analysis. High-resolution imaging datasets and intermediate data products, including simulation outputs, will require substantial storage capacity, estimated at approximately fifty terabytes for the duration of the project. Assistance from NCEMS and CYVERSE IT specialists will be critical for optimizing workflows, managing data transfers, and ensuring compatibility between datasets and computational tools. As part of the project’s data synthesis efforts, we also require support for systematic dataset curation and integration, including scouting and pooling smaller datasets from literature and open-source repositories. Proposed Timeline A two-year timeline is appropriate for the proposed work. Development of the core workflow for cell image feature space comparison for two-component test cases is expected to take at least six months. Further refinement, expansion to three-component systems, incorporation of additional data types, and integration across test cases will extend through the second year, depending on availability of data, resources, and personnel.",
            "proposal_status": "Rejected",
            "ranking": 12
        },
        {
            "proposal_id": "10",
            "proposal_title": "Transposable elements and the emergence of genomic innovation",
            "authors": ["Shaun Mahony"],
            "authors_departments": ["Biochemistry & Molecular Biology"],
            "abstract": "Our working group aims to comprehensively investigate how transposable elements (TEs) contribute to genomic innovation and species diversification in vertebrates through the generation of novel functional elements. While TEs comprise nearly half of the human genome and are increasingly recognized as sources of regulatory innovation, their repetitive nature has complicated their analysis using short-read sequencing technologies. We propose a two-phase approach: First, we will reprocess tens of thousands of publicly available genomic datasets (including ChIP-seq, ATAC-seq, RNA-seq, and Ribo-seq) using repeat-aware analysis pipelines and new telomere-to-telomere and pangenome assemblies. Second, we will synthesize this reprocessed data to understand how TEs generate new transcription factor binding sites, cell-specific and species-specific enhancers, novel transcripts, and protein-coding sequences. The project combines multidisciplinary expertise from multiple labs and will leverage advanced computational approaches to uncover complex relationships between TEs and regulatory networks. Our investigation represents a quintessential mesoscale emergence phenomenon, bridging molecular-level TE insertions with emergent regulatory networks and phenotypic innovations. The project will produce both a comprehensive public data resource of repeat-aware genomic analyses and new insights into how TEs drive biological innovation through regulatory network evolution.",
            "full_draft":"Introduction Transposable elements are mobile genetic elements that can insert themselves into new genomic locations, resulting in duplication of DNA. Nearly half of the human genome originates from transposon activity. Since their discovery by Barbara McClintock, their biological and evolutionary roles have been debated. Once dismissed as “junk DNA,” they are now recognized as key drivers of genomic innovation, creating new regulatory elements and contributing to evolutionary diversification. For instance, a large fraction of primate-specific transcription factor binding sites are derived from transposable elements. This means TEs have played a fundamental role in shaping species-specific gene regulation. However, their repetitive nature complicates functional characterization with sequencing-based assays. High-throughput methods like ChIP-seq, ATAC-seq, and RNA-seq rely on short reads that often map ambiguously to repetitive regions, producing multi-mapped reads that most pipelines discard. As a result, much of the genome’s repetitive and TE-rich regions remain underexplored in existing analyses. Recent advances in both computational and sequencing technologies now make it possible to revisit these regions with improved accuracy. Telomere-to-telomere and pangenome assemblies fill previous gaps in reference genomes, providing a more complete representation of repetitive DNA. New probabilistic read allocation methods, such as Allo, allow recovery of signals from multi-mapped reads, substantially increasing the detection of functional genomic elements associated with TEs. Preliminary analyses already show large gains in identified binding sites when accounting for such reads. Together, these technological advances provide an unprecedented opportunity to systematically characterize the functional roles of TEs across genomes and species. Goals This project aims to determine how transposable elements generate novel functional elements that drive phenotypic diversification across vertebrates. We will reprocess large-scale genomic, transcriptomic, and ribosome profiling datasets using repeat-aware pipelines and integrate them across data modalities, cell types, and species to answer fundamental biological questions. The key questions include: How do TEs generate new transcription factor binding sites, and which transcription factors preferentially bind to specific TE families? How do TEs give rise to cell-type- or species-specific enhancers? How do they contribute to novel transcripts and protein-coding sequences? Addressing these questions will illuminate how molecular-level TE insertions give rise to new regulatory networks and biological functions. Proposed Activities Phase 1: Reprocessing regulatory genomics data with repeat-aware pipelines We will work with computational staff to reprocess tens of thousands of publicly available genomic datasets using repeat-aware methods. This includes ATAC-seq, DNase-seq, ChIP-seq (for transcription factors and histone marks), RNA-seq, and Ribo-seq from major consortia such as ENCODE, Roadmap Epigenomics, FAANG, and GTEx, along with vertebrate comparative data. Both human and mouse datasets will be aligned to telomere-to-telomere assemblies for maximum genome coverage, and human data will also be aligned to pangenome assemblies to capture population-level structural variation. Reads will be mapped using standard aligners configured to retain multiple mapping locations, followed by probabilistic allocation of multi-mapped reads using tools such as Allo and Rcount. Peak calling and quantification will be performed on these reprocessed data to generate consistent repeat-aware maps of regulatory activity, transcription, and translation. This phase will produce a comprehensive collection of regulatory and expression datasets that accurately capture activity within repetitive regions of the genome. Phase 2: Data synthesis to understand the role of TEs in generating functional elements Building on Phase 1, we will integrate and analyze the repeat-aware datasets to uncover patterns of TE-derived functional innovation. We will identify TE-derived transcription factor binding sites, enhancers, transcripts, and translated sequences by intersecting data with RepeatMasker annotations and evaluating their conservation across species using multi-species alignments. Chromatin states will be annotated using hidden Markov models, enabling classification of TE-derived regions as promoters, enhancers, or other regulatory states across different cell types. Statistical analyses will assess associations between TE families, transcription factors, gene expression, and tissue specificity. Beyond correlation-based approaches, we will apply integrative methods such as tensor decomposition to identify higher-order dependencies between data modalities—for example, linking specific transcription factors with TE families that activate enhancer programs leading to tissue-specific gene expression. This integrative analysis will reveal how TEs collectively reshape regulatory landscapes. Outcomes The project will yield a comprehensive, publicly accessible collection of reprocessed datasets capturing regulatory and transcriptional activity in repetitive regions, along with reproducible analysis pipelines for repeat-aware genomics. The data and methods will enable researchers to study regions of the genome that were previously inaccessible and to explore how TEs contribute to genomic and phenotypic innovation. Phase 2 will result in research publications describing biological discoveries as well as review papers summarizing best practices for repeat-aware data processing and analysis. Rationale for a Working Group Approach This project brings together complementary expertise in transposon biology, regulatory genomics, evolutionary biology, and computational data integration. Members specialize in distinct but interconnected areas—TE functionalization, TE evolution, enhancer and network evolution, species-specific regulation, and multi-omic data modeling—making collaboration essential. The scale of data reprocessing and integration required exceeds the capabilities of any individual laboratory, underscoring the need for coordinated efforts and shared computational infrastructure. Rationale for NCEMS Support The scope of this project aligns directly with the mission of NCEMS: synthesizing large-scale data to uncover mesoscale emergence phenomena. TE-driven regulatory innovation exemplifies such emergence, linking molecular events like TE insertions to higher-order regulatory networks and organismal diversity. NCEMS support will provide the computational power, storage, and expert personnel required to reprocess over 75,000 datasets, implement integrative analyses, and coordinate multi-lab collaboration. Requested Resources The project requires approximately ten million CPU hours for large-scale data reprocessing and analysis, staff scientist support for pipeline execution and data management, and storage for intermediate and processed datasets. Additional resources will support collaborative meetings, data publication, and personnel exchanges between participating labs. Proposed Timeline The project will span two years, with Phase 1 dedicated to reprocessing datasets and Phase 2 focusing on data integration and biological interpretation. The first year will include an initial meeting to finalize analysis plans and pipelines, followed by repeat-aware reprocessing across data modalities. During the second year, integrated analyses will be conducted, and results will be synthesized into collaborative publications and data resources.",
            "proposal_status": "Accepted",
            "ranking": 1
        },
        {
            "proposal_id": "11",
            "proposal_title": "Searching the crosslinking mass spectrometry universe for new protein-protein interactions",
            "authors": ["Stephen D. Fried", "Yasset Perez-Riverol", "Henning Hermjakob"],
            "authors_departments": ["Synthetic biology", "Biophysics", "Bioinformatics", "Molecular Systems"],
            "abstract": "Crosslinking mass spectrometry (XL-MS) is a powerful structural proteomics method that can provide high-resolution structural information about complex mixtures of proteins in their native physiological context.1,2,3 The method works by chemically crosslinking two reactive amino acid residues that are close to one another in 3-D space, thereby freezing this spatial information into a covalent bond, and then retrieving this information by sequencing crosslinked peptides following enzymatic digest of the constituent proteins (like Hi-C,4 but for proteins). Initially developed in the 2000s to interrogate purified proteins, by the late-2010s, methods became available to interrogate protein-protein interactions (PPIs) on the proteome-scale and in the cellular environment.5 This project seeks to standardize, combine, and integrate existing publicly available crosslinking datasets from PRIDE to make these data more re-usable and useful for integrative and hybrid modeling. We will also create a meta-dataset to catalogue PPIs in human and other model organisms. Because XL-MS encodes information about both protein identity and interacting residues, these findings can be cross-validated to structural predictions of protein complexes using AlphaFold3, thereby providing stronger evidence for their existence compared to huMAP3.0, which predicts protein complexes without structural evidence. We will integrate these findings into the EBI Complex Portal to make it accessible to the life science community.",
            "full_draft":"Introduction Every protein is connected. The physical interactions made by proteins often define their functions; hence, to understand the whole proteome, we must also have a complete compendium of protein complexes. Several proteomics approaches can be used to infer the existence of a pairwise protein-protein interaction, the fundamental unit of data used to build a model of a protein complex, such as affinity purification mass spectrometry, proximity labeling, co-fractionation mass spectrometry, and photoactivated RNA crosslinking coupled to mass spectrometry. Large-scale efforts such as BioPlex have conducted tens of thousands of affinity purification experiments to catalog human protein complexes. huMAP integrates BioPlex data with co-fractionation and proximity labeling datasets, using machine learning to cluster protein pairs into a global network. This network estimates that most human proteins exist in some complex, and its predictions have been incorporated into the EBI Complex Portal, greatly expanding the manually curated human complexes. While these approaches are powerful, none provide structural information or evidence of which residues or interfaces are involved in a protein-protein interaction. Furthermore, these assays require that complexes survive lysis and remain intact in vitro, meaning transient interactions are often missed. Crosslinking mass spectrometry is an orthogonal method that encodes pairwise interactions by sequencing the two peptides derived from two proteins covalently connected through a chemical crosslinker. Originally applied to purified proteins, advances in crosslinker chemistry, mass spectrometry instrumentation, and search software have enabled proteome-wide applications. A contact between any reactive residue on one protein and any reactive residue on another can now be identified and validated. For many years, confidently identifying inter-protein crosslinks in complex samples was challenging because search software could not properly estimate false discovery rates. This limitation has been overcome by new algorithms capable of accurate validation. The XL-MS community has generally made raw data available on PRIDE, a major proteomics repository, but reusability remains limited. Different search algorithms, customized databases, and inconsistent metadata have created barriers to data reuse. Until recently, there was no unified format for reporting processed crosslinking data, and sample metadata were often poorly annotated. Proposed Activities This project has three main activities. The first aims to select large-scale XL-MS datasets from PRIDE, re-annotate them with standardized metadata, and reanalyze them using modern search algorithms to improve data quality and reusability. The second will mine reprocessed data to discover potentially new protein-protein interactions and provide additional validation for existing ones, integrating these validated interactions into the EBI Complex Portal. The third goal, if time permits, is to use the interaction data to suggest potential functions for uncharacterized proteins based on their interaction partners. Reanalysis of XL-MS data on PRIDE There are currently over a thousand PRIDE datasets mentioning crosslinking, though few include complete processed data. Many contain only partial or technical studies without biological context. We will compile a list of biologically meaningful, large-scale XL-MS datasets focused on protein complex and interaction analysis. Each legacy dataset will be reannotated using standardized metadata formats and reprocessed using modern algorithms such as Scout. The results will be redeposited to PRIDE under new accessions as complete standardized submissions, linked to the originals. These will populate a dedicated PRIDE Crosslinking database, providing consistent and accessible data for reuse. Conglomerate Pairwise Residue Interactions to Validate Predicted Complexes A curated subset of large-scale reprocessed datasets across multiple model organisms will be analyzed in depth. Criteria for inclusion will include proteome-wide design, use of cleavable crosslinkers, studies in model organisms such as yeast or human cells, and sufficient experimental scale. Inter-protein crosslinks from these datasets will be pooled and analyzed to validate predicted complexes. For human data, each detected interaction will be compared against huMAP predictions. If the proteins are linked in huMAP, structural modeling with AlphaFold will assess whether the crosslinked residues are spatially proximal in the predicted structure. Verified interactions will increase the evidence level for those complexes in the Complex Portal, annotated with structural and dataset references. Crosslinks not already in huMAP will also be structurally tested, and passing cases will be added to the portal with provisional evidence labels. Similar analyses will be extended to yeast datasets. Additional analyses may include reprocessing affinity purification data from other organisms using quantitative pipelines, clustering them into complexes, and comparing them to XL-MS data using the same validation framework. Proposing Functions for Unannotated Proteins Many human proteins still lack assigned functions. While most functional proteins contain recognizable catalytic domains, uncharacterized proteins may act as scaffolds that mediate specific interactions. To begin annotating these, we will examine XL-MS-derived interaction networks. If an uncharacterized protein interacts with one or more proteins of known function, we will compile examples to identify emerging patterns or network features that may guide hypothesis generation for future studies. Outcomes Reanalysis of PRIDE XL-MS data will yield roughly a hundred large-scale standardized datasets, greatly expanding the number of accessible, biologically relevant crosslinking experiments. These data will facilitate reuse, allow non-specialists to access residue-pair information directly, and ensure compatibility with hybrid modeling databases. Conglomerating residue-level interactions will increase confidence in predicted complexes, refine evidence levels in the Complex Portal, and identify new interactions supported by structural modeling. Preliminary results will support high-impact publications and public data resources. Analyses of uncharacterized proteins may yield hypotheses about their roles and guide future functional studies. Working Group Approach The project requires combined expertise in mass spectrometry, data standards, database engineering, and structural biology. Dr. Fried’s team brings experience in XL-MS and structural biology, while collaborators at the EBI provide critical infrastructure for PRIDE and the Complex Portal, enabling large-scale reanalysis and integration. Manual curation and automation tasks will be supported by dedicated data scientists. This collaboration ensures both scientific depth and technical feasibility. Rationale for NCEMS Support XL-MS data provide unique residue-level evidence of protein-protein interactions but remain underutilized due to inconsistent formats and metadata. NCEMS support is essential to harmonize, reprocess, and integrate these data, linking them to existing structural predictions and interaction networks. This integration aligns with ongoing developments in hybrid structural biology, providing valuable resources for the community. Requested Resources The project requires a data scientist for dataset curation, annotation, and reanalysis, with travel support for collaboration between NCEMS and EBI teams. Computational resources are needed for large-scale reprocessing and structural predictions, estimated at tens of thousands of CPU hours and thousands of AlphaFold runs. Personnel and computational support will ensure timely and reproducible execution. Proposed Timeline The project will span approximately fifteen months. The first six months will focus on dataset reannotation and reprocessing, led by a data scientist with oversight from EBI collaborators. The next six months will focus on structural validation and integration of results into the Complex Portal. The final phase will explore uncharacterized proteins and synthesize results for publication and public release.",
            "proposal_status": "Accepted",
            "ranking": 4
        },
        {
            "proposal_id": "12",
            "proposal_title": "Intelligent metadata compilation to enhance the reusability and discoverability of mass spectrometry-based proteomics data",
            "authors": ["Wout Bittremieux", "Iddo Friedberg"],
            "authors_departments": ["Machine learning", "Analytical Chemistry", "Bioinformatics"],
            "abstract": "Mass spectrometry-based proteomics generates vast amounts of data, yet the effective reuse and discovery of these datasets remain challenging due to incomplete and inconsistent metadata. Metadata, which provides critical contextual information such as experimental conditions, sample characteristics, and data processing details, is essential for making data FAIR (findable, accessible, interoperable, and reusable). To address these challenges, our working group aims to develop intelligent, automated workflows for comprehensive metadata extraction, harmonization, and integration into PRIDE, the largest public proteomics data repository. We will develop powerful bioinformatics and natural language processing tools to extract metadata directly from raw mass spectrometry files and scientific publications. By leveraging structured data standards, metadata will be enriched with controlled vocabulary terms, enhancing dataset transparency and usability. Community engagement through a machine-learning challenge will further drive innovation in metadata extraction techniques. The outcomes of the working group's activities will include automated metadata compilation workflows integrated into user-friendly tools, advanced dataset querying capabilities in PRIDE, and the creation of educational resources for the scientific community. These efforts will enable researchers to efficiently locate and reuse proteomics datasets, facilitating secondary analyses such as AI model training and large-scale reanalysis.",
            "full_draft":"Introduction and Goals In line with the FAIR principles—findable, accessible, interoperable, and reusable—omics data is increasingly accessible through public repositories tailored to diverse research domains. This surge in data availability has immense potential to accelerate scientific discovery and foster cross-disciplinary innovation. However, the effective findability and reusability of these datasets remain limited, primarily due to a lack of comprehensive metadata. Metadata, which includes details like experimental conditions, sample characteristics, data processing methods, and instrument settings, is critical for transforming raw data into scientifically valuable resources. Unfortunately, metadata is often incomplete, inconsistently structured, or buried in unstructured formats such as free-form text in publications. These shortcomings restrict automated processing, systematic querying, and the discovery of relevant datasets. To bridge this gap, the working group will develop automated methods to extract, refine, and associate high-quality metadata with omics data in public repositories. This initiative will lay the groundwork for improved data discoverability and expanded reusability. While metadata enhancement is relevant across all omics fields, the initial focus is on mass spectrometry-based proteomics due to its complex metadata needs and established annotation frameworks. Specifically, the group will leverage the SDRF-Proteomics format, a structured tab-delimited standard adopted from transcriptomics (MAGE-TAB) by the Proteomics Standards Initiative. SDRF-Proteomics improves data transparency and usability by specifying relationships between samples, assays, and data files. Despite its growing adoption, many datasets remain inconsistently annotated. The group proposes to address this by developing intelligent, automated workflows to systematically gather and associate metadata for proteomics experiments and integrate it into PRIDE, the largest proteomics data repository. The tools will extract technical and biological metadata directly from experimental data, open-access publications, and existing annotations. Extracted metadata will be harmonized using controlled vocabulary and ontology terms and linked to datasets in PRIDE, creating an enriched metadata layer accessible for secondary analyses. To ensure accessibility, a user-friendly interface will be implemented in PRIDE’s web portal, alongside an API for metadata-based queries. Enhanced metadata infrastructure will unlock powerful capabilities for proteomics researchers, such as identifying suitable training datasets for AI tools, enabling automated large-scale reanalyses like differential protein expression profiling, and facilitating re-interpretation of public data in light of new hypotheses. Proposed Activities The project will initially focus on standard bottom-up discovery proteomics workflows, encompassing data-dependent and data-independent acquisition with both label-free and label-based quantitation, as well as post-translational modification-enriched datasets. Time permitting, it will expand to specialized approaches such as crosslinking and hydrogen-deuterium exchange proteomics. This stepwise approach allows controlled validation and extension to other omics disciplines in the future. Metadata retrieval will combine extraction from raw mass spectrometry data with mining of complementary metadata from scientific literature. From raw data, proprietary formats will be converted to mzML, and essential metadata such as instrument type, parameters, and search settings (mass tolerances, digestion enzymes, isobaric labeling strategies) will be derived using existing tools such as Param-Medic. Organism identification will be obtained by matching against reference data, and digestion enzymes and modifications will be determined via open modification searching. These components will be integrated into an automated workflow using Nextflow to streamline extraction and organization. For literature-based metadata, critical contextual information not found in raw data (biological conditions, case/control groupings, disease relevance) will be gathered using a natural language processing pipeline built on large language models fine-tuned for scientific text mining. To ensure accuracy and transparency, the models will indicate unavailable elements, perform self-verification, and use ensemble cross-checking among LLMs. To engage the community, a machine-learning challenge will be launched to drive innovation in metadata extraction. Winning open-source algorithms will be integrated into workflows. Tutorial datasets and documentation will be shared through ProteomicsML, fostering education for both data scientists and proteomics practitioners. To develop and validate the approach, the group will use a curated set of public datasets with SDRF metadata as a gold standard, along with a manually curated private test set as an independent benchmark for evaluating metadata accuracy. Scoring will emphasize precision to ensure high-confidence results. Finally, the workflows will be integrated into user-friendly tools such as lesSDRF, enabling automated metadata compilation during PRIDE submissions. Retrospective metadata generation will also be applied to existing public datasets with support from PRIDE’s data curation team. Enhanced PRIDE functionality will include metadata-driven dataset querying via the website and API. Outcomes The project will deliver multiple key outcomes: automated workflows for metadata extraction from raw data and open-access publications using NLP; a community challenge to produce open-source metadata extraction solutions; educational resources and tutorials through ProteomicsML; enhanced metadata tools integrated with PRIDE submission workflows; advanced metadata-based querying and API retrieval in PRIDE; and several open-access publications describing methodologies and technical innovations. All workflows and outputs will follow open-science principles, with code released on GitHub and manuscripts preprinted with accompanying data. While initially focused on proteomics, this work will create a scalable template for addressing metadata challenges across omics disciplines, beginning with metabolomics and beyond. Rationale for a Working Group Approach The project combines expertise across bioinformatics, machine learning, NLP, proteomics, and data repository management, exceeding the capacity of any single research group. Machine-learning and NLP specialists will design algorithms to extract metadata from raw files and publications, while bioinformaticians will align metadata with structured data standards. Proteomics experts will ensure biological relevance, and repository teams will oversee integration with PRIDE. This coordinated, interdisciplinary effort enables robust, scalable, and biologically meaningful metadata solutions. Rationale for NCEMS Support The project aligns with NCEMS’s mission to leverage public data for large-scale synthesis and develop novel integrative strategies. NCEMS staff scientists will assist with SDRF metadata curation, community challenge support, and workflow integration. CyVerse computational resources will provide scalability for NLP and large-scale metadata extraction. Team science training, project management support, and coordination of tool integration across geographically distributed members will further enhance outcomes. NCEMS’s contribution ensures both technical infrastructure and sustainability for this collaborative initiative. Requested Resources The project requires staff scientist and assistant support specializing in metadata harmonization and proteomics experimental design. These roles will include curation of metadata, support for NLP corpus creation, management of the community challenge, technical guidance, and workflow integration. Computational resources include approximately 15 TB of storage for public data mirroring, 2,500 CPU hours for metadata extraction pipelines, and 500 GPU hours for local fine-tuning of LLMs. Proposed Timeline The project is planned for two years. The first year will focus on metadata compilation, workflow development, and a community challenge launch. The second year will include evaluation of challenge outcomes, integration of workflows into lesSDRF, retrospective metadata generation for PRIDE datasets, and implementation of metadata-based querying functionality. Final deliverables will include completed tools, datasets, and open-access publications disseminated to the research community.",
            "proposal_status": "Accepted",
            "ranking": 4
        }

    ]
}