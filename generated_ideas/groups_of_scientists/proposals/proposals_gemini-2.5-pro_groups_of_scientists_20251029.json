{
  "session_id": "gemini-2.5-pro_groups_of_scientists",
  "template_name": "groups_of_scientists",
  "generation_timestamp": "2025-10-29T09:52:32.861802",
  "total_proposals": 10,
  "proposals": [
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_01",
      "original_title": "The Chromatin Grammar of Cellular Identity: Emergence of Cell Fate from the Integrated Epigenome",
      "original_abstract": "A fundamental question in biology is how the ~200 distinct cell types in the human body, each sharing the same genome, emerge and maintain their unique identities. This identity is encoded not in the DNA sequence itself, but in the dynamic, multi-layered epigenome. We propose to form a working group to address the hypothesis that a 'chromatin grammar'—a set of combinatorial rules governing the interplay of DNA accessibility, histone modifications, and transcription factor binding—dictates cellular identity as an emergent property. This project will synthesize a vast repository of publicly available single-cell and bulk functional genomics data, including ATAC-seq, ChIP-seq, Hi-C, and RNA-seq from consortia like ENCODE, the Human Cell Atlas, and Roadmap Epigenomics. Our multidisciplinary team, comprising computational biologists, chromatin experts, developmental biologists, and machine learning specialists, will develop a novel integrative framework. Using advanced machine learning techniques, including graph neural networks and transformers, we will move beyond simple correlations to build predictive models that can infer cell type from chromatin state alone and simulate cell fate transitions. The project will deliver a publicly accessible 'Chromatin State Atlas' and a computational model of the emergent logic of cell identity. This work will provide profound insights into development, disease (like cancer, where identity is lost), and regenerative medicine, while training a new generation of scientists in large-scale data synthesis and modeling.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:38:30.046166",
      "proposal": {
        "title": "The Chromatin Grammar of Cellular Identity: Emergence of Cell Fate from the Integrated Epigenome",
        "abstract": "A fundamental question in biology is how the ~200 distinct cell types in the human body, each sharing the same genome, emerge and maintain their unique identities. This identity is encoded not in the DNA sequence itself, but in the dynamic, multi-layered epigenome. We propose to form a working group to address the hypothesis that a 'chromatin grammar'—a set of combinatorial rules governing the interplay of DNA accessibility, histone modifications, and transcription factor binding—dictates cellular identity as an emergent property. This project will synthesize a vast repository of publicly available single-cell and bulk functional genomics data, including ATAC-seq, ChIP-seq, Hi-C, and RNA-seq from consortia like ENCODE, the Human Cell Atlas, and Roadmap Epigenomics. Our multidisciplinary team, comprising computational biologists, chromatin experts, developmental biologists, and machine learning specialists, will develop a novel integrative framework. Using advanced machine learning techniques, including graph neural networks and transformers, we will move beyond simple correlations to build predictive models that can infer cell type from chromatin state alone and simulate cell fate transitions. The project will deliver a publicly accessible 'Chromatin State Atlas' and a computational model of the emergent logic of cell identity. This work will provide profound insights into development, disease (like cancer, where identity is lost), and regenerative medicine, while training a new generation of scientists in large-scale data synthesis and modeling.",
        "background_and_significance": "The differentiation of a single zygote into the hundreds of specialized cell types that constitute a complex organism is a marvel of biological self-organization. While every cell shares an identical genome, each cell type exhibits a unique and stable gene expression program that defines its identity and function. This cellular diversity arises from the epigenome, a complex layer of chemical modifications to DNA and its associated histone proteins that orchestrates genome function without altering the underlying DNA sequence. The epigenome encompasses DNA methylation, dozens of post-translational histone modifications, chromatin accessibility, and the three-dimensional folding of the genome. Understanding how these layers of information are integrated to produce stable cellular identities is a central challenge in modern biology. \n\nOver the past two decades, large-scale international consortia such as the ENCODE Project, the Roadmap Epigenomics Mapping Consortium, and the Human Cell Atlas have generated an unprecedented wealth of publicly available data cataloging these epigenetic features across a vast array of human cell types and tissues. These efforts have been instrumental in creating a 'parts list' for the human epigenome. For instance, specific histone modifications are strongly correlated with the activity of functional elements: H3K4me3 marks active promoters, while H3K27ac is a hallmark of active enhancers (Heintzman et al., 2007, Nature). Computational methods like ChromHMM and Segway have leveraged these correlations to segment the genome into a limited number of 'chromatin states' (e.g., 'active promoter', 'poised enhancer'), providing a valuable, albeit simplified, annotation of the non-coding genome (Ernst & Kellis, 2012, Nature Methods). These models have successfully linked genetic variants to disease by identifying their location within specific regulatory elements.\n\nHowever, this descriptive, correlational understanding has significant limitations. It treats epigenetic marks largely as independent features, failing to capture the complex, combinatorial, and context-dependent interplay between them. The 'histone code' hypothesis, which posited that specific combinations of modifications act in concert to signal downstream functions (Strahl & Allis, 2000, Nature), has proven difficult to decipher with simple linear models. We currently possess a 'dictionary' that links individual marks to functions, but we lack a 'grammar'—a set of rules that explains how these marks are combined in sequence and in three-dimensional space to compose the complex regulatory programs that define a cell. We do not yet understand how cellular identity emerges as a robust, system-level property from these local molecular interactions. Consequently, we lack the ability to predict a cell's identity from its chromatin state *de novo* or to simulate the dynamic epigenetic transitions that occur during development or disease.\n\nThis research is both important and timely due to the convergence of two key factors. First, the explosion of single-cell multi-omic technologies is providing data at a resolution that was previously unimaginable, allowing us to observe the epigenome's heterogeneity and dynamics within cell populations. Synthesizing these vast, disparate datasets is a community-scale challenge that no single lab can tackle alone. Second, recent breakthroughs in machine learning, particularly deep learning architectures like graph neural networks (GNNs) and transformers, provide powerful new tools for learning complex patterns and long-range dependencies in high-dimensional data. These models are perfectly suited to move beyond simple correlations and learn the non-linear, combinatorial rules of the chromatin grammar. By forming a multidisciplinary working group to synthesize existing public data with these advanced analytical strategies, we are poised to address this fundamental gap in knowledge, yielding profound insights into the logic of cellular identity with direct implications for developmental biology, cancer research, and regenerative medicine.",
        "research_questions_and_hypotheses": "This project is driven by the overarching hypothesis that cellular identity is an emergent property governed by a decipherable, predictive 'chromatin grammar'. We will test this central hypothesis through three specific, interconnected research aims, each addressing fundamental questions about the structure, function, and dynamics of the epigenome.\n\n**Aim 1: Define the fundamental units and syntax of the chromatin grammar.**\nThis aim seeks to move beyond annotating individual epigenetic marks to identifying the recurrent, combinatorial patterns that form the building blocks of regulatory programs. We will deconstruct the complex epigenome into its core components and the rules governing their assembly.\n*   **Research Question 1.1:** What are the fundamental, recurrent combinations of chromatin accessibility, histone modifications, and transcription factor (TF) binding that constitute the 'words' of the epigenome across diverse human cell types?\n*   **Hypothesis 1.1:** We hypothesize that a finite, learnable set of multi-modal 'chromatin motifs' exists, representing stereotyped regulatory states (e.g., a 'pluripotency enhancer' motif, a 'neuronal promoter' motif). These motifs are more informative than any single epigenetic mark alone. We predict that our unsupervised models will not only rediscover known patterns, such as bivalent promoters in stem cells (H3K4me3 and H3K27me3), but also uncover novel, cell-type-specific combinations that define unique regulatory functions.\n*   **Research Question 1.2:** What are the syntactic rules governing the arrangement of these chromatin motifs along the linear genome and their organization in 3D space to control gene expression?\n*   **Hypothesis 1.2:** We hypothesize that the spatial organization of chromatin motifs follows non-random, hierarchical rules. The 'syntax' of the grammar dictates which genes are expressed by governing enhancer-promoter communication within 3D topologically associating domains (TADs). We predict that graph-based models incorporating Hi-C data will reveal that cell-type-specific gene expression programs are encoded in the network topology of long-range chromatin interactions, and that disruptions to this syntax are associated with aberrant gene regulation.\n\n**Aim 2: Build a predictive model of cellular identity from the integrated epigenome.**\nThis aim will leverage the discovered grammar to construct a comprehensive, predictive model that maps chromatin state to cellular identity. This represents a critical test of our understanding, moving from description to prediction.\n*   **Research Question 2.1:** Can a machine learning model, trained on a multi-modal atlas of chromatin states, accurately predict a cell's identity (type, subtype, and state) from its epigenomic profile alone?\n*   **Hypothesis 2.1:** We hypothesize that the chromatin state contains sufficient information to uniquely and robustly specify cell identity. An integrative deep learning model will learn the high-dimensional mapping from the epigenome to cell type with greater accuracy than models based on single data modalities or gene expression profiles. We predict our model will successfully classify cells from lineages not seen during training, demonstrating its ability to learn generalizable principles of the grammar.\n*   **Research Question 2.2:** What is the minimal and sufficient set of genomic loci and associated chromatin features required to define a given cellular identity?\n*   **Hypothesis 2.2:** We hypothesize that cell identity is specified by a core set of 'master' regulatory loci whose chromatin states are both necessary and sufficient. Using model interpretability techniques (e.g., attention scores, in silico saturation mutagenesis), we will identify these key genomic 'hubs'. We predict that a model trained only on these core loci will retain high predictive accuracy, providing a condensed, mechanistic view of the epigenetic basis of cell identity.\n\n**Aim 3: Model the dynamics of cell fate transitions as a shift in chromatin grammar.**\nThis aim extends our static model to the dynamic processes of development and cellular reprogramming, treating cell fate transitions as programmatic shifts in the underlying chromatin grammar.\n*   **Research Question 3.1:** Can our framework model the ordered sequence of changes in the chromatin landscape during cellular differentiation and predict intermediate states?\n*   **Hypothesis 3.1:** We hypothesize that cell fate transitions are not random walks but follow specific trajectories through a high-dimensional 'chromatin state space', constrained by the rules of the grammar. We predict that by training our model on pseudotime-ordered single-cell data from developmental systems (e.g., hematopoiesis), it will learn a latent representation of this state space. This will allow us to simulate differentiation trajectories, identify key decision points (bifurcations), and predict the sequence of epigenetic events required to transition from one cell state to another. This predictive capability will be validated against held-out time-course datasets.",
        "methods_and_approach": "Our approach is a multi-year, multi-institutional effort centered on the synthesis of public data using a novel, integrated computational framework. The project is structured around our three research aims and is designed to be open, reproducible, and collaborative.\n\n**Data Acquisition, Harmonization, and Integration (Year 1, Q1-Q3)**\nThis foundational phase is critical for the project's success. We will aggregate a comprehensive collection of publicly available human functional genomics data from major consortia, including ENCODE, Roadmap Epigenomics, the Human Cell Atlas (HCA), and the 4D Nucleome (4DN) project, as well as individual studies from GEO/SRA.\n*   **Data Types:** We will focus on core data modalities that define the epigenome: ATAC-seq (chromatin accessibility), ChIP-seq for key histone modifications (activating: H3K27ac, H3K4me1, H3K4me3; repressive: H3K27me3, H3K9me3), ChIP-seq for the architectural protein CTCF, and Hi-C/Micro-C (3D genome architecture). We will also integrate corresponding RNA-seq data for model validation and interpretation.\n*   **Processing Pipeline:** To overcome heterogeneity from different experimental protocols and labs, we will establish a single, unified processing pipeline. This pipeline will be built using Nextflow for scalability and portability, incorporating best-practice tools (e.g., BWA, MACS2, Juicer) and adhering to ENCODE processing standards. The entire workflow will be containerized using Docker and Singularity, ensuring complete reproducibility. All raw and processed data will be meticulously annotated with standardized metadata (cell type, tissue, donor, experimental conditions) and organized into a cloud-based, queryable database using efficient formats like Zarr or HDF5.\n\n**Aim 1: Deciphering Chromatin Grammar (Year 1, Q4 - Year 2, Q4)**\nWe will develop a novel machine learning framework to learn the combinatorial rules of the epigenome.\n*   **Genomic Representation:** We will represent the genome as a multi-graph. Genomic bins (e.g., 500 bp) will serve as nodes. Each node will have a feature vector containing the normalized signals from all integrated data types (ATAC-seq, ChIP-seq, etc.). Edges will connect nodes in two ways: 1) 'sequential' edges connecting adjacent bins along the chromosome, and 2) 'long-range' edges connecting physically interacting bins, weighted by contact frequency from Hi-C data.\n*   **Modeling Approach:** We will employ a hybrid deep learning architecture. To capture local combinatorial patterns ('chromatin motifs'), we will use Graph Attention Networks (GATs), which can learn the importance of different features and neighboring nodes. To capture the linear syntax and long-range dependencies along the chromosome, we will adapt the Transformer architecture, which has excelled at learning context in natural language. By treating the sequence of genomic bins as a 'sentence' and chromatin motifs as 'words', the Transformer's self-attention mechanism can identify critical regulatory elements hundreds of kilobases apart. We will use unsupervised methods, such as clustering the learned node embeddings from the GAT/Transformer encoder, to systematically identify and classify the fundamental 'words' of the chromatin grammar.\n\n**Aim 2: Predictive Modeling of Cell Identity (Year 2, Q1 - Year 3, Q2)**\nWe will build and validate a supervised model to predict cell type from chromatin state.\n*   **Model Architecture:** The encoder developed in Aim 1 will serve as the foundation. We will add a classification head to this encoder and train the entire model end-to-end in a supervised fashion. The input will be the multi-modal chromatin state for a given genomic region (e.g., a 2 Mb window), and the output will be a probability distribution over a controlled vocabulary of cell types derived from the Cell Ontology.\n*   **Training and Validation:** We will use the harmonized data from hundreds of cell types for training. To ensure robustness and avoid batch effects, we will employ a rigorous cross-validation strategy, holding out entire donors or tissues for testing. Model performance will be evaluated using metrics like area under the precision-recall curve (AUPRC). We will perform extensive model interpretation using techniques like SHAP (SHapley Additive exPlanations) and attention map visualization to identify the genomic loci and feature combinations most predictive of each cell type, directly testing Hypothesis 2.2.\n\n**Aim 3: Modeling Cell Fate Dynamics (Year 3, Q1 - Year 4, Q2)**\nWe will extend our framework to model the dynamic transitions between cell states.\n*   **Data:** We will utilize public datasets that capture cellular differentiation, such as time-course single-cell multi-omic data from hematopoietic stem cell differentiation or directed differentiation of embryonic stem cells.\n*   **Modeling Approach:** We will employ a variational autoencoder (VAE) architecture. The model will learn to project the high-dimensional chromatin state of single cells into a low-dimensional latent space. By incorporating pseudotime information derived from trajectory inference algorithms (e.g., Palantir), we will structure this latent space to represent differentiation pathways as smooth trajectories. This will allow us to perform in silico experiments: we can 'walk' along these trajectories to predict the sequence of chromatin state changes, identify bifurcation points representing cell fate decisions, and simulate the effects of perturbing key TFs by observing the resulting shift in the trajectory.\n\n**Timeline and Milestones:**\n*   **Year 1:** Completion of data harmonization pipeline; first-generation chromatin motif catalog.\n*   **Year 2:** Release of Chromatin State Atlas v1.0; development and initial validation of the cell identity prediction model; first major publication.\n*   **Year 3:** Refined predictive model with full interpretability analysis; development of the dynamic VAE model for cell fate transitions; public release of the 'Chromatin Grammar Engine' software.\n*   **Year 4:** Validation of dynamic simulations; capstone publications summarizing the chromatin grammar; final release of all data, models, and web portal; final working group meeting and community workshop.",
        "expected_outcomes_and_impact": "This project will produce a suite of tangible deliverables and generate significant intellectual and practical impacts, fundamentally advancing the molecular and cellular biosciences. Our work is designed to create lasting resources for the scientific community and to train a new generation of data-savvy biologists, directly aligning with the core mission of the NCEMS program.\n\n**Expected Outcomes and Deliverables:**\n1.  **A Unified, Harmonized Human Epigenome Atlas:** Our first major outcome will be a comprehensive, consistently processed atlas of multi-modal epigenomic data from hundreds of human cell types. This resource, far exceeding what any single lab could produce, will eliminate a major barrier in the field—data heterogeneity—and serve as a foundational dataset for countless future studies on gene regulation, development, and disease.\n2.  **The 'Chromatin Grammar Engine':** We will deliver a powerful, open-source deep learning model capable of predicting cell identity from chromatin state and simulating cell fate transitions. The software will be fully documented, containerized, and made available on platforms like GitHub and Docker Hub, allowing any researcher to apply our state-of-the-art methods to their own data.\n3.  **A Publicly Accessible Web Portal:** To ensure our findings are accessible to the entire biological community, including those without computational expertise, we will create an interactive web portal. This portal will allow users to explore the identified chromatin motifs, visualize the grammatical rules, query the importance of specific genomic regions for defining cell identity, and browse the complete epigenome atlas.\n4.  **A Quantitative Framework for Cellular Identity:** The primary intellectual outcome will be a new, quantitative framework for understanding cellular identity as an emergent property of the epigenome. We will deliver a catalog of the fundamental 'words' (chromatin motifs) and 'syntactic rules' that constitute the chromatin grammar, shifting the field from a descriptive to a predictive science.\n\n**Broader Scientific and Societal Impact:**\n*   **Transforming Basic Biology:** By providing a predictive model of gene regulation, our work will offer profound insights into fundamental biological processes. It will provide a mechanistic basis for understanding how cell lineages are established during embryogenesis and how cellular identity is maintained with high fidelity throughout life.\n*   **Advancing Human Health:** The implications for medicine are significant. In **cancer research**, our model can be used to understand how epigenetic dysregulation leads to a loss of cellular identity and the acquisition of malignant, de-differentiated states. This could reveal novel diagnostic biomarkers or therapeutic strategies aimed at restoring a normal epigenetic state. In **regenerative medicine**, a predictive understanding of the chromatin grammar will provide a blueprint for designing more efficient and precise protocols for cellular reprogramming and directed differentiation, accelerating the development of cell-based therapies for diseases like Parkinson's, diabetes, and heart failure.\n*   **Training and Workforce Development:** This project is an exemplary training vehicle. Postdoctoral fellows and graduate students will form the core of the working group, receiving unique cross-disciplinary training at the intersection of computational biology, machine learning, and chromatin biology. Through co-mentorship across participating labs, they will build collaborative skills and professional networks essential for future leadership in data-intensive science. We will further broaden our impact by developing and hosting an annual summer workshop on data synthesis and computational epigenomics for the wider community.\n\n**Dissemination and Open Science:**\nWe are deeply committed to the principles of open, team, and reproducible science. All software will be developed openly on GitHub. All analysis workflows will be shared as portable containers. All data, models, and results will be deposited in public repositories (e.g., GEO, Zenodo) and made accessible through our web portal prior to publication. We plan to disseminate our findings through high-impact, open-access publications, presentations at major international conferences, and seminars at diverse institutions. This working group will establish a lasting collaborative network that will continue to pursue these fundamental questions, ensuring the long-term sustainability and impact of the project.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis project that is beyond the capabilities of any single research laboratory or existing collaboration. The sheer scale of data aggregation and harmonization, the computational demands of training state-of-the-art deep learning models, and the essential requirement for deep, integrated expertise from disparate scientific fields (machine learning, computational biology, chromatin biology, developmental biology) necessitate the unique collaborative and resource-intensive environment provided by an NCEMS Working Group. NCEMS support is critical for funding the dedicated personnel, high-performance computing resources, and collaborative infrastructure required to achieve our ambitious goals.\n\n**Budget Justification and Breakdown (Total Request over 4 Years)**\n\n**A. Personnel ($1,250,000):** The majority of the budget is allocated to personnel who will drive the project's day-to-day research and development.\n*   **Postdoctoral Scholars (3.0 FTE x 4 years):** We request support for three postdoctoral scholars, one based at each of the three lead PIs' institutions. Each will bring complementary expertise: Postdoc 1 (ML/AI specialist) will lead model architecture development; Postdoc 2 (Bioinformatics specialist) will manage the data harmonization pipeline; Postdoc 3 (Chromatin Biologist) will lead biological interpretation and validation of model outputs. This distributed model fosters deep collaboration.\n*   **Graduate Students (2.0 FTE x 4 years):** Support for two graduate students who will be co-mentored across labs. They will focus on specific aims, such as model interpretability and application to specific disease datasets, providing an outstanding cross-disciplinary training opportunity.\n*   **Data Manager / Research Scientist (0.5 FTE x 4 years):** A part-time professional staff member is essential for managing the petabyte-scale data atlas, maintaining the public web portal, and ensuring long-term data stewardship.\n\n**B. Travel ($120,000):**\n*   **Annual In-Person Working Group Meeting ($20,000/year):** Funds to bring all PIs, trainees, and the data manager together for an intensive 3-day workshop each year. These meetings are vital for strategic planning, data integration, and fostering a cohesive team spirit.\n*   **Conference Travel ($10,000/year):** To support travel for trainees to present project findings at key international conferences (e.g., ISMB, CSHL Biology of Genomes), which is crucial for dissemination and professional development.\n\n**C. Computational Resources ($200,000):**\n*   **Cloud Computing Credits ($50,000/year):** This is a critical need. Training large-scale GNN and Transformer models on genomic data is computationally expensive and requires access to high-end GPU clusters. Cloud platforms (e.g., AWS, Google Cloud) provide the necessary scalable infrastructure that is not available through standard institutional resources.\n\n**D. Other Direct Costs ($80,000):**\n*   **Publication Costs ($10,000/year):** To cover open-access fees for an anticipated 4-6 major publications, ensuring our findings are freely accessible to all.\n*   **Workshop and Training Materials ($10,000/year):** Funds to develop materials and support logistics for our annual summer training workshop, designed to disseminate our methods to the broader scientific community.\n\n**E. Indirect Costs:** Calculated based on the federally negotiated rates for the participating institutions on the modified total direct costs."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_02",
      "original_title": "Deconstructing Allostery: Mapping the Emergent Communication Networks of Macromolecular Machines",
      "original_abstract": "Allostery, the process by which a binding event at one site of a protein or complex affects a distant functional site, is a quintessential emergent property that underpins cellular regulation. However, the pathways of allosteric communication through large, dynamic macromolecular machines like the ribosome, proteasome, or spliceosome remain poorly understood. This working group will pioneer a community-scale effort to create a unified 'Allosteric Atlas' by systematically mapping these communication networks. We will not generate new experimental data, but instead integrate and synthesize three major public data types: 1) thousands of static structures from the Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) to build structural ensembles; 2) deep mutational scanning (DMS) data to identify functionally coupled residues; and 3) molecular dynamics simulation trajectories to capture conformational dynamics. Our team, uniting structural biologists, biophysicists, computer scientists, and biochemists, will develop novel computational methods based on network theory and information theory to trace the flow of information through these complexes. The goal is to build predictive models that can identify cryptic allosteric sites, forecast the functional consequences of mutations (including those associated with disease), and guide the rational design of allosteric drugs. This project will transform our understanding of molecular regulation from a one-site-at-a-time view to a holistic, network-based perspective.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:40:05.874081",
      "proposal": {
        "title": "Deconstructing Allostery: Mapping the Emergent Communication Networks of Macromolecular Machines",
        "abstract": "Allostery, the process by which a binding event at one site of a protein or complex affects a distant functional site, is a quintessential emergent property that underpins cellular regulation. However, the pathways of allosteric communication through large, dynamic macromolecular machines like the ribosome, proteasome, or spliceosome remain poorly understood. This working group will pioneer a community-scale effort to create a unified 'Allosteric Atlas' by systematically mapping these communication networks. We will not generate new experimental data, but instead integrate and synthesize three major public data types: 1) thousands of static structures from the Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) to build structural ensembles; 2) deep mutational scanning (DMS) data to identify functionally coupled residues; and 3) molecular dynamics simulation trajectories to capture conformational dynamics. Our team, uniting structural biologists, biophysicists, computer scientists, and biochemists, will develop novel computational methods based on network theory and information theory to trace the flow of information through these complexes. The goal is to build predictive models that can identify cryptic allosteric sites, forecast the functional consequences of mutations (including those associated with disease), and guide the rational design of allosteric drugs. This project will transform our understanding of molecular regulation from a one-site-at-a-time view to a holistic, network-based perspective.",
        "background_and_significance": "Allostery is a fundamental mechanism of biological regulation, enabling proteins and their complexes to act as sophisticated information processing devices. First conceptualized through the classic Monod-Wyman-Changeux (MWC) and Koshland-Nemethy-Filmer (KNF) models, allostery was initially described as a ligand-induced switch between discrete conformational states. While these models provided a powerful foundation, they are insufficient to describe the complex, continuous, and often subtle communication that occurs within large, multi-subunit macromolecular machines. Today, allostery is increasingly understood not as a simple mechanical switch, but as a quintessential emergent property arising from the complex interplay of a protein's structure, dynamics, and sequence evolution. It is the collective behavior of hundreds or thousands of residues, connected through a dense network of interactions, that gives rise to long-range communication. Understanding this emergent phenomenon is one of the grand challenges in molecular and cellular biology.\n\nThe current state of the field is characterized by a wealth of data and a diversity of powerful, yet fragmented, approaches. Experimentally, high-resolution structural methods like X-ray crystallography and cryo-electron microscopy provide static snapshots of different functional states, offering clues about conformational changes. Techniques like hydrogen-deuterium exchange mass spectrometry (HDX-MS) and NMR spectroscopy provide insights into protein dynamics. Crucially, the advent of deep mutational scanning (DMS) has enabled the high-throughput quantification of functional consequences for thousands of mutations, revealing complex epistatic relationships between residues that hint at underlying communication pathways. Computationally, molecular dynamics (MD) simulations can model the dynamic motions of proteins at atomic resolution, while methods like normal mode analysis (NMA) can describe low-frequency collective motions relevant to allostery. Network-based models, such as Protein Structure Networks (PSNs), have successfully identified allosteric pathways by representing proteins as graphs of interacting residues, building on pioneering work by Bahar, Nussinov, and others.\n\nDespite these advances, significant gaps in our knowledge persist. The primary limitation is data fragmentation. Structural data from the PDB/EMDB, functional data from DMS databases like MaveDB, and dynamic data from MD simulations exist in separate, unlinked repositories. There is no unifying framework to integrate these disparate data types into a single, coherent model of allosteric communication. This fragmentation prevents us from seeing the full picture. A second major gap is scale. Most detailed allosteric studies have focused on smaller, single-domain proteins or dimers. The principles governing allosteric communication across the vast distances and multiple interfaces of megadalton-scale machines like the ribosome, proteasome, or spliceosome remain largely uncharted territory. Finally, current approaches are often more descriptive than predictive. While we can sometimes rationalize observed allosteric effects, we lack the ability to reliably predict, *a priori*, which residues will form a communication pathway, where cryptic allosteric sites might be located, or how a novel mutation will impact function from a distance.\n\nThis research is critically important and timely for several reasons. First, the exponential growth of public data in structural, sequencing, and functional genomics databases has created an unprecedented opportunity for a large-scale synthesis project. For the first time, sufficient data exists to attempt a systematic mapping of allosteric networks. Second, advances in computational power and machine learning provide the necessary tools to integrate these massive, heterogeneous datasets. Third, a deeper understanding of allostery has profound biomedical implications. Allosteric drugs, which target sites other than the active site, can offer greater specificity and fewer side effects, representing a new frontier in pharmacology. Furthermore, many disease-causing mutations, particularly variants of uncertain significance (VUS), likely exert their pathogenic effects by disrupting allosteric regulation rather than by directly ablating catalytic activity. By creating a predictive, network-based framework for allostery, this project will provide a foundational resource to accelerate rational drug design and improve our interpretation of the human variome.",
        "research_questions_and_hypotheses": "The overarching goal of this working group is to establish a new paradigm for understanding allostery, moving from qualitative descriptions to a quantitative, predictive, and generalizable network-based framework. By synthesizing vast public datasets, we will construct and validate an 'Allosteric Atlas' for key macromolecular machines, addressing fundamental questions about the nature of molecular communication. Our research is structured around three central questions, each with testable hypotheses.\n\n**Research Question 1: How can structurally, dynamically, and functionally diverse data types be integrated to construct a unified, multi-layered representation of allosteric networks?**\nCurrent approaches typically rely on a single data modality (e.g., structure or dynamics), providing an incomplete view. We posit that a holistic model requires data fusion.\n*   **Hypothesis 1a:** A multi-layered network model, where nodes (residues) are connected by edges weighted by a composite score derived from structural proximity (PDB/EMDB), dynamic cross-correlations (MD simulations), co-evolutionary couplings (sequence alignments), and functional epistasis (DMS data), will capture allosteric pathways with significantly higher fidelity than any single-layer network.\n*   **Prediction:** Allosteric pathways identified using our integrated model will show a statistically significant higher enrichment for known functionally critical residues, disease-associated mutations (from ClinVar), and experimentally validated allosteric sites compared to pathways derived from structure-only or dynamics-only networks.\n*   **Validation:** We will rigorously benchmark our integrative method on a 'gold standard' set of well-characterized allosteric proteins (e.g., hemoglobin, GPCRs, protein kinases). Using metrics such as the area under the receiver operating characteristic curve (AUC-ROC), we will quantify the predictive power of our multi-layered approach versus single-layer models in identifying known allosteric residues.\n\n**Research Question 2: What are the conserved architectural principles and emergent properties of allosteric communication networks across different classes of macromolecular machines?**\nWe seek to determine if there are universal 'rules' governing information flow in proteins or if network architecture is tailored to specific biological functions.\n*   **Hypothesis 2a:** Allosteric communication does not occur through random walks but is channeled along evolutionarily conserved 'information highways' composed of residues with distinct biophysical properties, such as high mechanical stress, optimal packing, and low conformational entropy.\n*   **Hypothesis 2b:** The global topology of allosteric networks will differ between classes of molecular machines. For example, processive machines like the ribosome may feature linear, directional pathways, whereas regulatory hubs like the proteasome may exhibit more distributed, scale-free network architectures to integrate multiple signals.\n*   **Prediction:** A comparative analysis of the Allosteric Atlases for the ribosome (translation), proteasome (degradation), and spliceosome (RNA processing) will reveal both conserved network motifs (e.g., critical 'hub' residues at subunit interfaces) and distinct topological signatures (e.g., average path length, clustering coefficient) that correlate with their biological roles.\n*   **Validation:** We will employ a suite of graph theory metrics to characterize network topologies. We will test the hypothesis that residues identified as high-centrality hubs in our networks are significantly enriched for post-translational modification sites, disease mutations, and sites of evolutionary conservation, providing independent lines of evidence for their functional importance.\n\n**Research Question 3: Can our integrated network model be used to make actionable predictions about novel allosteric sites and the functional consequences of mutations?**\nA truly successful model must move beyond description to prediction, generating testable hypotheses for the broader scientific community.\n*   **Hypothesis 3a:** The propagation of allosteric signals can be modeled as information flow on our weighted graph, allowing for the quantitative prediction of a perturbation's (e.g., mutation or ligand binding) effect at a distant functional site using concepts from information theory, such as mutual information.\n*   **Hypothesis 3b:** By systematically calculating the information transfer efficiency from every residue to a known active site, we can generate whole-protein 'allosteric potential' maps, which will reveal cryptic or previously unknown allosteric sites suitable for therapeutic targeting.\n*   **Prediction:** Our model will identify specific, high-potential residues in the 26S proteasome, distant from the catalytic chamber, that are predicted to allosterically modulate its proteolytic activity. Furthermore, our model will classify a significant fraction of currently annotated VUS in disease-relevant proteins as likely pathogenic or benign based on their predicted disruption of critical allosteric pathways.\n*   **Validation:** While this project will not generate new experimental data, all predictions will be made publicly available through our web portal to be tested by the community. We will perform retrospective validation by assessing our model's ability to distinguish known pathogenic from benign variants in benchmark datasets. We will also track the reclassification of VUS in public databases over time to prospectively validate our predictions.",
        "methods_and_approach": "This project will be executed by a multidisciplinary working group composed of four collaborating labs with expertise in structural biology, computational biophysics, data science/machine learning, and biochemistry. The project is organized into a four-phase workflow, with significant cross-lab collaboration and trainee involvement at each stage.\n\n**Working Group Structure and Collaboration:** The four PIs and their trainees will form a cohesive unit, meeting virtually bi-weekly and in-person at an annual workshop. A shared computational infrastructure, including a centralized database, a common GitHub organization for code development, and a project-wide Slack channel, will facilitate seamless collaboration. Trainees (postdocs and graduate students) will be co-mentored and will lead specific sub-projects, ensuring they gain cross-disciplinary skills in data synthesis, computational modeling, and team science.\n\n**Phase 1: Systematic Data Curation and Harmonization (Months 1-9)**\nThis foundational phase focuses on aggregating and standardizing the public data that fuel our models. We will develop a robust, automated pipeline for this process.\n*   **Data Sources:** We will target three major macromolecular machines: the human ribosome, the 26S proteasome, and the spliceosome. For each, we will systematically gather: 1) **Structural Data:** All available X-ray, cryo-EM, and NMR structures from the PDB and EMDB, including different conformational states, species orthologs, and ligand-bound forms. 2) **Sequence/Evolutionary Data:** We will construct deep multiple sequence alignments (MSAs) for each subunit from the UniRef100 and TrEMBL databases. 3) **Functional Data:** We will mine public repositories like MaveDB for relevant deep mutational scanning (DMS) datasets. 4) **Dynamic Data:** We will collate publicly available MD simulation trajectories from sources like the PDB, aMD-share, and literature-associated repositories.\n*   **Data Integration Platform:** A key deliverable of this phase is a unified data schema. All data will be mapped onto a common reference structure for each complex using robust structural alignment algorithms. This process will handle challenges like different numbering schemes, missing domains, and species variations, creating a consistent, analysis-ready dataset.\n\n**Phase 2: Multi-Layered Allosteric Network Construction (Months 6-18)**\nUsing the harmonized data, we will construct a multi-layered network representation for each complex.\n*   **Layer 1 (Static Structural Network):** An ensemble of networks will be built from all curated structures. Nodes are Cα atoms, and edge weights will be derived from inter-residue distances, capturing the range of observed conformations.\n*   **Layer 2 (Dynamic Correlation Network):** From MD trajectories, we will calculate the dynamic cross-correlation matrix (DCCM) between all residue pairs, with edge weights representing the degree of correlated motion.\n*   **Layer 3 (Co-evolutionary Network):** Using our MSAs, we will apply direct coupling analysis (DCA) methods (e.g., GREMLIN) to compute co-evolutionary scores, which identify residues that evolve together, often due to functional or structural constraints.\n*   **Layer 4 (Functional Epistasis Network):** Where DMS data is available, we will calculate pairwise epistatic scores between mutations to build a network based on functional coupling.\n*   **Network Integration:** We will develop a novel machine learning framework (e.g., a weighted ensemble method or a graph neural network) to integrate these four layers. The final edge weight between any two residues will represent the synthesized evidence of their connection within an allosteric communication channel. This integration method is a core innovation of our proposal.\n\n**Phase 3: Network Analysis and Pathway Identification (Months 15-30)**\nWith the integrated networks constructed, we will identify and characterize allosteric pathways.\n*   **Pathway Algorithms:** We will implement and compare multiple algorithms to trace information flow. These will range from classic graph theory approaches like Dijkstra's shortest path algorithm to more sophisticated models based on current flow in resistor networks (e.g., Resistor Network Theory) and information theory. Specifically, we will calculate the mutual information between the states of residue pairs across the structural/dynamic ensemble to quantify information transfer capacity.\n*   **Identification of Critical Nodes and Edges:** We will use a suite of network centrality metrics (e.g., betweenness centrality, closeness centrality) to identify 'hub' residues and 'bottleneck' edges that are predicted to be critical for allosteric signal transduction.\n\n**Phase 4: Model Validation, Prediction, and Dissemination (Months 24-36)**\n*   **Validation:** We will rigorously validate our framework by testing its ability to recapitulate known biological features. We will quantify the overlap between our predicted high-centrality nodes/pathways and independently determined data, including: 1) known catalytic and binding sites; 2) sites of post-translational modifications; 3) known disease mutations from ClinVar and HGMD; and 4) experimentally determined allosteric sites from the literature.\n*   **Prediction and Atlas Generation:** For our target complexes, we will generate comprehensive, predictive 'Allosteric Atlases'. These atlases will consist of the integrated networks and maps of predicted communication pathways emanating from key functional sites. We will use these to predict novel, cryptic allosteric sites and to score the potential pathogenicity of VUS.\n*   **Timeline and Milestones:**\n    *   **Year 1:** Complete data curation pipeline; develop and benchmark network integration framework on test systems. First annual workshop.\n    *   **Year 2:** Construct integrated networks for the proteasome and ribosome; implement and compare pathway prediction algorithms. Submit methods-focused manuscript.\n    *   **Year 3:** Complete analysis of all target machines; generate predictive Allosteric Atlases; launch public web portal for data dissemination. Submit application-focused manuscripts. Host final dissemination workshop.",
        "expected_outcomes_and_impact": "This project is designed to produce transformative outcomes that will significantly advance the field of molecular and cellular biology, with broad impacts on biomedical research and workforce development. Our contributions will be both conceptual and practical, providing new knowledge, new tools, and a foundational resource for the scientific community.\n\n**Intellectual Merit and Contributions to the Field:**\n1.  **A New Paradigm for Allostery:** Our primary intellectual contribution will be to shift the conceptualization of allostery from a qualitative, protein-specific phenomenon to a quantitative, network-based science. By developing a generalizable framework, we will provide the language and tools to describe allostery as an emergent property of complex systems, enabling systematic comparison and the discovery of universal principles of molecular communication.\n2.  **Methodological Innovation in Data Synthesis:** We will pioneer a novel computational pipeline for the synthesis of heterogeneous data types—structural, dynamic, evolutionary, and functional. This integrative approach addresses a major bottleneck in modern biology, where data is abundant but often siloed. The methods we develop for data harmonization and multi-layer network integration will be broadly applicable to other complex biological questions beyond allostery.\n3.  **Creation of a Foundational 'Allosteric Atlas':** The project will deliver a unique, high-value resource for the scientific community. The Allosteric Atlas for the ribosome, proteasome, and spliceosome will be the first comprehensive map of information flow in these essential molecular machines. This resource, accessible via a public web portal, will serve as a hypothesis-generation engine, enabling researchers to explore communication pathways, interpret mutational data, and design new experiments.\n\n**Broader Impacts and Applications:**\n1.  **Accelerating Therapeutic Discovery:** A key practical outcome will be the identification of novel, cryptic allosteric sites on therapeutically important targets. Allosteric drugs offer the potential for greater specificity and novel modes of action compared to traditional active-site inhibitors. Our Allosteric Atlas will provide a rational basis for targeting complexes like the proteasome (a key cancer target) or the bacterial ribosome (a target for antibiotics), opening new avenues for drug development.\n2.  **Improving Understanding of Human Disease:** Our framework will provide a powerful tool for interpreting the functional consequences of genetic variation. Many disease-causing mutations, especially the vast number of 'variants of uncertain significance' (VUS), likely exert their effects by subtly disrupting allosteric regulation. By mapping these variants onto our communication networks, we can develop a mechanistic basis for predicting their pathogenicity, aiding in genetic diagnosis and personalized medicine.\n3.  **Training the Next Generation of Data-Savvy Scientists:** This project is an ideal training vehicle. Its inherently collaborative and interdisciplinary nature will equip graduate students and postdoctoral fellows with a unique skillset at the intersection of biophysics, computer science, and biology. They will gain hands-on experience in large-scale data analysis, computational modeling, open-source software development, and 'team science'—precisely the skills needed for the future scientific workforce. We will further amplify this impact through annual workshops and by making all our training materials publicly available.\n4.  **Commitment to Open and Reproducible Science:** This project is fundamentally committed to the principles of open science. All software developed will be released under a permissive open-source license on GitHub. All curated data, network models, and analysis workflows (e.g., as Jupyter notebooks) will be made publicly available. The final Allosteric Atlas will be disseminated through a user-friendly web portal, ensuring that our results are not only published but are also accessible, explorable, and reusable by the entire community. This commitment ensures the long-term impact and sustainability of our work.\n\n**Dissemination Plan:** Our findings will be disseminated broadly through high-impact, open-access publications, presentations at major international conferences (e.g., Biophysical Society, ISMB), and the aforementioned public web portal. In the final year, we will host a community workshop to train other researchers in the use of our tools and to foster new collaborations, ensuring the methods and resources from this project are widely adopted.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis effort that is beyond the scope and resources of any single research laboratory. It requires the deep integration of expertise from four distinct scientific disciplines—structural biology, computational biophysics, computer science, and biochemistry—and a coordinated effort to curate, integrate, and analyze massive public datasets. The development of a public-facing, sustainable resource like the Allosteric Atlas also requires dedicated support that falls outside the purview of traditional research grants. The NCEMS program, with its focus on catalyzing multidisciplinary teams for data synthesis, is uniquely suited to support this ambitious project.\n\n**Budget Justification:** The total requested budget is allocated to support the personnel, computational infrastructure, and collaborative activities essential for the project's success over a three-year period.\n\n*   **Personnel (65%):** The majority of the funds will support the dedicated researchers who will execute the project. This includes four Postdoctoral Fellows, one in each of the four collaborating PIs' labs. These fellows will form the core research team, driving the development of the computational pipeline and the analysis of the target systems. We also request support for four Graduate Students, who will focus on specific sub-projects while receiving invaluable cross-disciplinary training. Partial summer salary is requested for the four PIs to provide scientific oversight, coordinate the working group, and lead training activities. Finally, we request support for a part-time Data Manager/Software Engineer to ensure robust data management practices, oversee the development of the public web portal, and manage open-source code releases.\n\n*   **Computational Resources (15%):** While we will leverage the substantial high-performance computing (HPC) resources at our respective institutions, the sheer scale of the data aggregation and network analysis necessitates dedicated cloud computing resources (e.g., Amazon Web Services or Google Cloud Platform). These funds will be used for large-scale data storage, burstable computing capacity for intensive calculations (e.g., network construction from thousands of structures), and for hosting the robust, publicly accessible web portal and database.\n\n*   **Travel (10%):** Collaboration is the cornerstone of this project. We request funds to support an annual in-person workshop for the entire working group (PIs, postdocs, and students). These multi-day meetings are indispensable for intensive brainstorming, resolving technical challenges, strategic planning, and fostering a cohesive team environment. Funds are also included for trainees and PIs to travel to one major international conference each year to present our findings, disseminate our tools, and engage with the broader scientific community.\n\n*   **Publication and Dissemination (5%):** To adhere to our open science commitment, we request funds to cover open-access publication fees for our anticipated manuscripts. A portion of this budget is also allocated for the design, development, and long-term maintenance of the Allosteric Atlas web portal.\n\n*   **Indirect Costs (F&A):** Indirect costs are calculated based on the federally negotiated rates for each participating institution.\n\n**Existing Resources:** This proposal leverages significant institutional support, including access to university-maintained HPC clusters, data storage infrastructure, and institutional software licenses. The PIs' labs provide the necessary office and lab space. The requested NCEMS funds are specifically for the personnel and collaborative resources that make this large-scale synthesis project feasible."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_03",
      "original_title": "In Silico Origins: The Emergence of Autocatalytic Metabolic Networks from Prebiotic Chemistry",
      "original_abstract": "The transition from a non-living chemical environment to the first self-sustaining, metabolizing life form is one of the greatest unsolved puzzles in science. This emergence of life was likely predicated on the formation of autocatalytic chemical networks capable of self-replication and evolution. This working group proposes a novel, purely computational approach to explore the plausible pathways for the emergence of primordial metabolism. We will synthesize and integrate disparate public data sources into a unified 'Prebiotic Chemical Universe' knowledge base. This includes: 1) comprehensive chemical reaction data from databases like KEGG and Rhea; 2) quantum chemistry calculations on the feasibility of prebiotic reactions; and 3) metabolomics data from diverse extant organisms to identify conserved core metabolic motifs. Our transdisciplinary team of systems biologists, computational chemists, evolutionary theorists, and astrobiologists will employ network expansion algorithms and principles from chemical engineering to simulate the growth of chemical networks from simple precursor molecules. We will search for the spontaneous emergence of autocatalytic cycles, identify key molecular 'scaffolds' that enable network complexity, and determine the minimal conditions required for a self-propagating metabolic system. This project will provide a principled, data-driven framework for generating testable hypotheses about the origin of life and the fundamental principles governing the emergence of biological complexity.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:41:32.186365",
      "proposal": {
        "title": "In Silico Origins: The Emergence of Autocatalytic Metabolic Networks from Prebiotic Chemistry",
        "abstract": "The transition from a non-living chemical environment to the first self-sustaining, metabolizing life form is one of the greatest unsolved puzzles in science. This emergence of life was likely predicated on the formation of autocatalytic chemical networks capable of self-replication and evolution. This working group proposes a novel, purely computational approach to explore the plausible pathways for the emergence of primordial metabolism. We will synthesize and integrate disparate public data sources into a unified 'Prebiotic Chemical Universe' knowledge base. This includes: 1) comprehensive chemical reaction data from databases like KEGG and Rhea; 2) quantum chemistry calculations on the feasibility of prebiotic reactions; and 3) metabolomics data from diverse extant organisms to identify conserved core metabolic motifs. Our transdisciplinary team of systems biologists, computational chemists, evolutionary theorists, and astrobiologists will employ network expansion algorithms and principles from chemical engineering to simulate the growth of chemical networks from simple precursor molecules. We will search for the spontaneous emergence of autocatalytic cycles, identify key molecular 'scaffolds' that enable network complexity, and determine the minimal conditions required for a self-propagating metabolic system. This project will provide a principled, data-driven framework for generating testable hypotheses about the origin of life and the fundamental principles governing the emergence of biological complexity.",
        "background_and_significance": "The origin of life, or abiogenesis, represents the conceptual boundary between geochemistry and biochemistry, marking the emergence of complex, self-sustaining systems from a simpler, non-living chemical world. Understanding this transition is a fundamental challenge in science, with profound implications for biology, chemistry, and astrobiology. A central hypothesis in this field is the 'metabolism-first' model, which posits that self-propagating networks of chemical reactions, or protometabolism, preceded the emergence of template-replicating genetic molecules like RNA. These primordial metabolic networks would have needed to exhibit autocatalysis—the ability of a network's products to catalyze its own production—to achieve the persistence, growth, and eventual evolution necessary for life. Seminal theoretical work by pioneers like Oparin, Haldane, and later Kauffman on autocatalytic sets provided the conceptual framework, suggesting that given a sufficient diversity of molecules and reactions, self-sustaining cycles could emerge spontaneously. However, these early models were largely abstract and lacked a concrete chemical basis. Experimental work, from the classic Miller-Urey experiment demonstrating abiotic synthesis of amino acids to more recent studies on non-enzymatic glycolysis and reverse Krebs cycle analogs, has shown that many of the building blocks and reactions of modern metabolism are prebiotically plausible. Despite this progress, the field faces significant hurdles that prevent a comprehensive understanding of metabolic origins. A primary limitation is the fragmented nature of our knowledge. Data on plausible prebiotic reactions are scattered across disparate fields and databases, including organic chemistry literature, geochemical models of early Earth environments, and biological databases of extant metabolic pathways. There is no unified, computationally accessible repository that integrates these diverse data sources. Consequently, current computational models of protometabolism are often based on heavily curated subsets of modern biochemistry, which introduces a strong 'retrodictive' bias and may overlook novel, non-biological pathways that were critical stepping stones. Furthermore, the feasibility of many proposed prebiotic reactions under realistic conditions—considering temperature, pressure, pH, and the catalytic effects of mineral surfaces—is often assumed rather than rigorously evaluated. This leaves a critical gap between abstract network theory and concrete chemical reality. The combinatorial explosion of possible reactions from even a simple set of precursor molecules makes an exhaustive search for emergent properties computationally intractable without a principled, data-driven approach to constrain the search space. This project is both important and timely because it directly addresses these gaps through a large-scale data synthesis approach. The recent explosion in publicly available biochemical data (KEGG, Rhea), chemical reaction databases (Reaxys), and computational chemistry tools allows us, for the first time, to construct a comprehensive 'Prebiotic Chemical Universe' knowledge base. By integrating these datasets, we can move beyond biased, retrodictive models and instead simulate the forward evolution of chemical complexity from a set of plausible initial conditions. This project's scale and inherent transdisciplinarity—requiring expertise in systems biology, quantum chemistry, evolutionary theory, and data science—make it an ideal endeavor for a community-scale working group. It is beyond the scope of any single research lab and directly aligns with the call's focus on synthesizing public data to address fundamental questions of emergence in the molecular sciences.",
        "research_questions_and_hypotheses": "The overarching goal of this working group is to develop and implement a data-driven, computational framework to simulate the emergence of complex, self-sustaining metabolic networks from a simple prebiotic chemical environment. By synthesizing disparate public datasets into a unified reaction universe, we will explore the plausible pathways from geochemistry to biochemistry. This goal is broken down into four specific, interconnected research questions (RQs) and their corresponding testable hypotheses. \n\n**RQ1: What is the structure and scope of a plausible 'Prebiotic Chemical Universe' (PCU) reaction network when integrating data from geochemistry, computational chemistry, and modern biology?**\nThis question addresses the foundational need for a comprehensive, unbiased map of possible prebiotic reactions. \n*   **Hypothesis 1 (H1):** An integrated network of plausible prebiotic reactions, constrained by thermodynamic feasibility, will be topologically distinct from modern metabolic networks, exhibiting lower average connectivity and a different modular structure, yet will contain the seeds of biological complexity. \n*   **Prediction:** We predict the PCU graph will have a power-law degree distribution but with a different exponent and clustering coefficient compared to the KEGG network. We expect to find that certain reaction classes (e.g., aldol additions, redox reactions) form highly connected cores within this network.\n\n**RQ2: Under what initial conditions (e.g., starting molecule sets, catalytic environments, energy sources) do autocatalytic cycles and self-propagating networks spontaneously emerge from the PCU?**\nThis question probes the environmental and chemical factors that trigger the transition from a static collection of chemicals to a dynamic, growing system.\n*   **Hypothesis 2 (H2):** The emergence of autocatalysis is not a generic property of complex chemical systems but depends critically on a combination of specific mineral catalysts (e.g., iron-sulfur surfaces) and a continuous influx of a limited set of high-energy precursors (e.g., HCN, formaldehyde, phosphate).\n*   **Prediction:** Our network expansion simulations will demonstrate a phase transition-like behavior. Below a certain threshold of catalytic enhancement or substrate availability, networks will be small and terminate quickly. Above this threshold, we will observe the rapid formation of large, self-propagating networks containing autocatalytic cycles. We will quantify this threshold as a function of environmental parameters.\n\n**RQ3: What are the key topological features and molecular 'scaffolds' that facilitate the transition from simple linear pathways to complex, interconnected, and autocatalytic networks?**\nThis question seeks to identify the critical components—the 'linchpins'—that enable the bootstrapping of metabolic complexity.\n*   **Hypothesis 3 (H3):** A small subset of versatile molecules (e.g., pyruvate, glyoxylate, simple sugars) act as crucial network hubs or 'scaffolds,' connecting disparate chemical pathways and enabling the closure of cycles. These molecules are not necessarily the most abundant but are the most topologically important.\n*   **Prediction:** Using network centrality measures (e.g., betweenness centrality), we will identify a small set of high-scoring molecules in our emergent networks. *In silico* 'knockout' experiments, where we remove these molecules from the initial seed set or disallow their formation, will disproportionately cripple network growth and prevent autocatalysis compared to the removal of random molecules.\n\n**RQ4: How do the structures of emergent *in silico* protometabolic networks compare to the conserved core metabolic pathways observed across all domains of life?**\nThis question addresses the long-standing debate of whether core metabolism is a 'frozen accident' of evolution or a deterministic outcome of fundamental chemical principles.\n*   **Hypothesis 4 (H4):** The core logic of central carbon metabolism, particularly pathways like the reverse Krebs cycle (rTCA) and glycolysis/gluconeogenesis, represents a robust, convergent solution for carbon fixation and biosynthesis that will emerge repeatedly in simulations under reducing prebiotic conditions.\n*   **Prediction:** Using subgraph isomorphism and network alignment algorithms, we will find that a significant fraction of our successful, autocatalytic simulations independently converge on networks containing motifs that are topologically and chemically homologous to the rTCA cycle or other ancient pathways, even when starting from diverse initial conditions.",
        "methods_and_approach": "Our research plan is structured into three synergistic phases, executed by a transdisciplinary team over 24 months. The entire project will adhere to open science principles, with all code and data developed collaboratively on a shared platform like GitHub.\n\n**Phase 1: Construction of the 'Prebiotic Chemical Universe' (PCU) Knowledge Base (Months 1-6)**\nThis foundational phase focuses on data synthesis and integration. The PCU will be structured as a graph database (using Neo4j), where nodes are chemical compounds and edges are reactions.\n*   **Data Sources:** We will integrate several distinct, publicly available data types. 1) **Biochemical Reactions:** We will extract reaction data from KEGG, MetaCyc, and Rhea databases. Using established methods for generating generalized reaction rules (e.g., based on bond changes), we will infer plausible non-enzymatic analogs. 2) **General Chemical Reactions:** We will mine the Reaxys database to include a broader scope of organic reactions not typically found in biological contexts. 3) **Experimental Prebiotic Chemistry:** We will perform systematic literature mining of journals like *Origins of Life and Evolution of Biospheres* to curate a set of experimentally verified prebiotic reactions. 4) **Thermodynamic and Kinetic Data:** Each reaction edge will be annotated with Gibbs free energy (ΔG) values. These will be sourced from databases like Equilibrator or, for novel reactions, calculated using quantum chemistry methods (DFT at the B3LYP/6-31G* level) to assess feasibility under various temperature and pH conditions. 5) **Catalysis Data:** We will incorporate information on the catalytic potential of early Earth minerals (e.g., iron sulfides, clays) from geochemical databases and the literature, encoding this as a potential reduction in the activation energy for specific reaction classes.\n\n**Phase 2: Network Expansion Simulations (Months 7-18)**\nThis phase uses the PCU to simulate the growth of chemical networks from simple beginnings.\n*   **Algorithm:** We will implement a network expansion algorithm in Python, leveraging libraries like RDKit for chemical informatics and NetworkX for graph analysis. The simulation proceeds iteratively: \n    1.  Initialize the network with a 'seed set' of simple molecules presumed abundant on the early Earth (e.g., H₂O, CO₂, CH₄, NH₃, HCN). \n    2.  At each step, query the PCU to find all reactions whose substrates are all present in the current network and satisfy a thermodynamic threshold (e.g., ΔG < 0). \n    3.  Add the products of these feasible reactions to the network, creating new nodes. \n    4.  Repeat until the network ceases to grow or reaches a predefined complexity.\n*   **Computational Experiments:** We will conduct a large ensemble of simulations to robustly test our hypotheses. We will systematically vary key parameters, including: the composition of the initial seed set (reflecting different origin scenarios like hydrothermal vents vs. atmospheric synthesis), temperature and pH (which affect ΔG), and the inclusion of specific catalytic rules that mimic mineral surfaces. This systematic exploration will allow us to map the 'parameter space' that leads to the emergence of complexity.\n\n**Phase 3: Analysis of Emergent Networks (Months 12-24)**\nIn this phase, we will analyze the structure and properties of the networks generated in Phase 2.\n*   **Detection of Autocatalysis:** We will employ algorithms to identify autocatalytic motifs, from simple single-reaction cycles to complex, collectively autocatalytic RAF (Reflexively Autocatalytic and Food-generated) sets. This analysis is key to identifying self-sustaining systems.\n*   **Topological and Chemical Analysis:** We will use a suite of graph-theoretic metrics to characterize the emergent networks, including degree distribution, clustering, and modularity. We will use centrality measures (betweenness, eigenvector) to pinpoint the 'scaffold' molecules predicted in H3. \n*   **Comparative Analysis:** To test H4, we will use network alignment and subgraph isomorphism algorithms (e.g., an adaptation of BLAST for chemical networks) to compare the topology of our emergent networks against the core metabolic maps from KEGG. This will provide a quantitative measure of the similarity between *in silico* emergent pathways and extant biology.\n\n**Timeline and Milestones:**\n*   **M6:** Public release of PCU knowledge base v1.0 and associated API.\n*   **M12:** Completion of the first major ensemble of simulations; submission of a methods paper.\n*   **M18:** Comprehensive analysis of autocatalytic networks and identification of key molecular scaffolds; presentation at a major international conference.\n*   **M24:** Completion of comparative analysis with extant metabolism; submission of primary research articles; final working group meeting and public workshop.",
        "expected_outcomes_and_impact": "This project will generate significant outcomes that advance the field of origin of life studies and have broader impacts across several scientific disciplines. Our contributions are designed to be tangible, open, and foundational for future research.\n\n**Intellectual Merit and Contributions to the Field:**\n1.  **The Prebiotic Chemical Universe (PCU) Knowledge Base:** Our primary deliverable will be the PCU, a comprehensive, computationally accessible graph database of plausible prebiotic reactions. This will be an invaluable community resource, unifying fragmented data from chemistry, biology, and geochemistry. It will enable researchers to move beyond anecdotal evidence and build quantitative, testable models of prebiotic systems.\n2.  **Data-Driven Pathways for Metabolic Emergence:** Instead of relying on speculation or biased retrodiction from modern biology, our project will generate a set of the most plausible, thermodynamically constrained pathways for the emergence of protometabolism. We will provide a ranked list of emergent autocatalytic cycles, offering a principled answer to the question, 'What did the first metabolisms look like?'\n3.  **A New Methodological Framework:** Our integrated pipeline—combining data synthesis, network expansion, and graph-theoretic analysis—will establish a powerful new methodology for studying emergence in complex chemical systems. This framework can be adapted to investigate other emergent phenomena, such as the formation of protocells or the evolution of signaling networks.\n4.  **Testable Hypotheses for Experimentalists:** By identifying key molecular scaffolds and critical environmental conditions, our *in silico* findings will generate specific, high-priority hypotheses for experimental validation. For example, we might predict that a specific dicarboxylic acid is essential for closing a protometabolic cycle in the presence of iron-sulfide catalysts, a prediction that can be directly tested in the lab.\n\n**Broader Impacts:**\n*   **Astrobiology and the Search for Life:** By defining the fundamental chemical principles and minimal conditions required for metabolic emergence, our work will directly inform astrobiology. It will help constrain the environmental conditions necessary for life to arise and refine the types of molecular biosignatures that missions like the James Webb Space Telescope should search for on exoplanets.\n*   **Synthetic Biology and Biotechnology:** Understanding the principles of self-organizing chemical networks has direct applications in synthetic biology. Our findings could inspire novel designs for artificial metabolic pathways in engineered microbes for producing biofuels or pharmaceuticals, or contribute to the long-term goal of creating a synthetic protocell from non-living components.\n*   **Training and Workforce Development:** This project is an ideal cross-disciplinary training ground. Trainees (postdocs and graduate students) will gain a unique combination of skills in data science, computational chemistry, systems biology, and evolutionary theory. Through collaborative work in a distributed team, they will be trained to become the next generation of data-savvy scientists, perfectly aligning with the research call's objectives.\n*   **Public Outreach and Dissemination:** We are committed to open science. All data (PCU), software, and results will be made publicly available via FAIR-compliant repositories (e.g., GitHub, Zenodo). We will publish our findings in high-impact, open-access journals. Furthermore, the topic of life's origins has broad public appeal. We will develop interactive web-based visualizations of our network simulations to engage students and the public, making complex scientific concepts accessible.\n\n**Long-Term Vision:** This project lays the groundwork for a comprehensive *in silico* model of abiogenesis. The working group established here will be uniquely positioned to secure future funding to extend this framework, integrating it with models of membrane encapsulation and the emergence of primitive genetic polymers, ultimately aiming for a complete, dynamic simulation of a protocell.",
        "budget_and_resources": "The proposed research requires a synergistic, multi-institutional collaboration that is beyond the capacity of a single research lab, making it an ideal project for NCEMS support. The synthesis of vast, heterogeneous datasets from chemistry, biology, and geochemistry, coupled with the need for expertise in quantum chemistry, network theory, and evolutionary biology, necessitates the formation of a dedicated working group. NCEMS resources are critical for coordinating this effort, facilitating the deep integration required for success, and providing a platform for training the next generation of interdisciplinary scientists. The requested budget is for a 24-month period.\n\n**1. Personnel ($380,000):** This constitutes the largest portion of the budget, dedicated to supporting the researchers who will execute the project.\n*   **Postdoctoral Scholars (2 FTEs):** $240,000. Two postdocs will be hired for the project duration. Postdoc 1 (based with PI 1) will specialize in network biology and algorithm development. Postdoc 2 (based with PI 2) will specialize in computational chemistry and data curation. This provides the core effort for the project.\n*   **Graduate Student Support (2 students, 50% time):** $80,000. Partial support for two graduate students who will assist with data integration, running simulations, and analysis, providing them with an exceptional training opportunity.\n*   **PI Summer Salary (3 PIs, 1 month/year):** $60,000. To allow the PIs to dedicate focused time to project management, analysis, and manuscript preparation during the summer months.\n\n**2. Travel ($40,000):** Essential for fostering collaboration and disseminating results.\n*   **Annual Working Group Meetings:** $25,000. Funds to bring the entire team (PIs and trainees) together for two in-person, multi-day workshops. These meetings are vital for intensive brainstorming, resolving technical challenges, and strategic planning.\n*   **Conference Travel:** $15,000. To support travel for trainees and PIs to present findings at major international conferences such as ISSOL, GRC on Origins of Life, or ISMB, ensuring broad dissemination of our work.\n\n**3. Computational Resources ($45,000):**\n*   **HPC Cluster Access:** $25,000. To purchase allocation on high-performance computing clusters for running thousands of network expansion simulations and the necessary DFT calculations for thermodynamic parameters.\n*   **Cloud Computing and Data Storage:** $20,000. For cloud-based services (e.g., AWS) to host the public-facing PCU graph database, manage large datasets, and support collaborative development platforms.\n\n**4. Materials and Supplies ($10,000):**\n*   Includes software licenses and subscriptions to chemical databases required for data mining.\n\n**5. Publication and Dissemination ($25,000):**\n*   **Open Access Fees:** $15,000. To cover article processing charges for publishing our results in high-impact open-access journals.\n*   **Final Workshop:** $10,000. To host a workshop at the end of the project to share our tools (PCU, software) and findings with the broader scientific community.\n\n**Total Direct Costs:** $500,000\n**Indirect Costs (F&A) at 50%:** $250,000\n**Total Requested Budget:** $750,000"
      }
    },
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_04",
      "original_title": "The Cancer Kinome's Emergent Logic: Predicting Therapeutic Resistance and Plasticity through Network Synthesis",
      "original_abstract": "The ability of cancer cells to resist targeted therapies and adapt to treatment is an emergent property of their rewired signaling networks. The protein kinome, a network of over 500 kinases, is a central hub for this rewiring, yet its complexity makes therapeutic responses difficult to predict. This working group will address this challenge by building dynamic, context-specific models of the cancer kinome. We will synthesize a massive collection of public data, including phosphoproteomics, genomics, and transcriptomics from The Cancer Genome Atlas (TCGA) and the Clinical Proteomic Tumor Analysis Consortium (CPTAC), alongside drug sensitivity data from the GDSC and CTRP databases for thousands of cancer cell lines. Our team, composed of cancer biologists, systems biologists, bioinformaticians, and clinical pharmacologists, will develop a machine learning framework to learn the rules of kinome rewiring in different cancer subtypes and in response to specific kinase inhibitors. The primary goal is to create predictive models that can identify emergent feedback loops and bypass pathways that lead to drug resistance before they arise. These models will be used to systematically predict synergistic drug combinations that can exploit network vulnerabilities and overcome the emergent robustness of the cancer kinome. This project will produce a powerful open-source platform for in silico drug screening and hypothesis generation, accelerating the development of more durable cancer therapies.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:42:59.628132",
      "proposal": {
        "title": "The Cancer Kinome's Emergent Logic: Predicting Therapeutic Resistance and Plasticity through Network Synthesis",
        "abstract": "The ability of cancer cells to resist targeted therapies and adapt to treatment is an emergent property of their rewired signaling networks. The protein kinome, a network of over 500 kinases, is a central hub for this rewiring, yet its complexity makes therapeutic responses difficult to predict. This working group will address this challenge by building dynamic, context-specific models of the cancer kinome. We will synthesize a massive collection of public data, including phosphoproteomics, genomics, and transcriptomics from The Cancer Genome Atlas (TCGA) and the Clinical Proteomic Tumor Analysis Consortium (CPTAC), alongside drug sensitivity data from the GDSC and CTRP databases for thousands of cancer cell lines. Our team, composed of cancer biologists, systems biologists, bioinformaticians, and clinical pharmacologists, will develop a machine learning framework to learn the rules of kinome rewiring in different cancer subtypes and in response to specific kinase inhibitors. The primary goal is to create predictive models that can identify emergent feedback loops and bypass pathways that lead to drug resistance before they arise. These models will be used to systematically predict synergistic drug combinations that can exploit network vulnerabilities and overcome the emergent robustness of the cancer kinome. This project will produce a powerful open-source platform for in silico drug screening and hypothesis generation, accelerating the development of more durable cancer therapies.",
        "background_and_significance": "The advent of targeted cancer therapies, particularly protein kinase inhibitors, has revolutionized oncology, offering remarkable efficacy in patient subsets with specific molecular alterations. Drugs like imatinib for CML and gefitinib for EGFR-mutant lung cancer exemplify the power of precision medicine. However, the initial success of these therapies is frequently undermined by the development of resistance, which remains a formidable clinical challenge. Resistance is not a simple, monolithic event but an emergent property of the complex, adaptive system of cellular signaling networks. At the heart of this system lies the protein kinome, comprising over 500 kinases that regulate virtually all cellular processes. In cancer, this network is extensively rewired, creating a robust and plastic system that can rapidly adapt to therapeutic insults. Understanding and predicting this adaptive rewiring is paramount to developing more durable cancer treatments.\n\nCurrent understanding of resistance is largely based on two classes of mechanisms: on-target alterations, such as secondary mutations in the drug's target kinase (e.g., the T790M 'gatekeeper' mutation in EGFR), and off-target rewiring. The latter is far more complex and involves the activation of parallel or downstream signaling pathways that bypass the inhibited node. For instance, MET amplification can confer resistance to EGFR inhibitors by activating the ERBB3-PI3K pathway independently of EGFR. Similarly, feedback loops, where the inhibition of a downstream kinase leads to the paradoxical reactivation of upstream signaling, are common. A classic example is the reactivation of the MAPK pathway through RAF dimerization following BRAF inhibitor treatment in melanoma. While these individual mechanisms are well-documented, they represent a reductionist view of a system-level problem. We lack a holistic understanding of the 'logic' of kinome rewiring—the generalizable principles and network motifs that govern how the entire system responds to perturbation.\n\nThe past decade has witnessed an explosion of publicly available, large-scale molecular and pharmacological data. Projects like The Cancer Genome Atlas (TCGA) and the Clinical Proteomic Tumor Analysis Consortium (CPTAC) have generated comprehensive genomic, transcriptomic, and proteomic maps for thousands of tumors across dozens of cancer types. Concurrently, pharmacogenomic screens such as the Genomics of Drug Sensitivity in Cancer (GDSC) and the Cancer Therapeutics Response Portal (CTRP) have profiled the sensitivity of over 1,000 cancer cell lines to hundreds of small molecules. These datasets represent an unprecedented resource, yet their full potential has not been realized. Most analyses have focused on identifying simple biomarker-drug associations (e.g., BRAF mutation predicts BRAF inhibitor sensitivity), which often fail to capture the network context that dictates the ultimate therapeutic outcome. Previous modeling efforts have either been limited to small, well-characterized pathways or have used statistical models that treat genes as independent features, ignoring the intricate network topology that defines their function. The key gap in the field is the absence of a comprehensive, data-driven framework that can synthesize these disparate data types to model the kinome as an integrated, dynamic system. Such a model is essential to move beyond predicting sensitivity to single agents and begin predicting the emergent, adaptive responses that lead to resistance. This project is timely and critical because it leverages the confluence of massive public data availability and recent advances in network-based machine learning to address this fundamental gap. By building a predictive model of kinome plasticity, we aim to systematically uncover the vulnerabilities of this adaptive system, providing a rational basis for the design of robust combination therapies.",
        "research_questions_and_hypotheses": "This working group is founded on the central premise that therapeutic resistance is an emergent property of kinome network dynamics that can be modeled and predicted through the synthesis of large-scale public data. Our research is structured around three specific, interconnected questions designed to deconstruct this complexity and translate the resulting knowledge into actionable therapeutic hypotheses.\n\n**Research Question 1: What are the conserved and cancer-type-specific patterns of kinome rewiring that mediate adaptive resistance to targeted kinase inhibitors?**\nWhile individual resistance mechanisms have been identified, a global, systematic map of the network-level adaptations is lacking. We seek to identify recurrent 'rewiring modules'—sets of kinases and downstream effectors whose activities are coordinately altered—that represent common solutions evolved by cancer cells to bypass therapeutic inhibition. \n*   **Hypothesis 1:** We hypothesize that despite the genetic heterogeneity of cancers, the functional space of kinome resistance mechanisms is constrained, leading to the recurrent activation of a finite set of rewiring modules across diverse cancer contexts. These modules will involve coordinated changes in kinase phosphorylation, protein expression, and transcriptional programs that restore critical downstream cellular functions (e.g., proliferation, survival) that were suppressed by the drug.\n*   **Testing and Validation:** We will develop a multi-view matrix factorization and clustering approach to integrate phosphoproteomic, transcriptomic, and drug sensitivity data from CPTAC and GDSC. This method will identify modules of co-regulated genes and proteins whose activation state correlates with resistance to specific classes of inhibitors. The biological significance of these predicted modules will be validated by testing for enrichment of known signaling pathways (e.g., KEGG, Reactome) and by assessing their prognostic value in independent clinical datasets from TCGA. We expect to deliver a comprehensive atlas of these resistance modules, providing a functional blueprint of kinome plasticity.\n\n**Research Question 2: Can a network-based machine learning model, which explicitly encodes the physical and functional relationships between kinases, predict cellular response to kinase inhibitors more accurately than feature-based models?**\nStandard machine learning approaches for drug sensitivity prediction often treat genomic or transcriptomic features as an unstructured 'bag of features,' ignoring the underlying network topology. We propose that a model that 'understands' the structure of the kinome will learn a more robust and interpretable representation of cellular state.\n*   **Hypothesis 2:** We hypothesize that a Graph Neural Network (GNN) model, built upon a comprehensive protein-protein interaction and kinase-substrate network, will outperform traditional models (e.g., Elastic Net, Random Forest) in predicting drug sensitivity. The GNN's architecture allows it to learn how signals propagate from a drug's target through the network, integrating information from the entire molecular context (mutations, expression levels) of a cell to predict the system's response.\n*   **Testing and Validation:** We will construct a baseline kinome graph using data from STRING, PhosphoSitePlus, and other databases. Node features will be derived from multi-omic data (mutations, CNV, gene expression) for ~1,000 cell lines. The GNN will be trained to predict IC50 values for hundreds of kinase inhibitors from GDSC. Performance will be rigorously evaluated using cross-validation and on held-out test sets of cell lines and drugs. The model's interpretability will be assessed by using graph attention mechanisms to identify the specific kinases and subnetworks most influential in predicting response to a given drug. The deliverable will be a validated, open-source predictive model of single-agent efficacy.\n\n**Research Question 3: Can in silico perturbation of our dynamic kinome model systematically identify synergistic drug combinations that preemptively block adaptive resistance pathways?**\nThe ultimate goal is to use our understanding of network rewiring to design more effective therapies. Synergistic combinations are thought to work by targeting parallel pathways or by blocking feedback mechanisms. Our model provides a platform to test this concept systematically.\n*   **Hypothesis 3:** We hypothesize that by simulating the effect of a single kinase inhibitor in our trained GNN model, we can predict the emergent resistance state of the kinome. The nodes (kinases) that become most 'activated' in this simulated resistant state are prime targets for a second, synergistic drug. We predict that drug combinations targeting the primary driver and the predicted bypass pathway will exhibit significant synergy.\n*   **Testing and Validation:** For each primary inhibitor, we will perform an in silico perturbation within the GNN. We will analyze the resulting network state to identify and rank predicted bypass kinases. This will generate a ranked list of thousands of potential synergistic drug pairs. This list of predictions will be validated against large-scale experimental combination screening data, such as the NCI-ALMANAC database, which serves as a massive, independent truth set. The expected outcome is a validated computational pipeline for prioritizing novel combination therapies for experimental testing.",
        "methods_and_approach": "Our research plan is a multi-phase, integrative strategy that progresses from data synthesis to predictive modeling and therapeutic hypothesis generation. The entire workflow is designed to be reproducible, scalable, and compliant with open science principles.\n\n**Phase 1: Comprehensive Data Acquisition, Harmonization, and Integration (Months 1-9)**\nThis foundational phase focuses on assembling a unified, analysis-ready data resource. This task is substantial and requires dedicated bioinformatic expertise, making it ideal for a collaborative working group.\n*   **Data Sources:** We will aggregate data from multiple public repositories. (1) **Molecular Profiles:** Genomics (somatic mutations, copy number variations) and transcriptomics (RNA-Seq) from The Cancer Genome Atlas (TCGA) and the Cancer Cell Line Encyclopedia (CCLE). (2) **Proteomics:** Global and phosphoproteomics data (Mass Spectrometry, RPPA) from the Clinical Proteomic Tumor Analysis Consortium (CPTAC) and CCLE. (3) **Pharmacology:** Drug sensitivity metrics (IC50, AUC) for hundreds of compounds across >1,000 cell lines from the Genomics of Drug Sensitivity in Cancer (GDSC), Cancer Therapeutics Response Portal (CTRP), and the PRISM Repurposing Screen. (4) **Network Priors:** Protein-protein interactions (PPIs) from STRING, BioGRID, and IntAct. Kinase-substrate relationships from PhosphoSitePlus, SIGNOR, and NetworKIN.\n*   **Harmonization Pipeline:** A systematic pipeline will be developed to process these heterogeneous datasets. This includes: quality control, consistent normalization methods (e.g., TPM for RNA-Seq, z-scoring for proteomics), batch effect correction where applicable, and mapping all genes, proteins, and compounds to standardized identifiers (e.g., HGNC, UniProt, PubChem). This curated, multi-modal data matrix will be a key deliverable for the community.\n\n**Phase 2: Construction of a Dynamic, Context-Specific Kinome Network Model (Months 6-18)**\nThis phase addresses our first two research questions by developing our core predictive model.\n*   **Graph Construction:** We will construct a large-scale graph representing the human kinome. Nodes will be proteins (primarily kinases), and edges will represent known interactions (PPIs, kinase-substrate). This base graph will be generic.\n*   **Context-Specific Feature Engineering:** For each of the ~1,000 cell lines, we will decorate this graph with context-specific features. Each node will be assigned a feature vector containing its basal gene expression, mutation status, and copy number state. Edge weights can be modulated by evidence of co-expression or correlated phosphorylation patterns within a specific cancer type.\n*   **Graph Neural Network (GNN) Development:** We will implement a Graph Attention Network (GAT), a state-of-the-art GNN architecture. The GAT is chosen for its ability to learn the relative importance of different neighbors in the network, making it highly interpretable. The model will take a cell line's feature-annotated kinome graph as input and will be trained to output a vector of predicted drug sensitivity values (AUCs) for a panel of ~200 kinase inhibitors. The loss function will be the mean squared error between predicted and experimental AUCs.\n*   **Training and Validation:** The model will be trained on 80% of the cell lines. Hyperparameters will be tuned using a 10% validation set. Final performance will be evaluated on the remaining 10% held-out test set of cell lines. We will also perform out-of-sample validation on drugs not seen during training to assess the model's ability to generalize. Performance will be benchmarked against standard machine learning models (e.g., Elastic Net, Random Forest) to quantify the added value of the graph-based approach.\n\n**Phase 3: In Silico Perturbation for Synergy Prediction and Mechanistic Insight (Months 18-30)**\nThis phase leverages the trained model to address our third research question.\n*   **Perturbation Simulation:** To simulate the effect of a drug, we will perform an in silico 'knockdown' on its target node(s) in the input graph. This can be done by masking the node's features or by adding a strong negative bias to its initial state. We will then perform a forward pass through the trained GNN to predict the post-perturbation network state. The difference between the basal and perturbed internal node embeddings will represent the network's adaptive response.\n*   **Identification of Resistance Pathways:** We will analyze the predicted post-perturbation state to identify kinases whose activity (as represented by their learned embeddings) is most significantly increased. These nodes represent the hubs of the emergent resistance network.\n*   **Systematic Synergy Prediction:** This process will be automated. For each of the ~200 kinase inhibitors as a 'primary' drug, we will identify the top-k predicted resistance kinases. We will then score all possible 'secondary' drugs from our panel based on whether they target one of these resistance kinases. This will generate a comprehensive, ranked matrix of predicted synergistic combinations.\n*   **Validation against Experimental Data:** Our primary validation set will be the NCI-ALMANAC database, which contains experimental synergy data for thousands of pairs. We will assess the enrichment of experimentally verified synergistic pairs in the top quantiles of our predicted synergy scores, using metrics like ROC-AUC and Precision-Recall curves.\n\n**Phase 4: Dissemination and Platform Development (Months 24-36)**\n*   **Timeline and Milestones:**\n    *   **Year 1:** Complete data harmonization pipeline (M9). First prototype of the GNN model trained and benchmarked (M12).\n    *   **Year 2:** Final validated single-drug prediction model (M18). Completion of systematic in silico perturbation screen (M24). First manuscript submitted.\n    *   **Year 3:** Validation of synergy predictions against NCI-ALMANAC (M30). Launch of public web portal with query and visualization capabilities (M33). Final project report and publications (M36).",
        "expected_outcomes_and_impact": "This project is designed to produce a transformative shift in our understanding of cancer therapy resistance, moving the field from a static, component-level view to a dynamic, system-level perspective. The expected outcomes are multifaceted, spanning fundamental scientific insights, powerful new computational tools, actionable therapeutic hypotheses, and significant contributions to training and open science, directly aligning with the core mission of the NCEMS research call.\n\n**Intellectual Merit and Contributions to the Field:**\nThe primary contribution will be a fundamentally new understanding of the emergent logic governing kinome plasticity. We will move beyond cataloging individual resistance mechanisms to defining the principles of network adaptation.\n1.  **An Atlas of Kinome Rewiring:** We will deliver the first comprehensive, data-driven map of the functional modules that cancer cells use to adapt to therapeutic pressure. This 'rewiring atlas' will serve as a foundational resource for cancer biologists, revealing conserved vulnerabilities and context-specific dependencies across dozens of cancer types.\n2.  **A Novel Predictive Framework:** Our Graph Neural Network model will represent a significant methodological advance for computational oncology. By explicitly incorporating network topology, it will provide more accurate and, crucially, more interpretable predictions than existing methods. The model's ability to identify the specific subnetworks driving a prediction will offer mechanistic insights that are absent in 'black-box' approaches.\n3.  **High-Confidence Therapeutic Hypotheses:** Unlike purely correlational studies, our in silico perturbation approach provides a mechanistic rationale for each predicted synergistic drug combination. We will produce a prioritized list of novel combinations, complete with their predicted molecular mechanism of action (e.g., blocking a specific feedback loop). This will provide a rich set of testable hypotheses for the broader cancer research community, significantly de-risking and accelerating the preclinical drug development pipeline.\n\n**Broader Impacts and Applications:**\nThe impact of this work will extend far beyond the immediate working group, providing resources and knowledge that will catalyze research across academia and industry.\n*   **Accelerating Translational Research:** The open-source platform and web portal will empower individual cancer researchers, who may lack the computational expertise to perform such large-scale analyses, to explore kinome dynamics. A biologist could, for example, query the model to predict sensitivity to a new inhibitor in their cell line of interest or to find the best combination partner for an existing drug, generating immediate, testable hypotheses for their lab.\n*   **Informing Clinical Strategy:** While our work is preclinical, it lays the groundwork for network-based patient stratification. In the long term, models like ours could be adapted to use patient tumor data to predict optimal combination therapies, contributing to the vision of personalized medicine.\n*   **Adherence to Open and Team Science:** This project is intrinsically collaborative and open. By synthesizing public data, we maximize its value and return on investment. All our methods, code, and derived data products will be made publicly available through platforms like GitHub and Zenodo. This commitment ensures our results are reproducible, transparent, and can be built upon by others, fostering a community-wide effort to solve the problem of drug resistance.\n\n**Training and Dissemination:**\nA core objective is to train the next generation of interdisciplinary scientists. Trainees (graduate students and postdocs) will be at the center of the collaboration, working across labs and disciplines. They will gain invaluable skills in large-scale data science, network biology, machine learning, and cancer pharmacology—a skill set in high demand. We will host annual project-wide workshops and hackathons to foster a collaborative environment and promote cross-pollination of ideas. Our dissemination strategy is aggressive and multi-pronged: we will publish our findings in high-impact, open-access journals, present at key international conferences (e.g., AACR, ISMB, ASCO), and, most importantly, release our user-friendly web portal as a persistent, community-facing resource. This ensures our work has a lasting and broad impact. The long-term vision is for this platform to become a living resource, continuously updated with new datasets and model improvements, serving as a central hub for network-based cancer pharmacology.",
        "budget_and_resources": "The proposed research represents a large-scale synthesis effort that is beyond the scope and resources of any single research laboratory. The project's success hinges on the integration of diverse expertise—from cancer biology and clinical pharmacology to network systems biology and machine learning—and requires significant, dedicated resources for personnel, computation, and collaboration. NCEMS support is therefore essential to assemble this multidisciplinary working group and provide the necessary infrastructure for a community-scale project of this magnitude.\n\n**Personnel (Total: $980,000 over 3 years)**\nThis is the largest and most critical component of the budget, supporting the dedicated effort of trainees and staff who will perform the research.\n*   **Postdoctoral Fellows (2):** $390,000. One fellow will specialize in bioinformatics and data harmonization, managing the complex data integration pipeline. The second will specialize in machine learning and computational modeling, leading the development and implementation of the GNN framework. (Salary: $55,000/year + 20% fringe benefits x 2 fellows x 3 years).\n*   **Data Scientist/Software Engineer (1):** $270,000. A full-time data scientist is crucial for building and maintaining the robust, user-friendly web portal and the underlying database, ensuring the project's primary deliverable is a sustainable community resource. (Salary: $75,000/year + 20% fringe x 1 FTE x 3 years).\n*   **Graduate Students (2):** $240,000. Two graduate students will be trained through this project, contributing to model development, validation, and analysis. This directly supports the call's goal of training the future data-savvy workforce. (Stipend + Tuition: $40,000/year x 2 students x 3 years).\n*   **Principal Investigator Support:** $80,000. Modest summer support for the four lead PIs to dedicate time for project management, scientific oversight, and trainee mentorship (0.5 months/year x 4 PIs).\n\n**Computational Resources (Total: $90,000)**\nTraining deep learning models like GNNs on thousands of high-dimensional samples is computationally intensive.\n*   **Cloud Computing Credits:** $75,000. Funds for AWS or Google Cloud Platform are required for access to high-memory nodes and, critically, GPU instances necessary for efficient model training ($25,000/year).\n*   **Data Storage:** $15,000. Secure and redundant storage for the harmonized multi-terabyte dataset ($5,000/year).\n\n**Travel and Collaboration (Total: $60,000)**\nFostering a cohesive and collaborative team is paramount.\n*   **Working Group Meetings:** $36,000. Funds to support twice-yearly, in-person meetings for the entire team (PIs and trainees) to facilitate deep collaboration, brainstorming, and project planning ($6,000/meeting x 2 meetings/year x 3 years).\n*   **Conference Travel:** $24,000. To support trainees and PIs in disseminating findings at major international conferences like AACR, ISMB, or Keystone Symposia ($4,000/year x 2 attendees x 3 years).\n\n**Dissemination and Publication (Total: $20,000)**\n*   **Open Access Publication Fees:** $20,000. To ensure all findings are published in high-impact, open-access journals, maximizing their reach and impact (approx. 4-5 publications).\n\n**Total Direct Costs:** $1,150,000\n**Indirect Costs (F&A) at 50%:** $575,000\n**Total Requested Budget:** $1,725,000"
      }
    },
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_05",
      "original_title": "Sociomicrobiology: Modeling the Emergence of Gut Ecosystem Function from Community-Scale Metabolic Interactions",
      "original_abstract": "The human gut microbiome is a complex ecosystem whose collective metabolic activities profoundly impact host health. These activities are not the sum of individual microbes but are emergent properties arising from a dense web of metabolic competition, cross-feeding, and collaboration. This working group aims to decipher the principles governing the emergence of community-level function in the gut microbiome. We will integrate and synthesize multi-omic data from the world's largest public microbiome repositories, including the Human Microbiome Project (HMP) and the American Gut Project. This includes metagenomic data to identify community composition, metatranscriptomic data to assess gene activity, and metabolomic data to measure metabolic outputs. Our diverse team, including microbial ecologists, systems biologists, computer scientists, and nutritionists, will go beyond simple taxonomic cataloging. We will employ novel computational pipelines to reconstruct genome-scale metabolic models for thousands of microbial species and assemble them into community-scale models. Using ecological theory and flux balance analysis, we will simulate nutrient flow, identify keystone species and metabolic handoffs, and predict how the system responds to perturbations like diet shifts or antibiotic administration. The project will deliver a dynamic, predictive model of the gut ecosystem, providing a framework for understanding dysbiosis and designing precision interventions (e.g., probiotics, prebiotics) to rationally engineer this critical emergent system.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:44:51.453379",
      "proposal": {
        "title": "Sociomicrobiology: Modeling the Emergence of Gut Ecosystem Function from Community-Scale Metabolic Interactions",
        "abstract": "The human gut microbiome is a complex ecosystem whose collective metabolic activities profoundly impact host health. These activities are not the sum of individual microbes but are emergent properties arising from a dense web of metabolic competition, cross-feeding, and collaboration. This working group aims to decipher the principles governing the emergence of community-level function in the gut microbiome. We will integrate and synthesize multi-omic data from the world's largest public microbiome repositories, including the Human Microbiome Project (HMP) and the American Gut Project. This includes metagenomic data to identify community composition, metatranscriptomic data to assess gene activity, and metabolomic data to measure metabolic outputs. Our diverse team, including microbial ecologists, systems biologists, computer scientists, and nutritionists, will go beyond simple taxonomic cataloging. We will employ novel computational pipelines to reconstruct genome-scale metabolic models for thousands of microbial species and assemble them into community-scale models. Using ecological theory and flux balance analysis, we will simulate nutrient flow, identify keystone species and metabolic handoffs, and predict how the system responds to perturbations like diet shifts or antibiotic administration. The project will deliver a dynamic, predictive model of the gut ecosystem, providing a framework for understanding dysbiosis and designing precision interventions (e.g., probiotics, prebiotics) to rationally engineer this critical emergent system.",
        "background_and_significance": "The human gut microbiome represents one of the most complex and densely populated ecosystems on Earth, comprising trillions of microorganisms that collectively encode a metabolic potential far exceeding that of their host. It is now unequivocally established that this microbial community is not a passive passenger but an active, integral component of human physiology, influencing nutrition, immune system development, and even neurological function. The collective behavior of this system is a quintessential example of emergence, where community-level functions, such as the production of neuroactive short-chain fatty acids (SCFAs) or the resistance to pathogen invasion, arise from a vast network of interactions between hundreds of microbial species. These functions cannot be predicted by studying microbes in isolation. Early microbiome research, driven by advances in DNA sequencing, focused on cataloging the taxonomic composition of the gut, successfully linking shifts in community structure (dysbiosis) to a wide range of chronic diseases, including inflammatory bowel disease (IBD), obesity, type 2 diabetes, and colorectal cancer. Seminal projects like the Human Microbiome Project (HMP) and MetaHIT provided foundational datasets that revealed the immense inter-individual variability and the existence of a 'core' functional capacity despite taxonomic heterogeneity. However, this descriptive phase of microbiome science has yielded primarily correlational insights, leaving a critical gap in our understanding of the underlying mechanisms. The central challenge has shifted from 'who is there?' to 'what are they doing, how, and why?'. To answer this, the field has embraced multi-omics, integrating metagenomics (genetic potential), metatranscriptomics (gene expression), proteomics (protein activity), and metabolomics (metabolic output). While these data types provide unprecedented depth, their integration remains a formidable challenge. Most analyses remain siloed, failing to capture the causal chain from gene to function within a complex community context. A key limitation of current approaches is the lack of a predictive, mechanistic framework. Ecological models based on Lotka-Volterra equations can describe population dynamics but often lack biochemical detail. In parallel, the field of systems biology has developed genome-scale metabolic models (GEMs), which are mathematical representations of an organism's entire metabolic network. Constraint-based modeling techniques like flux balance analysis (FBA) can use GEMs to predict metabolic fluxes and growth rates under specific environmental conditions. This approach has been successfully extended to model small microbial communities, revealing principles of metabolic cross-feeding (syntrophy) and competition. For instance, studies have shown how methanogens and sulfate-reducing bacteria depend on hydrogen produced by fermenters, a classic example of an emergent metabolic process. However, scaling these methods to the complexity of the human gut microbiome has been computationally and methodologically prohibitive. Existing community models are often limited to a few dozen well-characterized organisms, failing to capture the diversity of the system. Furthermore, they are typically static and do not adequately incorporate dynamic constraints from other omics data, such as gene expression levels from metatranscriptomics. This project is timely and crucial because we are at a technological and data-driven inflection point. The public availability of massive, multi-omic datasets from thousands of individuals provides the raw material for an unprecedented synthesis effort. Concurrently, advances in bioinformatics, including high-throughput metagenome-assembled genome (MAG) recovery and automated GEM reconstruction tools (e.g., CarveMe, gapseq), make it feasible to build models for thousands of previously uncultured organisms. By uniting a transdisciplinary team of microbial ecologists, systems biologists, and computer scientists, this working group is uniquely positioned to bridge the gap between descriptive multi-omics and predictive, mechanistic understanding. We will move beyond correlation to causation, building a dynamic model of the gut ecosystem that can predict the emergence of function from structure. This will provide a foundational framework for the rational engineering of the microbiome, transforming our approach to nutrition and medicine.",
        "research_questions_and_hypotheses": "The overarching goal of this working group is to decipher the fundamental principles by which community-scale metabolic interactions generate emergent functional properties in the human gut microbiome. We will achieve this by developing and applying a predictive, multi-omic-constrained computational framework. Our research is structured around three specific, interconnected aims, each addressing key questions and testable hypotheses.\n\n**Aim 1: Develop a scalable, integrated pipeline for constructing context-specific community metabolic models from public multi-omic data.** This aim establishes the core methodological foundation of our project.\n*   **Research Question 1.1:** Can we systematically and accurately reconstruct thousands of high-quality, species-specific genome-scale metabolic models (GEMs) from metagenomic data and integrate them into robust, sample-specific community models?\n*   **Hypothesis 1.1:** We hypothesize that a hybrid computational pipeline, which combines the speed of automated GEM reconstruction tools with a data-driven, semi-automated curation process informed by phylogenetic context and known physiology, will produce models with significantly higher predictive accuracy for microbial growth phenotypes and metabolic capabilities compared to purely automated approaches. \n*   **Validation:** Reconstructed GEMs for cultured species will be validated against experimental data from the literature (e.g., Biolog growth assays). The predictive accuracy of the community models will be benchmarked by their ability to recapitulate known community-level functions in well-defined synthetic communities.\n*   **Research Question 1.2:** How can metatranscriptomic and metabolomic data be systematically integrated to constrain community models, transforming them from static representations of genetic potential to dynamic models of in situ metabolic activity?\n*   **Hypothesis 1.2:** We hypothesize that integrating metatranscriptomic data to define reaction flux bounds and metabolomic data to set environmental nutrient conditions will dramatically improve the model's fidelity in predicting community-level metabolic outputs (e.g., fecal short-chain fatty acid profiles) compared to models constrained by metagenomic abundance alone.\n*   **Validation:** Using datasets with matched multi-omics (e.g., HMP2), we will perform k-fold cross-validation. We will train the model constraints on a subset of samples and test its ability to predict the metabolomic profiles of held-out samples, measuring the correlation between predicted and observed metabolite concentrations.\n\n**Aim 2: Identify the organizing principles and keystone components of the gut metabolic network.** This aim uses our validated models to uncover fundamental ecological and metabolic rules.\n*   **Research Question 2.1:** What are the dominant patterns of metabolic interaction (e.g., competition, syntrophy, parasitism) that structure the gut community, and how do these patterns differ across host health states (e.g., healthy vs. IBD)?\n*   **Hypothesis 2.1:** We hypothesize that the gut metabolic network is not random but is organized into functional guilds—groups of taxonomically diverse species that perform similar metabolic roles (e.g., primary fiber degraders, lactate consumers, butyrate producers). We predict that ecosystem stability is conferred by high functional redundancy within these guilds and that dysbiosis is characterized by a loss of specific guilds or a breakdown in key metabolic handoffs between them.\n*   **Validation:** We will apply network analysis algorithms to the simulated metabolic exchange fluxes from hundreds of community models. We will identify modules in this network that correspond to our hypothesized guilds and show that guild structure is more conserved across healthy individuals than taxonomic structure.\n*   **Research Question 2.2:** Can we identify 'keystone species' or, more importantly, 'keystone functions' that have a disproportionate impact on the stability and emergent functions of the community?\n*   **Hypothesis 2.2:** We hypothesize that metabolic keystones are organisms or enzymatic functions that uniquely provide or consume critical intermediate metabolites (e.g., H2, formate, essential amino acids) that link major metabolic pathways. Their in-silico removal will cause a significant, non-linear decrease in critical community functions, such as total biomass production or SCFA synthesis, that is far greater than predicted by their abundance alone.\n*   **Validation:** We will perform systematic in-silico knockout simulations, removing one species at a time from the community models and quantifying the impact on key functional outputs. The results will allow us to rank species by their 'keystone index'.\n\n**Aim 3: Build and validate a predictive model of microbiome response to defined perturbations.** This aim leverages our framework to make forward predictions, moving from explanation to prediction.\n*   **Research Question 3.1:** How do dietary shifts (e.g., changes in fiber, fat, or protein content) and antibiotic administration alter the metabolic interaction network and emergent functions of the gut microbiome?\n*   **Hypothesis 3.1:** We hypothesize that our constrained community models can accurately predict the direction and magnitude of change in key metabolic outputs (e.g., butyrate/propionate ratio) in response to a simulated dietary intervention. For example, a simulated increase in dietary fiber will increase flux through fermentation pathways and select for butyrate-producing guilds, a prediction we can test against real-world data.\n*   **Validation:** We will use our models to predict the outcomes of published dietary intervention studies where longitudinal multi-omic data is available. We will initialize our models with baseline data, simulate the dietary change by altering the nutrient input conditions, and compare the model's predicted endpoint metabolite and taxonomic profiles with the experimentally observed data.",
        "methods_and_approach": "This project will synthesize vast, publicly available multi-omic datasets using a novel, rigorous, and reproducible computational workflow. Our approach is designed to be modular, scalable, and transparent, adhering to the highest standards of open science. The transdisciplinary nature of our team is essential for the success of this multi-faceted methodology.\n\n**Data Sources and Harmonization:**\nWe will leverage several of the world's largest and most comprehensive public microbiome datasets. Our primary sources include:\n1.  **The Human Microbiome Project (HMP1 & HMP2/iHMP):** Provides metagenomic, metatranscriptomic, and metabolomic data from hundreds of healthy individuals and those with IBD, including valuable longitudinal data for tracking dynamic changes.\n2.  **The American Gut Project (AGP):** Offers metagenomic data from over 10,000 individuals, providing immense statistical power for identifying generalizable patterns across a diverse population.\n3.  **Other Cohorts:** We will incorporate data from other well-phenotyped cohorts with publicly available multi-omics, such as LifeLines-DEEP and TwinsUK, to enhance the robustness and generalizability of our findings.\nA significant initial effort (Year 1, Q1-Q2) will be dedicated to data harmonization. We will develop a unified metadata schema to standardize variables such as host diet, disease status, age, sex, and medication use across all projects. A dedicated workflow, built using Snakemake, will automate the downloading, quality control (FastQC, Trimmomatic), and processing of all raw sequencing data from repositories like NCBI SRA.\n\n**Computational Pipeline:**\nOur core analytical pipeline consists of five interconnected modules:\n*   **Module 1: Genome Recovery from Metagenomes:** For each metagenomic sample, we will perform de novo assembly using MEGAHIT. We will then use a suite of binning tools (MetaBAT2, MaxBin2, CONCOCT) to reconstruct Metagenome-Assembled Genomes (MAGs). The resulting bins will be consolidated and refined using DAS Tool and evaluated for quality with CheckM. Only high-quality MAGs (Completeness > 90%, Contamination < 5%) will be retained, forming our comprehensive gut microbial genome catalog.\n*   **Module 2: Genome-Scale Metabolic Model (GEM) Reconstruction and Curation:** This is a cornerstone of our project. For each MAG and publicly available reference genome, we will reconstruct a GEM. We will employ a hybrid strategy: initial draft models will be generated using automated tools like CarveMe and gapseq. These drafts will then undergo a rigorous, semi-automated curation process. This involves using algorithms to identify and fill metabolic gaps, ensuring the model can produce biomass, and comparing model predictions to known metabolic capabilities. Our team's microbial physiologists and systems biologists will perform manual curation on key pathways (e.g., fermentation, vitamin biosynthesis) to ensure biochemical accuracy. All models will be standardized in SBML format, version-controlled, and housed in a public repository.\n*   **Module 3: Community Model Assembly and Simulation:** For each host sample, we will construct a personalized community metabolic model. First, we will determine the relative abundance of each microbe in our catalog by mapping metagenomic reads from the sample back to the genomes. These abundances will define the composition of the community model. We will use the `micom` Python framework, which is specifically designed for efficient FBA of large microbial communities. The simulation environment (the 'diet') will be defined using a standard in-silico representation of a Western diet (e.g., from the Virtual Metabolic Human database), which specifies the nutrient influx into the system.\n*   **Module 4: Multi-omic Constraint Integration:** To capture the in situ activity of the microbiome, we will integrate other omics data. Metatranscriptomic reads from a given sample will be mapped to the genes in our community's genomes. The resulting expression levels will be used to constrain the maximum allowable flux through the corresponding reactions in the model, using established algorithms like GIMME. This ensures that highly expressed pathways are more active in the simulation. Where available, fecal metabolomic data will be used to set boundary conditions for nutrient uptake and metabolite secretion, further grounding the model in experimental reality.\n*   **Module 5: Network Analysis and Perturbation Simulation:** With the constrained community models, we will simulate metabolic activity to address our scientific questions. We will calculate pairwise metabolic exchange fluxes to construct a 'sociometabolic' network graph for each community. Graph theory methods will be used to identify keystone species, metabolic guilds (modules), and critical metabolic handoffs. To test hypotheses about system response, we will perform in-silico perturbations. Dietary shifts will be simulated by altering the nutrient composition of the input media. Antibiotic effects will be modeled by removing susceptible species from the community. The resulting changes in predicted community growth, stability, and metabolic outputs (e.g., SCFA production) will form the basis of our predictions.\n\n**Timeline and Milestones:**\n*   **Year 1:** Establish data acquisition and MAG recovery pipeline. Reconstruct and curate the first 500 high-quality GEMs. Develop and benchmark the community model assembly workflow. Hold the first annual in-person working group meeting.\n*   **Year 2:** Scale GEM reconstruction to over 1,500 species. Fully implement and validate the metatranscriptomic data integration module. Perform initial network analysis across the HMP cohort to identify core metabolic guilds. Submit the first manuscript on the methodology and model repository.\n*   **Year 3:** Conduct comprehensive perturbation simulations (diet and antibiotics). Validate model predictions against longitudinal data from the HMP2 IBD cohort. Finalize and release the open-source software package. Disseminate findings through high-impact publications and presentations at international conferences. Host a training workshop for the broader community.\n\n**Reproducibility and Training:**\nAll analysis workflows will be encoded in Snakemake or Nextflow, and all software will be containerized using Docker/Singularity to ensure full reproducibility. All code, models, and derived data will be shared via GitHub and public data archives (e.g., Zenodo). Trainees (graduate students and postdocs) will be central to this process, receiving hands-on training in data science, systems biology, and collaborative research through bi-weekly project meetings and intensive annual hackathons.",
        "expected_outcomes_and_impact": "This working group will generate significant and lasting contributions that advance the molecular and cellular sciences by providing a novel, mechanistic framework for understanding emergence in complex biological systems. The impact will span conceptual, methodological, and translational domains, directly addressing the core goals of the NCEMS research call.\n\n**Intended Contributions to the Field:**\n1.  **A Conceptual Shift from Correlation to Causation:** Our primary contribution will be to move the field of microbiome research beyond descriptive, correlational studies. By creating a predictive, mechanism-based model of the gut ecosystem, we will provide a powerful tool for testing causal hypotheses about how microbial community structure leads to emergent function. This will establish a new paradigm for studying host-microbe systems, grounded in the principles of systems biology and ecological theory.\n2.  **A Novel, Open-Source Analytical Platform:** We will deliver a fully documented, open-source computational pipeline for building multi-omic-constrained community metabolic models. This platform will be a valuable resource for the entire research community, enabling other scientists to apply our methods to their own data and questions, thus democratizing this powerful analytical approach.\n3.  **The Most Comprehensive Gut Microbial Metabolic Resource:** We will produce and publicly release the largest, most highly curated database of genome-scale metabolic models for human gut microbes, many derived from previously uncultured MAGs. This 'Sociomicrobiology Model Kit' will be an invaluable standalone resource, accelerating research in microbial physiology, synthetic biology, and drug discovery.\n4.  **Discovery of Fundamental Organizing Principles:** Our analysis will uncover the fundamental 'rules' of metabolic organization in the gut. We will identify conserved metabolic guilds, quantify the importance of functional redundancy for ecosystem stability, and pinpoint keystone species and metabolites that are critical control points in the system. These discoveries will provide a new, function-centric roadmap of the gut ecosystem.\n\n**Broader Impacts and Applications:**\n*   **Translational Medicine and Personalized Nutrition:** The predictive power of our models has direct translational potential. They can serve as the foundation for developing personalized interventions. For example, a clinician could use a patient's microbiome data to run in-silico simulations to predict which specific prebiotic fiber would most effectively increase their butyrate production, or to design a probiotic consortium to restore a missing metabolic function. This represents a critical step towards the rational engineering of the microbiome for therapeutic purposes.\n*   **Drug Development and Toxicology:** Our models can be used as an in-silico platform to screen for off-target effects of new drug candidates on the gut microbiome. By simulating the inhibition of microbial enzymes targeted by a drug, we can predict potential disruptions to the gut ecosystem, potentially reducing adverse effects and improving drug safety profiles.\n*   **Training the Next Generation of Scientists:** This project is intrinsically designed to train a new generation of data-savvy biologists. Trainees will gain invaluable cross-disciplinary skills at the intersection of microbiology, computational biology, and data science. The collaborative, team-science environment fostered by the working group will prepare them for the future of large-scale, integrative biological research.\n\n**Dissemination and Sustainability:**\nOur dissemination strategy is multi-pronged. We will publish our findings in high-impact, open-access journals (e.g., *Nature Biotechnology*, *Cell Host & Microbe*, *PLOS Computational Biology*). We will actively present our work at major international conferences (e.g., ISMB, ISME, Keystone Symposia). Crucially, all code, models, and workflows will be made publicly available via GitHub and Zenodo, adhering to FAIR data principles. To ensure long-term impact, we will host a training workshop in the final year to disseminate our tools and techniques to the broader community. The collaborative network established by this working group will be self-sustaining, serving as a nucleus for future collaborative grant proposals (e.g., NIH U01s, NSF Biology Integration Institutes) to experimentally validate model predictions and expand the model to include host-microbe metabolic interactions. Our long-term vision is to create a 'digital twin' of the gut microbiome, a tool that will transform basic research and clinical practice. This project lays the essential foundation for that vision.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis effort that is impossible for a single laboratory or a small collaboration to undertake. The project's success hinges on the integration of diverse expertise, the management of massive datasets, and the need for significant computational power, making it an ideal fit for the NCEMS Working Group program. The requested budget is designed to support the personnel, collaborative activities, and computational infrastructure essential for achieving our ambitious goals.\n\n**Justification for NCEMS Support:**\nThis project's scope is defined by its scale and complexity. We will be synthesizing petabytes of public data, reconstructing and curating thousands of genome-scale models, and running millions of CPU-hours of complex simulations. This requires a coordinated, multidisciplinary team composed of microbial ecologists, systems biologists, bioinformaticians, and computer scientists from multiple institutions. NCEMS support is critical for (1) **Personnel:** Funding dedicated postdoctoral fellows and graduate students who will perform the day-to-day research and are the focus of our training mission; (2) **Collaboration and Coordination:** Supporting the essential in-person meetings and a part-time project manager to ensure this geographically distributed team works as a cohesive and efficient unit; and (3) **Computational Resources:** Providing dedicated funds for the high-performance computing (HPC) and cloud resources necessary for large-scale data analysis and simulation.\n\n**Budget Breakdown (3-Year Total: $749,000):**\n\n*   **A. Personnel ($456,000):**\n    *   **Postdoctoral Scholars (2.0 FTE):** $306,000. Two postdocs for three years, one specializing in metagenomic bioinformatics and the other in metabolic modeling. They will lead the development of the core computational pipeline. (Based on an average salary of $55,000/year + 40% fringe benefits).\n    *   **Graduate Students (2.0 FTE):** $90,000. Support for two graduate students for three years, who will focus on data curation, model validation, and network analysis, representing a key training component. (Based on a stipend of $30,000/year + $15,000/year tuition).\n    *   **Project Manager (0.2 FTE):** $60,000. A part-time manager to coordinate meetings, track milestones, manage reporting, and facilitate communication across the working group. (Based on a salary of $80,000/year + 40% fringe).\n\n*   **B. Travel ($60,000):**\n    *   **Annual Working Group Meetings:** $20,000 per year. To bring the entire team (~10 PIs and trainees) together for an intensive 3-day in-person workshop and hackathon. This is vital for fostering deep collaboration, resolving complex technical challenges, and cross-training.\n\n*   **C. Computational Resources & Data ($90,000):**\n    *   **HPC/Cloud Computing:** $75,000 ($25,000 per year). For purchasing compute cycles on a national HPC resource (e.g., XSEDE) or a commercial cloud provider (e.g., AWS) for metagenomic assemblies and large-scale FBA simulations.\n    *   **Data Storage and Archiving:** $15,000 ($5,000 per year). For robust, long-term storage of processed data and public archiving of final data products in repositories like Zenodo, ensuring compliance with FAIR principles.\n\n*   **D. Materials & Supplies ($15,000):**\n    *   **Publication Costs:** $15,000. To cover open-access publication fees for an anticipated 5-6 peer-reviewed articles in high-impact journals.\n\n*   **E. Total Direct Costs: $621,000**\n\n*   **F. Indirect Costs (F&A) ($128,000):**\n    *   Calculated based on a blended rate across participating institutions on a modified total direct cost base, as per NCEMS guidelines.\n\n**Institutional Commitment:**\nThe participating investigators' institutions are committed to the success of this project and will provide faculty salaries, benefits, office and laboratory space, and access to local computing infrastructure, which will supplement the resources requested in this proposal. This cost-sharing demonstrates a strong institutional investment in our collaborative research program."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_06",
      "original_title": "Cracking the Splicing Code: A Deep Learning Approach to the Emergence of Proteomic Diversity",
      "original_abstract": "Alternative splicing of pre-mRNA allows a limited number of genes to produce a vast repertoire of protein isoforms, a key source of biological complexity. The 'splicing code' that governs this process—the set of rules by which cis-regulatory elements and trans-acting RNA-binding proteins (RBPs) guide the spliceosome to generate tissue-specific isoform patterns—remains elusive. This working group will tackle this grand challenge by treating the splicing code as a language to be learned from data. We will synthesize petabytes of public data, including: 1) RNA-seq data from the Genotype-Tissue Expression (GTEx) project and single-cell atlases to quantify isoform expression across thousands of samples; 2) eCLIP-seq data from ENCODE to map RBP binding sites; and 3) RNA structure probing data to understand the role of local RNA conformation. Our team, bringing together RNA biologists, computational linguists, and machine learning experts, will develop and train a novel deep learning architecture, inspired by large language models like transformers. This model will learn the complex, combinatorial grammar of splicing directly from DNA/RNA sequence and the cellular context of RBP expression. The ultimate goal is a predictive tool that, given a gene sequence and a cell type, can accurately predict the resulting mRNA isoforms. This would revolutionize our ability to interpret genetic variants affecting splicing in disease and understand the emergence of proteomic diversity.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:46:35.353553",
      "proposal": {
        "title": "Cracking the Splicing Code: A Deep Learning Approach to the Emergence of Proteomic Diversity",
        "abstract": "Alternative splicing of pre-mRNA allows a limited number of genes to produce a vast repertoire of protein isoforms, a key source of biological complexity. The 'splicing code' that governs this process—the set of rules by which cis-regulatory elements and trans-acting RNA-binding proteins (RBPs) guide the spliceosome to generate tissue-specific isoform patterns—remains elusive. This working group will tackle this grand challenge by treating the splicing code as a language to be learned from data. We will synthesize petabytes of public data, including: 1) RNA-seq data from the Genotype-Tissue Expression (GTEx) project and single-cell atlases to quantify isoform expression across thousands of samples; 2) eCLIP-seq data from ENCODE to map RBP binding sites; and 3) RNA structure probing data to understand the role of local RNA conformation. Our team, bringing together RNA biologists, computational linguists, and machine learning experts, will develop and train a novel deep learning architecture, inspired by large language models like transformers. This model will learn the complex, combinatorial grammar of splicing directly from DNA/RNA sequence and the cellular context of RBP expression. The ultimate goal is a predictive tool that, given a gene sequence and a cell type, can accurately predict the resulting mRNA isoforms. This would revolutionize our ability to interpret genetic variants affecting splicing in disease and understand the emergence of proteomic diversity.",
        "background_and_significance": "The central dogma of molecular biology, while foundational, belies the immense complexity that emerges from a finite set of genes. The discovery that the human genome contains only ~20,000 protein-coding genes, a number comparable to that of the nematode C. elegans, presented a profound puzzle: how does this limited genetic toolkit generate the vast phenotypic complexity of human biology? The answer, in large part, lies in alternative splicing (AS), a post-transcriptional regulatory mechanism by which exons from a single pre-mRNA transcript are differentially joined. This process, occurring in over 95% of human multi-exon genes, generates a multiplicity of mRNA isoforms from a single gene, which are then translated into distinct protein variants, dramatically expanding the proteomic landscape. This proteomic diversity is fundamental to cellular differentiation, tissue identity, and developmental programs. The rules governing this process, collectively termed the 'splicing code,' remain one of the great unsolved problems in molecular biology. Unlike the triplet-based genetic code, the splicing code is a complex, combinatorial, and context-dependent language. It is written in the sequence of the pre-mRNA itself, in the form of cis-regulatory elements such as exonic and intronic splicing enhancers and silencers (ESEs, ESSs, ISEs, ISSs). This code is read and interpreted by a dynamic cohort of trans-acting factors, primarily RNA-binding proteins (RBPs), which bind to these cis-elements and guide the core spliceosome machinery to select specific splice sites. The combinatorial binding of dozens to hundreds of RBPs creates a regulatory logic of staggering complexity, enabling the precise tuning of isoform ratios in a cell-type-specific and condition-dependent manner. Early efforts to decipher this code focused on identifying short, consensus sequence motifs for splice sites and regulatory elements. While informative, these approaches failed to capture the combinatorial nature and long-range interactions inherent to splicing regulation. The advent of high-throughput sequencing has provided an unprecedented wealth of data. Large-scale projects like the Encyclopedia of DNA Elements (ENCODE) and the Genotype-Tissue Expression (GTEx) project have generated massive public datasets, including transcriptomes from thousands of human samples (RNA-seq), and genome-wide binding maps for hundreds of RBPs (eCLIP-seq). These resources have fueled the development of computational models to predict splicing outcomes. Initial machine learning models, such as those based on support vector machines or random forests, demonstrated some success but were limited by their inability to learn complex sequence features automatically. More recently, deep learning, particularly convolutional neural networks (CNNs), has shown significant promise. Models like SpliceAI can accurately predict if a genetic variant will disrupt a splice site, a major step forward for clinical genetics. However, significant gaps remain. Current models struggle to predict quantitative isoform ratios (i.e., the Percent Spliced In, or PSI value) across diverse cellular contexts. They often fail to integrate the crucial context of trans-acting RBP expression levels and do not adequately model the long-range dependencies between distal regulatory elements, which can be kilobases apart. Furthermore, the role of local RNA secondary structure, known to influence RBP binding and splice site accessibility, is frequently ignored. We are at a critical juncture where the confluence of massive, multimodal public datasets and revolutionary advances in artificial intelligence, specifically the development of transformer-based large language models (LLMs), makes it possible to address these limitations. Transformers, with their self-attention mechanism, are uniquely suited to learning the long-range, contextual 'grammar' of complex sequential data. This project is therefore timely and important, proposing to synthesize these disparate data modalities within a novel LLM-inspired framework to finally crack the splicing code. Success will not only represent a fundamental breakthrough in our understanding of gene regulation but will also provide a powerful tool to interpret genetic disease and engineer RNA-based therapeutics.",
        "research_questions_and_hypotheses": "The overarching goal of this working group is to develop and validate a comprehensive, predictive model of alternative splicing, named 'SpliceFormer,' that can accurately predict tissue-specific isoform ratios from genomic sequence and cellular context. By treating the splicing code as a formal language, we aim to decipher its grammar and uncover the emergent principles that govern proteomic diversity. To achieve this, we will address three central research questions, each associated with specific, testable hypotheses.\n\n**Research Question 1: Can a transformer-based deep learning architecture effectively learn the complex, combinatorial, and long-range grammar of the splicing code from integrated multi-modal public data?**\nThis question addresses the core technical challenge of building a model capable of understanding the intricate rules of splicing. We hypothesize that the limitations of previous models stem from their inability to capture the full complexity of the input data.\n*   **Hypothesis 1a:** A deep learning model built upon a transformer architecture will significantly outperform current state-of-the-art models (e.g., CNN-based architectures like SpliceAI) in the quantitative prediction of isoform ratios (PSI values) across diverse human tissues. We predict this is because the self-attention mechanism inherent to transformers can model dependencies between regulatory elements and splice sites separated by thousands of nucleotides, a known feature of splicing regulation that is poorly captured by local-feature-focused CNNs.\n*   **Hypothesis 1b:** Explicitly conditioning the model on the cellular context, defined by the expression profile of key RBPs and splicing factors, will be essential for achieving tissue-specific predictive accuracy. We hypothesize that a model receiving only DNA/RNA sequence as input will predict a generic splicing outcome, while a model provided with the trans-acting factor milieu will correctly predict tissue-specific isoform switches.\n*   **Hypothesis 1c:** The integration of RNA secondary structure information as an additional input modality will further refine model predictions. We hypothesize that for a specific subset of splicing events known to be regulated by RNA structure, incorporating this information will correct prediction errors made by a structure-agnostic model.\n*   **Validation:** These hypotheses will be tested by systematically training and evaluating model variants. We will compare the performance (e.g., Pearson correlation between predicted and observed PSI) of our full SpliceFormer model against baseline CNNs and ablated versions of our own model (e.g., without the transformer layers, without RBP context, without structure data) on a held-out test set of genes and tissues from the GTEx project.\n\n**Research Question 2: What are the key cis-regulatory elements and trans-acting factor combinations that define tissue-specific splicing programs?**\nBeyond prediction, our goal is to extract biological knowledge from the trained model. We aim to use the model as an in silico laboratory to probe the mechanisms of splicing regulation.\n*   **Hypothesis 2a:** Model interpretation techniques, specifically in silico saturation mutagenesis, will identify the precise nucleotide-level functional impact of cis-regulatory sequences, allowing for the de novo discovery of novel splicing enhancers and silencers with higher accuracy than motif-based searches.\n*   **Hypothesis 2b:** Analysis of the model's internal mechanisms, such as its attention maps and feature importance scores (e.g., SHAP values), will reveal the 'grammar' of splicing regulation, including synergistic and antagonistic interactions between RBPs that govern specific splicing decisions. For example, we hypothesize we can identify specific RBP combinations that define brain-specific versus muscle-specific exon inclusion patterns.\n*   **Validation:** Cis-element predictions will be validated by comparing our functional scores with large-scale experimental data from massively parallel reporter assays (MPRAs) and known pathogenic splicing mutations from databases like ClinVar. Predicted RBP interactions will be cross-referenced with protein-protein interaction databases and co-localization patterns from ENCODE eCLIP-seq data.\n\n**Research Question 3: How does the splicing code drive the emergence of cellular identity and how is it perturbed in human disease?**\nThis question seeks to apply our validated model to address fundamental questions in cell biology and translational medicine.\n*   **Hypothesis 3a:** The SpliceFormer model can accurately predict the functional consequences of non-coding genetic variants on splicing, enabling the systematic prioritization of disease-causing variants from genome-wide association studies (GWAS).\n*   **Hypothesis 3b:** By applying the model to single-cell RNA-seq data, we can map the dynamic landscape of splicing regulation during cellular differentiation and identify critical splicing 'switches' that are necessary for cell fate decisions.\n*   **Validation:** We will use SpliceFormer to score all common variants in the human genome for their predicted effect on splicing. These scores will be tested for enrichment in GWAS loci for various diseases. For single-cell applications, we will apply the model to public datasets of hematopoiesis or neurogenesis and validate predicted splicing switches against known lineage-defining isoform changes in the literature.\n\n**Deliverables:** The expected outcomes include: (1) The open-source SpliceFormer software package and pre-trained models; (2) A public web portal for predicting splicing outcomes; (3) A comprehensive atlas of predicted functional cis-elements and RBP regulatory networks across human tissues; and (4) High-impact publications detailing the model and its biological insights.",
        "methods_and_approach": "This project will be executed by a multidisciplinary working group comprising experts in RNA biology, machine learning, computational linguistics, and bioinformatics. The collaborative structure is essential for integrating the diverse datasets and developing a conceptually novel modeling approach. The research plan is organized into three synergistic aims, with a detailed timeline and milestones.\n\n**Aim 1: Curation and Synthesis of a Multi-modal Splicing Data Compendium.**\nThe foundation of our project is the large-scale integration of publicly available data. This task requires a robust, reproducible data processing pipeline managed by a dedicated data scientist within the team.\n*   **Transcriptomic Data:** We will utilize the Genotype-Tissue Expression (GTEx) project (v8), which contains over 17,000 RNA-seq datasets from 54 human tissues. Raw data will be re-processed through a uniform pipeline using STAR for alignment and rMATS for the robust identification and quantification of alternative splicing events (e.g., skipped exons, mutually exclusive exons), yielding Percent Spliced In (PSI) values. This will form our primary training and testing dataset. Additionally, we will integrate data from single-cell atlases (e.g., Human Cell Atlas) to derive cell-type-specific RBP expression profiles and validate model predictions at higher resolution.\n*   **RBP Binding Data:** We will leverage the ENCODE project's comprehensive collection of enhanced CLIP-seq (eCLIP-seq) datasets, covering over 150 RBPs. Uniformly processed peak data will be mapped to pre-mRNA coordinates to create a high-resolution map of RBP-RNA interactions. For tissues lacking direct eCLIP data, we will develop an imputation model that predicts RBP binding probability based on local sequence motifs and the RBP's expression level in that tissue.\n*   **RNA Structure Data:** We will incorporate data from in vivo structure probing experiments (e.g., SHAPE-MaP, DMS-seq). These datasets provide nucleotide-resolution information on RNA secondary structure. We will process these data to generate a probability track for each nucleotide being in a paired or unpaired state, which will serve as an input channel to our model.\n*   **Genomic and Annotation Data:** The human reference genome (GRCh38) and GENCODE gene annotations will provide the scaffold for all data integration.\n\n**Aim 2: Development and Training of the SpliceFormer Model.**\nThis aim constitutes the core technical innovation of the proposal, led by our machine learning experts with guidance from computational linguists on architectural design.\n*   **Input Representation:** For each splicing event, the model will receive a multi-channel input tensor. This includes: (1) The one-hot encoded DNA sequence of the target exon and ~1kb of flanking intronic sequence on each side; (2) A parallel vector representing RNA secondary structure probabilities; and (3) A concatenated 'context vector' containing the normalized expression values (TPM) of ~200 key RBPs and splicing factors for the specific tissue sample.\n*   **Model Architecture:** The SpliceFormer architecture will consist of three main components. First, a set of convolutional layers will scan the sequence and structure inputs to learn low-level features like splice sites and RBP binding motifs. Second, the output of the convolutional layers will be fed into a multi-head self-attention Transformer encoder. This core component will learn the long-range dependencies and combinatorial relationships between all features in the sequence. Third, the tissue-specific RBP context vector will be integrated using a feature-wise linear modulation (FiLM) layer, which allows the trans-acting factor profile to dynamically gate the flow of information through the network. The final layers will be a multi-layer perceptron that outputs a single value, the predicted PSI.\n*   **Training, Validation, and Benchmarking:** The curated GTEx dataset will be split into training (80%), validation (10%), and testing (10%) sets, ensuring no gene overlap. The model will be trained on a distributed GPU cluster using the Adam optimizer and a mean squared error loss function. We will perform extensive hyperparameter tuning. The final model's performance will be rigorously benchmarked against existing methods, including SpliceAI and other CNN-based models, on the held-out test set.\n\n**Aim 3: Model Interpretation, Biological Discovery, and Dissemination.**\nOnce trained, the model will be used as a tool for biological discovery, led by the RNA biologists in the team.\n*   **Interpretation Techniques:** We will employ a suite of interpretation methods. In silico saturation mutagenesis will be performed by systematically mutating every position in a sequence and recording the predicted change in PSI, generating high-resolution functional maps. We will visualize the transformer's attention maps to identify which distal elements the model uses to make predictions for a given splice site. Feature attribution methods like SHAP will be used to determine the relative importance of specific RBPs for splicing decisions in different tissues.\n*   **Web Portal and Open Science:** To ensure broad utility, we will develop a user-friendly web portal where researchers can submit a gene and select a tissue context to receive splicing predictions and visualizations. All code will be made available on GitHub under a permissive license, and all processed data and trained models will be deposited in public repositories like Zenodo, adhering to FAIR data principles.\n\n**Timeline:**\n*   **Year 1:** Data aggregation, processing pipeline finalization, and initial model prototyping. First annual working group meeting.\n*   **Year 2:** Full-scale model training, hyperparameter optimization, rigorous benchmarking, and initial interpretation analyses. Development of the web portal begins.\n*   **Year 3:** In-depth biological discovery using the model, application to disease variant and single-cell data, manuscript preparation, and public release of all tools and resources. Final working group meeting and community training workshop.",
        "expected_outcomes_and_impact": "This project is poised to deliver transformative outcomes that will significantly advance the fields of molecular biology, computational biology, and translational medicine. The impact will be felt through the creation of a paradigm-shifting predictive tool, the generation of a foundational biological resource, and the training of a new generation of interdisciplinary scientists.\n\n**Intellectual Merit and Contribution to the Field:**\nThe primary outcome will be the **SpliceFormer model**, a powerful, publicly accessible tool that represents a fundamental leap forward in our ability to understand and predict gene regulation. Unlike previous models, SpliceFormer will be the first to integrate sequence, RNA structure, and the trans-acting cellular environment to predict quantitative, tissue-specific splicing outcomes. This moves the field beyond simple classification of splice sites towards a truly quantitative and mechanistic understanding of the process. The conceptual framing of the splicing code as a 'language' to be learned by a transformer architecture is a novel approach that could serve as a blueprint for modeling other complex biological systems.\n\nA second major outcome will be the creation of a **comprehensive, dynamic atlas of the human splicing code**. Through in silico mutagenesis and model interpretation, we will generate the most detailed map to date of functional cis-regulatory elements across the entire human transcriptome. Crucially, this map will be dynamic, showing how the activity of these elements is modulated by the combinatorial interplay of RBPs across dozens of human tissues. This atlas will serve as a foundational resource for RNA biologists for years to come, enabling countless new hypotheses about gene regulation in health and disease.\n\n**Broader Impacts and Applications:**\nThe societal and clinical impact of this research will be substantial.\n*   **Revolutionizing Clinical Genetics:** A major challenge in genomics is the interpretation of variants of unknown significance (VUS), particularly those in non-coding regions. SpliceFormer will provide a powerful tool to predict whether any given genetic variant disrupts splicing, providing a direct mechanistic link to disease. This will aid in the diagnosis of rare genetic disorders and improve the clinical utility of whole-genome sequencing.\n*   **Accelerating Therapeutic Development:** The model will have direct applications in the design of RNA-targeted therapies. For diseases caused by mis-splicing, such as Spinal Muscular Atrophy or certain cancers, SpliceFormer can be used to design and optimize antisense oligonucleotides (ASOs) that correct the splicing defect. It can predict both on-target efficacy and potential off-target effects, streamlining the pre-clinical development pipeline.\n*   **Enabling Advances in Synthetic Biology:** A predictive understanding of the splicing code will allow for the forward engineering of genetic constructs with precisely controlled, cell-type-specific splicing patterns. This will be invaluable for creating sophisticated gene circuits for applications in cell-based therapies, regenerative medicine, and biotechnology.\n\n**Dissemination, Open Science, and Training:**\nThis working group is deeply committed to the principles of open and reproducible science. All software developed will be open-source and hosted on GitHub. All data, model weights, and results will be shared through public repositories (e.g., Zenodo, GEO) and a dedicated project website. Findings will be published in high-impact, open-access journals. To maximize community engagement, we will host a final-year workshop to train other researchers in the use of our tools. This project is an ideal training environment for graduate students and postdocs, who will gain invaluable cross-disciplinary experience at the cutting edge of big data, machine learning, and molecular biology, directly addressing the call's goal of fostering a data-savvy workforce.\n\n**Long-Term Vision and Sustainability:**\nThe long-term vision is to establish SpliceFormer as a 'foundation model' for RNA processing, analogous to what AlphaFold has become for protein structure. The framework we develop is extensible and can be adapted in future work to incorporate other layers of gene regulation, such as transcription kinetics, chromatin state, and polyadenylation. This project will not only answer a long-standing fundamental question but will also lay the groundwork for a new era of predictive, personalized genomics.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis project that is beyond the capabilities of a single research laboratory or existing collaboration. The sheer scale of the data integration, the computational expense of developing and training a novel deep learning architecture, and the essential requirement for deep, synergistic expertise from disparate scientific fields (RNA biology, machine learning, computational linguistics) necessitate the working group structure and support provided by the NCEMS program. Standard funding mechanisms are insufficient to support the required personnel, computational infrastructure, and collaborative coordination.\n\n**Budget Justification:**\nThe budget is designed for a three-year project period and reflects the intensive computational and collaborative nature of the work. The primary costs are for personnel who will drive the project's success and for the substantial computational resources required for large-scale model training.\n\n**Detailed Budget Breakdown (3-Year Total):**\n\n1.  **Personnel ($945,000):** This is the largest budget category, reflecting the project's reliance on dedicated, highly skilled researchers.\n    *   **Postdoctoral Fellows (2 FTEs):** $450,000. Two fellows will be hired, one with expertise in bioinformatics and RNA biology, and the other in machine learning. They will lead the data processing and model development efforts, respectively. (Based on $75,000 salary + 25% fringe per year).\n    *   **Graduate Students (2 FTEs):** $240,000. Two students will support the postdocs, focusing on data validation, model benchmarking, and application of the model to biological questions. (Based on $40,000 stipend + benefits/tuition per year).\n    *   **Data Scientist (0.5 FTE):** $180,000. A part-time data scientist is critical for managing the petabyte-scale data harmonization pipeline, ensuring data integrity, and maintaining the project's cloud and HPC infrastructure. (Based on $120,000 full-time salary + 25% fringe per year).\n    *   **Principal Investigator Support:** $75,000. Summer salary (1 month/year) for the lead PI to provide scientific oversight and manage the working group.\n\n2.  **Computational Resources ($150,000):**\n    *   **Cloud Computing Credits (AWS/Google Cloud):** $120,000 ($40,000/year). Essential for accessing GPU instances (e.g., A100s) required for training large transformer models. This also covers cloud storage for the multi-terabyte processed dataset.\n    *   **HPC Cluster Access:** $30,000 ($10,000/year). To support data pre-processing and analysis tasks on local institutional high-performance computing clusters.\n\n3.  **Travel and Collaboration ($60,000):**\n    *   **Annual Working Group Meeting:** $45,000 ($15,000/year). To bring the entire team (PIs, trainees, staff) together for an intensive 3-day in-person workshop to foster deep collaboration, resolve challenges, and plan future directions.\n    *   **Conference Travel:** $15,000 ($5,000/year). To enable trainees to present project findings at key international conferences (e.g., RNA Society, ISMB, NeurIPS), disseminating our work and providing valuable professional development.\n\n4.  **Publications and Dissemination ($20,000):**\n    *   **Open-Access Publication Fees:** $15,000. To cover article processing charges for publishing our findings in high-impact open-access journals.\n    *   **Web Portal Hosting & Maintenance:** $5,000. To cover costs associated with hosting and maintaining the public-facing web server for the SpliceFormer tool.\n\n**Total Direct Costs:** $1,175,000\n**Indirect Costs (IDC):** (Calculated at a hypothetical 55% of modified total direct costs, excluding tuition): ~$590,000\n**Total Requested Budget:** ~$1,765,000"
      }
    },
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_07",
      "original_title": "The Phase-Separation Atlas: Predicting the Emergence of Membraneless Organelles from Proteome-wide Features",
      "original_abstract": "Cells organize their cytoplasm using not only membrane-bound organelles but also membraneless organelles or biomolecular condensates, which form via liquid-liquid phase separation (LLPS). The composition, regulation, and material properties of these condensates are emergent phenomena driven by multivalent interactions among proteins and nucleic acids. This working group will develop a predictive framework to map the 'phase-separation potential' of the entire human proteome. We will synthesize diverse public datasets, including protein sequences from UniProt, structural disorder predictions, post-translational modification sites, and protein-protein interaction networks from STRING and BioGRID. These data will be used to train machine learning models, benchmarked against curated experimental databases of phase-separating proteins (e.g., PhaSepDB, PhaSePro). Our multidisciplinary team of cell biologists, polymer physicists, biophysicists, and data scientists will develop a multi-scale model that predicts not only the intrinsic LLPS propensity of individual proteins but also the combinatorial logic of how these proteins assemble into specific, compositionally distinct condensates. The project will deliver a publicly accessible, interactive 'Phase-Separation Atlas' that predicts which proteins form condensates, with whom they interact, and how disease-associated mutations or post-translational modifications might alter these emergent cellular structures. This will provide a powerful resource for understanding cellular organization and the molecular basis of diseases like neurodegeneration and cancer.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:48:04.060815",
      "proposal": {
        "title": "The Phase-Separation Atlas: Predicting the Emergence of Membraneless Organelles from Proteome-wide Features",
        "abstract": "Cells organize their cytoplasm using not only membrane-bound organelles but also membraneless organelles or biomolecular condensates, which form via liquid-liquid phase separation (LLPS). The composition, regulation, and material properties of these condensates are emergent phenomena driven by multivalent interactions among proteins and nucleic acids. This working group will develop a predictive framework to map the 'phase-separation potential' of the entire human proteome. We will synthesize diverse public datasets, including protein sequences from UniProt, structural disorder predictions, post-translational modification sites, and protein-protein interaction networks from STRING and BioGRID. These data will be used to train machine learning models, benchmarked against curated experimental databases of phase-separating proteins (e.g., PhaSepDB, PhaSePro). Our multidisciplinary team of cell biologists, polymer physicists, biophysicists, and data scientists will develop a multi-scale model that predicts not only the intrinsic LLPS propensity of individual proteins but also the combinatorial logic of how these proteins assemble into specific, compositionally distinct condensates. The project will deliver a publicly accessible, interactive 'Phase-Separation Atlas' that predicts which proteins form condensates, with whom they interact, and how disease-associated mutations or post-translational modifications might alter these emergent cellular structures. This will provide a powerful resource for understanding cellular organization and the molecular basis of diseases like neurodegeneration and cancer.",
        "background_and_significance": "The textbook view of cellular organization, dominated by membrane-enclosed compartments, has been fundamentally expanded by the discovery of membraneless organelles (MLOs). These dynamic, protein- and RNA-rich bodies, also known as biomolecular condensates, are critical hubs for biochemical reactions and information processing, including stress response, RNA metabolism, and ribosome biogenesis. The formation of these MLOs is an emergent property of the system, driven by a physical process known as liquid-liquid phase separation (LLPS). This process is governed by the collective effect of many weak, multivalent interactions among macromolecules, primarily proteins with intrinsically disordered regions (IDRs) and nucleic acids. Seminal work by Brangwynne, Hyman, Rosen, Pappu, and others has established the physicochemical principles of LLPS, demonstrating how specific sequence features—such as charge patterning, aromatic residues, and low-complexity domains—can encode the potential for a protein to phase separate. This paradigm shift has profound implications, as the material properties and composition of condensates are not fixed but are dynamically regulated by the cell, and their dysregulation is increasingly linked to devastating human diseases, including amyotrophic lateral sclerosis (ALS), Alzheimer's disease, and various cancers. The field has rapidly advanced from descriptive cell biology to quantitative biophysics, yet a critical gap remains in our ability to predict and understand phase separation at a proteome-wide scale. Current experimental methods for identifying phase-separating proteins are low-throughput and often performed in vitro, divorced from the complex cellular milieu. While several computational tools have emerged to predict LLPS propensity from protein sequence (e.g., PScore, catGRANULE, FuzDrop), they face significant limitations. First, they primarily focus on the intrinsic properties of individual proteins, often overlooking the combinatorial logic of how multiple components assemble into a specific, functional condensate. Cellular LLPS is not a solo act; it is a collective, emergent phenomenon. Second, existing models often rely on a limited set of sequence features and have not fully leveraged the explosion of structural, post-translational, and network-level data. Third, they lack the capacity to systematically predict how cellular context, such as post-translational modifications (PTMs) or the presence of specific RNA molecules, tunes phase behavior. Consequently, we lack a comprehensive 'parts list' of the human condensatome, let alone the 'assembly instructions' that govern its formation, composition, and regulation. There is no integrated resource that allows a researcher to query a protein and predict not only if it can phase separate, but with which partners, under what conditions, and how this behavior might be altered by a disease-associated mutation. This project is both important and timely because it addresses this critical gap directly. The unprecedented availability of high-quality public data—from proteome sequences (UniProt), interaction networks (STRING, BioGRID), predicted structures (AlphaFold DB), and clinical variants (ClinVar)—provides a historic opportunity for a large-scale data synthesis effort. By convening a multidisciplinary working group of cell biologists, polymer physicists, and data scientists, we can integrate these disparate datasets to build a predictive model of cellular organization that is far beyond the scope of any single research lab. This project will transform our understanding of the emergent principles governing cellular compartmentalization and provide an invaluable resource to accelerate research into the molecular basis of health and disease.",
        "research_questions_and_hypotheses": "This project is motivated by the central question: Can we move beyond predicting the intrinsic LLPS propensity of individual proteins to create a multi-scale, predictive framework that maps the emergent, combinatorial logic of the entire human condensatome? To address this overarching goal, we have formulated three specific, interconnected research questions, each with testable hypotheses that will guide our data synthesis and modeling efforts. \n\n**Research Question 1 (RQ1): What are the universal and context-specific molecular features that determine a protein's intrinsic capacity to undergo LLPS and how is this capacity regulated?**\nWhile current predictors identify some key features, they lack the sophistication to capture the full regulatory grammar encoded in protein sequence, structure, and modification state. We hypothesize that a more comprehensive integration of features within an advanced machine learning framework will yield a superior predictive model.\n*   **Hypothesis 1a:** A deep learning model that integrates a rich feature set—including amino acid n-gram frequencies, physicochemical properties (e.g., sequence charge decoration), predicted structural motifs from AlphaFold, and consensus disorder predictions—will significantly outperform existing algorithms in predicting the intrinsic LLPS propensity of a protein when benchmarked against curated experimental databases.\n*   **Hypothesis 1b:** Post-translational modifications (PTMs) function as a dynamic regulatory code that systematically tunes LLPS. We hypothesize that specific PTMs, such as phosphorylation of serine/threonine residues or arginine methylation, predictably alter a protein's phase diagram by modifying its net charge, valency, and interaction surfaces. Our model will be trained to predict the direction and magnitude of this effect.\n\n**Research Question 2 (RQ2): How do multivalent interactions among proteins and with RNA molecules specify the composition and identity of distinct biomolecular condensates?**\nAn individual protein's LLPS propensity is necessary but not sufficient to explain the formation of specific MLOs. We posit that condensate identity emerges from the underlying network of molecular interactions.\n*   **Hypothesis 2a:** The architecture of the protein-protein interaction (PPI) network, when weighted by the intrinsic LLPS scores of its constituent proteins (from RQ1), contains modules that correspond to known biomolecular condensates. We predict that community detection algorithms will identify distinct protein clusters, each defined by a core of high-propensity 'scaffold' proteins and a periphery of lower-propensity 'client' proteins, which recapitulate the known compositions of organelles like stress granules, P-bodies, and the nucleolus.\n*   **Hypothesis 2b:** Specific RNA molecules, particularly long non-coding RNAs, act as essential architectural elements for a subset of condensates. We hypothesize that integrating protein-RNA interaction data will reveal that certain RNAs serve as critical hubs, scaffolding specific protein communities and thereby defining the condensate's ultimate composition and function.\n\n**Research Question 3 (RQ3): How do disease-associated genetic mutations perturb the proteome's phase-separation landscape, leading to cellular dysfunction?**\nThe link between aberrant phase transitions and disease is well-established, but a systematic method to predict the impact of mutations is lacking. We hypothesize that our framework can mechanistically stratify mutations based on their predicted effect on LLPS.\n*   **Hypothesis 3a:** Pathogenic missense mutations found in databases like ClinVar, when mapped onto proteins with high predicted LLPS propensity, will disproportionately alter key physicochemical features governing multivalency (e.g., charge, aromaticity, disorder) compared to benign polymorphisms from gnomAD.\n*   **Hypothesis 3b:** Our integrated model can predict the functional consequence of a mutation, classifying it as gain-of-function (e.g., promoting aggregation and solidification, as seen in FUS/TDP-43), loss-of-function (e.g., disrupting necessary condensate formation), or neutral. This predictive capability will provide a powerful tool for interpreting variants of unknown significance.\n\nOur primary deliverable will be the 'Phase-Separation Atlas,' a public web resource integrating these predictive tiers. Validation will be computational, using rigorous cross-validation, testing on held-out datasets not used for training, and benchmarking against orthogonal evidence from proteomics studies of purified MLOs.",
        "methods_and_approach": "This project is designed as a community-scale synthesis effort, leveraging the diverse expertise of our working group, which includes cell biologists, biophysicists, polymer physicists, data scientists, and software engineers. Our approach is organized into three synergistic aims, built upon a foundational data integration platform. The entire project will adhere to open science principles, with all data, code, and models made publicly available.\n\n**Foundational Step: Data Acquisition, Harmonization, and Integration**\nOur first task is to construct a comprehensive, multi-modal knowledge graph. This involves aggregating and harmonizing data from numerous public repositories. \n*   **Data Sources:** We will use the human proteome from UniProt/Swiss-Prot as our base. This will be annotated with: (1) sequence-derived features; (2) consensus disorder predictions from MobiDB; (3) predicted 3D structures from the AlphaFold Database; (4) experimentally verified PTMs from PhosphoSitePlus; (5) high-confidence PPIs from STRING and BioGRID; (6) protein-RNA interactions from ENCODE and starBase; and (7) human genetic variants from ClinVar, COSMIC, and gnomAD. For model training and benchmarking, we will compile a gold-standard set of LLPS-positive and -negative proteins from PhaSepDB, PhaSePro, and extensive manual literature curation.\n*   **Integration:** All data will be mapped to unique UniProt identifiers and stored in a Neo4j graph database. This structure is ideal for representing the complex, multi-layered relationships between proteins, RNAs, PTMs, and their emergent properties.\n\n**Aim 1: Developing a State-of-the-Art Predictor for Intrinsic LLPS Propensity**\nTo address RQ1, we will develop a machine learning model to compute a continuous LLPS score for every protein in the human proteome.\n*   **Feature Engineering:** For each protein, we will compute a rich vector of over 200 features, including amino acid composition, sequence complexity, charge patterning (e.g., SCD, κ), hydrophobicity, aromatic residue content, and predicted structural features like coil/helix content and solvent accessibility derived from AlphaFold models.\n*   **Model Architecture:** We will employ a transformer-based architecture, which excels at capturing long-range dependencies in sequential data. This model will be trained on our gold-standard dataset to predict LLPS propensity. Its performance will be rigorously compared against simpler models (e.g., Gradient Boosting) and existing tools using 10-fold cross-validation and testing on a held-out set of recently published LLPS proteins.\n*   **PTM Modeling:** We will simulate the effect of common PTMs by altering the feature vector (e.g., phosphorylation adds two negative charges and a bulky group) and predicting the resulting change in the LLPS score, thereby quantifying the regulatory potential of each modification.\n\n**Aim 2: Mapping the Condensatome through Network Analysis**\nTo address RQ2, we will use the LLPS scores from Aim 1 to analyze the human interactome.\n*   **Network Construction:** We will construct a weighted human PPI network where nodes are proteins and edges represent high-confidence interactions. Each node will be annotated with its intrinsic LLPS score.\n*   **Community Detection:** We will apply network clustering algorithms (e.g., Louvain method) to identify densely interconnected modules. Our central hypothesis is that modules with a high average LLPS score represent protein communities that co-assemble into specific condensates. We will validate these predicted communities by testing for enrichment of proteins known to co-localize in specific MLOs using GO terms and other annotation databases.\n*   **RNA Scaffolding:** We will extend this network by adding RNA nodes and protein-RNA edges. This will allow us to identify RNAs that act as hubs or bridges, organizing specific protein communities into functional ribonucleoprotein granules.\n\n**Aim 3: Building and Disseminating the Interactive Phase-Separation Atlas**\nTo address RQ3 and serve the broader community, we will create a user-friendly web portal.\n*   **Backend/Frontend:** A RESTful API will provide access to our integrated database and predictive models. The frontend will be developed using React and D3.js for interactive data visualization.\n*   **Functionality:** The Atlas will allow users to: (1) search for any human protein and view its predicted LLPS score and key driving features; (2) explore its predicted condensate partners in an interactive network view; (3) visualize a 'mutability map' highlighting residues where mutations are predicted to most strongly impact LLPS; and (4) input specific mutations (e.g., from clinical sequencing) to get a prediction of their impact on phase separation.\n\n**Timeline and Milestones:**\n*   **Year 1:** Data integration complete. Version 1.0 of the intrinsic LLPS predictor developed and benchmarked. First annual working group meeting.\n*   **Year 2:** Network-based condensate prediction model complete. Beta version of the Phase-Separation Atlas web portal released for community feedback. Training workshop for trainees.\n*   **Year 3:** Full integration of all predictive tiers. Public launch of the Atlas. Systematic analysis of disease mutations. Final working group meeting and submission of primary manuscript.",
        "expected_outcomes_and_impact": "The successful completion of this project will yield significant outcomes that will fundamentally advance the molecular and cellular biosciences, with broad impacts on human health research. Our work is designed to create not just new knowledge, but also enduring resources that will catalyze research across the scientific community, perfectly aligning with the mission of this funding organization.\n\n**Intellectual Merit and Contribution to the Field:**\nThis project will deliver the first comprehensive, multi-scale predictive map of the human condensatome. Our primary intellectual contribution is the shift from a reductionist, single-protein view of phase separation to a systems-level, emergent framework. By integrating sequence, structure, PTMs, and interaction networks, we will elucidate the combinatorial 'code' that governs the assembly of membraneless organelles. This will provide a quantitative, mechanistic foundation for understanding how cells use phase separation to organize their cytoplasm. We will generate a ranked and prioritized list of hundreds of novel candidate LLPS proteins, providing a rich set of experimentally testable hypotheses for the broader cell biology community. Furthermore, our framework for predicting the functional consequences of mutations will establish a new paradigm for interpreting genetic variation in the context of cellular organization.\n\n**Broader Impacts and Applications:**\n*   **Accelerating Disease Research:** The Phase-Separation Atlas will be an invaluable hypothesis-generation tool. A researcher studying a protein implicated in Alzheimer's disease, ALS, or a specific cancer can immediately use our resource to assess its likelihood of phase separation, identify its potential interaction partners within a condensate, and predict how patient-derived mutations might alter its behavior. This will dramatically lower the barrier to entry for studying LLPS and guide experimental design, saving time and resources.\n*   **Informing Therapeutic Strategies:** By identifying the key 'scaffold' proteins that nucleate specific disease-relevant condensates and the critical interactions that maintain them, the Atlas will highlight novel targets for therapeutic intervention. This could inspire strategies aimed at dissolving pathological aggregates or restoring the function of essential condensates.\n*   **Training the Next Generation of Data-Savvy Biologists:** This project is intrinsically a training vehicle. Graduate students and postdocs within the working group will gain unique cross-disciplinary expertise at the interface of cell biology, biophysics, and machine learning. Through our planned annual workshops, we will disseminate these skills and our analytical pipelines to the wider community, fostering a more computationally fluent workforce.\n\n**Dissemination, Data Sharing, and Long-Term Vision:**\nWe are deeply committed to open science principles. \n*   **The Phase-Separation Atlas:** Our primary deliverable will be a freely accessible, user-friendly web portal. We will secure institutional commitments to maintain this resource for at least five years beyond the funding period.\n*   **Open-Source Code and Data:** All source code for our predictive models and the web portal, along with all processed data and the final trained models, will be made available through public repositories like GitHub and Zenodo under permissive licenses (e.g., MIT, CC-BY).\n*   **Publications and Presentations:** We will publish our findings in high-impact, open-access journals. We will also present our work at major international conferences (e.g., ASCB, Biophysical Society) to ensure broad dissemination.\n\nOur long-term vision is for the Atlas to become a community-driven, living resource. We will build it with the capacity for future updates, allowing for the integration of new experimental data and model refinement over time. This project will not only answer our proposed research questions but will also create a powerful, extensible platform that empowers the entire scientific community to explore the emergent world of biomolecular condensates.",
        "budget_and_resources": "The proposed research represents a large-scale, multidisciplinary synthesis effort that requires a dedicated team and significant computational resources, far exceeding the capacity of a single research laboratory or existing collaboration. The budget is designed to support a highly collaborative and trainee-focused working group, ensuring the successful execution of the project and the dissemination of its outcomes. The total requested budget for the three-year project is $XXX,XXX.\n\n**1. Personnel ($XXX,XXX):**\nThis is the largest and most critical component of the budget. The funds will support the trainees and technical staff who will perform the data integration, model development, and analysis.\n*   **Postdoctoral Fellows (2.0 FTE):** We request support for two postdoctoral fellows. One will specialize in machine learning and computational biology, leading the development of the predictive models (Aim 1). The second will have expertise in network biology and bioinformatics, focusing on the interactome analysis and condensate mapping (Aim 2). Their combined expertise is essential for bridging the different scales of the project.\n*   **Graduate Students (3.0 FTE):** Support for three graduate students is requested. They will be embedded within the PIs' labs and will be integral to all aspects of the project, from data curation and feature engineering to model validation and web portal testing. This aligns with the call's goal of training the future data-savvy workforce.\n*   **Data Manager/Software Engineer (0.5 FTE):** We request support for a part-time professional to oversee the construction and maintenance of the integrated database and the public-facing web portal (Aim 3). This ensures the creation of a robust, sustainable, and user-friendly community resource.\n\n**2. Travel ($XX,XXX):**\nTo foster the deep collaboration required for a synthesis project of this nature, we request funds for one annual in-person working group meeting for all PIs, postdocs, and students. This dedicated time is crucial for strategic planning, problem-solving, and cross-pollination of ideas. We also request funds to support travel for each trainee to present their work at one major national or international conference per year, facilitating dissemination and professional development.\n\n**3. Computational Resources ($XX,XXX):**\nTraining deep learning models on proteome-scale data and hosting a dynamic web portal are computationally intensive tasks. We request funds for cloud computing services (e.g., Amazon Web Services or Google Cloud Platform) to provide the necessary GPU access for model training and the server infrastructure for data storage and web hosting.\n\n**4. Materials and Supplies ($X,XXX):**\nThis includes costs for software licenses and subscriptions to relevant databases or services.\n\n**5. Publication and Dissemination Costs ($XX,XXX):**\nFunds are requested to cover open-access publication fees for at least three major manuscripts. We also request a modest budget to support the hosting of our annual virtual training workshop, covering costs for web platforms and material preparation.\n\n**6. Indirect Costs (F&A) ($XXX,XXX):**\nIndirect costs are calculated at the federally negotiated rates for the participating institutions.\n\n**Justification for NCEMS Support:** The scale of data integration, the need for diverse and sustained expertise from multiple disciplines, and the focus on creating a lasting community resource make this project an ideal fit for the NCEMS working group program. No single PI has the resources or breadth of expertise to undertake this challenge alone. This budget directly supports the collaborative, training-oriented, and computationally-driven nature of the proposed work."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_08",
      "original_title": "The Digital Cell: Unifying Image and Omics Data to Model the Emergence of Cellular Morphology",
      "original_abstract": "A cell's shape and internal organization are fundamental to its function, yet they are emergent properties arising from staggeringly complex interactions between the cytoskeleton, membranes, and organelles. Understanding how morphology is robustly encoded and dynamically regulated requires a holistic, quantitative approach. This working group proposes to build a 'digital twin' of a human cell by integrating public data from two powerful but often disconnected domains: imaging and omics. We will synthesize 3D live-cell imaging data from resources like the Allen Cell Explorer and the Image Data Resource, which contain terabytes of high-resolution movies of fluorescently-tagged structures. We will combine this with proteomics, transcriptomics, and protein-protein interaction data that define the molecular parts list and their connections. Our team, uniting cell biologists, computer vision experts, biophysicists, and applied mathematicians, will develop novel deep learning algorithms to automatically extract a 'morphological feature space' from thousands of images. We will then build an integrated computational model that links these morphological features to the underlying molecular networks. The goal is to create a predictive model that can simulate how cellular morphology emerges from molecular-level rules and how it changes in response to genetic perturbations cataloged in resources like the DepMap. This will provide an unprecedented platform for understanding the principles of cellular self-organization.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:49:37.658930",
      "proposal": {
        "title": "The Digital Cell: Unifying Image and Omics Data to Model the Emergence of Cellular Morphology",
        "abstract": "A cell's shape and internal organization are fundamental to its function, yet they are emergent properties arising from staggeringly complex interactions between the cytoskeleton, membranes, and organelles. Understanding how morphology is robustly encoded and dynamically regulated requires a holistic, quantitative approach. This working group proposes to build a 'digital twin' of a human cell by integrating public data from two powerful but often disconnected domains: imaging and omics. We will synthesize 3D live-cell imaging data from resources like the Allen Cell Explorer and the Image Data Resource, which contain terabytes of high-resolution movies of fluorescently-tagged structures. We will combine this with proteomics, transcriptomics, and protein-protein interaction data that define the molecular parts list and their connections. Our team, uniting cell biologists, computer vision experts, biophysicists, and applied mathematicians, will develop novel deep learning algorithms to automatically extract a 'morphological feature space' from thousands of images. We will then build an integrated computational model that links these morphological features to the underlying molecular networks. The goal is to create a predictive model that can simulate how cellular morphology emerges from molecular-level rules and how it changes in response to genetic perturbations cataloged in resources like the DepMap. This will provide an unprecedented platform for understanding the principles of cellular self-organization.",
        "background_and_significance": "The principle that structure dictates function is a cornerstone of biology. At the cellular level, this principle manifests in the intricate and dynamic morphology of the cell—its overall shape, the spatial organization of its organelles, and the architecture of its cytoskeleton. These are not static, pre-programmed structures but emergent properties arising from the collective behavior of millions of molecules. Understanding how a cell’s genotype and molecular state translate into its physical phenotype is one of the most fundamental, long-standing challenges in molecular and cellular science. For decades, research has progressed along two parallel, powerful, but largely disconnected tracks: the visual and the molecular. The imaging revolution, driven by advances like lattice light-sheet and spinning-disk confocal microscopy, has provided breathtaking views into the living cell. Large-scale public repositories such as the Allen Cell Explorer and the Image Data Resource (IDR) now house petabytes of high-resolution, three-dimensional movies, cataloging the localization and dynamics of thousands of proteins. These resources offer an unprecedented visual encyclopedia of cellular organization. Concurrently, the omics revolution has provided a comprehensive 'parts list' and 'wiring diagram' of the cell. Projects like the Human Proteome Map, STRING, and BioGRID have systematically cataloged protein abundances and their physical interactions, while resources like GTEx and ENCODE have mapped the transcriptional landscape. Furthermore, large-scale perturbation screens, most notably the Dependency Map (DepMap), have functionally linked thousands of genes to cellular fitness and other phenotypes through systematic CRISPR and RNAi screens. The critical gap in our knowledge lies at the intersection of these two domains. We have a wealth of data on what the cell looks like and what it is made of, but we lack a quantitative, predictive framework that connects the molecular 'wiring diagram' to the emergent physical form. Current computational models in cell biology are often focused on specific subsystems, such as actin polymerization or mitotic spindle assembly. While incredibly insightful, these models do not capture the holistic, cell-wide coordination that governs overall morphology. The analysis of large-scale imaging data often relies on a limited set of pre-defined, 'hand-crafted' features (e.g., cell area, nuclear eccentricity), which may miss subtle or complex aspects of cellular organization. We are at a unique inflection point where this grand challenge can finally be addressed. The confluence of three key developments makes this project both timely and feasible: the maturity of massive, public imaging and omics datasets; the dramatic advances in artificial intelligence, particularly deep learning for automated feature extraction from complex images; and the growing culture of collaborative, open team science. This working group proposes to bridge the gap between the molecular and the morphological by synthesizing these disparate public data streams. By developing a unified computational framework, we will move beyond correlative studies to build a predictive model of cellular self-organization. This 'Digital Cell' will serve as a powerful, community-accessible platform to perform in silico experiments, generating testable hypotheses about how molecular perturbations impact the physical structure of the cell. This research directly addresses the funding call's focus on emergence phenomena, leveraging data synthesis and multidisciplinary collaboration to solve a foundational puzzle in cellular bioscience.",
        "research_questions_and_hypotheses": "This working group will address the overarching question: How do the collective interactions of a cell's molecular components robustly encode and dynamically regulate its three-dimensional morphology? To deconstruct this complex problem, we have formulated three specific, interconnected aims, each with a central research question and a testable hypothesis. Our approach is designed to create a hierarchical framework, starting with a quantitative description of morphology, then linking it to the underlying molecular state, and finally using this link for prediction.\n\n**Aim 1: Define a comprehensive, quantitative 'morphospace' of the human cell.**\n*   **Research Question 1:** Can we develop a unified, low-dimensional feature space that captures the salient morphological variations across thousands of live-cell 3D images, encompassing cell shape, organelle organization, and cytoskeletal architecture, without relying on biased, manually engineered features?\n*   **Hypothesis 1:** A deep generative model, specifically a 3D Convolutional Variational Autoencoder (CVAE), trained on a large corpus of 3D cell images, can learn a continuous, compressed latent representation (the 'morphospace'). We hypothesize that this learned space will be more powerful than traditional feature sets because it will capture complex, multi-scale relationships between subcellular structures. Proximity within this space will correspond to holistic morphological similarity.\n*   **Validation:** The validity of the morphospace will be tested rigorously. First, the CVAE's decoder must be able to reconstruct high-fidelity cell images from latent space vectors, demonstrating that the representation is comprehensive. Second, we will test if the space is biologically meaningful by projecting cells with known phenotypes (e.g., cells in different phases of the cell cycle) into the space and verifying that they form distinct, separable clusters. Third, we will perform interpolations between distant points in the morphospace and show, through visual inspection by expert cell biologists in our team, that the generated intermediate images represent biologically plausible morphological transitions.\n\n**Aim 2: Build a predictive model linking the molecular state (omics) to the morphological state (morphospace).**\n*   **Research Question 2:** To what extent can the abundance and interaction patterns of proteins and transcripts predict a cell's position within the defined morphospace? Which molecular pathways are the primary determinants of specific morphological axes?\n*   **Hypothesis 2:** A multi-modal machine learning model, integrating protein-protein interaction (PPI) networks with gene expression and protein abundance data, can accurately predict a cell's coordinates in the morphospace. We propose using a Graph Neural Network (GNN), as it is explicitly designed to learn from the relational structure of the PPI network. We hypothesize that this integrated model will outperform models based on gene expression alone, as it captures the functional context of molecular components.\n*   **Validation:** We will train the model using data from sources like the Human Protein Atlas, where imaging and omics data are available for the same cell lines. The model's predictive accuracy will be assessed on a held-out test set using metrics like cosine similarity between predicted and true morphospace vectors. To identify key molecular drivers, we will employ feature attribution methods (e.g., SHAP, integrated gradients) to rank genes and pathways by their influence on different morphological axes.\n\n**Aim 3: Use the integrated model to simulate the morphological consequences of genetic perturbations.**\n*   **Research Question 3:** Can our integrated model accurately predict the specific morphological changes that result from systematic gene knockdowns or knockouts, as cataloged in large-scale public screens?\n*   **Hypothesis 3:** By computationally simulating a genetic perturbation (e.g., removing a gene's node from the GNN input) and propagating this change through our trained omics-to-morphospace model, we can forecast the resulting displacement vector in morphospace. We hypothesize that these *in silico* predicted phenotypic shifts will quantitatively match experimentally observed morphological changes from high-content imaging screens.\n*   **Validation:** This hypothesis will be tested directly against public data. We will use our model to predict the morphological outcomes for genes targeted in the DepMap project. These predictions will be compared to the measured morphological feature changes in corresponding high-content Cell Painting datasets. We will quantify the model's predictive power by calculating the correlation between the predicted and observed phenotypic vectors across thousands of genetic perturbations. Success in this aim will validate our model as a powerful engine for hypothesis generation.",
        "methods_and_approach": "Our project is a multi-year, multi-phase effort centered on the synthesis and modeling of public data. The methodology is designed to be modular, with clear milestones and validation steps at the conclusion of each phase. Our entire workflow will adhere to open science principles, with all code, models, and derived data being made publicly available.\n\n**Phase 1: Data Acquisition, Curation, and Harmonization (Months 1-9)**\nThis foundational phase requires the expertise of our entire team to collate and standardize disparate data types. \n*   **Imaging Data Sources:** We will primarily utilize the Allen Cell Explorer dataset, which contains over 40,000 3D live-cell image sets of human induced pluripotent stem cells (hiPSCs) with fluorescently tagged structures. This will be our core training set due to its consistency and high quality. We will supplement this with datasets from the Image Data Resource (IDR), specifically high-content screens like the Cell Painting assays, which provide morphological data linked to genetic or chemical perturbations. We will also incorporate 2D confocal images from the Human Protein Atlas to link protein localization to our model. \n*   **Omics and Perturbation Data Sources:** We will construct a comprehensive molecular interaction network using data from STRING and BioGRID. Basal gene expression and protein abundance profiles for relevant cell lines will be sourced from GTEx, CCLE, and the Human Proteome Map. For perturbation analysis, we will use the CRISPR and RNAi screening data from the DepMap project and the associated imaging phenotypes from the Broad Institute's Cell Painting datasets.\n*   **Curation Pipeline:** A systematic pipeline will be developed to process these data. For images, this includes metadata standardization, intensity normalization, and segmentation of cell and nuclear boundaries using pre-trained models like Cellpose. For omics data, this involves mapping all gene/protein identifiers to a common namespace (Ensembl) and constructing a unified, weighted graph representing the molecular interaction network.\n\n**Phase 2: Morphospace Construction via Deep Learning (Months 6-18)**\n*   **Model Architecture:** We will implement a 3D Convolutional Variational Autoencoder (CVAE). The encoder will consist of a series of 3D convolutional layers that downsample an input 3D image stack (e.g., 64x128x128 voxels) into a low-dimensional latent vector (e.g., 128 dimensions). The decoder will be a symmetric network of 3D transposed convolutional layers that reconstructs the image from the latent vector. The VAE framework is chosen for its ability to learn a continuous and generative latent space, which is essential for our goals of interpolation and simulation.\n*   **Training and Validation:** The CVAE will be trained on the curated Allen Cell Explorer dataset using a combined loss function of reconstruction error and the Kullback-Leibler divergence to regularize the latent space. Training will be performed on a high-performance computing (HPC) cluster with multiple GPUs. Validation will proceed as described in the previous section: assessing reconstruction quality, clustering of known cell states, and expert evaluation of morphological interpolations.\n\n**Phase 3: Linking Omics to Morphospace with Graph Neural Networks (Months 15-27)**\n*   **Model Architecture:** We will build a predictive model using a Graph Neural Network (GNN). The input to the GNN will be our curated molecular interaction graph. Node features will include basal gene expression and protein abundance levels. The GNN will use message-passing layers to learn embeddings for each node that incorporate both its own features and the features of its network neighbors. These node embeddings will then be aggregated to produce a single graph-level embedding, which will be mapped via a multi-layer perceptron to predict the 128-dimensional morphospace vector.\n*   **Training and Validation:** The model will be trained in a supervised fashion, using the morphospace vectors generated in Phase 2 as the ground truth labels for corresponding cell lines with available omics data. We will use a held-out test set and cross-validation to evaluate performance, using cosine similarity and mean squared error as our primary metrics. Feature attribution techniques (e.g., GNNExplainer) will be used to identify the molecular subnetworks most predictive of specific morphological features.\n\n**Phase 4: In Silico Perturbation and Model Deployment (Months 24-36)**\n*   **Simulation Pipeline:** We will develop a computational pipeline to simulate genetic perturbations. A gene knockout will be modeled by removing the corresponding node and its edges from the input graph fed into the trained GNN. The model will then predict the new morphospace vector. The difference between the perturbed and unperturbed vectors represents the predicted morphological phenotype.\n*   **Large-Scale Validation:** We will perform thousands of these *in silico* perturbations, corresponding to the genes targeted in the DepMap screens. The predicted phenotypic vectors will be quantitatively compared to the experimental vectors derived from Cell Painting data, providing a robust, large-scale validation of our model's predictive power.\n*   **Timeline and Deliverables:**\n    *   **Year 1:** Complete data curation pipeline; first-generation morphospace model (CVAE) trained and validated.\n    *   **Year 2:** Refined morphospace model; GNN linking model developed and trained; initial integration and validation.\n    *   **Year 3:** Perturbation simulation framework finalized; large-scale validation against DepMap; public web portal for community use developed; publications and dissemination.\nThis project structure ensures a logical progression from data description to predictive modeling, with clear validation points that mitigate risk and ensure the robustness of our final integrated 'Digital Cell' model.",
        "expected_outcomes_and_impact": "The successful completion of this project will yield significant outcomes that advance the fields of cell biology, computational biology, and data science, with broad impacts on basic research and translational medicine. The project is designed not only to answer a fundamental scientific question but also to create a lasting, extensible resource for the scientific community, perfectly aligning with the goals of the NCEMS program.\n\n**Intellectual Merit and Contributions to the Field:**\n1.  **A New Paradigm for Cellular Phenotyping:** We will move the field beyond qualitative descriptions and simple scalar measurements of cell morphology. Our learned 'morphospace' will provide a holistic, quantitative, and data-driven framework for describing cellular form. This represents a fundamental shift in how cell state is defined and measured.\n2.  **Solving the Genotype-Phenotype Gap:** This work will provide the first large-scale, predictive model that quantitatively links the molecular state of a cell (genotype, expression, interactions) to its emergent physical form (phenotype). This directly addresses the long-standing puzzle of cellular self-organization and provides a mechanistic, data-driven understanding of emergence.\n3.  **Novel Computational Methodologies:** We will develop and disseminate novel, open-source deep learning tools. The 3D CVAE for morphological analysis and the GNN for integrating network and imaging data will be powerful, generalizable methods applicable to a wide range of biological questions beyond the scope of this initial project.\n\n**Broader Impacts and Applications:**\n1.  **A Foundational Resource for the Research Community:** The primary outcome will be the 'Digital Cell' model, accessible via a user-friendly web portal. This will empower any researcher, regardless of their computational expertise, to perform *in silico* experiments. For example, a biologist studying a novel protein can use our tool to generate a testable hypothesis about its role in organizing a specific organelle, guiding future experiments and accelerating the pace of discovery. This directly fulfills the call's goal of developing innovative research strategies.\n2.  **Applications in Disease Modeling and Drug Discovery:** Aberrant cell morphology is a hallmark of numerous diseases, including cancer, fibrosis, and neurodegeneration. Our model will provide a platform to understand the molecular basis of these morphological defects. It can be used to predict the physical consequences of disease-associated mutations or to screen for molecular targets that could restore a healthy morphology, opening new avenues for computational diagnostics and therapeutics.\n3.  **Training the Next Generation of Data-Savvy Scientists:** This project is an ideal training environment. Graduate students and postdocs will work at the nexus of cell biology, computer vision, and biophysics, gaining invaluable cross-disciplinary skills. They will learn to manage large-scale data, develop sophisticated computational models, and work within a collaborative, open-science framework. We will further amplify this impact by hosting annual workshops to train the broader community on our tools and methods, fulfilling the call's mandate to train the future workforce.\n4.  **Stimulating Cross-Disciplinary Collaboration:** The very structure of our working group, uniting cell biologists, computer scientists, and biophysicists from different institutions and career stages, embodies the collaborative spirit of the NCEMS program. The project's success is contingent on this deep integration of expertise, demonstrating a model for future community-scale synthesis projects. The need for NCEMS support is clear, as coordinating this effort, supporting dedicated personnel, and funding the required computational resources is beyond the capacity of any single lab or standard grant mechanism.\n\n**Dissemination and Long-Term Vision:**\nWe will pursue a multi-pronged dissemination strategy including high-impact publications, presentations at major international conferences, and the release of all code and models through public repositories (GitHub, Zenodo). Our long-term vision is to create an extensible framework. The initial model, built on hiPSCs, will serve as a scaffold. We envision the community contributing new data—from different cell types, tissues, or with new data modalities—to progressively expand the 'Digital Cell' into a multi-scale 'Digital Organism,' ensuring the project's lasting impact and sustainability.",
        "budget_and_resources": "The proposed research represents a large-scale, multi-institutional synthesis effort that requires dedicated resources beyond the scope of a standard research grant. The budget is designed to support the personnel, computational infrastructure, and collaborative activities essential for the project's success over a three-year period. The requested funds are critical for coordinating the diverse expertise of our working group and creating a robust, high-value community resource from publicly available data.\n\n**1. Personnel (Approximately 65% of total budget):**\nThe intellectual core of this project is the people who will perform the data integration and modeling. \n*   **Postdoctoral Fellows (2 FTEs):** We request support for two full-time postdoctoral fellows who will be the primary drivers of the research. One fellow will have expertise in computer vision and machine learning, leading the development of the morphospace model (Aim 1). The second will be a computational biologist/bioinformatician, leading the development of the GNN linking model and perturbation analysis (Aims 2 & 3).\n*   **Graduate Students (2 FTEs):** Support for two graduate students will ensure the project's continuity and contribute to training the next generation of scientists. They will work closely with the postdocs and PIs on all aspects of the project.\n*   **Principal Investigators (1 month summer salary/year for 4 PIs):** Partial summer support is requested for the PIs to dedicate significant time to scientific oversight, cross-institutional coordination, mentoring of trainees, and dissemination of results.\n*   **Project Manager/Data Scientist (0.5 FTE):** To ensure the smooth operation of a geographically distributed team, we request support for a part-time manager to handle data logistics, manage public repositories, maintain the project website, and coordinate meetings and workshops.\n\n**2. Computational Resources (Approximately 15%):**\nTraining deep learning models on terabytes of 3D image data is computationally intensive.\n*   **Cloud Computing Credits:** We request funds for cloud computing services (e.g., AWS, Google Cloud Platform). This provides on-demand access to high-end GPUs, which is essential for model training and hyperparameter tuning. It also facilitates reproducibility and allows us to share our computational environments with the broader community.\n*   **Data Storage:** A budget is allocated for robust, high-speed data storage solutions to host the curated and harmonized datasets, which will exceed 100 TB.\n\n**3. Travel (Approximately 10%):**\nEffective collaboration in a multi-disciplinary working group requires regular face-to-face interaction.\n*   **Working Group Meetings:** We request funds for the entire team (PIs, postdocs, students) to meet in person twice per year. These intensive, multi-day workshops are critical for brainstorming, problem-solving, and ensuring all components of the project remain integrated.\n*   **Conference Travel:** Funds are included for trainees and PIs to present our findings at key national and international conferences (e.g., ASCB, NeurIPS), facilitating dissemination and feedback from the community.\n\n**4. Training and Dissemination (Approximately 5%):**\n*   **Annual Workshop:** We request funds to host one public workshop per year to train external researchers on our tools and methodologies. This budget covers logistical costs and travel support for a limited number of participants from underrepresented institutions.\n*   **Publication Costs:** Funds are allocated for open-access publication fees to ensure our findings are freely accessible to all.\n\n**5. Indirect Costs (F&A):**\nIndirect costs are calculated based on the federally negotiated rates for each participating institution and are applied to their portion of the direct costs."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_09",
      "original_title": "A Pan-Viral Synthesis of the Host-Pathogen Arms Race: Uncovering the Emergent Rules of Molecular Conflict",
      "original_abstract": "The perpetual conflict between viruses and their hosts is a primary driver of molecular evolution, leading to the emergence of sophisticated host immune defenses and viral countermeasures. While individual examples are well-studied, the universal principles governing these molecular arms races remain poorly defined. This working group will perform the first comprehensive, pan-viral synthesis of host-virus evolution. We will integrate massive public datasets, including all available viral genomes from NCBI, hundreds of host genomes, and comprehensive databases of host-pathogen protein-protein interactions. Our team of virologists, evolutionary biologists, immunologists, and computational biologists will develop a powerful phylogenomic pipeline to systematically identify genes under positive selection—the molecular signatures of genetic conflict—across thousands of host-virus pairs. By mapping these rapidly evolving sites onto protein structures and interaction networks, we will uncover the emergent strategies of this conflict. We will identify conserved functional hotspots on host restriction factors targeted by diverse viral families and discover novel viral protein families dedicated to immune evasion. This project will produce a 'Molecular Conflict Atlas' that charts the evolutionary history and predicts future trajectories of host-virus interactions, providing a powerful framework for understanding viral emergence, pathogenesis, and the development of broad-spectrum antiviral strategies.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:51:05.219445",
      "proposal": {
        "title": "A Pan-Viral Synthesis of the Host-Pathogen Arms Race: Uncovering the Emergent Rules of Molecular Conflict",
        "abstract": "The perpetual conflict between viruses and their hosts is a primary driver of molecular evolution, leading to the emergence of sophisticated host immune defenses and viral countermeasures. While individual examples are well-studied, the universal principles governing these molecular arms races remain poorly defined. This working group will perform the first comprehensive, pan-viral synthesis of host-virus evolution. We will integrate massive public datasets, including all available viral genomes from NCBI, hundreds of host genomes, and comprehensive databases of host-pathogen protein-protein interactions. Our team of virologists, evolutionary biologists, immunologists, and computational biologists will develop a powerful phylogenomic pipeline to systematically identify genes under positive selection—the molecular signatures of genetic conflict—across thousands of host-virus pairs. By mapping these rapidly evolving sites onto protein structures and interaction networks, we will uncover the emergent strategies of this conflict. We will identify conserved functional hotspots on host restriction factors targeted by diverse viral families and discover novel viral protein families dedicated to immune evasion. This project will produce a 'Molecular Conflict Atlas' that charts the evolutionary history and predicts future trajectories of host-virus interactions, providing a powerful framework for understanding viral emergence, pathogenesis, and the development of broad-spectrum antiviral strategies.",
        "background_and_significance": "The co-evolutionary struggle between hosts and their viral pathogens is a central engine of molecular innovation and a key determinant of species' health and survival. This incessant 'arms race,' often described by the Red Queen hypothesis, forces the rapid evolution of both host defense mechanisms and viral countermeasures, leaving indelible signatures in their respective genomes. The study of this conflict has yielded profound insights into fundamental biological processes, from the basic mechanics of protein-protein interactions to the diversification of entire gene families. Seminal studies over the past two decades have beautifully illustrated this dynamic in specific host-virus systems. For instance, the discovery of primate TRIM5α as a restriction factor against retroviruses, and the subsequent identification of the viral capsid as its target, revealed a history of recurrent positive selection in both proteins at their interaction interface. Similarly, the APOBEC3 family of cytidine deaminases, which lethally mutates viral genomes, has undergone dramatic expansion and diversification in primates, driven by antagonism from viral proteins like HIV's Vif, which itself is one of the most rapidly evolving genes in the viral genome. These canonical examples, alongside others like the protein kinase R (PKR) and myxovirus resistance (Mx) protein systems, have established a powerful paradigm: genes involved in host-virus conflicts can be identified by searching for the molecular signature of positive selection, where the rate of nonsynonymous substitution significantly exceeds the rate of synonymous substitution (dN/dS > 1). This approach has become a cornerstone of 'evolutionary immunology,' successfully identifying novel antiviral factors and pinpointing the precise molecular surfaces of conflict. However, our current understanding remains fragmented and anecdotal, largely derived from in-depth studies of a few well-chosen host lineages (primarily primates) and viral families (primarily retroviruses and lentiviruses). This narrow focus presents a major gap in our knowledge. We lack a systematic, global understanding of the principles governing these molecular arms races across the vast diversity of viruses and their hosts. Are the same host pathways repeatedly targeted by unrelated viruses? Do viruses convergently evolve similar molecular solutions to overcome conserved host defenses? Are there 'hotspots' of conflict in the host proteome that serve as a crucible for evolutionary innovation? Answering these questions has been impossible due to the immense scale of the required data and the need for deep, cross-disciplinary expertise. The current state of the field is a collection of fascinating, but disconnected, stories. This project aims to synthesize these stories into a coherent, universal narrative. The timeliness of this research cannot be overstated. We are living in an era of unprecedented data availability. Public repositories now contain millions of viral sequences from every conceivable environment and hundreds of high-quality host genomes spanning the tree of life. Concurrently, advances in computational power and phylogenomic methods have made it feasible to analyze these massive datasets at a scale previously unimaginable. The recent COVID-19 pandemic serves as a stark reminder of the critical need to understand the general principles of viral emergence and host adaptation. By moving beyond single-system studies to a comprehensive, pan-viral synthesis, we can uncover the emergent, predictive rules of molecular conflict. This project will transform the field from descriptive to predictive, providing a foundational framework for anticipating future pandemic threats and designing novel, broad-spectrum antiviral therapies.",
        "research_questions_and_hypotheses": "This working group will address the central question: What are the universal principles and emergent properties of the molecular arms race between viruses and their hosts? To deconstruct this overarching goal, we have formulated three specific, interconnected research questions, each with testable hypotheses and clear, predictable outcomes. \n\n**Question 1: What are the conserved molecular battlegrounds? A systematic identification of host and viral proteins under recurrent, intense evolutionary conflict.**\nWhile we know of individual host restriction factors, we lack a global inventory of the genes most frequently engaged in viral conflict across diverse host taxa. Similarly, we have a limited catalog of the viral protein families dedicated to antagonizing these defenses.\n*   **Hypothesis 1a:** A discrete subset of host protein functional classes (e.g., nucleic acid binding, ubiquitination machinery, membrane remodeling) are universally enriched for genes undergoing positive selection due to their central role in the host-virus interface.\n*   **Prediction:** A genome-wide dN/dS scan across hundreds of host genomes will reveal that genes showing the strongest and most frequent signatures of positive selection are not randomly distributed but are significantly enriched in specific Gene Ontology (GO) terms related to innate immunity and core cellular processes co-opted by viruses.\n*   **Hypothesis 1b:** We can discover novel, uncharacterized viral immune evasion gene families by identifying viral ortholog groups that consistently exhibit high rates of positive selection across diverse viral lineages.\n*   **Prediction:** Our pan-viral phylogenomic screen will identify numerous viral ortholog groups with high dN/dS ratios that currently lack functional annotation. The taxonomic distribution of these rapidly evolving viral genes will correlate with specific host clades, suggesting adaptation to a particular host's immune repertoire.\n\n**Question 2: Are there convergent 'rules of engagement' at the structural level? Uncovering the shared strategies of molecular antagonism.**\nThe physical interfaces between host and viral proteins are the atomic arenas of conflict. We seek to determine if evolution repeatedly finds similar structural solutions to win these battles.\n*   **Hypothesis 2a:** Across diverse host-virus systems, positive selection will convergently target structurally and functionally equivalent 'hotspots' on orthologous host proteins. These hotspots represent vulnerable surfaces critical for protein function that viruses must engage to disable them.\n*   **Prediction:** When we map positively selected sites onto 3D protein structures (from PDB or AlphaFold), we will find that these sites are not randomly distributed on the protein surface. Instead, they will form statistically significant spatial clusters. Furthermore, the locations of these clusters on, for example, all mammalian orthologs of PKR, will be conserved, even when these mammals are targeted by different viruses.\n*   **Hypothesis 2b:** The sites of rapid evolution on a viral antagonist and its host target will be in direct physical proximity, forming a co-evolving 'molecular scar' at the protein-protein interface.\n*   **Prediction:** For known host-virus interacting pairs, structural modeling will show that the majority of positively selected residues on the host protein are within a small physical distance (e.g., <10 Å) of the positively selected residues on the viral protein.\n\n**Question 3: Can the evolutionary history of conflict predict its future trajectory and outcomes?**\nBy integrating the patterns of selection across a broad evolutionary timescale, we aim to build a framework that has predictive power for host range, viral emergence, and pathogenesis.\n*   **Hypothesis 3a:** The 'evolutionary dynamism' of a host species' immune arsenal—defined by the number of positively selected genes and the intensity of selection—can predict its susceptibility to viral spillover.\n*   **Prediction:** Host species with a larger and more rapidly evolving repertoire of antiviral genes will harbor a narrower range of endemic viruses and will be identified as the source of fewer zoonotic events compared to species with less dynamic immune genomes.\n*   **Deliverables:** The primary deliverable will be the 'Molecular Conflict Atlas,' a comprehensive, open-access database and web portal. This resource will integrate our findings, allowing users to query genes, species, and viruses to visualize evolutionary histories, selection pressures, and structural hotspots. We will also deliver a suite of validated, containerized computational workflows for large-scale evolutionary analysis, and a series of high-impact publications.",
        "methods_and_approach": "This project is a pure data synthesis effort, leveraging publicly available data and requiring no new experimental data generation. Our approach is organized into three synergistic aims that integrate phylogenomics, structural biology, and network science. The scale of this analysis—integrating millions of viral sequences with hundreds of host genomes—necessitates the collaborative, multidisciplinary team and NCEMS resources proposed.\n\n**Data Acquisition and Curation**\nOur foundation will be a meticulously curated collection of public data:\n1.  **Host Genomes:** We will select ~500 representative, high-quality vertebrate and invertebrate genome assemblies from NCBI Genomes and Ensembl, prioritizing taxonomic breadth and assembly contiguity. We will use the official gene annotations for each species.\n2.  **Viral Genomes:** We will download the entirety of the NCBI Viral Genomes Resource, encompassing all complete viral sequences from RefSeq and a curated subset from GenBank. This dataset comprises millions of sequences, which will be clustered into viral orthologous groups (VOGs) using established tools like vConTACT2.\n3.  **Interaction Data:** We will integrate data from the Host-Pathogen Interaction Database (HPIDB), IntAct, and BioGRID to create a reference set of known host-virus protein-protein interactions (PPIs). This set will be used to validate our methods and to seed analyses of co-evolution.\n4.  **Structural Data:** We will utilize all available empirical structures from the Protein Data Bank (PDB) and supplement them with high-quality predicted models from the AlphaFold Database for proteins lacking experimental structures.\n\n**Aim 1: A Global Map of Host and Viral Genes Under Positive Selection**\nThis aim will build the foundational dataset of genes involved in molecular conflict.\n*   **Step 1: Host Orthogroup Identification:** We will use OrthoFinder to identify all one-to-one orthologs of protein-coding genes across our 500 host species. We will initially focus on a candidate list of ~2,000 genes with known or predicted immune function, later expanding to a genome-wide scale.\n*   **Step 2: Phylogenomic Pipeline:** For each orthogroup, we will create multiple sequence alignments using MAFFT, perform quality trimming with Gblocks, and reconstruct gene trees using IQ-TREE under appropriate models of evolution. These steps will be automated in a Nextflow pipeline for scalability and reproducibility.\n*   **Step 3: Positive Selection Analysis:** We will systematically apply the codeml program from the PAML package to each orthogroup alignment and gene tree. We will use site-models (M8 vs. M7) to identify specific codons under positive selection (dN/dS > 1) and use the Bayes Empirical Bayes (BEB) analysis to calculate posterior probabilities. To ensure robustness, we will corroborate significant findings with alternative methods like MEME and FUBAR from the HyPhy package. A stringent false discovery rate (FDR < 0.05) will be applied to correct for multiple testing across thousands of genes.\n*   **Step 4: Viral Selection Analysis:** A parallel pipeline will be applied to the VOGs identified from the viral genome dataset. This represents a significant computational challenge and a primary justification for NCEMS support.\n\n**Aim 2: Structural and Co-evolutionary Analysis of Conflict Interfaces**\nThis aim will translate sequence-level data into mechanistic, structural insights.\n*   **Step 1: Structural Mapping:** All statistically significant positively selected sites identified in Aim 1 will be mapped onto the corresponding protein structures (PDB or AlphaFold models).\n*   **Step 2: Hotspot Identification:** We will employ spatial statistics to determine if these sites are randomly distributed or form significant 3D clusters. We will use a density-based clustering algorithm (e.g., DBSCAN) on the 3D coordinates of the alpha-carbons of selected sites to identify 'evolutionary hotspots.'\n*   **Step 3: Co-evolutionary Analysis:** For known and predicted interacting pairs, we will test for correlated evolutionary histories. This will involve comparing branch-specific dN/dS rates along the host and viral phylogenies and using methods like the Mirror-Tree server or custom phylogenetic correlation analyses to detect signatures of tightly coupled evolution.\n\n**Aim 3: Synthesis, Prediction, and Creation of the Molecular Conflict Atlas**\nThis aim will integrate all data into a unified, predictive framework.\n*   **Step 1: Network Construction:** We will build a bipartite network where nodes represent host genes and viral gene families. Edges will be drawn based on evidence of interaction (from databases or co-evolutionary analysis) and weighted by the intensity of positive selection.\n*   **Step 2: Atlas Development:** We will develop a public web portal, the 'Molecular Conflict Atlas,' built on a robust database backend (e.g., PostgreSQL). This portal will feature interactive visualizations of phylogenies, protein structures with selection hotspots highlighted, and network graphs, allowing users to explore the data dynamically.\n\n**Timeline and Milestones**\n*   **Year 1:** Data acquisition and curation. Development and validation of the phylogenomic pipeline on a pilot dataset (e.g., primate genomes and their associated viruses). First in-person working group meeting.\n*   **Year 2:** Full-scale execution of host and viral selection scans. Initial structural mapping and hotspot analysis. Development of the database schema for the Atlas. Mid-project meeting and trainee workshop.\n*   **Year 3:** Co-evolutionary and network analyses. Completion and public launch of the Molecular Conflict Atlas. Manuscript preparation and dissemination of results. Final working group meeting.",
        "expected_outcomes_and_impact": "This project will fundamentally shift our understanding of host-virus evolution from a collection of specific case studies to a comprehensive, data-driven science. By synthesizing the vast repository of public genomic data, we will uncover the emergent, generalizable principles of molecular conflict. The expected outcomes will have a profound and lasting impact on molecular and cellular biology, with direct applications in public health and pandemic preparedness.\n\n**Intended Contributions to the Field**\n1.  **A Foundational Resource: The Molecular Conflict Atlas:** The primary outcome will be a publicly accessible, dynamic web resource that integrates our findings. This 'Atlas' will be the first of its kind, providing a queryable database of host and viral genes under selection, their evolutionary histories, their structural 'hotspots' of conflict, and their interaction networks. It will serve as a hypothesis-generation engine for the entire virology and immunology community for years to come, enabling researchers to instantly look up the evolutionary history of their gene of interest.\n2.  **A Global Inventory of Molecular Arms Dealers:** We will produce the first comprehensive catalog of host defense genes and viral immune evasion factors identified through their evolutionary signatures across the tree of life. This will undoubtedly reveal hundreds of novel players in this conflict, opening up entirely new avenues of experimental research.\n3.  **Discovery of General Principles and Convergent Evolution:** Our synthesis will move beyond individual examples to reveal the 'rules of engagement.' We will determine which host cellular pathways are the most common battlegrounds and whether viruses from different families have convergently evolved similar structural solutions to antagonize them. This will provide a new framework for understanding protein function and adaptation under intense selective pressure.\n\n**Broader Impacts and Applications**\n1.  **Informing Antiviral Therapies:** By identifying the conserved functional hotspots on host proteins that are repeatedly targeted by viruses, we can pinpoint ideal targets for host-directed antiviral therapies. Conversely, identifying the rapidly evolving interfaces on viral proteins can help predict and mitigate viral escape from targeted drugs or vaccines.\n2.  **Pandemic Preparedness and 'Evolutionary Surveillance':** Our framework will enable a new form of surveillance. By analyzing the genome of a newly discovered animal virus, we can assess its evolutionary trajectory. Does it show signatures of positive selection in genes known to interact with human orthologs? This information can help prioritize research and public health resources on animal viruses that show the most evolutionary potential for zoonotic spillover.\n3.  **Training the Next Generation of Data-Savvy Biologists:** This project is an ideal training vehicle. Graduate students and postdoctoral fellows will be at the heart of this cross-disciplinary effort, gaining invaluable skills in phylogenomics, computational biology, big data analysis, and collaborative science. We will host a dedicated workshop to disseminate our methods and train a wider community, directly addressing the research call's goal of developing the future workforce.\n\n**Dissemination and Open Science**\nWe are deeply committed to open science principles. All computational pipelines and custom code will be open-source and shared on GitHub with containerized (Docker/Singularity) workflows for full reproducibility. All curated data and results will be deposited in public repositories (e.g., Dryad) and made available through the Molecular Conflict Atlas. We will disseminate our findings through high-impact publications in leading journals (e.g., Nature, Science, Cell Host & Microbe), presentations at major international conferences (e.g., Gordon Research Conferences on Viruses & Cells, American Society for Virology), and seminars at diverse institutions. The long-term vision is for the Atlas to become a community-sustained resource, with a plan for continued updates as new genomic data becomes available, ensuring its lasting value to the scientific community.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis project whose scope, computational demands, and multidisciplinary nature far exceed the capabilities of any single research lab or existing collaboration. The need to process millions of viral genomes and hundreds of host genomes in a unified phylogenomic framework requires a level of computational infrastructure and dedicated personnel that can only be supported by a dedicated initiative like NCEMS. This budget reflects the resources necessary to coordinate a geographically and scientifically diverse team to tackle this grand challenge.\n\n**Budget Justification**\nThe primary costs are for personnel to drive the project, computational resources to perform the analysis, and travel to facilitate the essential in-person collaboration that sparks innovation.\n\n**1. Personnel (Total: ~$600,000 over 3 years)**\n*   **Postdoctoral Fellows (2 FTE):** We request support for two postdoctoral fellows who will be the intellectual drivers of the project. Postdoc 1 will specialize in phylogenomics and pipeline development. Postdoc 2 will focus on structural biology, network analysis, and development of the Atlas. Their salaries are budgeted at standard NIH levels with fringe benefits.\n*   **Data Manager/Scientist (0.5 FTE):** The scale of the data requires professional management. We request partial support for a data scientist to oversee data acquisition, manage the project's database, and lead the back-end development of the web portal.\n*   **Principal Investigators (1 month summer salary/year for 4 PIs):** Summer salary is requested for the PIs to dedicate significant focused time to project oversight, analysis, and manuscript preparation, particularly during the summer months.\n*   **Graduate Student Support:** We request travel and computational resource funds for two graduate students who will be supported by institutional funds but will be integral to the project.\n\n**2. Travel (Total: ~$90,000)**\n*   **Working Group Meetings:** We request funds for three in-person, two-day meetings for the entire team (5 PIs, 2 postdocs, 2 students). These meetings are critical for data integration, brainstorming, and collaborative analysis. Budget includes airfare, lodging, and meals.\n*   **Conference Dissemination:** Funds are requested for each trainee (postdoc/student) and one PI to attend one major international conference per year to present findings and network with the broader community.\n\n**3. Computational Resources (Total: ~$120,000)**\n*   **High-Performance Computing (HPC):** The pan-viral selection scan is computationally massive. We request funds for purchasing a significant allocation on a national supercomputing cluster (e.g., via XSEDE/ACCESS) or equivalent cloud computing credits (e.g., AWS, Google Cloud). This is a critical need that cannot be met by typical institutional resources.\n*   **Data Storage and Server:** Funds for a dedicated local server with large storage capacity (~100 TB) and high memory for data curation, intermediate analysis, and hosting the Atlas database.\n\n**4. Other Direct Costs (Total: ~$45,000)**\n*   **Publication Costs:** Open-access publication fees for an anticipated 3-4 major manuscripts.\n*   **Trainee Workshop:** Funds to host a 3-day summer workshop in Year 2 for trainees and outside participants, covering materials, and speaker travel.\n\n**5. Indirect Costs (F&A):** Calculated based on the negotiated rates of the lead institution.\n\nThis budget is essential for assembling the necessary human talent and computational power to synthesize a truly global view of host-virus evolution, a goal perfectly aligned with the mission of NCEMS."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_groups_of_scientists_gemini-2.5-pro_10",
      "original_title": "The Cellular Phenotype Project: Predicting Emergent Cellular Behavior by Integrating the Functional Genomics Universe",
      "original_abstract": "The ultimate goal of cellular biology is to predict phenotype from genotype. Achieving this requires a systems-level understanding of how molecular components interact to produce emergent cellular behaviors. This working group proposes an audacious 'grand challenge' project: to build a single, unified computational model of a human cell by synthesizing the entirety of public functional genomics data. We will create a multi-layered knowledge graph integrating data from dozens of major consortia, including ENCODE (regulatory elements), GTEx (tissue-specific expression), the Human Protein Atlas (protein localization), DepMap (CRISPR-based gene dependencies), STRING/BioGRID (protein interactions), Reactome (pathways), and ChEMBL (drug-target data). This graph will represent the most comprehensive in silico model of a cell ever constructed. Our team, comprising world leaders in systems biology, large-scale data integration, and artificial intelligence, will then deploy cutting-edge graph neural networks and other AI methodologies to learn the emergent rules of cellular function from this integrated network. The model will be trained to predict the phenotypic consequences of perturbations, such as the system-wide effects of knocking out a gene or introducing a drug. This project will serve as a community resource, providing a powerful, open-source platform to generate hypotheses, interpret complex datasets, and move biology toward a truly predictive science.",
      "model_name": "gemini-2.5-pro",
      "template_name": "groups_of_scientists",
      "generation_timestamp": "2025-10-29T09:52:32.861271",
      "proposal": {
        "title": "The Cellular Phenotype Project: Predicting Emergent Cellular Behavior by Integrating the Functional Genomics Universe",
        "abstract": "The ultimate goal of cellular biology is to predict phenotype from genotype. Achieving this requires a systems-level understanding of how molecular components interact to produce emergent cellular behaviors. This working group proposes an audacious 'grand challenge' project: to build a single, unified computational model of a human cell by synthesizing the entirety of public functional genomics data. We will create a multi-layered knowledge graph integrating data from dozens of major consortia, including ENCODE (regulatory elements), GTEx (tissue-specific expression), the Human Protein Atlas (protein localization), DepMap (CRISPR-based gene dependencies), STRING/BioGRID (protein interactions), Reactome (pathways), and ChEMBL (drug-target data). This graph will represent the most comprehensive in silico model of a cell ever constructed. Our team, comprising world leaders in systems biology, large-scale data integration, and artificial intelligence, will then deploy cutting-edge graph neural networks and other AI methodologies to learn the emergent rules of cellular function from this integrated network. The model will be trained to predict the phenotypic consequences of perturbations, such as the system-wide effects of knocking out a gene or introducing a drug. This project will serve as a community resource, providing a powerful, open-source platform to generate hypotheses, interpret complex datasets, and move biology toward a truly predictive science.",
        "background_and_significance": "The quest to understand the mapping from genotype to phenotype is the central challenge of modern biology. While the sequencing of the human genome provided the 'parts list' for a cell, understanding how these parts interact to create complex, emergent behaviors remains a formidable task. The post-genomic era has been characterized by an explosion of high-throughput functional genomics data, generated by large-scale international consortia. Projects like the Encyclopedia of DNA Elements (ENCODE) have mapped regulatory regions; the Genotype-Tissue Expression (GTEx) project has cataloged gene expression across human tissues; the Human Protein Atlas (HPA) has systematically determined protein localization; and the Dependency Map (DepMap) has identified genes essential for cancer cell survival. These resources, among many others, offer unprecedented, multi-faceted views into cellular function. However, a critical limitation persists: these monumental datasets are largely analyzed in isolation. Each provides a single, incomplete projection of an immensely complex system. Consequently, our understanding remains fragmented, and our ability to predict the system-level consequences of genetic or chemical perturbations is limited. This fragmentation represents a major barrier to translating genomic information into therapeutic advances and a deeper understanding of human health and disease. Early attempts in systems biology to create holistic models, such as those based on ordinary differential equations (ODEs) or Boolean networks, were powerful for small, well-characterized pathways but failed to scale to the complexity of an entire cell. The subsequent rise of network biology, focusing on protein-protein interaction (PPI) networks, provided a more scalable framework. Seminal work demonstrated that network topology could reveal functional modules and predict gene function. However, these models typically represented only one data type (e.g., physical interactions) and lacked the multi-modal context necessary to capture the full spectrum of cellular regulation. More recently, knowledge graphs (KGs) have emerged as a powerful paradigm for integrating heterogeneous biological data. Projects like Hetionet have successfully integrated disparate databases to predict novel drug-disease relationships, demonstrating the power of connecting diverse information types. Yet, even these efforts have not fully incorporated the richness of modern functional genomics, such as genome-wide genetic dependency screens or comprehensive perturbational transcriptomics. The key gap this proposal addresses is the lack of a single, unified computational framework that can synthesize the full breadth of public functional genomics data to learn the fundamental rules of cellular behavior. This research is critically timely for two reasons. First, we have reached a critical mass of high-quality, publicly available data. The sheer volume and diversity of information from projects like DepMap, GTEx, and ENCODE now make a comprehensive synthesis not only possible but necessary to extract maximal value. Second, recent breakthroughs in artificial intelligence, particularly the development of graph neural networks (GNNs), provide the ideal computational tool for this challenge. GNNs are specifically designed to learn from complex, relational data structured as graphs, enabling them to model the intricate web of interactions within a cell. By combining the vast repository of public data with cutting-edge AI, we are poised to move beyond descriptive, correlational studies towards a truly predictive and mechanistic model of the cell. This project directly aligns with the research call's mission to catalyze multidisciplinary teams to synthesize public data, address a long-standing puzzle in cellular science, and develop innovative analytical strategies that are beyond the capacity of any single research lab.",
        "research_questions_and_hypotheses": "This working group will address the overarching scientific challenge of building a predictive, in silico model of a human cell. Our research is structured around four fundamental questions, each associated with specific, testable hypotheses that will guide our data synthesis and model development efforts. Our primary goal is to create a model that not only recapitulates known biology but also generates novel, experimentally verifiable predictions about emergent cellular behavior. \n\n**Research Question 1 (Integration):** What is the optimal computational architecture for synthesizing the universe of public functional genomics data into a single, multi-layered, and biologically coherent knowledge graph? While many databases exist, their integration is non-trivial, requiring harmonization of identifiers, ontologies, and data structures. We will explore different graph schemas to determine how best to represent diverse molecular entities (genes, proteins, compounds) and their complex relationships (regulation, interaction, localization, etc.).\n*   **Hypothesis 1.1:** A heterogeneous knowledge graph, incorporating at least ten distinct molecular data types (e.g., gene expression, protein interactions, genetic dependencies, chromatin accessibility) and representing entities as distinct node types, will yield significantly higher performance in downstream predictive tasks compared to models built on homogeneous graphs or smaller subsets of data. We predict that the richness of the integrated data will allow the model to learn more robust and generalizable biological rules.\n*   **Hypothesis 1.2:** The inclusion of context-specific edges, such as tissue-specific expression (from GTEx) or cell-line-specific gene dependencies (from DepMap), will enable the model to make more accurate context-dependent predictions than a generic, context-agnostic model.\n\n**Research Question 2 (Learning):** Can advanced graph neural network (GNN) architectures learn the complex, non-linear patterns of molecular interactions from this integrated graph to effectively model the rules of cellular function?\n*   **Hypothesis 2.1:** A Graph Transformer or Relational Graph Convolutional Network (R-GCN) model, trained on our Cellular Phenotype Knowledge Graph (CP-KG), will outperform current state-of-the-art machine learning models (e.g., random forests, gradient boosting) and models trained on individual data types in predicting held-out experimental data. Specifically, we predict a >15% improvement in the Area Under the Receiver Operating Characteristic (AUROC) curve for predicting gene essentiality.\n*   **Hypothesis 2.2:** The node embeddings (latent representations) learned by the GNN will capture deep, biologically meaningful information. We hypothesize that genes with similar embeddings, even if not directly connected in the input graph, will share functional roles. We will test this by evaluating whether the cosine similarity of gene embeddings correlates with semantic similarity in Gene Ontology (GO) terms.\n\n**Research Question 3 (Prediction):** How accurately can the trained model predict the phenotypic outcomes of novel genetic and chemical perturbations that were not seen during training?\n*   **Hypothesis 3.1:** Our model can predict the essentiality of genes in cell lines held out from the training set with high accuracy. We will validate this by training the model on 80% of the cell lines in DepMap and testing its performance on the remaining 20%, demonstrating its ability to generalize across different genetic backgrounds.\n*   **Hypothesis 3.2:** The model can predict cellular responses to drugs. By representing drugs as nodes and training the model on known drug sensitivity data (e.g., from the GDSC database), we hypothesize it can predict the efficacy of novel compounds or the response of untested cell lines to existing drugs. We will validate these predictions against new, publicly released drug screen data.\n\n**Research Question 4 (Emergence):** Can we interrogate the trained model to uncover novel, higher-order principles of cellular organization and emergent biological phenomena?\n*   **Hypothesis 4.1:** Using model interpretability techniques (e.g., GNNExplainer), we can identify the specific subgraphs and molecular pathways that are most influential for a given prediction (e.g., why a cell is sensitive to a particular drug). We hypothesize these identified pathways will be enriched for known mechanisms of action and will also reveal novel, off-target effects.\n*   **Hypothesis 4.2:** In silico perturbation experiments—simulating the effect of a gene knockout by computationally removing its corresponding node from the graph—will accurately predict the resulting changes in the expression of other genes. We will validate these predictions against experimental data from perturbational datasets like the Connectivity Map (CMap L1000). This will demonstrate the model's capacity to predict the cascading, system-wide effects that define emergent phenotypes.",
        "methods_and_approach": "Our methodology is organized into three synergistic aims, forming a comprehensive plan to construct, train, and interrogate a predictive model of the cell. This project is exclusively computational and relies entirely on the synthesis of publicly available data, perfectly aligning with the research call's focus. The collaborative, multi-lab structure is essential for integrating the diverse expertise required for each aim.\n\n**Aim 1: Construct the Cellular Phenotype Knowledge Graph (CP-KG).**\nThis foundational aim focuses on the aggregation, harmonization, and integration of disparate public datasets into a unified, machine-readable graph structure. This task requires significant bioinformatic expertise and is a community-scale effort.\n*   **Data Sources:** We will integrate data from over a dozen major consortia and databases. Key sources include: (1) **Genomic/Epigenomic:** ENCODE (transcription factor binding sites, chromatin accessibility), JASPAR (TF motifs); (2) **Transcriptomic:** GTEx (baseline tissue expression), TCGA (cancer expression), Connectivity Map L1000 (gene expression post-perturbation); (3) **Proteomic:** Human Protein Atlas (subcellular localization, abundance), BioPlex/HuRI (physical protein-protein interactions); (4) **Functional/Genetic:** DepMap/Project Achilles (genome-wide CRISPR gene dependency scores), ClinVar (disease-associated variants); (5) **Pathways/Interactions:** Reactome, KEGG (curated pathways), STRING, BioGRID (functional and physical interaction networks); (6) **Chemical Biology:** ChEMBL, DrugBank (drug-target relationships), GDSC/CTRP (drug sensitivity screens).\n*   **Integration and Harmonization:** We will establish a formal graph schema with defined node and edge types. Node types will include Gene, Protein, Chemical Compound, Disease, Pathway, and Cell Line. Edge types will represent relationships like `regulates_expression`, `physically_interacts_with`, `is_localized_in`, `is_essential_in`, `targets`, and `is_associated_with`. All biological entities will be mapped to standardized identifiers (e.g., Ensembl, UniProt, ChEBI) to resolve ambiguity. The integrated graph will be stored in a Neo4j database for efficient querying and exported to formats compatible with machine learning libraries like PyTorch Geometric (PyG) and Deep Graph Library (DGL).\n\n**Aim 2: Develop and Train Predictive AI Models on the CP-KG.**\nThis aim leverages our team's expertise in artificial intelligence to build models that learn the rules of cellular function from the CP-KG.\n*   **Model Architecture:** We will primarily use Heterogeneous Graph Neural Networks (GNNs), such as the Heterogeneous Graph Transformer (HGT), which are specifically designed to handle the diverse node and edge types in our CP-KG. The HGT model uses a meta-path-based attention mechanism to learn the importance of different relationship types when aggregating information across the graph.\n*   **Training Strategy:** We will employ a two-stage training process. First, we will use self-supervised pre-training, where the model learns fundamental biological relationships by performing tasks like link prediction (predicting masked interactions) on the entire graph. This allows the model to learn rich, general-purpose embeddings for all nodes. Second, we will fine-tune the pre-trained model on specific supervised tasks using held-out datasets. Key tasks include: (1) **Gene Essentiality Prediction:** Predicting the CRISPR dependency score for each gene-cell line pair from DepMap. (2) **Drug Sensitivity Prediction:** Predicting the IC50 value for a given drug-cell line pair from GDSC. (3) **Perturbation Outcome Prediction:** Predicting the L1000 differential expression signature resulting from a specific genetic or chemical perturbation.\n*   **Validation and Controls:** All models will be evaluated using rigorous k-fold cross-validation. To ensure we are testing for true generalization, folds will be stratified to hold out entire cell lines, drugs, or gene families, preventing trivial memorization. As a baseline control, we will compare our GNN's performance against simpler models (e.g., logistic regression, random forest) trained on non-integrated data features. We will also perform temporal validation by training on older data releases and testing on newly discovered interactions or functional annotations.\n\n**Aim 3: Interrogate the Model to Uncover Emergent Biological Principles.**\nWith a validated model, this aim focuses on using it as a virtual laboratory to generate novel hypotheses.\n*   **Model Interpretability:** We will use post-hoc explanation methods like GNNExplainer and Integrated Gradients to dissect model predictions. For example, when the model predicts a gene is essential, we will identify the minimal subgraph of interactions (e.g., a specific pathway) that was most influential in that decision.\n*   **In Silico Perturbations:** We will systematically simulate perturbations by altering the graph structure or node features. For a gene knockout, we will remove the corresponding gene node and its edges and use the GNN to predict the resulting changes in the embeddings and properties of all other nodes in the graph. This allows us to simulate the cascading, system-wide effects of perturbations and identify critical nodes that mediate cellular robustness.\n\n**Timeline:**\n*   **Year 1 (Months 1-12):** Data acquisition, harmonization, and construction of CP-KG v1.0. Development and benchmarking of data processing pipelines. Initial implementation of the HGT model architecture. Milestone: Public release of CP-KG v1.0 schema and data.\n*   **Year 2 (Months 13-24):** Self-supervised pre-training and supervised fine-tuning of models for all predictive tasks. Rigorous cross-validation and benchmarking. First manuscript detailing the CP-KG and predictive framework. Milestone: Public release of trained models and open-source code.\n*   **Year 3 (Months 25-36):** In-depth model interrogation, large-scale in silico perturbation studies, and biological interpretation of findings. Development of a user-friendly web portal for community access. Final manuscripts and dissemination at international conferences. Milestone: Launch of the Cellular Phenotype Project web portal.",
        "expected_outcomes_and_impact": "The Cellular Phenotype Project is a high-risk, high-reward endeavor designed to create a paradigm shift in how cellular biology is studied. Its success will yield transformative outcomes and have a profound impact on both fundamental science and translational medicine. This project's scale and ambition directly address the research call's goal of tackling grand challenges through community-scale data synthesis.\n\n**Expected Outcomes and Contributions to the Field:**\n1.  **A Foundational Community Resource:** The primary deliverable will be the Cellular Phenotype Knowledge Graph (CP-KG) and the associated predictive models. This will be the most comprehensive, integrated in silico model of a human cell ever created. We will adhere strictly to Open Science principles, making the graph, all model code, and a user-friendly web portal publicly available. This resource will democratize systems-level analysis, enabling researchers worldwide to query the integrated knowledge base, generate hypotheses, and perform virtual experiments that would be impossible in a wet lab setting.\n2.  **A New Paradigm for Predictive Biology:** This project will move the field beyond descriptive genomics and correlational network analysis towards a truly predictive science. By demonstrating that an AI model can learn the rules of cellular function from integrated data, we will establish a new framework for understanding the genotype-phenotype map. This will serve as a blueprint for future efforts to model more complex biological systems, such as tissues, organs, or even entire organisms.\n3.  **Novel Biological Discoveries:** The model is not just a repository of known information but a discovery engine. We expect to identify thousands of novel, high-confidence predictions, including: new functions for uncharacterized genes, previously unknown pathways mediating drug response, unexpected cross-talk between signaling pathways, and key molecular players that govern cellular robustness and disease states. These in silico discoveries will provide a rich source of testable hypotheses for the broader experimental community.\n\n**Broader Impacts and Applications:**\n*   **Accelerating Drug Discovery and Development:** The pharmaceutical industry invests billions in identifying new drug targets and predicting patient responses. Our platform will provide a powerful tool for in silico target validation by predicting the system-wide consequences of inhibiting a specific protein. It can also be used to predict drug efficacy and toxicity across diverse genetic backgrounds, stratify patient populations for clinical trials, and identify novel drug repurposing opportunities.\n*   **Advancing Personalized Medicine:** The ultimate vision is to create patient-specific models. By inputting a patient's genomic and transcriptomic data, future iterations of this model could predict their individual susceptibility to disease or their likely response to a panel of treatments, paving the way for truly personalized therapeutic strategies.\n*   **Training the Next Generation of Scientists:** This project is an ideal training environment that sits at the intersection of biology, computer science, and data science. Graduate students and postdocs involved will gain invaluable cross-disciplinary skills, preparing them to be leaders in the future data-driven workforce. We will further amplify this impact by hosting annual workshops and releasing tutorials to train the wider community in using our tools and methods.\n\n**Dissemination and Sustainability:**\nOur dissemination strategy is multi-faceted. We will publish our findings in high-impact journals such as *Nature*, *Science*, and *Cell*, with methods-focused papers in journals like *Nature Methods* or *Nature Machine Intelligence*. All publications will be open-access. We will present our work at leading international conferences (e.g., ISMB, RECOMB, AACR). The long-term sustainability of the project will be ensured by building an active user community around our open-source tools and by seeking follow-up funding from federal agencies (e.g., NIH) to expand the model to include new data types (metabolomics, single-cell omics) and to build disease-specific versions (e.g., a 'Cancer Cell Phenotype Project'). This project will establish a living, evolving resource that will grow in value as more public data becomes available.",
        "budget_and_resources": "The proposed research represents a large-scale, multidisciplinary effort that is beyond the capabilities of a single research laboratory or existing collaboration. The scope of data integration, the computational intensity of the AI modeling, and the need for diverse, coordinated expertise necessitate the support and resources of the NCEMS Working Group program. The budget reflects the personnel and infrastructure required to execute this ambitious three-year project.\n\n**Personnel (Total: $1,250,000)**\nThis is the largest component of the budget, reflecting the collaborative and training-focused nature of the project.\n*   **Co-Principal Investigators (4 x 1 month summer salary/year):** $240,000. To support the dedicated time of the four PIs from different institutions, who bring essential, non-overlapping expertise in systems biology, bioinformatics, machine learning, and proteomics.\n*   **Postdoctoral Fellows (3 FTEs for 3 years):** $630,000. Three postdoctoral researchers will form the core research team. One will specialize in large-scale data integration and bioinformatics; the second will focus on GNN model development and training; the third will lead model interrogation, validation, and biological interpretation.\n*   **Graduate Students (2 FTEs for 3 years):** $240,000. Two graduate students will support the postdocs, taking lead on specific sub-projects, such as the integration of a particular data modality or the development of a specific predictive task.\n*   **Project Manager / Software Engineer (0.5 FTE for 3 years):** $140,000. A part-time professional is critical for managing the complex project timeline across multiple institutions, overseeing data releases, and leading the development of the public-facing web portal.\n\n**Computational Resources (Total: $150,000)**\n*   **High-Performance Computing (HPC) / Cloud Credits:** $120,000. Training graph neural networks on a graph with billions of nodes and edges is computationally prohibitive on local hardware. This allocation will provide access to a national supercomputing center or commercial cloud provider (e.g., AWS, Google Cloud) for access to high-memory, multi-GPU nodes required for model training and large-scale in silico experiments.\n*   **Data Storage:** $30,000. To cover the costs of robust, long-term storage (~100 TB) for the raw data, the integrated CP-KG, model checkpoints, and simulation results.\n\n**Other Direct Costs (Total: $100,000)**\n*   **Travel:** $60,000. To support biannual in-person meetings of the entire working group to foster deep collaboration and strategic planning. This also includes funds for trainees and PIs to present findings at one major international conference per year.\n*   **Workshops and Training:** $25,000. To host one annual open workshop to train the broader scientific community on our tools and to foster a collaborative ecosystem around the project.\n*   **Publication Costs:** $15,000. To cover open-access fees for an anticipated 3-4 major publications.\n\n**Total Direct Costs:** $1,500,000\n**Indirect Costs (F&A) (Calculated at a hypothetical 50% rate):** $750,000\n**Total Requested Budget:** $2,250,000\n\n**Justification for NCEMS Support:** This project is the epitome of a community-scale synthesis project. No single lab possesses the combined expertise in functional genomics, network biology, and cutting-edge AI, nor the resources to undertake the massive data integration and computational modeling effort required. The NCEMS framework is essential to bring together this diverse team, provide the necessary computational infrastructure, and support the collaborative environment needed to tackle this 'grand challenge' in cellular biology."
      }
    }
  ]
}