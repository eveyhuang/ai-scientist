{
  "session_id": "gemini-2.5-pro_single_scientist",
  "template_name": "single_scientist",
  "generation_timestamp": "2025-10-28T16:37:07.912308",
  "total_proposals": 10,
  "proposals": [
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_01",
      "original_title": "The Chromatin Compiler: Emergence of Cellular Identity from Genome Architecture",
      "original_abstract": "A fundamental question in biology is how a single genome can give rise to hundreds of distinct, stable cell types. This emergent property, cellular identity, is encoded not just in the DNA sequence but in its dynamic, three-dimensional folding within the nucleus. We propose to create a 'Chromatin Compiler,' a predictive framework that synthesizes vast, publicly available datasets to understand how chromatin architecture dictates cell fate. This Working Group will integrate multi-modal data including Hi-C, ChIA-PET, and Micro-C from the 4D Nucleome portal to map genome topology; single-cell ATAC-seq and RNA-seq from cell atlases (e.g., Human Cell Atlas) to link chromatin state to gene expression at cellular resolution; and ChIP-seq data from ENCODE and Roadmap Epigenomics for histone modifications and transcription factor binding. Our transdisciplinary team, comprising polymer physicists, computational biologists, developmental biologists, and machine learning experts, will develop novel graph-based neural networks and dynamical systems models. These models will aim to uncover the 'grammatical rules' governing how interactions between regulatory elements, topological associating domains (TADs), and nuclear compartments orchestrate complex gene expression programs during differentiation. The project's success will yield a powerful in silico tool to predict how perturbations to genome architecture—such as those caused by structural variants in cancer or developmental disorders—lead to aberrant cell identities. This work will provide unprecedented insight into the emergent logic of cellular programming and establish a new paradigm for studying genome function.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:23:47.383679",
      "proposal": {
        "title": "The Chromatin Compiler: Emergence of Cellular Identity from Genome Architecture",
        "abstract": "A fundamental question in biology is how a single genome can give rise to hundreds of distinct, stable cell types. This emergent property, cellular identity, is encoded not just in the DNA sequence but in its dynamic, three-dimensional folding within the nucleus. We propose to create a 'Chromatin Compiler,' a predictive framework that synthesizes vast, publicly available datasets to understand how chromatin architecture dictates cell fate. This Working Group will integrate multi-modal data including Hi-C, ChIA-PET, and Micro-C from the 4D Nucleome portal to map genome topology; single-cell ATAC-seq and RNA-seq from cell atlases (e.g., Human Cell Atlas) to link chromatin state to gene expression at cellular resolution; and ChIP-seq data from ENCODE and Roadmap Epigenomics for histone modifications and transcription factor binding. Our transdisciplinary team, comprising polymer physicists, computational biologists, developmental biologists, and machine learning experts, will develop novel graph-based neural networks and dynamical systems models. These models will aim to uncover the 'grammatical rules' governing how interactions between regulatory elements, topological associating domains (TADs), and nuclear compartments orchestrate complex gene expression programs during differentiation. The project's success will yield a powerful in silico tool to predict how perturbations to genome architecture—such as those caused by structural variants in cancer or developmental disorders—lead to aberrant cell identities. This work will provide unprecedented insight into the emergent logic of cellular programming and establish a new paradigm for studying genome function.",
        "background_and_significance": "The establishment and maintenance of cellular identity is a cornerstone of multicellular life and a classic example of an emergent biological phenomenon. How hundreds of distinct, stable cell types arise from a single zygotic genome remains one of the most fundamental questions in biology. While the central dogma describes the flow of information from DNA to RNA to protein, it is the complex regulatory landscape controlling this flow that specifies cell type. This landscape is not merely linear; it is profoundly shaped by the three-dimensional (3D) architecture of the genome within the nucleus. The past decade has revolutionized our understanding of this architecture, revealing a hierarchical organization from nucleosomes to chromatin loops, Topologically Associating Domains (TADs), and large-scale compartments (Lieberman-Aiden et al., 2009; Dixon et al., 2012). This structure is not random; it is intimately linked to function. TADs, for instance, act as insulated neighborhoods that constrain enhancer-promoter interactions, ensuring genes are regulated appropriately. The disruption of these boundaries by structural variants is now a known mechanism in developmental disorders and cancer (Lupiáñez et al., 2015). Concurrently, large-scale international consortia such as the Encyclopedia of DNA Elements (ENCODE), the Roadmap Epigenomics Project, the 4D Nucleome (4DN) Program, and the Human Cell Atlas (HCA) have generated an unprecedented wealth of publicly available data. We now have access to multi-modal datasets spanning 3D genome conformation (Hi-C, Micro-C), chromatin accessibility (ATAC-seq), histone modifications and transcription factor binding (ChIP-seq), and gene expression (RNA-seq) across a vast array of human cell types, often at single-cell resolution. This data explosion presents a monumental opportunity for synthesis. Despite these advances, critical gaps in our knowledge persist, preventing a truly predictive understanding of genome function. First, our view of genome architecture is largely static. We have snapshots of different cell types but lack a comprehensive, dynamic model of how architecture reconfigures during cell fate transitions. Second, existing computational models are often siloed, focusing on predicting one aspect of genome organization (e.g., TAD boundaries from sequence) but failing to integrate multi-modal data to predict a holistic, functional outcome like a cell-type-specific gene expression program. Third, and most critically, our understanding remains descriptive rather than predictive. We can characterize the architectural differences between a neuron and a lymphocyte, but we cannot yet formulate a set of generalizable 'rules' or a computational model that can predict, *ab initio*, the functional consequences of a specific architectural perturbation. There is no 'compiler' to translate the language of genome structure into the language of cellular function. This project is both important and timely because it directly addresses these gaps. It is important because a predictive model of genome function would transform our ability to interpret genetic variation, understand disease mechanisms, and engineer cell fates for therapeutic purposes. It is timely because the necessary ingredients are finally in place: the requisite large-scale public data is available, and advances in machine learning, particularly deep learning on graphs, provide the powerful analytical tools needed to tackle this complexity. This proposal outlines a community-scale synthesis project to build this 'Chromatin Compiler,' a framework that will learn the grammatical rules of genome architecture and provide a quantitative, predictive model of how cellular identity emerges from the dynamic folding of the genome.",
        "research_questions_and_hypotheses": "Our overarching goal is to develop and validate a predictive computational framework, the 'Chromatin Compiler,' that formalizes the relationship between 3D genome architecture and the emergent property of cellular identity. This ambitious objective is broken down into three specific, interconnected research questions, each with testable hypotheses.\n\n**Research Question 1 (RQ1): What are the fundamental architectural principles and regulatory 'grammar' that define a stable cellular identity?**\nWe posit that the complex, multi-modal data describing a cell's chromatin state can be distilled into a core set of predictive features and interaction rules. \n*   **Hypothesis 1a:** A stable cellular identity is encoded by a unique, multi-scale 'signature' of chromatin interactions. This signature is not defined by any single feature but by the synergistic combination of specific long-range enhancer-promoter contacts, the precise positioning and insulation strength of TAD boundaries, and the global pattern of genomic compartmentalization. \n*   **Hypothesis 1b:** A finite set of 'grammatical rules' governs these interactions. We hypothesize that cell-type-specific transcription factors (TFs) and epigenetic marks act as a regulatory 'syntax,' determining which genomic loci can interact and modulating the probability and stability of these interactions. For example, the presence of H3K27ac and pioneer TFs at distal elements may be a rule that 'permits' looping to a promoter marked by H3K4me3.\n*   **Testing and Validation:** We will test these hypotheses by building a graph neural network (GNN) that integrates multi-modal data (Hi-C, ATAC-seq, ChIP-seq) to classify cell types. Success will be defined by high classification accuracy (>95%) on held-out test data. We will then use model interpretability techniques (e.g., GNNExplainer) to extract the features and interactions (the 'grammar') most salient for each cell-type classification, thereby validating our hypotheses.\n\n**Research Question 2 (RQ2): How do dynamic changes in genome architecture orchestrate cell fate transitions during differentiation?**\nWe propose that differentiation is not a random walk but a directed, programmatic process of architectural remodeling.\n*   **Hypothesis 2a:** Cell differentiation follows a stereotyped trajectory through a high-dimensional 'chromatin state space.' This trajectory is characterized by the ordered and sequential formation, dissolution, and rewiring of specific chromatin loops and TADs, which precedes and directs changes in gene expression.\n*   **Hypothesis 2b:** Master developmental transcription factors (e.g., PAX6 in neurogenesis) act as primary drivers of these architectural changes. They function by binding to key nodes in the chromatin network, initiating local changes in accessibility and recruiting chromatin remodeling complexes, which then propagate through the network to establish a new, stable architectural state.\n*   **Testing and Validation:** We will use publicly available time-series multi-omic data from well-characterized *in vitro* differentiation systems (e.g., embryonic stem cells to cardiomyocytes). We will develop a spatio-temporal GNN or a neural ODE model to learn the transition rules between successive time points. The model will be validated by its ability to accurately predict future chromatin and expression states from earlier ones. We will perform *in silico* 'knockout' experiments by removing the signal of a master TF from the input data and predicting the resulting deviation from the normal differentiation trajectory, comparing these predictions to published experimental results.\n\n**Research Question 3 (RQ3): Can the Chromatin Compiler predict the functional consequences of architectural perturbations, such as pathogenic structural variants (SVs), on cellular identity?**\nWe aim to move from descriptive correlation to causal prediction, using our model as a tool for *in silico* genetics.\n*   **Hypothesis 3a:** Pathogenic non-coding SVs exert their effects primarily by altering the 'grammatical rules' of chromatin folding. Deletions of TAD boundaries, for example, lead to 'enhancer hijacking' by allowing enhancers from one domain to ectopically activate oncogenes in an adjacent domain. Inversions and translocations can create novel, disease-driving enhancer-promoter contacts.\n*   **Hypothesis 3b:** Our trained Chromatin Compiler can accurately predict the aberrant gene expression patterns resulting from a given SV. By inputting the altered genomic sequence and structure, the model will predict the new 3D contact map and, consequently, which genes will be misregulated.\n*   **Testing and Validation:** We will curate a validation set of well-characterized pathogenic SVs from literature and databases (e.g., COSMIC, ClinVar). We will implement these SVs *in silico* within our model's framework and predict their impact on gene expression in the relevant cell type. Predictions will be quantitatively compared against ground-truth experimental data (e.g., RNA-seq, 4C-seq) from patient-derived cells or engineered cell lines harboring these SVs. The model's predictive power will be assessed by the correlation between predicted and observed changes in gene expression.",
        "methods_and_approach": "This project is founded on the synthesis of public data and the development of novel computational methods, executed by a transdisciplinary Working Group. Our approach is organized into four synergistic aims that directly address our research questions.\n\n**Working Group Structure and Collaboration:** The project's success hinges on the deep integration of diverse expertise. The team comprises four PIs and their trainees: **PI 1 (Computational Biology)** will lead data acquisition, harmonization, and the development of reproducible analysis pipelines. **PI 2 (Machine Learning)** will spearhead the design, implementation, and training of the core deep learning models. **PI 3 (Polymer Physics)** will provide theoretical guidance on chromatin biophysics, ensuring our models are physically plausible and aiding in the interpretation of architectural changes. **PI 4 (Developmental Biology)** will provide crucial biological context, guiding the selection of cellular systems and validating the biological relevance of model predictions. The group will convene for biannual, intensive in-person workshops, supplemented by weekly virtual meetings, to foster continuous collaboration. Trainees will be co-mentored and will spend time in partner labs to gain cross-disciplinary skills.\n\n**Aim 1: Unified Multi-modal Chromatin Data Compendium.** The foundation of our project is a comprehensive, harmonized database of publicly available human genomics data. We will systematically mine repositories including 4D Nucleome, ENCODE, Roadmap Epigenomics, GEO, and the Human Cell Atlas. Data types will include: 1) **3D Architecture:** Hi-C, Micro-C, ChIA-PET; 2) **1D Epigenomics:** ChIP-seq for key histone marks (H3K27ac, H3K4me3, H3K27me3) and architectural proteins (CTCF, RAD21), ATAC-seq, and DNase-seq; and 3) **Transcriptomics:** Bulk and single-cell RNA-seq. We will focus on curating data from at least 100 distinct cell types and several well-documented differentiation time courses (e.g., ESCs to neurons, HSCs to hematopoietic lineages). All data will be processed through a single, containerized (Docker/Singularity) and version-controlled (Nextflow) pipeline to ensure maximal reproducibility and eliminate batch effects from variable processing. The final output will be a unified data resource where all modalities are mapped to a common genomic coordinate system, ready for machine learning integration.\n\n**Aim 2: Modeling Static Cellular Identity with Graph Neural Networks (GNNs).** To address RQ1, we will represent the genome as a graph. Genomic regions (e.g., 10kb bins) will serve as nodes. Node features will be vectors of 1D epigenomic signals from Aim 1. Edges between nodes will be weighted by their 3D contact frequency from Hi-C data. We will develop a Graph Attention Network (GAT), a GNN architecture adept at learning the importance of different neighbors in a graph. The model will be trained on our compendium of cell types with the objective of predicting the cell type label from the input chromatin graph. The trained model will serve as our initial 'Chromatin Compiler.' We will employ model interpretation tools (e.g., saliency maps, attention weight analysis) to dissect the trained model and extract the predictive genomic features and interactions that constitute the 'grammatical rules' of each cell identity.\n\n**Aim 3: Modeling Dynamic Cell Fate Transitions.** To address RQ2, we will extend our static model to capture dynamics. Using time-series data from differentiation, we will build a spatio-temporal GNN. This model will incorporate a recurrent component (like an LSTM or GRU) or be framed as a Neural Ordinary Differential Equation (Neural ODE) to learn the continuous-time evolution of the chromatin state. The model will learn a function `f` that maps the chromatin graph at time `t` to the graph at time `t+1`. Validation will involve predicting later time points from earlier ones and assessing the model's ability to reconstruct the known differentiation trajectory. This dynamic model will allow us to identify critical 'bifurcation points' in the chromatin state space where cell fate decisions are made and to pinpoint the key TF and regulatory events that drive these transitions through *in silico* perturbation experiments.\n\n**Aim 4: Predictive Modeling of Architectural Perturbations.** To address RQ3, we will build a predictive module on top of our trained models. This module will take a structural variant (SV) as input (e.g., coordinates of a deletion, inversion, or translocation). It will algorithmically modify the genomic graph representation to reflect the SV. The perturbed graph will then be fed into the trained 'Chromatin Compiler' from Aim 2. We will add a regression head to the GNN, trained to predict gene expression levels from the surrounding chromatin graph state. The model's output will be a prediction of the new 3D contact landscape and the resulting changes in gene expression for all genes near the SV. We will validate these predictions against a curated set of known pathogenic SVs, comparing our *in silico* results with published experimental data.\n\n**Timeline:** **Year 1:** Completion of Aim 1 data compendium; development and initial training of the static GNN model (Aim 2). **Year 2:** Refinement and interpretation of the static model; development and training of the dynamic model (Aim 3); first major publication on the static compiler. **Year 3:** Validation of the dynamic model; development and validation of the SV prediction module (Aim 4); release of the open-source 'Chromatin Compiler' software package and web portal; final publications.",
        "expected_outcomes_and_impact": "The 'Chromatin Compiler' project is a high-risk, high-reward endeavor poised to fundamentally shift the paradigm of genome biology from a descriptive to a predictive science. Its success will yield profound intellectual contributions, significant broader impacts on human health, and a powerful new platform for the scientific community, directly aligning with the goals of the NCEMS research call.\n\n**Intellectual Merit and Contribution to the Field:** The primary outcome of this work will be a validated, predictive computational framework that quantitatively links 3D genome architecture to cellular identity. This represents a significant leap beyond current correlative studies. We will deliver: 1) A comprehensive, harmonized multi-modal atlas of the human epigenome and 3D genome. 2) A novel class of machine learning models (spatio-temporal GNNs) tailored for integrative genomics. 3) A catalogue of the architectural 'grammatical rules' that define and maintain specific cell types. 4) The first dynamic models that map the architectural trajectories of cell differentiation. By formalizing how a complex biological property—cellular identity—emerges from the collective interactions of molecular components, this project provides a concrete solution to a central question in emergence phenomena, a core focus of this call. It will establish a new field of 'predictive 4D genomics.'\n\n**Broader Impacts and Biomedical Applications:** The long-term impact of the Chromatin Compiler on biomedical research will be substantial. The ability to predict the functional consequences of genomic variants is a central goal of personalized medicine. Our framework will provide a powerful tool for: 1) **Interpreting Disease Genomics:** Over 90% of disease-associated variants from GWAS lie in non-coding regions. Our tool will help prioritize and mechanistically explain how these variants, particularly structural variants, contribute to disease by altering genome architecture. 2) **Understanding Developmental Disorders:** Many congenital diseases are caused by mutations that disrupt long-range gene regulation. The Compiler will provide a means to diagnose and understand these 'chromatinopathies.' 3) **Cancer Biology:** We can use the model to understand how somatic SVs in cancer genomes lead to oncogene activation or tumor suppressor inactivation through mechanisms like enhancer hijacking, providing insights for novel therapeutic strategies. 4) **Synthetic Biology:** A deep understanding of the genome's operating principles is a prerequisite for rational cell engineering. The 'rules' we uncover could guide the design of synthetic chromosomes and customized cell types for regenerative medicine.\n\n**Alignment with NCEMS Goals:** This project is perfectly aligned with the NCEMS mission. It is a community-scale synthesis project that is impossible for a single lab to undertake due to the immense data scale and the need for tightly integrated, diverse expertise (ML, physics, computational and developmental biology). It leverages exclusively public data to answer a fundamental question. We are committed to **Open Science**; all code will be open-source (GitHub), data will be shared via public repositories (Zenodo), and models will be accessible through a user-friendly web portal. Our dissemination plan includes high-impact, open-access publications and presentations at major international conferences. Furthermore, the project is designed to **Train the Next Generation** of data-savvy scientists. Trainees will be at the heart of the collaboration, gaining unique interdisciplinary skills that are in high demand. We will host an annual open hackathon to disseminate our tools and train the broader community, tapping into diverse new talent. The collaborative partnership we have assembled spans multiple institutions, career stages, and scientific disciplines, ensuring a rich and innovative research environment.",
        "budget_and_resources": "The proposed research requires a level of coordinated effort, computational power, and diverse expertise that extends far beyond the capacity of a single research lab or a traditional multi-PI grant. The NCEMS Working Group mechanism is uniquely suited to provide the necessary support and collaborative infrastructure for this community-scale synthesis project.\n\n**Justification for NCEMS Support:** The primary challenge of this project is the integration of two distinct elements: massive, heterogeneous datasets and deep, diverse intellectual expertise. A single lab may possess expertise in one area but not all (e.g., machine learning but not developmental biology). NCEMS support is critical for assembling our geographically dispersed team of experts in computational biology, machine learning, polymer physics, and developmental biology, and providing the dedicated resources for them to function as a single, cohesive unit. Furthermore, the computational costs associated with training deep learning models on petabyte-scale genomic data are substantial and often exceed the budget of standard research grants. NCEMS funding will provide the necessary cloud computing resources and support for data management, which are central to the project's success. Finally, the NCEMS framework for regular in-person meetings and workshops is essential for fostering the intense, cross-disciplinary brainstorming required to develop truly innovative models and interpret their biological meaning.\n\n**Budget Breakdown (3-Year Total):**\n*   **Personnel ($480,000):** This constitutes the largest portion of the budget, reflecting our focus on training and dedicated effort. This includes salary and benefits for two full-time Postdoctoral Fellows who will be the primary technical leads, and stipend support for two Graduate Students. It also includes one month of summer salary for each of the four PIs to ensure dedicated time for management, supervision, and intellectual leadership.\n*   **Computational Resources ($150,000):** This is a critical component for a project of this scale. Funds are requested for cloud computing credits (e.g., AWS/GCP) for GPU/TPU access required for training large-scale graph neural networks. This also includes costs for long-term cloud storage of the harmonized data compendium.\n*   **Travel ($90,000):** To facilitate deep collaboration, we request funds for biannual, three-day in-person meetings of the entire Working Group (4 PIs, 4 trainees). Funds are also allocated for each trainee and PI to attend one major international conference per year to present their findings and network with the broader community.\n*   **Training and Dissemination ($60,000):** This includes costs to organize and host an annual two-day workshop/hackathon open to the wider scientific community, providing training on our tools and methods. It also covers costs for developing and maintaining a public web portal for the Chromatin Compiler and open-access publication fees for an anticipated 4-5 manuscripts.\n*   **Indirect Costs (IDC):** Calculated based on the respective institutional rates, applied to the direct costs.\n\n**Total Direct Costs:** $780,000\n\n**Institutional Resources:** The PIs' home institutions will provide significant in-kind support, including faculty and administrative salaries, office and lab space, and access to institutional high-performance computing clusters for model development and data preprocessing, thereby leveraging existing infrastructure and demonstrating strong institutional commitment to the project."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_02",
      "original_title": "Deconstructing the Condensatome: A Predictive Atlas of Liquid-Liquid Phase Separation in Cellular Function and Disease",
      "original_abstract": "The discovery of biomolecular condensates, membraneless organelles formed via liquid-liquid phase separation (LLPS), has revolutionized our understanding of cellular organization. These dynamic compartments emerge from multivalent interactions among proteins and nucleic acids, concentrating components to regulate key processes like transcription and stress response. However, the sequence-level rules governing which molecules enter which condensate, and how condensate properties yield specific functions, remain elusive. This Working Group will build a predictive atlas of the 'condensatome' by synthesizing disparate public data. We will integrate protein sequence and domain information (UniProt, Pfam), experimentally determined and predicted structures (PDB, AlphaFold DB), protein-protein interaction networks (STRING, BioGRID), and LLPS-specific databases (PhaSepDB, DrLLPS). Our team of biophysicists, protein biochemists, cell biologists, and AI specialists will employ advanced machine learning models, including large language models trained on protein sequences and graph neural networks on interaction data, to predict LLPS propensity and composition. We will correlate these predictions with cell-type-specific transcriptomic and proteomic data to understand how the cellular context modulates condensate formation. A key goal is to identify the 'emergent grammar' of LLPS and predict how mutations, suchas those found in neurodegenerative diseases like ALS and Alzheimer's, disrupt condensate dynamics and lead to pathological aggregation. This project will produce a publicly accessible, predictive platform that will transform our ability to understand and therapeutically target the emergent biology of cellular compartmentalization.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:25:21.827266",
      "proposal": {
        "title": "Deconstructing the Condensatome: A Predictive Atlas of Liquid-Liquid Phase Separation in Cellular Function and Disease",
        "abstract": "The discovery of biomolecular condensates, membraneless organelles formed via liquid-liquid phase separation (LLPS), has revolutionized our understanding of cellular organization. These dynamic compartments emerge from multivalent interactions among proteins and nucleic acids, concentrating components to regulate key processes like transcription and stress response. However, the sequence-level rules governing which molecules enter which condensate, and how condensate properties yield specific functions, remain elusive. This Working Group will build a predictive atlas of the 'condensatome' by synthesizing disparate public data. We will integrate protein sequence and domain information (UniProt, Pfam), experimentally determined and predicted structures (PDB, AlphaFold DB), protein-protein interaction networks (STRING, BioGRID), and LLPS-specific databases (PhaSepDB, DrLLPS). Our team of biophysicists, protein biochemists, cell biologists, and AI specialists will employ advanced machine learning models, including large language models trained on protein sequences and graph neural networks on interaction data, to predict LLPS propensity and composition. We will correlate these predictions with cell-type-specific transcriptomic and proteomic data to understand how the cellular context modulates condensate formation. A key goal is to identify the 'emergent grammar' of LLPS and predict how mutations, suchas those found in neurodegenerative diseases like ALS and Alzheimer's, disrupt condensate dynamics and lead to pathological aggregation. This project will produce a publicly accessible, predictive platform that will transform our ability to understand and therapeutically target the emergent biology of cellular compartmentalization.",
        "background_and_significance": "The paradigm of cellular organization has been fundamentally reshaped by the discovery of biomolecular condensates, membraneless compartments formed through liquid-liquid phase separation (LLPS). These dynamic assemblies, such as the nucleolus, stress granules, and processing bodies, emerge from multivalent interactions among proteins and nucleic acids, creating distinct biochemical environments that regulate a vast array of cellular processes. This emergent phenomenon of self-organization challenges the classical view of a cell governed solely by membrane-bound organelles. The formation of condensates is primarily driven by proteins containing intrinsically disordered regions (IDRs) and low-complexity domains (LCDs), which engage in a network of transient, specific interactions. The 'sticker-and-spacer' model provides a powerful conceptual framework, where 'stickers' (e.g., aromatic residues, charged motifs) mediate interactions and 'spacers' dictate the dynamics and material properties of the resulting condensate. The functional consequences are profound; by concentrating specific molecules and excluding others, condensates can enhance reaction rates, sequester components, and act as hubs for signal transduction and gene regulation. The significance of LLPS extends to pathophysiology. A growing body of evidence links aberrant phase transitions, particularly the conversion of dynamic liquid condensates into solid, pathological aggregates, to the etiology of numerous diseases. In neurodegenerative disorders like amyotrophic lateral sclerosis (ALS) and frontotemporal dementia (FTD), mutations in RNA-binding proteins such as FUS and TDP-43 promote their aggregation within stress granules. Similarly, the pathological aggregation of Tau protein in Alzheimer's disease is now understood through the lens of LLPS. Despite this rapid progress, the field faces critical knowledge gaps that hinder our ability to predict and control condensate behavior. Current understanding of the 'condensatome'—the complete set of condensates and their components—is fragmented and largely descriptive. We lack a comprehensive, predictive model that can answer fundamental questions: What are the precise sequence and structural features that determine a protein's partitioning into a specific condensate? How does the cellular context, including protein concentrations and post-translational modifications, modulate the composition and function of the condensatome? And how do disease-associated mutations disrupt this delicate equilibrium? Existing computational tools have made initial strides but are limited. Predictors like PScore, catGRANULE, and FuzDrop primarily rely on amino acid sequence features to predict a generic propensity for LLPS. While useful, they often fail to capture the specificity of interactions that dictate which proteins co-assemble. Furthermore, they do not typically integrate structural information or the broader context of protein-protein interaction networks. Databases such as PhaSepDB and DrLLPS are invaluable repositories of experimentally validated phase-separating proteins, but they represent a sparse sampling of the proteome and lack the integrative framework needed for system-wide prediction. This project is both important and timely due to a confluence of factors. First, the explosion of publicly available biological data—from comprehensive protein sequences (UniProt) and interaction maps (BioGRID, STRING) to the revolutionary availability of accurate predicted structures for entire proteomes (AlphaFold DB)—provides the raw material for an unprecedented data synthesis effort. Second, recent breakthroughs in artificial intelligence, particularly the development of protein large language models (pLMs) and graph neural networks (GNNs), offer powerful new methodologies to learn complex patterns from multi-modal biological data. This Working Group will address these gaps by synthesizing these disparate data streams within a unified machine learning framework. By building a predictive atlas of the condensatome, we will move beyond describing individual components to understanding the emergent, system-level logic of cellular organization, providing a transformative resource for basic science and therapeutic discovery.",
        "research_questions_and_hypotheses": "This Working Group is organized around a central overarching question: Can we decipher the multi-modal 'grammar'—encoded in protein sequence, structure, and interaction networks—that governs the assembly, composition, and function of the cellular condensatome? To address this, we have formulated three specific, interconnected research aims, each with testable hypotheses and clear validation strategies.\n\n**Aim 1: Develop a unified, multi-modal machine learning framework to predict protein LLPS propensity and condensate-specific partitioning.**\nThis aim addresses the fundamental limitation of current predictors by integrating diverse data types to achieve a more holistic and accurate model of LLPS. \n*   **Research Question 1.1:** Can a model that integrates protein sequence, 3D structure, and network context significantly outperform single-modality approaches in predicting a protein's intrinsic ability to undergo LLPS?\n*   **Hypothesis 1.1:** We hypothesize that an integrative machine learning model, combining embeddings from a protein language model (pLM) for sequence, a geometric deep learning model for structure (from AlphaFold DB), and a graph neural network (GNN) for protein-protein interaction (PPI) network topology, will yield superior predictive performance for LLPS propensity. The synergy of these modalities will capture complementary information: pLMs excel at context-rich sequence patterns, structural models reveal surface properties and multivalency, and GNNs encode the system-level context of potential binding partners.\n*   **Validation 1.1:** The model will be trained on a curated set of known phase-separating proteins (positives) from PhaSepDB and LLPSDB and a rigorously selected set of negatives. Performance will be evaluated using receiver operating characteristic (ROC) and precision-recall (PR) curves under strict cross-validation and on held-out test sets. We will benchmark our multi-modal approach against a suite of state-of-the-art, single-modality predictors.\n*   **Research Question 1.2:** Beyond general propensity, can we predict the specific condensate(s) a protein is likely to partition into?\n*   **Hypothesis 1.2:** We hypothesize that the learned multi-modal representations contain signatures specific to different condensate types (e.g., nucleolus, stress granule, P-body). By training a multi-label classifier on these representations using proteins with known subcellular locations from high-quality, condensate-specific proteomic datasets, we can accurately predict a protein's 'condensate fingerprint'.\n*   **Validation 1.2:** We will use proteins with experimentally verified, exclusive localization to specific condensates as a gold-standard test set. We will assess classification accuracy and use model interpretability techniques (e.g., SHAP) to identify the key sequence, structural, and network features that distinguish different condensate families.\n\n**Aim 2: Elucidate how cellular context modulates the composition and dynamics of the condensatome.**\nLLPS is not a fixed property but an emergent behavior sensitive to the cellular environment. This aim seeks to model that context-dependency.\n*   **Research Question 2.1:** How do variations in protein and RNA expression levels across different cell types and physiological states alter the predicted landscape of biomolecular condensates?\n*   **Hypothesis 2.1:** We hypothesize that the probability of a condensate forming and its final composition are critically dependent on the stoichiometry of its components. By integrating cell-type-specific transcriptomic and proteomic data (from GTEx, Human Protein Atlas) into our GNN framework, we can create 'context-aware' predictions. We predict that condensates will be more stable in cell types where their core scaffold proteins are highly expressed, and their client composition will reflect the abundance of available binding partners.\n*   **Validation 2.1:** We will generate predicted condensatomes for well-characterized cell lines (e.g., U2OS, HeLa, HEK293). These predictions will be systematically compared against published, experimentally determined condensate proteomes for these same cell lines, assessing the overlap and correlation of component enrichment.\n\n**Aim 3: Predict the functional impact of genetic variants on condensate integrity and link disruptions to disease pathology.**\nThis aim leverages our predictive framework to bridge the gap between genotype and phenotype in condensate-related diseases.\n*   **Research Question 3.1:** Can our model accurately quantify how disease-associated mutations alter a protein's LLPS behavior and its interactions within the condensatome?\n*   **Hypothesis 3.1:** We hypothesize that pathogenic mutations found in neurodegenerative diseases (e.g., in FUS, TDP-43, Tau from ClinVar) will receive high 'disruption scores' from our model. These scores will reflect predicted changes in LLPS propensity, interaction partner affinity, or a shift towards aggregation-prone states. In contrast, benign polymorphisms from gnomAD will have minimal predicted impact.\n*   **Validation 3.1:** We will perform a large-scale in silico saturation mutagenesis analysis on key condensate proteins. Our predicted disruption scores will be benchmarked against published experimental data (e.g., changes in saturation concentration, droplet fusion dynamics, or fibrillization rates) for a subset of these mutations. We will also assess the ability of our score to discriminate between known pathogenic and benign variants from clinical databases, a critical test of its translational potential.",
        "methods_and_approach": "This project will be executed by a multidisciplinary Working Group composed of three synergistic teams, ensuring that diverse expertise is leveraged to tackle this complex data synthesis challenge. The teams are: Team A (Data Curation & Integration), comprising computational biologists; Team B (Machine Learning & Model Development), led by AI specialists; and Team C (Biological Interpretation & Validation), consisting of cell biologists and biophysicists. Trainees (graduate students and postdocs) will be embedded within each team and will participate in regular cross-team meetings and annual in-person workshops to foster collaboration and provide comprehensive training, directly aligning with the NCEMS mission.\n\n**Phase 1: Unified Data Curation and Integration (Months 1-6)**\nThe foundation of this project is the creation of a comprehensive, harmonized knowledge graph. Team A will be responsible for this critical first step.\n*   **Data Sources:** We will aggregate data from a wide array of public repositories. \n    *   **Sequence & Function:** UniProtKB/Swiss-Prot (canonical human sequences, PTMs, functional annotations), Pfam and InterPro (protein domain definitions).\n    *   **Structure:** The Protein Data Bank (PDB) for experimentally determined structures and the AlphaFold DB for high-quality, comprehensive predicted structures of the human proteome.\n    *   **Interactions:** BioGRID and the Human Reference Interactome (HuRI) for curated physical protein-protein interactions (PPIs). STRING will be used for functional associations, filtered for high-confidence evidence channels.\n    *   **LLPS Ground Truth:** PhaSepDB, LLPSDB, and DrLLPS will be integrated to form a gold-standard set of known phase-separating proteins, their interacting partners, and experimental conditions.\n    *   **Cellular Context:** Gene- and protein-level expression data from the Genotype-Tissue Expression (GTEx) project and the Human Protein Atlas will provide tissue- and cell-type-specific context.\n    *   **Genetic Variation:** ClinVar and gnomAD will be our primary sources for disease-associated and population variants, respectively.\n*   **Integration Pipeline:** Data will be programmatically downloaded, parsed, and cleaned. We will implement a rigorous entity resolution protocol to map identifiers across databases. The integrated data will be loaded into a Neo4j graph database, where proteins are nodes and their diverse relationships (e.g., physical interaction, domain co-occurrence, functional association) are represented as typed edges with associated properties (e.g., confidence scores, experimental evidence).\n\n**Phase 2: Multi-Modal Model Development and Training (Months 7-18)**\nTeam B, in close consultation with Team C, will develop and validate our core predictive models.\n*   **Feature Engineering:** We will generate rich, multi-modal feature representations for every human protein.\n    *   **Sequence Embeddings:** We will use a state-of-the-art, pre-trained protein large language model (pLM), such as ESM-2, to generate fixed-length vector embeddings from protein sequences. These embeddings capture latent evolutionary and biophysical information.\n    *   **Structural Embeddings:** Using the AlphaFold structures, we will employ a geometric deep learning model (e.g., a graph convolutional network operating on the protein structure graph) to learn features describing surface charge distribution, hydrophobicity, and the spatial arrangement of potential 'sticker' residues.\n    *   **Network Embeddings:** A graph neural network (GNN), such as GraphSAGE, will be applied to our integrated PPI network to generate embeddings that encode a protein's network topology and local neighborhood.\n*   **Aim 1 Model (Propensity & Partitioning):** The sequence, structural, and network embeddings will be concatenated and passed through a deep neural network classifier. This model will first be trained as a binary classifier for general LLPS propensity. Subsequently, using condensate-specific proteomic data as labels, it will be adapted for multi-label classification to predict partitioning into specific condensates.\n*   **Aim 2 Model (Context-Awareness):** To model cellular context, we will create tissue-specific PPI graphs by weighting the edges based on the co-expression levels of the interacting proteins in that tissue. The GNN will be re-trained on these weighted graphs to produce context-specific network embeddings, which will then be used to make context-dependent predictions of condensate stability and composition.\n*   **Aim 3 Model (Mutation Impact):** We will build an in silico mutagenesis pipeline. For a given variant, the mutated protein sequence will be generated. Its 3D structure will be re-predicted using efficient tools like ColabFold. The new sequence and structure will be passed through our feature extractors and trained models. The difference between the output scores of the mutant and wild-type proteins will yield a 'disruption score,' quantifying the mutation's predicted impact on LLPS.\n\n**Phase 3: Atlas Construction, Validation, and Dissemination (Months 19-24)**\nTeam C will lead the biological interpretation of model outputs and the development of the public-facing resource.\n*   **The Condensatome Atlas:** We will build a user-friendly web portal with an intuitive interface. Users will be able to search for their protein of interest and view its predicted LLPS score, its predicted condensate partners, the features driving the prediction, and the predicted impact of known clinical variants. The portal will feature network visualizations of predicted condensate compositions across different cell types.\n*   **Open Science Commitment:** In adherence with NCEMS principles, all software will be open-sourced on GitHub with permissive licenses. All curated datasets, trained model weights, and genome-wide predictions will be deposited in Zenodo. A comprehensive API will be developed to allow programmatic access to the atlas data, facilitating its integration into other analysis pipelines.\n\n**Timeline and Milestones:**\n*   **Month 6:** Completion of the integrated data warehouse and feature extraction pipeline.\n*   **Month 12:** Version 1.0 of the multi-modal LLPS propensity and partitioning predictor is trained, benchmarked, and internally validated.\n*   **Month 18:** Context-aware and mutation-impact prediction models are finalized. Beta version of the web portal is deployed for internal testing.\n*   **Month 24:** Public launch of the Condensatome Atlas web portal and API. Submission of primary manuscripts describing the resource and key biological findings.",
        "expected_outcomes_and_impact": "The successful completion of this project will yield transformative outcomes and exert a significant, long-term impact on the molecular and cellular biosciences. Our work will not only generate a powerful new resource for the scientific community but will also fundamentally advance our understanding of the principles of cellular organization.\n\n**Intellectual Merit and Contributions to the Field:**\n1.  **A Predictive, Mechanistic Atlas of the Condensatome:** The primary outcome will be the 'Condensatome Atlas,' a first-of-its-kind, publicly accessible platform. Unlike existing static databases, our atlas will be predictive and dynamic. It will provide the community with system-wide predictions of LLPS propensity, condensate composition, and context-dependent modulation for the entire human proteome. This resource will empower researchers to move from observation to hypothesis, enabling them to query how their protein of interest behaves within the complex landscape of cellular compartmentalization.\n2.  **Deciphering the 'Grammar' of LLPS:** By applying interpretable AI techniques to our multi-modal models, we will deconstruct the complex interplay of factors that govern condensate assembly. This will allow us to move beyond simple correlations (e.g., IDR content) to a more nuanced understanding of the emergent 'grammar' of LLPS. We expect to identify novel sequence motifs, define the specific structural contexts that confer multivalency, and uncover the network topologies that stabilize distinct condensates. This represents a major step towards a comprehensive, first-principles understanding of biological self-assembly.\n3.  **A New Paradigm for Data Synthesis in Cell Biology:** This project will serve as a blueprint for applying integrative, multi-modal machine learning to other complex problems in biology. Our methodology for harmonizing sequence, structure, network, and expression data to predict an emergent cellular phenotype will be broadly applicable to areas such as signal transduction, protein complex formation, and metabolic network analysis.\n\n**Broader Impacts and Applications:**\n1.  **Accelerating Disease Research and Therapeutic Development:** The atlas will have immediate translational relevance. By providing a tool to systematically predict the impact of mutations on condensate integrity, we will help researchers prioritize variants of unknown significance and elucidate disease mechanisms. For neurodegenerative diseases like ALS and Alzheimer's, our platform can identify critical nodes in the network of pathological phase transitions, revealing novel targets for therapeutic intervention aimed at restoring condensate homeostasis or preventing aggregation.\n2.  **Enabling Rational Design in Synthetic Biology:** A deep, predictive understanding of the rules of LLPS will empower the field of synthetic biology. Engineers will be able to use our atlas and models to design synthetic proteins and RNA molecules that form artificial condensates with bespoke properties. These synthetic organelles could be used to create novel bioreactors, sequester toxic metabolites, or control complex metabolic pathways within engineered cells.\n3.  **Training a New Generation of Data-Savvy Biologists:** In line with the NCEMS mission, our Working Group is structured to provide exceptional training opportunities. Graduate students and postdocs will gain hands-on experience at the cutting edge of computational biology, machine learning, and data science, all while being deeply embedded in a collaborative, transdisciplinary environment. Through our open-source tools and public workshops, we will disseminate these skills to the broader community, helping to build the future data-savvy workforce.\n\n**Dissemination and Open Science:**\nOur commitment to open science is unwavering. All outcomes will be made immediately and broadly available. We plan to publish our findings in high-impact, open-access journals (e.g., Cell, Nature Methods). The Condensatome Atlas web portal and its underlying API will be freely accessible without restriction. All source code will be maintained in a public GitHub repository, and all curated data and model weights will be deposited in Zenodo with detailed documentation to ensure full reproducibility. We will actively promote the resource through presentations at major international conferences (e.g., ASCB, ISMB) and by hosting virtual tutorials and workshops.\n\n**Long-Term Vision:**\nThe NCEMS support is critical to catalyze this ambitious synthesis effort, which is far beyond the scope of a single research lab. The collaborative network and computational infrastructure established by this project will create a durable hub for condensate research. We envision the atlas as a living resource, sustained long-term by the lead institution and updated with new data from the community. This foundational work will seed numerous follow-up projects, including experimental validation of novel predictions and the pursuit of large-scale center grants to further explore the therapeutic potential of targeting the condensatome.",
        "budget_and_resources": "The proposed budget is designed to support a highly collaborative, computationally intensive, three-PI Working Group for a 24-month period. The resources requested are essential for the project's success and reflect the community-scale nature of the research, which surpasses the capabilities of any single lab or standard research grant.\n\n**1. Personnel (Approximately 65% of total budget):**\nThe primary investment is in dedicated personnel who will drive the project's research and development activities.\n*   **Postdoctoral Fellows (3.0 FTEs):** We request support for three full-time postdoctoral fellows, one based in each collaborating PI's lab. Each postdoc will lead one of the core teams: Data Integration, ML Model Development, and Biological Interpretation/Validation. Their focused effort is critical for executing the ambitious data synthesis and modeling tasks.\n*   **Graduate Students (3 x 0.5 FTEs):** To fulfill the NCEMS training mission, we request partial support (stipend and tuition) for three graduate students. These trainees will be mentored by the PIs and postdocs, contributing directly to the project's aims while receiving unparalleled cross-disciplinary training in data science and molecular biology.\n*   **Principal Investigator Effort (3 x 0.5 summer months/year):** We request modest summer salary support for the three PIs to ensure they have dedicated time for project management, intensive mentoring, cross-team coordination, and manuscript preparation.\n\n**2. Computational Resources (Approximately 15%):**\nTraining state-of-the-art machine learning models on proteome-scale datasets is a significant computational expense.\n*   **Cloud Computing Credits / HPC Access:** We request a substantial allocation for GPU-enabled cloud computing (e.g., AWS or Google Cloud) or for purchasing dedicated GPU nodes for an institutional high-performance computing (HPC) cluster. This is essential for training protein language models and large-scale graph neural networks.\n*   **Data Storage and Servers:** Funds are requested for a dedicated server to host the integrated Neo4j database and the public-facing web portal, including costs for long-term, robust data storage and backup (estimated 5-10 TB).\n\n**3. Travel (Approximately 10%):**\nFostering genuine collaboration is paramount.\n*   **Working Group Meetings:** We request funds to hold in-person meetings for the entire team (PIs, postdocs, students) twice per year. These intensive, multi-day workshops are indispensable for brainstorming, resolving technical challenges, and strengthening the collaborative fabric of the group.\n*   **Conference Travel:** Support is requested for all trainees and PIs to attend one major international conference annually to present their findings, disseminate the project's outcomes, and receive feedback from the scientific community.\n\n**4. Other Direct Costs (Approximately 5%):**\n*   **Publication Fees:** Funds are allocated to cover open-access publication charges for the anticipated 3-4 high-impact manuscripts resulting from this work, ensuring adherence to open science principles.\n*   **Software Licenses:** Costs for any necessary commercial software licenses (e.g., for data visualization or specialized analysis tools).\n\n**5. Indirect Costs (F&A):**\nIndirect costs are calculated based on the federally negotiated rates for each of the three participating institutions.\n\n**Justification for NCEMS Support:**\nThis project is uniquely suited for the NCEMS program. The synthesis of vast, disparate public datasets to address a fundamental question of emergent biological organization requires a collaborative, multi-lab effort with diverse expertise that cannot be assembled or funded through traditional mechanisms like an NIH R01. The need for dedicated computational resources (HPC/cloud) and specialized personnel (postdocs with hybrid expertise) goes beyond the scope of a single lab. NCEMS support is the catalyst required to bring together this specific team, provide them with the necessary resources, and enable a transformative project that will produce a lasting, open resource for the entire biosciences community."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_03",
      "original_title": "The Metabolic Symphony of the Tumor Microenvironment: Emergence of Drug Resistance and Immunosuppression",
      "original_abstract": "The tumor microenvironment (TME) is a complex ecosystem where cancer cells, immune cells, fibroblasts, and other stromal cells engage in intricate metabolic cross-talk. Emergent properties of this system, such as profound immunosuppression and therapeutic resistance, are major barriers to effective cancer treatment. This Working Group aims to deconstruct this 'metabolic symphony' by building a spatially-resolved, multi-cellular model of TME metabolism. We will synthesize publicly available single-cell and spatial transcriptomics data (from TCGA, GEO, and specialized atlases), proteomics data, and public metabolomics datasets. Our collaborative team, uniting cancer biologists, immunologists, computational systems biologists, and spatial data scientists, will develop novel computational methods to integrate these data layers. We will use this integrated data to parameterize community-scale, genome-scale metabolic models (GEMs) that simulate nutrient competition and metabolite exchange between every cell type within a spatially-defined TME. The goal is to identify critical metabolic dependencies and feedback loops that drive emergent system-level behaviors. For example, how does lactate produced by glycolytic cancer cells reprogram macrophages to an immunosuppressive M2 phenotype? How do cancer-associated fibroblasts fuel tumor growth? By simulating metabolic perturbations, we will predict novel therapeutic strategies that disrupt these symbiotic interactions, aiming to break drug resistance and reinvigorate anti-tumor immunity. This synthesis project will provide a systems-level blueprint of TME metabolism, offering a powerful new lens through which to view and treat cancer.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:26:47.744116",
      "proposal": {
        "title": "The Metabolic Symphony of the Tumor Microenvironment: Emergence of Drug Resistance and Immunosuppression",
        "abstract": "The tumor microenvironment (TME) is a complex ecosystem where cancer cells, immune cells, fibroblasts, and other stromal cells engage in intricate metabolic cross-talk. Emergent properties of this system, such as profound immunosuppression and therapeutic resistance, are major barriers to effective cancer treatment. This Working Group aims to deconstruct this 'metabolic symphony' by building a spatially-resolved, multi-cellular model of TME metabolism. We will synthesize publicly available single-cell and spatial transcriptomics data (from TCGA, GEO, and specialized atlases), proteomics data, and public metabolomics datasets. Our collaborative team, uniting cancer biologists, immunologists, computational systems biologists, and spatial data scientists, will develop novel computational methods to integrate these data layers. We will use this integrated data to parameterize community-scale, genome-scale metabolic models (GEMs) that simulate nutrient competition and metabolite exchange between every cell type within a spatially-defined TME. The goal is to identify critical metabolic dependencies and feedback loops that drive emergent system-level behaviors. For example, how does lactate produced by glycolytic cancer cells reprogram macrophages to an immunosuppressive M2 phenotype? How do cancer-associated fibroblasts fuel tumor growth? By simulating metabolic perturbations, we will predict novel therapeutic strategies that disrupt these symbiotic interactions, aiming to break drug resistance and reinvigorate anti-tumor immunity. This synthesis project will provide a systems-level blueprint of TME metabolism, offering a powerful new lens through which to view and treat cancer.",
        "background_and_significance": "The treatment of cancer has been revolutionized by immunotherapies, yet a significant fraction of patients fails to respond, largely due to the complex and immunosuppressive nature of the tumor microenvironment (TME). The TME is not merely a passive scaffold for malignant cells but a dynamic, multicellular ecosystem where cancer cells, immune cells, fibroblasts, and endothelial cells engage in a complex network of interactions. A growing body of evidence indicates that metabolism is a central organizing principle of this ecosystem. The concept of metabolic reprogramming in cancer, first described by Otto Warburg nearly a century ago, has evolved from a cancer-cell-centric view to one that encompasses the entire TME. It is now clear that the metabolic activities of all cellular constituents are deeply intertwined, creating a unique metabolic landscape that promotes tumor growth, angiogenesis, and immune evasion. Key examples of this metabolic cross-talk are well-documented. Cancer cells, through aerobic glycolysis, consume vast amounts of glucose and secrete lactate, leading to a nutrient-poor, acidic milieu that cripples the function of effector T cells, which also rely on glucose for their anti-tumor activity. This lactate is not merely a waste product; it acts as a signaling molecule that polarizes macrophages towards an immunosuppressive, pro-tumorigenic M2 phenotype. Similarly, the enzyme indoleamine 2,3-dioxygenase (IDO), often expressed by cancer or stromal cells, depletes local tryptophan, an amino acid essential for T cell proliferation, thereby inducing immune tolerance. Beyond competition, metabolic symbiosis is also a critical feature. The 'reverse Warburg effect' posits that cancer-associated fibroblasts (CAFs) undergo glycolysis and export lactate and other nutrients, which are then taken up and utilized by adjacent cancer cells for oxidative phosphorylation, effectively 'feeding' the tumor. These individual examples, while insightful, represent only single threads in a much larger, more intricate tapestry. The fundamental limitation of current research is its reductionist approach, which typically focuses on a single pathway or the interaction between two cell types. This fails to capture the emergent properties of the TME—system-level behaviors like robust immunosuppression and therapeutic resistance that arise from the collective, nonlinear interactions of all components. The advent of high-throughput omics technologies, particularly single-cell and spatial transcriptomics, has provided unprecedented, high-resolution snapshots of the TME's cellular composition and spatial organization. We can now identify dozens of cell subtypes and map their locations relative to one another. However, these data are largely descriptive. A critical gap exists in our ability to translate these static maps into a dynamic, mechanistic understanding of the metabolic fluxes and interactions that govern TME function. We lack integrated, predictive models that can simulate how the spatial arrangement of cells dictates local metabolic niches and how perturbations to one component ripple through the entire system. This proposal is both important and timely because it directly addresses this gap. The explosion of publicly available, multi-modal TME datasets from consortia like TCGA, CPTAC, and the Human Tumor Atlas Network provides a rich, untapped resource for data synthesis. Concurrently, computational systems biology approaches, such as genome-scale metabolic modeling, have matured to a point where they can be adapted to tackle multi-cellular systems. By convening a multidisciplinary Working Group of cancer immunologists, computational biologists, and spatial data scientists, we are uniquely positioned to synthesize these disparate data streams into the first spatially-resolved, multi-cellular metabolic model of the TME. This project will move the field beyond correlative observations to a predictive, systems-level understanding of TME metabolism, uncovering novel vulnerabilities and paving the way for next-generation metabolic therapies to overcome immunotherapy resistance.",
        "research_questions_and_hypotheses": "The overarching goal of this Working Group is to develop and apply a novel computational framework that synthesizes public multi-omics data to build a predictive, spatially-resolved model of tumor microenvironment (TME) metabolism. This model will serve as an in silico laboratory to dissect the emergent properties of the TME, specifically immunosuppression and therapeutic resistance, and to identify novel metabolic targets to reverse these states. To achieve this goal, we will address three central research questions, each associated with a specific, testable hypothesis. \n\n**Research Question 1: How does the spatial organization of distinct cell types within the TME orchestrate local metabolic niches and give rise to emergent, system-level immunosuppressive functions?**\nThe arrangement of cells is not random; it creates micro-domains with unique metabolic properties. A glycolytic tumor cell cluster will create a different metabolic environment than a region rich in oxidative CAFs. We hypothesize that the functional state of immune cells is critically dependent on their spatial 'metabolic zip code'.\n*   **Hypothesis 1:** The emergence of functionally distinct, immunosuppressive TME domains is a spatially-determined phenomenon driven by localized gradients of key immunomodulatory metabolites (e.g., lactate, kynurenine, adenosine, protons). We predict that our spatially-resolved model will reveal specific, recurring spatial motifs, such as the co-localization of lactate-secreting cancer cells with M2-polarized, arginase-expressing macrophages. We will test this by simulating metabolite concentrations across the spatial map and correlating these predictions with the observed functional states of immune cells inferred from transcriptomic data. We further predict that in silico disruption of these spatial motifs (e.g., by moving T-cells away from lactate 'hotspots') will revert their simulated metabolic and functional state towards an anti-tumor phenotype.\n\n**Research Question 2: What are the critical metabolic dependencies and symbiotic cross-feeding interactions between cancer cells and the diverse stromal and immune cell populations that are essential for sustained tumor growth and immune evasion?**\nTumors are complex ecosystems that thrive on metabolic cooperation. Identifying the keystone interactions that support the entire system is crucial for developing effective therapies. We aim to move beyond known interactions to create a comprehensive, unbiased map of these metabolic exchanges.\n*   **Hypothesis 2:** Cancer-associated fibroblasts (CAFs) and immunosuppressive myeloid cells function as metabolic 'hubs,' reprogramming their own metabolism to supply limiting nutrients and anabolic precursors to cancer cells, thereby sustaining proliferation under nutrient stress. We hypothesize that our multi-cellular model will identify specific, high-flux metabolic exchange pathways (e.g., transfer of specific amino acids, lipids, or TCA cycle intermediates) from CAFs or M2 macrophages to cancer cells. We will test this by performing in silico 'knockout' experiments. We predict that blocking the efflux of a key metabolite from CAFs in our model will have a more profound inhibitory effect on cancer cell biomass production than blocking the synthesis of that same metabolite within the cancer cell itself, thus revealing a critical, non-cell-autonomous dependency.\n\n**Research Question 3: Can we computationally identify and validate novel metabolic checkpoints that, when targeted, synergize with existing immunotherapies to overcome resistance by remodeling the TME?**\nThe ultimate goal is to translate our systems-level understanding into therapeutic strategies. The metabolic state of the TME is a key determinant of response to immune checkpoint inhibitors (ICIs).\n*   **Hypothesis 3:** The metabolic signature of the TME is a robust predictor of response to immunotherapy, and targeted metabolic interventions can sensitize non-responsive tumors to ICIs. We will parameterize our models using public datasets from patients treated with ICIs, separating them into responder and non-responder cohorts. We predict the non-responder models will exhibit distinct metabolic features, such as higher lactate production and greater nutrient competition between tumor and T-cells. We will test this by simulating the effects of various metabolic inhibitors in our 'non-responder' models. We predict that in silico inhibition of specific enzymes (e.g., lactate dehydrogenase A in cancer cells, arginase 1 in myeloid cells, or fatty acid oxidation pathways in regulatory T-cells) will not only directly impact tumor cells but will also remodel the simulated TME to be more permissive to T-cell function (e.g., increased glucose/glutamine availability for T-cells), thereby creating a synergistic anti-tumor effect when combined with simulated anti-PD-1 therapy. The deliverable will be a ranked list of metabolic targets predicted to have the highest synergistic potential.",
        "methods_and_approach": "This project is founded on the synthesis of publicly available data and the collaborative expertise of our multidisciplinary Working Group, which unites computational systems biologists, cancer immunologists, and spatial data scientists. Our approach is organized into three sequential but interconnected aims, designed to build a robust, predictive model of TME metabolism from the ground up. The entire workflow will be developed as an open-source, reproducible pipeline.\n\n**Data Sources and Curation:**\nWe will exclusively use publicly available data, obviating the need for new data generation. Our primary data sources include:\n1.  **Single-Cell RNA-seq (scRNA-seq):** Datasets from repositories like the Gene Expression Omnibus (GEO), the Human Tumor Atlas Network (HTAN), and The Cancer Genome Atlas (TCGA). We will focus on major cancer types with rich data availability, such as melanoma, non-small cell lung cancer, and breast cancer, ensuring we have data from both primary tumors and metastases.\n2.  **Spatial Transcriptomics (ST):** We will leverage publicly available datasets generated using platforms like 10x Genomics Visium, which provide gene expression data overlaid on a tissue histology image. These datasets are critical for providing the spatial scaffold for our models.\n3.  **Proteomics:** Data from the Clinical Proteomic Tumor Analysis Consortium (CPTAC) will be used to constrain our metabolic models, as enzyme abundance is often more directly related to metabolic flux than mRNA levels.\n4.  **Metabolomics:** Publicly available mass spectrometry imaging (MSI) and bulk metabolomics data from resources like the Metabolomics Workbench will be used not for model construction, but for validation of our model's predictions of spatial metabolite distributions.\n5.  **Knowledge Bases:** Our models will be built upon the foundation of human metabolic reconstructions like Recon3D and HMR2.0, with pathway information from KEGG and Reactome.\n\n**Aim 1: Data Harmonization and Multi-Modal Integration (Months 1-9)**\nThe first major challenge is to process and integrate these diverse data types. We will develop a standardized computational pipeline to:\n*   Process raw scRNA-seq data using tools like Seurat or Scanpy for quality control, normalization, and cell type annotation based on canonical marker genes. This will yield cell-type-specific expression profiles for all major TME constituents (e.g., malignant cells, T-cell subtypes, macrophages, CAFs, endothelial cells).\n*   Process ST data using tools like Squidpy and Giotto. This involves aligning ST spots to the histology image and performing cell-type deconvolution, an analytical process that estimates the proportion of each cell type within each spatial spot, by integrating the scRNA-seq data.\n*   Integrate proteomics data by mapping proteins to their corresponding genes and using these abundance levels to further refine expression estimates.\nThe output of Aim 1 will be a unified data object for each tumor sample, containing cell type definitions, their spatial locations, and their associated transcriptomic and proteomic profiles.\n\n**Aim 2: Construction of Spatially-Resolved, Multi-Cellular Genome-Scale Metabolic Models (GEMs) (Months 7-24)**\nThis aim forms the core of our proposal. We will build our TME model in three steps:\n1.  **Cell-Type-Specific GEMs:** For each cell type identified in Aim 1, we will generate a context-specific metabolic model. Using its unique expression profile, we will 'prune' a generic human GEM (e.g., Recon3D) to retain only the reactions and pathways active in that cell type, using established algorithms like iMAT or GIMME. This results in a unique metabolic network for each cell type.\n2.  **Spatial Scaffolding:** We will use the processed ST data to create a 2D grid that represents the tissue slice. Each location (spot) on this grid will be populated with the specific GEMs corresponding to the cell types found there, weighted by their predicted abundance from the deconvolution step.\n3.  **Community Simulation:** We will implement a community metabolic modeling framework, such as COMETS (Computation of Microbial Ecosystems in Time and Space), adapted for a mammalian system. This framework simulates the metabolic activity of all cells simultaneously. All cells in the grid share a common extracellular environment, allowing them to compete for nutrients (e.g., glucose, oxygen, amino acids) and exchange metabolites (e.g., lactate, acetate). We will use Flux Balance Analysis (FBA) to predict metabolic fluxes for each cell, defining biologically relevant objective functions (e.g., maximizing biomass for cancer cells, maximizing cytokine production for T-cells). This integrated model will allow us to simulate the emergent metabolic state of the entire TME.\n\n**Aim 3: In Silico Simulation, Hypothesis Testing, and Target Identification (Months 19-36)**\nWith the model built, we will use it as a virtual laboratory:\n*   **Model Validation:** We will first validate the model by comparing its predictions (e.g., spatial distribution of lactate) against withheld metabolomics data and established experimental findings from the literature.\n*   **Hypothesis Testing:** We will systematically address our research questions. To test Hypothesis 1, we will analyze the predicted spatial metabolite gradients and their correlation with immune cell states. For Hypothesis 2, we will perform in silico gene knockouts for hundreds of metabolic enzymes and transporters in each cell type to map the network of metabolic dependencies. For Hypothesis 3, we will build models of immunotherapy responders and non-responders and simulate the systemic effects of combining metabolic inhibitors with anti-PD-1 therapy, quantifying synergy.\n\n**Timeline and Milestones:**\n*   **Year 1:** Completion of the data integration pipeline (M9); generation of first-generation, non-spatial multi-cellular models (M12).\n*   **Year 2:** First fully spatially-resolved TME model for melanoma completed and validated (M18); comprehensive dependency mapping and initial therapeutic target list generated (M24).\n*   **Year 3:** Models for lung and breast cancer completed (M30); synergistic combination therapies predicted and prioritized (M32); public release of open-source software, models, and web portal (M36).",
        "expected_outcomes_and_impact": "This project, through its large-scale synthesis of public data, is poised to deliver transformative outcomes and have a significant impact on both basic cancer biology and translational oncology. Our contributions will be methodological, conceptual, and therapeutic.\n\n**Expected Outcomes:**\n1.  **A Novel, Open-Source Computational Platform:** A primary deliverable will be our complete, documented, and containerized (e.g., Docker) computational pipeline for integrating spatial and single-cell multi-omics data to build spatially-resolved metabolic models. This platform will be a powerful resource for the broader research community, adaptable to other cancer types and even other complex tissue environments like those in fibrosis or neurodegeneration.\n2.  **A Spatially-Resolved Metabolic Atlas of the TME:** We will generate the first comprehensive, dynamic maps of metabolic activity across different tumor types. These atlases will visualize nutrient consumption, metabolite exchange, and pathway activity with cellular and spatial resolution. This will be made accessible through an interactive web portal, allowing researchers to explore metabolic heterogeneity in the TME without requiring computational expertise.\n3.  **New Mechanistic Insights into TME Biology:** Our systems-level approach will uncover emergent properties of TME metabolism that are invisible to reductionist methods. We expect to identify novel metabolic symbioses, unappreciated nutrient dependencies, and the precise mechanisms by which spatial organization dictates immune function. For example, we may discover that a specific metabolic exchange between endothelial cells and regulatory T-cells is a key, previously unknown driver of immunosuppression.\n4.  **A Prioritized List of Novel Therapeutic Targets:** The ultimate translational outcome will be a high-confidence, computationally validated list of metabolic enzymes and transporters predicted to be critical nodes in the TME network. Crucially, these will not just be targets in cancer cells, but also in stromal and immune cells. We will also provide predictions for which of these targets are most likely to synergize with existing immunotherapies, providing a strong rationale for future preclinical and clinical investigation.\n\n**Broader Impact and Applications:**\n*   **Advancing Cancer Research:** This project will shift the paradigm of cancer metabolism research from a cell-centric to an ecosystem-level perspective. Our findings and tools will enable countless new research directions for cancer biologists and immunologists, providing a framework to interpret their own experimental data in a richer, systemic context.\n*   **Accelerating Therapeutic Development:** The in silico platform can serve as a screening tool to rapidly test and prioritize novel metabolic drug targets and combination strategies, reducing the time and cost associated with preclinical research. This can help de-risk the development of new cancer therapies for both academic labs and pharmaceutical partners.\n*   **Training the Next Generation of Scientists:** This project is intrinsically multidisciplinary and collaborative, providing an ideal training environment. Graduate students and postdoctoral fellows will gain unique, highly sought-after skills at the intersection of big data analysis, computational modeling, and cancer biology. In line with the research call's mission, we will actively train a new generation of data-savvy scientists through hands-on research, workshops, and the development of open-source educational materials.\n*   **Commitment to Open and Reproducible Science:** By adhering strictly to open science principles—making all code, data, and models publicly available—we will ensure our work is transparent, reproducible, and a lasting resource for the entire scientific community. This fosters a culture of collaboration and accelerates the pace of discovery.\n\n**Dissemination and Long-Term Vision:**\nOur dissemination strategy includes high-impact publications, presentations at major international conferences (AACR, SITC, ISMB), and annual workshops to train users on our platform. Our long-term vision is to establish this Working Group as a durable hub for TME systems biology. The developed framework will be continuously expanded to incorporate new data types (e.g., epigenomics, proteomics) and new cancer types. The collaborative network forged by this project will catalyze future research, ensuring that the impact of this NCEMS-supported initiative extends far beyond the initial funding period, creating a self-sustaining community focused on solving complex biological problems through data synthesis.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis project that is beyond the scope and capabilities of a single research laboratory or a typical multi-PI R01 grant. The project's success hinges on the deep integration of diverse expertise—cancer immunology, computational systems biology, and spatial data science—and the dedicated effort required to synthesize vast, heterogeneous public datasets. This aligns perfectly with the mission of the NCEMS to catalyze multidisciplinary teams to address fundamental questions through data synthesis. Traditional funding mechanisms prioritize new data generation, whereas our project exclusively leverages existing data, requiring significant person-hours for data curation, methods development, and large-scale computational analysis. NCEMS support is therefore essential to provide the protected time, collaborative infrastructure, and specialized personnel needed for a project of this magnitude.\n\n**Budget Justification (Total Request over 3 Years):**\n\n**1. Personnel (~75% of total budget):** This is the largest and most critical component of the budget, reflecting the project's focus on analysis and integration rather than experimental costs.\n*   **Principal Investigators (3):** 1.0 month of summer salary per year for each PI. This provides protected time for project leadership, intellectual direction, data interpretation, and manuscript preparation.\n*   **Postdoctoral Fellows (2):** We request support for two full-time postdocs for three years. Postdoc 1 will specialize in genome-scale metabolic modeling and simulation. Postdoc 2 will focus on the upstream analysis of single-cell and spatial omics data and the biological interpretation of model outputs from an immunological perspective. Their salaries are budgeted with full fringe benefits.\n*   **Graduate Students (2):** Support for two graduate students for three years, including stipend, tuition, and health insurance. The students will work collaboratively across the PIs' labs, receiving unique cross-disciplinary training.\n*   **Data Scientist/Software Engineer (0.5 FTE):** Support for a part-time professional staff member dedicated to building and maintaining the public-facing web portal, ensuring the project's deliverables are robust, user-friendly, and sustainable.\n\n**2. Equipment (~5%):**\n*   **High-Performance Computing:** We request funds to purchase a dedicated high-performance computing server with multiple CPUs and high-memory GPUs. This is essential for the computationally intensive tasks of model construction, parameterization, and running thousands of in silico perturbation simulations.\n\n**3. Travel (~5%):**\n*   **Working Group Meetings:** Funds to allow the entire team (PIs, postdocs, students) to meet in person twice per year for intensive workshops and strategic planning sessions.\n*   **Scientific Dissemination:** Funds for each trainee and PI to attend and present at one major international conference per year (e.g., AACR, SITC, ISMB) to share our findings with the community.\n\n**4. Other Direct Costs (~5%):**\n*   **Publication Fees:** To cover open-access fees for an anticipated 4-5 peer-reviewed publications in high-impact journals.\n*   **Software and Cloud Computing:** Costs for specialized software licenses (e.g., Gurobi optimization solver, MATLAB) and cloud computing credits (AWS/Google Cloud) for scalable data storage and on-demand computational bursts.\n*   **Workshop Organization:** Modest funds to support the organization of an annual training workshop for the broader community.\n\n**5. Indirect Costs (F&A):** Calculated based on the federally negotiated rates for each participating institution.\n\nThis budget is designed to support a highly collaborative and productive team, ensuring we can achieve our ambitious goals and deliver high-impact, open-source resources to the scientific community."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_04",
      "original_title": "Allostery Across the Proteome: Uncovering the Hidden Communication Network of Cellular Regulation",
      "original_abstract": "Allostery, the process by which binding at one site on a protein affects a distant functional site, is a fundamental emergent property that enables complex biological regulation. While well-studied in individual proteins, a proteome-wide understanding of allosteric communication pathways is lacking. This Working Group will create a comprehensive, predictive map of allostery across the human proteome. We will synthesize a massive corpus of public data, including static protein structures (PDB), structural ensembles (NMR, cryo-EM), and predicted structures (AlphaFold DB); molecular dynamics simulation trajectories from repositories like MDTraj and MDsrv; and large-scale sequence data with evolutionary couplings (Pfam, EVcouplings). We will also integrate data on disease- and population-level mutations (ClinVar, gnomAD) to identify variants that likely function by disrupting allosteric regulation. Our team, composed of structural biologists, computational chemists, bioinformaticians, and machine learning experts, will develop a novel framework based on graph theory and geometric deep learning. This framework will model proteins as dynamic networks, identifying conserved pathways of communication ('allosteric wires') that are invisible to static structural analysis. The ultimate goal is to create a publicly accessible 'Allosterome Atlas' that allows researchers to query any protein and visualize its potential allosteric sites and communication pathways. This resource will revolutionize drug discovery by revealing novel druggable sites on challenging targets and provide a new framework for interpreting the functional impact of genetic variation.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:28:11.303398",
      "proposal": {
        "title": "Allostery Across the Proteome: Uncovering the Hidden Communication Network of Cellular Regulation",
        "abstract": "Allostery, the process by which binding at one site on a protein affects a distant functional site, is a fundamental emergent property that enables complex biological regulation. While well-studied in individual proteins, a proteome-wide understanding of allosteric communication pathways is lacking. This Working Group will create a comprehensive, predictive map of allostery across the human proteome. We will synthesize a massive corpus of public data, including static protein structures (PDB), structural ensembles (NMR, cryo-EM), and predicted structures (AlphaFold DB); molecular dynamics simulation trajectories from repositories like MDTraj and MDsrv; and large-scale sequence data with evolutionary couplings (Pfam, EVcouplings). We will also integrate data on disease- and population-level mutations (ClinVar, gnomAD) to identify variants that likely function by disrupting allosteric regulation. Our team, composed of structural biologists, computational chemists, bioinformaticians, and machine learning experts, will develop a novel framework based on graph theory and geometric deep learning. This framework will model proteins as dynamic networks, identifying conserved pathways of communication ('allosteric wires') that are invisible to static structural analysis. The ultimate goal is to create a publicly accessible 'Allosterome Atlas' that allows researchers to query any protein and visualize its potential allosteric sites and communication pathways. This resource will revolutionize drug discovery by revealing novel druggable sites on challenging targets and provide a new framework for interpreting the functional impact of genetic variation.",
        "background_and_significance": "Allostery is a fundamental emergent property of biological macromolecules, enabling the regulation of protein function through ligand binding or covalent modification at sites distal to the primary functional site. This 'action at a distance' is the linchpin of cellular signaling, metabolic feedback, and genetic regulation, allowing proteins to act as sophisticated information processors. The classical models of Monod-Wyman-Changeux (MWC) and Koshland-Nemethy-Filmer (KNF) first conceptualized allostery in terms of discrete conformational state transitions. However, our modern understanding, informed by decades of biophysical research, has evolved to view proteins as dynamic conformational ensembles (Frauenfelder et al., 1991). In this paradigm, allostery arises from a perturbation of the protein's free energy landscape, where a binding event at one site shifts the equilibrium distribution of conformational states, thereby altering the activity at a distant site (Cooper & Dryden, 1984). This dynamic view is critical, as it implies that allosteric communication pathways are not necessarily encoded in a single static structure but are emergent properties of the protein's collective motions.\n\nDespite its central importance, our knowledge of allostery remains fragmented and largely anecdotal, confined to a few well-studied protein systems like hemoglobin, lac repressor, and protein kinases. The methods developed to study these systems have provided invaluable insights but are not scalable to the proteome level. Experimental techniques such as NMR spectroscopy, hydrogen-deuterium exchange mass spectrometry (HDX-MS), and double-mutant cycles are low-throughput and resource-intensive. Computationally, several approaches have been developed to probe allosteric mechanisms. Molecular Dynamics (MD) simulations can, in principle, map the complete conformational landscape, but are computationally expensive. Network-based models, such as Protein Structure Networks (PSNs), represent proteins as graphs of interacting residues and use network theory metrics to identify communication pathways (Bahar et al., 2010). Sequence-based methods, like Statistical Coupling Analysis (SCA) and Direct Coupling Analysis (DCA), leverage the vast amount of sequence data to identify co-evolving residues, which are often functionally and allosterically linked (Lockless & Ranganathan, 1999; Morcos et al., 2011). More recently, machine learning models have shown promise in predicting allosteric sites from sequence and structural features.\n\nA critical gap in the field is the absence of a unified, systematic framework to map allosteric communication networks across an entire proteome. Current approaches suffer from several key limitations. First, they typically rely on a single data modality—either static structure, sequence co-evolution, or limited dynamics—failing to integrate these complementary sources of information. Allostery is a multi-faceted phenomenon, and a holistic understanding requires synthesizing structural, dynamic, and evolutionary data. Second, most studies are based on single, static crystal structures, which represent only one snapshot of a dynamic ensemble and may completely obscure the pathways of communication. Third, the connection between genetic variation and allosteric disruption is profoundly under-explored. A vast number of disease-associated mutations, particularly Variants of Uncertain Significance (VUS), are located far from active sites, and it is highly probable that many of them exert their pathogenic effects by perturbing allosteric regulation. \n\nThis research is exceptionally timely. We are at a unique confluence of data availability and methodological advancement. The AlphaFold database provides high-quality structural models for the entire human proteome, solving the structural coverage problem. Public repositories of MD simulations are growing, and genomic databases like ClinVar and gnomAD contain millions of annotated human variants. Concurrently, advances in geometric deep learning provide a powerful new toolkit for learning complex patterns from 3D structural and graph-based data. This project directly addresses the research call's mandate by proposing to synthesize these massive, publicly available datasets using a transdisciplinary team to answer a fundamental question about an emergent cellular phenomenon. By creating a proteome-wide map of allostery, we will provide a new layer of functional annotation, revolutionize our ability to target proteins therapeutically, and establish a new paradigm for interpreting the impact of genetic variation.",
        "research_questions_and_hypotheses": "The overarching goal of this Working Group is to transition the study of allostery from a case-by-case endeavor to a systematic, proteome-wide science. We aim to develop a novel computational framework to synthesize multi-modal public data and generate a predictive, comprehensive map of allosteric communication networks across the human proteome—the 'Allosterome Atlas.' This endeavor is guided by a set of specific, interconnected research questions and testable hypotheses.\n\n**Research Question 1: Can a unified computational framework that integrates static structure, conformational dynamics, and evolutionary information systematically and accurately identify allosteric communication pathways on a proteome-wide scale?**\nCurrently, methods for predicting allostery are fragmented, each leveraging a different data type. We propose that a holistic approach that learns from these disparate but complementary data streams will yield a more accurate and robust model of allosteric communication than any single method alone.\n*   **Hypothesis 1 (H1):** A multi-modal geometric deep learning model, trained on integrated features from experimental structures (PDB), predicted structures (AlphaFold), molecular dynamics (MD) simulations, and evolutionary couplings (EVcouplings), will significantly outperform existing single-modality methods in predicting experimentally validated allosteric sites and pathways.\n*   **Testing and Validation:** We will benchmark our model's performance against established tools (e.g., AlloSite-Pro, SPACER) using curated datasets like ASBench. The primary metric will be the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) for identifying known allosteric sites. We will further validate the model by performing retrospective analyses on well-characterized allosteric drugs, predicting their binding sites and the communication pathways to the active site, and comparing these predictions with extensive experimental literature.\n\n**Research Question 2: Do conserved architectural principles and recurring motifs ('design patterns') govern the wiring of allosteric networks across diverse protein families?**\nJust as certain folds are reused throughout the proteome, it is plausible that nature has evolved common solutions for transmitting allosteric signals. Identifying these patterns would reveal fundamental principles of protein design and evolution.\n*   **Hypothesis 2 (H2):** Allosteric communication pathways are not random walks but are enriched in specific structural motifs (e.g., chains of beta-strands, alpha-helical interfaces) and are composed of residues that are evolutionarily conserved and dynamically coupled. These pathways will exhibit topological similarities across unrelated proteins that share similar regulatory functions.\n*   **Testing and Validation:** After computing allosteric pathways for the entire proteome, we will perform a large-scale statistical analysis to identify over-represented secondary structure elements, interface types, and residue properties within these pathways. We will use graph clustering algorithms to group proteins based on the topology of their allosteric networks and test whether these clusters correlate with functional classifications (e.g., GO terms, KEGG pathways) more strongly than simple fold-based classifications.\n\n**Research Question 3: To what extent do disease-associated genetic variants, particularly those distal to functional sites, exert their pathogenic effects by disrupting allosteric communication?**\nA significant challenge in clinical genetics is interpreting Variants of Uncertain Significance (VUS). We posit that a large fraction of these variants are 'allosteric mutations' that cause disease by subtly rewiring the internal communication of a protein.\n*   **Hypothesis 3 (H3):** Pathogenic missense variants cataloged in ClinVar, especially those classified as VUS, are significantly more likely to map onto predicted allosteric pathways and disrupt network connectivity than benign population variants from gnomAD.\n*   **Testing and Validation:** We will develop a quantitative 'Allosteric Disruption Score' (ADS) that measures the predicted change in pathway integrity upon in silico mutation. We will compute this score for all variants in ClinVar and gnomAD. Our hypothesis will be confirmed if the distribution of ADS for pathogenic variants is statistically significantly higher than for benign variants. We will validate this by correlating high ADS scores with known loss-of-function or gain-of-function effects for specific disease-causing mutations documented in the literature (e.g., mutations in glucokinase causing diabetes or in FGFRs causing craniosynostosis).\n\n**Expected Outcomes and Deliverables:** The successful completion of this project will yield: (1) A novel, open-source computational framework for multi-modal allostery prediction. (2) The 'Allosterome Atlas,' a public web portal for visualizing allosteric networks. (3) A comprehensive catalog of predicted allosteric sites and pathways for the human proteome. (4) A prioritized list of VUS reclassified based on their predicted impact on allosteric regulation. (5) Several high-impact publications detailing our methodology, findings, and the utility of the Atlas.",
        "methods_and_approach": "This project is a large-scale data synthesis effort that requires a multidisciplinary team and a phased, systematic approach. Our Working Group comprises experts in structural biology, computational chemistry, bioinformatics, and machine learning, ensuring all facets of the project are handled with rigor. The project is organized into three primary Aims.\n\n**Aim 1: Curation and Synthesis of a Multi-Modal Proteome-Scale Dataset.**\nThe foundation of our project is the aggregation and harmonization of diverse, publicly available datasets into a unified data structure suitable for machine learning. \n*   **Data Sources:** We will leverage a comprehensive set of databases. (1) **Structural Data:** All human protein structures from the Protein Data Bank (PDB), supplemented by the complete human proteome from the AlphaFold Database (v4). (2) **Dynamic Data:** We will collate structural ensembles from the Biological Magnetic Resonance Bank (BMRB) and cryo-EM maps from the Electron Microscopy Data Bank (EMDB). Crucially, we will mine public MD simulation repositories such as MDsrv, BioSimGrid, and the MoDEL database. For key protein families lacking dynamic data, we will perform new, standardized all-atom MD simulations (using GROMACS/AMBER with the CHARMM36m force field) on high-performance computing (HPC) resources. (3) **Evolutionary Data:** We will generate Multiple Sequence Alignments (MSAs) for each protein family in Pfam and compute evolutionary couplings using established tools like EVcouplings and GREMLIN. (4) **Genetic and Functional Data:** We will use variant data from ClinVar and gnomAD, and functional site annotations (active sites, binding sites, PTMs) from UniProt and FireDB.\n*   **Data Integration Pipeline:** We will develop an automated Snakemake workflow to process these data. For each protein, the pipeline will generate a unified graph representation. Residues will be nodes, and edges will represent spatial proximity, covalent bonds, and dynamic correlations. Each node and edge will be decorated with a rich feature vector containing information from all data modalities: static structural properties (secondary structure, solvent accessibility), dynamic properties (B-factors, RMSF from MD), evolutionary properties (conservation, co-evolutionary scores), and functional annotations.\n\n**Aim 2: Development and Validation of a Geometric Deep Learning Framework for Allostery Prediction.**\nWe will develop a novel machine learning model to learn the principles of allosteric communication from our integrated dataset.\n*   **Model Architecture:** We will employ a Geometric Graph Neural Network (GNN), specifically an equivariant message-passing network (e.g., E(n)-GNN). This architecture is ideal as it naturally operates on 3D graph data and respects the rotational and translational symmetries inherent to protein structures. The model will take our multi-modal protein graphs as input and learn to predict an 'allosteric potential' score for each residue.\n*   **Training:** The model will be trained in a supervised manner on a gold-standard set of ~500 proteins with experimentally validated allosteric sites curated from databases like ASBench and AlloReg. We will use a combination of binary cross-entropy loss for site prediction and self-supervised learning objectives to enforce biophysical realism.\n*   **Pathway Identification:** Once trained, the model's learned edge weights will represent the strength of allosteric communication. We will apply graph traversal algorithms (e.g., Dijkstra's algorithm) to this learned graph to identify the optimal, highest-probability communication pathways ('allosteric wires') between any two residues, such as a predicted allosteric site and a known active site.\n*   **Validation:** The framework will be rigorously validated on a held-out test set. We will compare its performance to existing methods and conduct in-depth case studies on well-understood allosteric systems (e.g., GPCRs, kinases, nuclear hormone receptors) to ensure our predicted pathways align with experimental evidence from NMR, HDX-MS, and mutational studies.\n\n**Aim 3: Proteome-Scale Deployment, Variant Impact Analysis, and Creation of the Allosterome Atlas.**\nWith a validated framework, we will perform the first-ever allosteric analysis of an entire proteome.\n*   **High-Throughput Analysis:** The analysis pipeline will be deployed on HPC resources to predict allosteric sites and pathways for every protein in the human proteome.\n*   **Variant Impact Scoring:** We will develop an in silico mutagenesis protocol within our framework. For each missense variant from ClinVar/gnomAD, we will compute an Allosteric Disruption Score (ADS) by quantifying the difference in network properties (e.g., pathway probabilities, centrality measures) between the wild-type and mutant protein graphs. This will allow us to systematically test Hypothesis 3.\n*   **The Allosterome Atlas:** The final output will be a publicly accessible web portal. Built with modern frameworks (e.g., React, D3.js, NGL viewer), the Atlas will allow users to search for any human protein, visualize its structure annotated with predicted allosteric sites and pathways, and query the predicted allosteric impact of known genetic variants.\n\n**Timeline:**\n*   **Year 1:** Complete data aggregation pipeline (M6). Develop and train initial GNN model prototype (M12).\n*   **Year 2:** Finalize and validate the predictive framework (M18). Begin proteome-scale deployment (M20). Launch internal prototype of the Allosterome Atlas (M24).\n*   **Year 3:** Complete proteome-wide analysis and variant scoring (M30). Populate and publicly launch the Allosterome Atlas (M32). Submit primary manuscripts for publication (M36).",
        "expected_outcomes_and_impact": "This project will generate transformative outcomes that will significantly advance the molecular and cellular biosciences, with profound impacts on basic research, therapeutic development, and personalized medicine. Our contributions will extend far beyond the specific scientific questions addressed, creating a lasting resource and a new conceptual framework for the entire research community.\n\n**Intellectual Merit and Contributions to the Field:**\nThe primary outcome will be the **Allosterome Atlas**, the first-ever comprehensive map of allosteric communication networks across the human proteome. This represents a paradigm shift, moving the study of allostery from a qualitative, case-by-case analysis to a quantitative, systems-level science. This Atlas will serve as a new functional annotation layer for the human genome, providing mechanistic insights that are orthogonal to existing annotations like gene ontology or pathway databases. We will uncover the prevalence and diversity of allosteric regulation, potentially revealing that it is a far more ubiquitous mechanism than currently appreciated. Our large-scale analysis will identify conserved 'design principles' of allosteric communication—common structural motifs and network topologies that nature has evolved to transmit information within proteins. This will provide fundamental insights into protein evolution and biophysics.\n\nMethodologically, we will deliver a novel, open-source **multi-modal deep learning framework**. This framework, which integrates structural, dynamic, and evolutionary data, will be a significant contribution to computational biology. Its success will demonstrate the power of data synthesis and will be adaptable to other challenging problems in protein science, such as predicting the effects of post-translational modifications or designing protein-protein interactions.\n\n**Broader Impacts and Applications:**\nThe societal and economic impacts of this research will be substantial, particularly in medicine and biotechnology.\n*   **Revolutionizing Drug Discovery:** The Allosterome Atlas will be a powerful engine for identifying novel therapeutic targets. Allosteric drugs offer significant advantages over traditional active-site inhibitors, including higher specificity and the ability to modulate, rather than simply block, protein function. Our Atlas will systematically reveal previously unknown allosteric sites on thousands of proteins, including high-value targets that have been deemed 'undruggable' due to flat, featureless active sites (e.g., transcription factors, scaffolding proteins). This will open up vast new therapeutic opportunities for cancer, neurodegenerative disorders, and metabolic diseases.\n*   **Advancing Personalized Medicine:** Our framework for calculating an 'Allosteric Disruption Score' for genetic variants will have a direct clinical impact. It will provide a powerful tool for interpreting the functional consequences of Variants of Uncertain Significance (VUS) identified in patient genomes. By providing a mechanistic hypothesis—disruption of allosteric regulation—we can help reclassify VUS, improve diagnostic accuracy, and guide the development of personalized therapies.\n*   **Enabling Rational Protein Engineering:** A detailed map of a protein's internal 'wiring' is invaluable for protein engineering. Researchers in synthetic biology and biotechnology can use the Atlas to rationally design mutations that fine-tune protein activity, stability, or substrate specificity for applications ranging from industrial enzyme production to the development of novel biosensors.\n\n**Dissemination, Data Sharing, and Training:**\nIn line with the research call's emphasis on open science, all outcomes will be made broadly and freely available. The Allosterome Atlas will be a public, user-friendly web portal. All software will be released as open-source code on GitHub, and all generated data will be deposited in public repositories (e.g., Zenodo). We plan to publish our findings in high-impact journals (e.g., Nature, Cell) and present at major international conferences. Furthermore, this project is an ideal training vehicle. The graduate students and postdocs in the Working Group will receive unique cross-disciplinary training at the interface of biophysics, data science, and genomics, preparing them to be leaders in the future data-savvy workforce. We will host annual workshops to train the broader community on using our tools and resources, ensuring maximum impact and fostering a collaborative ecosystem around the study of the allosterome.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis project that is beyond the capabilities of any single research laboratory. Its successful execution requires the deep integration of expertise from four distinct scientific domains—structural biology, computational chemistry, bioinformatics, and machine learning—and the synthesis of petabyte-scale public datasets. The computational demands for large-scale molecular dynamics simulations, deep learning model training on 3D graph data, and the analysis of the entire human proteome far exceed the resources typically available to individual PIs. Therefore, the support and collaborative framework provided by NCEMS are essential for the project's feasibility and success.\n\n**Budget Justification (3-Year Project Total: $1,250,000)**\n\n*   **A. Personnel ($780,000):** The intellectual core of the project will be driven by dedicated trainees. We request support for two Postdoctoral Scholars for three years ($100,000/yr each, salary + fringe). One postdoc will have expertise in molecular dynamics and computational biophysics, leading the dynamic data generation and analysis. The second postdoc will be a machine learning expert responsible for developing and implementing the geometric GNN framework. We also request support for two Graduate Student Research Assistants for three years ($60,000/yr each, stipend, tuition, fees). These students will focus on the data integration pipeline and the development of the Allosterome Atlas web portal, respectively. Finally, we request one month of summer salary per year for each of the four collaborating PIs ($15,000/PI/yr) to support their dedicated effort in project oversight, management, and trainee mentorship.\n\n*   **B. Computational Resources ($210,000):** This project is computationally intensive. We request $70,000 per year to cover costs for high-performance computing. This includes an allocation on a national supercomputing resource (e.g., ACCESS) for large-scale simulations and model training on GPU clusters. Funds will also be used for cloud computing credits (AWS/GCP) for flexible development and prototyping, and for purchasing dedicated high-capacity data storage solutions to manage the massive integrated dataset.\n\n*   **C. Travel ($90,000):** Collaboration is key. We budget for the full Working Group (4 PIs + 4 trainees) to meet in person twice annually ($15,000/meeting) to facilitate intensive collaboration, strategic planning, and cross-training. We also allocate funds for each trainee to travel to one major international conference per year (e.g., ISMB, BPS) to present their findings and network with the broader community.\n\n*   **D. Publication and Dissemination ($30,000):** We request funds to cover open-access publication fees for an anticipated 4-5 major manuscripts ($5,000/publication). An additional $5,000 is budgeted for costs associated with the long-term hosting and maintenance of the Allosterome Atlas web portal.\n\n*   **E. Indirect Costs (F&A) ($140,000):** This is an estimated amount based on a blended rate across the collaborating institutions and is subject to negotiation based on the final direct cost base. This support is critical for the institutional infrastructure that enables this research."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_05",
      "original_title": "Emergence of Robustness and Plasticity in Cellular Signaling Networks",
      "original_abstract": "Cellular signaling networks face a fundamental design challenge: they must be robust enough to buffer against noise and perturbations, yet plastic enough to adapt to new environmental cues. How these dual, seemingly contradictory, emergent properties arise from the underlying molecular interactions is a central puzzle in systems biology. This Working Group will address this question by synthesizing diverse, large-scale public datasets to uncover the general design principles of robust and plastic networks. We will integrate phosphoproteomic and transcriptomic time-series data following various perturbations (e.g., from LINCS, GEO), comprehensive protein-protein interaction maps (BioGRID, IntAct), and kinetic parameters curated from the literature (SABIO-RK). Our multidisciplinary team of systems biologists, mathematicians specializing in dynamical systems, control theory engineers, and computational scientists will develop and apply novel analytical strategies. We will use network topology analysis, information theory, and machine learning-based parameter inference to build predictive dynamical models of key signaling pathways (e.g., MAPK, NF-κB). By comparing network architectures and dynamics across different pathways and organisms, we will identify recurring motifs, feedback/feedforward loop structures, and parameter regimes that confer robustness versus plasticity. The project will deliver a set of generalizable principles that explain how cells achieve this critical balance, with profound implications for understanding diseases where signaling is dysregulated (e.g., cancer, autoimmune disorders) and for the rational design of synthetic biological circuits.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:29:43.843844",
      "proposal": {
        "title": "Emergence of Robustness and Plasticity in Cellular Signaling Networks",
        "abstract": "Cellular signaling networks face a fundamental design challenge: they must be robust enough to buffer against noise and perturbations, yet plastic enough to adapt to new environmental cues. How these dual, seemingly contradictory, emergent properties arise from the underlying molecular interactions is a central puzzle in systems biology. This Working Group will address this question by synthesizing diverse, large-scale public datasets to uncover the general design principles of robust and plastic networks. We will integrate phosphoproteomic and transcriptomic time-series data following various perturbations (e.g., from LINCS, GEO), comprehensive protein-protein interaction maps (BioGRID, IntAct), and kinetic parameters curated from the literature (SABIO-RK). Our multidisciplinary team of systems biologists, mathematicians specializing in dynamical systems, control theory engineers, and computational scientists will develop and apply novel analytical strategies. We will use network topology analysis, information theory, and machine learning-based parameter inference to build predictive dynamical models of key signaling pathways (e.g., MAPK, NF-κB). By comparing network architectures and dynamics across different pathways and organisms, we will identify recurring motifs, feedback/feedforward loop structures, and parameter regimes that confer robustness versus plasticity. The project will deliver a set of generalizable principles that explain how cells achieve this critical balance, with profound implications for understanding diseases where signaling is dysregulated (e.g., cancer, autoimmune disorders) and for the rational design of synthetic biological circuits.",
        "background_and_significance": "Cellular life depends on the faithful transmission and processing of information. Signaling networks are the intricate communication systems that allow cells to sense their environment and internal state, and to execute appropriate responses, such as proliferation, differentiation, or apoptosis. A central challenge in the design of these networks is the need to balance two opposing emergent properties: robustness and plasticity. Robustness is the ability to maintain stable function and produce a reliable output despite perturbations, such as thermal noise, stochastic fluctuations in component concentrations, or genetic mutations. This property ensures the fidelity of critical cellular processes. Conversely, plasticity, or adaptability, is the capacity to alter signaling responses in the face of new or persistent environmental cues, enabling cells to learn from experience and adapt to changing conditions. How cells achieve this delicate and seemingly paradoxical balance is a fundamental, unanswered question in molecular and cellular biology.\n\nEarly work in systems biology identified key network motifs that contribute to specific dynamic behaviors. Negative feedback loops (NFLs) were shown to promote homeostasis and accelerate response times, key features of robust systems. Positive feedback loops (PFLs) were found to generate bistability and ultrasensitivity, enabling switch-like, decisive cell fate decisions. Incoherent feedforward loops (FFLs) can act as pulse generators or fold-change detectors, buffering against fluctuations in input signal amplitude. These foundational studies, pioneered by researchers like Uri Alon and others, provided a 'parts list' of network motifs, but a comprehensive understanding of how these parts are assembled into systems that are simultaneously robust and plastic remains elusive. Seminal studies have provided deep insights into individual pathways. For instance, the work of Barkai and Leibler on yeast chemotaxis demonstrated perfect adaptation, a powerful form of robustness, achieved through integral feedback control. Similarly, the oscillatory dynamics of the NF-κB transcription factor, elucidated by Hoffmann and colleagues, revealed how temporal coding can regulate gene expression, a mechanism that blends robust signal transmission with plastic, context-dependent interpretation.\n\nDespite these advances, the field faces significant limitations that this Working Group is uniquely positioned to address. First, most research has been siloed, focusing intensely on a single pathway in a specific model organism. This has yielded deep but narrow insights, leaving a critical gap in our understanding of the generalizable principles that govern signaling network design across diverse biological contexts. Second, many analyses have relied on static protein-protein interaction maps, which ignore the highly dynamic and context-dependent nature of signaling. Third, the construction of predictive dynamical models, which are essential for understanding emergent properties, has been severely hampered by the 'curse of dimensionality' and the scarcity of well-constrained kinetic parameters. Finally, the explosion of publicly available high-throughput data—including transcriptomic and proteomic time-series—has created an unprecedented opportunity for a synthesis-based approach, yet these datasets are often heterogeneous and noisy, requiring sophisticated integration strategies beyond the scope of a single research lab.\n\nThis research is therefore both important and timely. It is important because a failure to properly balance robustness and plasticity is a hallmark of numerous human diseases. In cancer, signaling pathways become pathologically robust, locked into a proliferative state and resistant to apoptotic signals. In autoimmune diseases, immune cell signaling can become hyper-plastic, overreacting to self-antigens. A principled understanding of how this balance is achieved could unveil novel therapeutic strategies aimed at re-tuning network dynamics rather than simply inhibiting a single protein. The research is timely because we are at a confluence of data availability and methodological innovation. The maturation of public data repositories like GEO, LINCS, and BioGRID, combined with advances in machine learning for parameter inference and network science, makes it possible, for the first time, to systematically compare network architectures and dynamics across dozens of pathways and cell types. This project will synthesize these disparate resources to extract fundamental design principles, moving the field from a descriptive to a predictive science of cellular signaling.",
        "research_questions_and_hypotheses": "The overarching goal of this Working Group is to elucidate the generalizable design principles that enable cellular signaling networks to simultaneously achieve robustness and plasticity. To address this complex challenge, we have formulated three specific, interconnected research questions, each associated with a set of testable hypotheses that will guide our data synthesis and modeling efforts.\n\n**Research Question 1 (RQ1): How do network topology and modular organization quantitatively contribute to the balance between robustness and plasticity?**\nWhile specific motifs like feedback loops are known to influence network behavior, a systematic, cross-pathway understanding of how entire network architectures are tuned for robustness versus plasticity is lacking. We hypothesize that these two properties are encoded in distinct, quantifiable features of the network graph.\n*   **Hypothesis 1a:** Robustness to intrinsic noise and parameter variation is primarily conferred by a high density of fast-acting negative feedback loops and incoherent feedforward loops. We predict that pathways known for homeostatic function (e.g., metabolic regulation) will show significant enrichment for these motifs compared to pathways involved in developmental decision-making.\n*   **Hypothesis 1b:** Plasticity is enabled by a modular network structure. We hypothesize that networks are organized into a robust 'core' module responsible for signal propagation, which is peripherally regulated by 'tuning' modules. These tuning modules, often containing positive feedback loops and slower transcriptional components, can re-wire or re-parameterize the core response without compromising its fundamental integrity. We predict that the degree of modularity will correlate with the known adaptive capacity of a given pathway.\n*   **Validation:** We will test these hypotheses by performing a large-scale comparative analysis of network topologies reconstructed from integrated public data. We will quantify motif enrichment and modularity scores for dozens of signaling pathways and correlate these topological metrics with functional measures of robustness (e.g., low cell-to-cell response variability from single-cell data) and plasticity (e.g., the degree of adaptive desensitization from time-series data).\n\n**Research Question 2 (RQ2): What are the dynamic and information-theoretic principles that govern the trade-off between reliable signal transmission and adaptive capacity?**\nSignaling networks are not just wires, but information channels. How they process information dynamically is key to their function. We propose that the robustness-plasticity trade-off can be rigorously framed using the language of information theory.\n*   **Hypothesis 2a:** Robust signaling pathways operate as high-fidelity communication channels optimized to maximize mutual information between a specific input and its downstream output. Plasticity, in this framework, represents a mechanism for dynamically reallocating the channel's bandwidth, for instance, by changing its sensitivity or dynamic range in response to a secondary, contextual cue.\n*   **Hypothesis 2b:** The temporal coding of signals is a key mechanism for separating robust and plastic responses. We hypothesize that robust, essential information is encoded in simple, easily decodable features (e.g., signal amplitude), while plastic, adaptive information is encoded in more complex dynamic features (e.g., frequency, pulse duration), as exemplified by the MAPK and NF-κB systems, respectively.\n*   **Validation:** We will apply information-theoretic tools to the curated time-series datasets to quantify the channel capacity of different pathways. We will build dynamical models to simulate how network modifications affect information flow. By comparing pathways with different known functions, we will test whether their dynamic encoding strategies align with our predictions.\n\n**Research Question 3 (RQ3): How do kinetic parameter landscapes and multi-scale feedback determine whether a network operates in a robust or plastic regime?**\nThe structure of a network defines its potential behaviors, but the specific kinetic parameters determine its actual function. We hypothesize that robustness and plasticity are emergent properties of the system's location in a high-dimensional parameter space.\n*   **Hypothesis 3a:** Robustness arises when a system operates in a 'flat' region of its parameter landscape, where the system's output is insensitive to large variations in most individual kinetic parameters. This property, known as structural robustness, is often achieved through mechanisms like enzyme saturation and integral feedback.\n*   **Hypothesis 3b:** Plasticity is enabled by the existence of nearby bifurcations in the parameter space. We predict that slow-acting feedback loops (e.g., transcriptional regulation) can push the system across these bifurcations, causing qualitative shifts in its dynamic behavior (e.g., from a stable steady state to an oscillation or a bistable switch), which underlies long-term adaptation.\n*   **Validation:** Using machine learning-based inference on time-series data, we will estimate posterior distributions for the parameters of our dynamical models. We will then perform global sensitivity analysis (e.g., Sobol indices) to identify sloppy vs. stiff parameter directions, testing Hypothesis 3a. We will use bifurcation analysis to map the dynamic regimes of our models and identify the parameters that act as 'levers' for plasticity, testing Hypothesis 3b.",
        "methods_and_approach": "Our research plan is a multi-pronged, integrative strategy designed to systematically address our research questions. The project is organized into four sequential but interconnected aims, forming a comprehensive workflow from data aggregation to the generation of generalizable principles. This project will exclusively use publicly available data, in full compliance with the research call.\n\n**Aim 1: Data Curation, Integration, and Construction of a Pan-Pathway Signaling Atlas.**\nThe foundation of this project is the synthesis of diverse, large-scale datasets. We will develop a reproducible computational pipeline using Nextflow to automate the retrieval, processing, and normalization of data from multiple public repositories.\n*   **Data Sources:** We will target three main classes of data. (1) **Dynamic Response Data:** Time-series transcriptomic (from GEO and ArrayExpress) and phosphoproteomic (from PRIDE, CPTAC, and the LINCS L1000/P100 projects) data capturing cellular responses to a wide range of perturbations (e.g., growth factors, cytokines, small molecule inhibitors) in well-characterized human cell lines (e.g., MCF7, HeLa, A549). (2) **Network Scaffolds:** Comprehensive protein-protein interaction data from BioGRID and IntAct, kinase-substrate interactions from PhosphoSitePlus and SIGNOR, and transcription factor-target interactions from ENCODE and TRRUST. (3) **Kinetic Parameters:** Experimentally measured enzyme kinetic parameters (Km, kcat, Ki) from SABIO-RK and BRENDA will be curated to serve as priors for our dynamic models.\n*   **Integration:** Raw data will be processed into a standardized format. Interaction data will be integrated using a weighted scheme, where edge weights reflect the amount and type of evidence. This will result in a comprehensive, multi-layered 'Signaling Atlas' that will serve as the foundational data structure for all subsequent analyses. All processing steps will be containerized (e.g., using Docker) to ensure full reproducibility.\n\n**Aim 2: Comparative Topological and Information-Theoretic Analysis.**\nUsing the Signaling Atlas, we will extract context-specific networks for canonical signaling pathways (e.g., MAPK, NF-κB, PI3K/Akt, Wnt, TGF-β) and perform systematic analyses to test Hypotheses 1a, 1b, 2a, and 2b.\n*   **Topological Analysis:** For each pathway, we will use established algorithms implemented in Python libraries (NetworkX) and Cytoscape to quantify global network properties (e.g., degree distribution, clustering coefficient) and local features. We will employ motif-finding algorithms (e.g., mfinder) to detect the enrichment of feedback and feedforward loops. Community detection algorithms (e.g., Louvain) will be used to identify modular structures and test Hypothesis 1b.\n*   **Information-Theoretic Analysis:** We will apply methods from information theory to the curated time-series data. Mutual information will be calculated between stimulus and response time-series to estimate the channel capacity of each pathway. Transfer entropy will be used to infer the direction and magnitude of information flow between network components, providing a dynamic, data-driven view of network connectivity.\n\n**Aim 3: Development, Parameterization, and Analysis of Predictive Dynamical Models.**\nTo move beyond static correlations and understand mechanism, we will build and analyze mechanistic models of core signaling modules.\n*   **Model Formulation:** We will use Ordinary Differential Equations (ODEs) as our primary modeling formalism, leveraging tools like PySB for programmatic model construction. This allows us to explicitly represent biochemical reactions and their kinetics. For each pathway, we will construct a series of models of increasing complexity, starting with a core topology and adding regulatory loops.\n*   **Parameter Inference:** This is a critical challenge we will address with state-of-the-art methods. We will employ Bayesian inference, specifically Markov Chain Monte Carlo (MCMC) methods (e.g., via the PyMC library), to fit our ODE models to the relevant time-series data. This approach has two key advantages: it can integrate prior knowledge (from SABIO-RK), and it yields full posterior distributions for each parameter, naturally capturing our uncertainty. This probabilistic approach is essential for robustly testing our hypotheses about parameter landscapes.\n*   **Model Analysis:** Once parameterized, the models will be our primary tool for in silico experimentation. We will perform global sensitivity analysis to identify stiff (plasticity-conferring) and sloppy (robustness-conferring) parameter combinations. Bifurcation analysis will be used to map the different dynamic regimes (e.g., stable, bistable, oscillatory) accessible to the network, revealing the mechanisms of plasticity. We will simulate the models under noisy conditions to quantify robustness and directly test its relationship with topological and parametric features.\n\n**Timeline and Milestones:**\n*   **Year 1:** Complete the data processing pipeline and release version 1.0 of the Signaling Atlas. Perform initial topological and information-theoretic analyses across 5-10 pathways. Publish a data descriptor paper.\n*   **Year 2:** Develop and parameterize robust dynamical models for 3-4 core pathways (e.g., ERK, NF-κB). Perform initial sensitivity and bifurcation analyses. Hold the first in-person Working Group meeting to synthesize results. Present preliminary findings at a major conference.\n*   **Year 3:** Expand modeling to a larger set of pathways. Conduct the final cross-pathway comparative analysis to synthesize general principles. Prepare and submit manuscripts to high-impact journals. Release all models, code, and analysis workflows as a comprehensive, open-source package. Hold the final Working Group meeting.",
        "expected_outcomes_and_impact": "This project is designed to produce a series of high-impact outcomes that will significantly advance the field of molecular and cellular biology, while also providing valuable resources and training opportunities for the broader scientific community.\n\n**Intellectual Merit and Contributions to the Field:**\nThe primary intellectual contribution will be the elucidation of a set of generalizable design principles that explain how cellular signaling networks resolve the fundamental trade-off between robustness and plasticity. This will represent a significant step towards a predictive understanding of cellular regulation. By moving beyond the study of single pathways, our comparative, synthesis-based approach will uncover common strategies that cells use to manage information, providing a unifying framework for systems biology. We expect to deliver:\n1.  **A Quantitative Catalog of Design Principles:** A set of rules linking specific network topologies (e.g., motif combinations, modularity), dynamic strategies (e.g., temporal coding), and parameter landscapes (e.g., sloppiness, proximity to bifurcations) to the emergent properties of robustness and plasticity.\n2.  **A Novel, Integrative Analytical Framework:** A fully documented, open-source computational pipeline for integrating multi-modal 'omics data to construct, parameterize, and analyze dynamical models of signaling networks. This will lower the barrier for other researchers to conduct similar synthesis projects.\n3.  **A Rich Community Resource:** The curated Pan-Pathway Signaling Atlas and the library of validated, parameterized models will be made publicly available through a user-friendly web portal. This will serve as a foundational resource for hypothesis generation and in silico experimentation by the wider cell biology community.\n\n**Broader Impacts and Applications:**\nThe implications of this research extend far beyond basic science. Understanding the principles of robust and plastic design has profound relevance for human health and biotechnology.\n*   **Translational Medicine:** The dysregulation of signaling robustness and plasticity is a root cause of many diseases. In cancer, pathways become rigidly robust to death signals; in autoimmune disorders, they are excessively plastic and hyper-responsive. Our findings will provide a 'network-level' perspective on disease, identifying novel therapeutic targets and strategies. For example, instead of simply inhibiting a kinase, one might design drugs that push a network away from a pathologically robust state, re-sensitizing it to other treatments. This aligns with the growing field of network medicine.\n*   **Synthetic Biology:** The rational design of synthetic biological circuits is often hampered by their fragility and lack of adaptability. Our work will provide a 'design manual' for engineers to build more sophisticated and reliable circuits. By incorporating the principles of robustness and plasticity we uncover, synthetic biologists can create cells with novel functions, such as smart therapeutics that can adapt their behavior to the state of a patient's disease or robust biosensors for environmental monitoring.\n*   **Training and Education:** This project is an ideal training vehicle for the next generation of data-savvy scientists. Trainees involved will gain a unique, interdisciplinary skillset spanning data science, bioinformatics, mathematical modeling, and systems biology. The collaborative Working Group structure will foster team science and provide mentorship opportunities for graduate students and postdocs from PIs with diverse expertise.\n\n**Dissemination and Open Science:**\nWe are deeply committed to the principles of open and reproducible science. All products of this research will be made rapidly and broadly available.\n*   **Publications:** We will publish our findings in high-impact, peer-reviewed journals (e.g., Cell Systems, Nature Communications). We will budget for open-access fees to ensure maximum accessibility.\n*   **Public Repositories:** All code will be developed in a version-controlled GitHub repository with a permissive open-source license. All curated data and models will be deposited in community-standard repositories like Zenodo, GEO, and the BioModels Database, with rich metadata to ensure they are FAIR (Findable, Accessible, Interoperable, and Reusable).\n*   **Community Engagement:** We will present our work at national and international conferences (e.g., ISMB, Q-Bio) and will organize workshops and tutorials to train the community on how to use our software and data resources. This will ensure the long-term impact and sustainability of our work.",
        "budget_and_resources": "The proposed research represents a large-scale synthesis effort that is beyond the capabilities of any single research laboratory. It requires a unique convergence of expertise in systems biology, dynamical systems theory, control engineering, and data science, as well as significant dedicated personnel time and computational resources for data integration and modeling. The NCEMS Working Group mechanism is therefore the ideal framework to support this project, as it is specifically designed to catalyze the kind of deep, multidisciplinary collaboration required for success. The requested budget reflects the personnel- and computation-intensive nature of this synthesis project.\n\n**Budget Justification (Total Request over 3 Years):**\n\n*   **A. Personnel ($390,000):** The bulk of the budget is allocated to personnel who will perform the day-to-day research.\n    *   **Postdoctoral Scholars (2 FTE for 3 years):** We request support for two postdoctoral fellows who will be the primary drivers of the project. One will specialize in bioinformatics and data integration, managing the data pipeline and topological analyses. The second will specialize in computational modeling and dynamical systems, leading the development and analysis of ODE models. (Approx. $65,000/year salary + benefits per scholar).\n    *   **Graduate Students (Partial support for 2 students):** We request stipend support for two graduate students. Their involvement is critical for training the next generation of scientists and for exploring specific sub-projects in depth. (Approx. $20,000/year per student).\n\n*   **B. Travel ($45,000):** Collaboration and dissemination are key to the project's success.\n    *   **Working Group Meetings ($25,000):** Funds to support two in-person, multi-day meetings for the entire team (4 PIs, 2 postdocs, 2 students). These intensive workshops are essential for integrating the different project arms, resolving challenges, and strategic planning.\n    *   **Conference Travel ($20,000):** Support for trainees and PIs to present findings at one major international conference per year (e.g., ISMB, ICSB), facilitating dissemination of results and fostering new collaborations.\n\n*   **C. Computational Resources ($30,000):**\n    *   **Cloud Computing ($20,000):** Credits for a commercial cloud provider (e.g., AWS or Google Cloud) are required for storing the large integrated datasets and for performing computationally demanding tasks, particularly the Bayesian parameter inference (MCMC), which can require thousands of CPU hours per model.\n    *   **Data Storage and Servers ($10,000):** Funds for long-term data archiving and hosting of the public web portal for our Signaling Atlas and model repository.\n\n*   **D. Publication Costs ($15,000):**\n    *   Funds to cover open-access publication fees for an anticipated 3-4 major manuscripts in high-quality journals, ensuring our findings are freely accessible to all.\n\n*   **E. Indirect Costs (IDC):** Calculated based on the federally negotiated rate for the lead institution and applied to the modified total direct costs.\n\n**Existing Resources:** The collaborating PIs will contribute their existing laboratory space, equipment, and access to institutional high-performance computing clusters. The institutions provide significant support through library access, administrative support, and IT services. The unique value provided by NCEMS is the dedicated funding and framework to unite these distributed resources and expertise into a cohesive and highly productive collaborative unit."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_06",
      "original_title": "Reconstructing Eukaryogenesis: A Data-Driven Synthesis of the Great Evolutionary Transition",
      "original_abstract": "The origin of the eukaryotic cell, with its nucleus, mitochondria, and complex endomembrane system, represents one of the most profound and enigmatic emergent events in the history of life. While hypotheses abound, a comprehensive, data-driven reconstruction of this transition is lacking. This Working Group will synthesize the explosion of genomic, proteomic, and structural data from across the tree of life to build a detailed, step-by-step model of eukaryogenesis. We will integrate complete genomes and proteomes from diverse eukaryotes, archaea (with a focus on the recently discovered Asgardarchaeota, our closest prokaryotic relatives), and bacteria from NCBI and UniProt. This will be combined with protein family phylogenies (OrthoDB), and a vast repository of protein structures (PDB, AlphaFold DB) to trace the origin and evolution of key eukaryotic signature proteins (ESPs) involved in membrane trafficking, cytoskeletal dynamics, and nuclear organization. Our team, a unique collaboration of evolutionary biologists, cell biologists, structural bioinformaticians, and computational phylogenomicists, will use sophisticated computational methods to reconstruct ancestral protein functions and interaction networks. We aim to pinpoint the specific molecular innovations and horizontal gene transfers that enabled the emergence of eukaryotic complexity, resolving long-standing debates about the roles of endosymbiosis and autogenous evolution. This project will produce the most detailed data-synthesized narrative of our own cellular origins, providing a foundational resource for understanding the principles of biological innovation.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:31:13.957037",
      "proposal": {
        "title": "Reconstructing Eukaryogenesis: A Data-Driven Synthesis of the Great Evolutionary Transition",
        "abstract": "The origin of the eukaryotic cell, with its nucleus, mitochondria, and complex endomembrane system, represents one of the most profound and enigmatic emergent events in the history of life. While hypotheses abound, a comprehensive, data-driven reconstruction of this transition is lacking. This Working Group will synthesize the explosion of genomic, proteomic, and structural data from across the tree of life to build a detailed, step-by-step model of eukaryogenesis. We will integrate complete genomes and proteomes from diverse eukaryotes, archaea (with a focus on the recently discovered Asgardarchaeota, our closest prokaryotic relatives), and bacteria from NCBI and UniProt. This will be combined with protein family phylogenies (OrthoDB), and a vast repository of protein structures (PDB, AlphaFold DB) to trace the origin and evolution of key eukaryotic signature proteins (ESPs) involved in membrane trafficking, cytoskeletal dynamics, and nuclear organization. Our team, a unique collaboration of evolutionary biologists, cell biologists, structural bioinformaticians, and computational phylogenomicists, will use sophisticated computational methods to reconstruct ancestral protein functions and interaction networks. We aim to pinpoint the specific molecular innovations and horizontal gene transfers that enabled the emergence of eukaryotic complexity, resolving long-standing debates about the roles of endosymbiosis and autogenous evolution. This project will produce the most detailed data-synthesized narrative of our own cellular origins, providing a foundational resource for understanding the principles of biological innovation.",
        "background_and_significance": "The emergence of the eukaryotic cell from prokaryotic ancestors stands as one of the most significant and complex events in evolutionary history, a true singularity that paved the way for all macroscopic life. This transition involved a radical increase in cellular complexity, including the origin of the nucleus, mitochondria, a dynamic endomembrane system, a versatile cytoskeleton, and linear chromosomes. Understanding this 'great evolutionary transition' is not merely a historical curiosity; it is fundamental to comprehending the principles that govern biological innovation and the emergence of complexity from simpler components. For decades, the field has been dominated by conceptual models, often based on limited, fragmentary evidence. The classic endosymbiotic theory, championed by Lynn Margulis, correctly identified the bacterial origin of mitochondria, but the nature of the host cell and the sequence of events that led to its complex architecture have remained fiercely debated. Broadly, hypotheses fall into two camps: autogenous models, which propose that complexity arose gradually within a single prokaryotic lineage before any major symbiotic event, and symbiogenetic models, which posit that the symbiotic merger itself was the catalyst for complexification. The 'inside-out' model, for instance, is a sophisticated autogenous theory suggesting that the nucleus and cytoplasm evolved from extracellular blebs formed by an archaeal ancestor. Conversely, symbiogenetic models like the 'hydrogen hypothesis' argue that the metabolic dependency between an archaeal host and a hydrogen-producing alphaproteobacterial symbiont drove the engulfment that initiated eukaryogenesis. A central point of contention has been the 'mitochondria-early' versus 'mitochondria-late' debate: did a complex 'proto-eukaryote' engulf the mitochondrion, or did the energy and genetic material from the endosymbiont fuel the evolution of complexity? The discovery of the Asgardarchaeota superphylum has revolutionized this field. Phylogenomic analyses by Zaremba-Niedzwiedzka et al. (2017) and others have robustly placed the eukaryotic lineage as a sister group to, or branching from within, the Asgardarchaeota. This discovery effectively ended the three-domain debate in favor of a two-domain tree of life, confirming the archaeal nature of the host cell. Crucially, Asgardarchaeal genomes contain an unprecedented number of 'eukaryotic signature proteins' (ESPs)—genes previously thought to be unique to eukaryotes. These include homologs of actin, tubulin, ESCRT proteins involved in membrane remodeling, and components of the ubiquitin system. This finding suggests the host was not a simple, passive partner but was 'primed' with a toolkit of proteins that could be co-opted for eukaryotic functions. However, this discovery has also raised new, more nuanced questions. What were these proteins doing in a prokaryotic context? How were their functions repurposed during eukaryogenesis? Did the Asgard ancestor possess a rudimentary cytoskeleton or endomembrane system? Answering these questions requires moving beyond simply cataloging genes. The key gap in our current knowledge is the lack of a holistic, integrated model that connects the evolution of genes to the evolution of cellular systems and functions. Previous studies have often focused on individual protein families or specific cellular processes in isolation. A comprehensive synthesis that integrates genomic, phylogenetic, structural, and functional data is required to reconstruct the step-by-step emergence of the eukaryotic cell. This project is exceptionally timely due to a confluence of factors. First, the torrent of publicly available genomic data from diverse microbial eukaryotes and newly discovered prokaryotic lineages provides an unprecedented dataset for comparative analysis. Second, the advent of highly accurate protein structure prediction through AlphaFold has opened the door to inferring the function of ancestral proteins for which no experimental data exists. Third, advances in computational phylogenetics and network biology provide the analytical tools necessary to synthesize these heterogeneous datasets. This project will leverage these advances to build the first data-driven, systems-level reconstruction of our own cellular origins, addressing a foundational question in molecular and cellular biosciences.",
        "research_questions_and_hypotheses": "The overarching goal of this Working Group is to synthesize publicly available data to construct a high-resolution, temporally-ordered model of the molecular events that transpired during the transition from a prokaryotic ancestor to the Last Eukaryotic Common Ancestor (LECA). We will move beyond qualitative models to a quantitative, evidence-based reconstruction of this emergent phenomenon. To achieve this, we have formulated four central research questions (RQs), each associated with specific, testable hypotheses.\n\n**RQ1: What was the molecular toolkit of the Asgard-eukaryote common ancestor, and how did it function in a prokaryotic context?** While we know Asgardarchaeota possess many ESPs, their ancestral functions and interactions remain unknown. We aim to reconstruct the ancestral proteome and protein-protein interaction (PPI) network to understand the cellular capabilities of the eukaryotic host just before the mitochondrial endosymbiosis.\n*   **Hypothesis 1 (The 'Primed Host' Hypothesis):** The Asgardarchaeal ancestor of eukaryotes already possessed a rudimentary, dynamic cytoskeleton and a basic membrane-remodeling system, which were essential prerequisites for engulfing the proto-mitochondrion. This system was likely involved in generating cell surface complexity and interacting with the environment.\n    *   *Prediction:* Ancestral state reconstruction will place the origin of key membrane-remodeling proteins (e.g., ESCRTs, BAR domains) and profilin/gelsolin-regulated actin homologs *before* the massive influx of genes from the alphaproteobacterial endosymbiont. Ancestral protein structure modeling will show that these proteins had the core biophysical properties (e.g., membrane curvature sensing, filament polymerization) necessary for these functions.\n\n**RQ2: How did the acquisition and integration of the alphaproteobacterial endosymbiont reshape the host's proteome and cellular architecture?** The endosymbiosis was not just a metabolic merger but also a massive genetic event, involving the transfer of hundreds of genes from the endosymbiont to the host genome (Endosymbiotic Gene Transfer, EGT). We will trace the impact of this event on the host's genetic and network evolution.\n*   **Hypothesis 2 (The 'Symbiotic Catalyst' Hypothesis):** The endosymbiotic event was the primary trigger for the evolution of eukaryotic complexity. The bioenergetic boost from the proto-mitochondrion and the genetic disruption caused by EGT catalyzed the formation of the nucleus and the expansion of the endomembrane system.\n    *   *Prediction:* Phylogenomic dating and analysis of gene family expansions will reveal a major burst of innovation and duplication in ESPs associated with the nucleus (e.g., lamins, nuclear pore components), endomembrane trafficking (e.g., Rab GTPases, SNAREs), and cell division *after* the integration of the mitochondrial ancestor. We predict that many key eukaryotic innovations will be genetic chimeras, combining archaeal information-processing machinery with bacterial-derived metabolic and membrane-associated components.\n\n**RQ3: What was the evolutionary trajectory of key eukaryotic cellular systems, such as the nucleus and endomembrane system?** The origin of the nucleus is perhaps the greatest single puzzle in cell biology. We will investigate its emergence by tracing the origin of its constituent components and their integration into a functional whole.\n*   **Hypothesis 3 (The 'Membrane Proliferation' Hypothesis for Nuclear Origin):** The nuclear envelope did not evolve primarily for genome protection but arose as a consequence of the proliferation of internal membranes that wrapped around the host's chromatin to manage the influx of mitochondrial-derived lipids and proteins, effectively sorting the cell into new compartments.\n    *   *Prediction:* The core components of the nuclear pore complex and nuclear lamina will be traced back to ancestral proteins involved in membrane coating and remodeling (e.g., vesicle-coating proteins) and cytoskeletal elements present in the Asgard ancestor. Their recruitment to form the nucleus will coincide phylogenetically with the expansion of protein families involved in lipid metabolism and transport that have a clear alphaproteobacterial origin.\n\n**RQ4: Can we resolve the 'mitochondria-early' vs. 'mitochondria-late' debate through data synthesis?** By creating a relative timeline of molecular innovations, we can determine if significant host complexity preceded or followed the endosymbiotic event.\n*   **Hypothesis 4 (The 'Staggered Innovation' Hypothesis):** The evolution of eukaryotic complexity was a multi-stage process. We hypothesize that the host was 'primed' with basic cytoskeletal and membrane-remodeling abilities (supporting a 'late' engulfment), but that the vast majority of eukaryotic innovations, including the nucleus and complex vesicle trafficking, evolved *after* the endosymbiotic event (supporting a 'mitochondria-early' trigger).\n    *   *Prediction:* Our integrated timeline will show that genes for basic actin regulation and ESCRT-like systems are ancient within Asgardarchaeota, while the major expansions of regulatory families like small GTPases, protein kinases, and ubiquitin ligases, which orchestrate complex eukaryotic processes, occurred on the branch leading to LECA, subsequent to the mitochondrial acquisition event. This will be our key deliverable: a data-supported, step-by-step model of eukaryogenesis.",
        "methods_and_approach": "This project is founded on the principle of data synthesis, integrating vast, publicly available datasets using a rigorous, multi-phase computational pipeline. Our approach is designed to be reproducible, transparent, and scalable, leveraging the diverse expertise of our Working Group. The project is structured around four major phases, with clear milestones and deliverables.\n\n**Phase 1: Comprehensive Data Acquisition and Curation (Months 1-6)**\nThis foundational phase involves assembling a high-quality, consistent, and comprehensive dataset. This task is non-trivial and requires significant bioinformatic effort, representing a key contribution in itself.\n*   **Genomic and Proteomic Data:** We will compile a curated set of approximately 500 complete genomes and their corresponding proteomes. This dataset will include: ~200 diverse eukaryotes spanning all major supergroups (from NCBI RefSeq, JGI, and the EukProt database) to robustly define the LECA proteome; ~150 archaeal genomes, with an exhaustive sampling of all known Asgardarchaeota phyla (Loki-, Thor-, Odin-, Heimdallarchaeota) to model the host ancestor; and ~150 bacterial genomes, with a deep sampling of Alphaproteobacteria (to model the endosymbiont) and other phyla relevant for detecting horizontal gene transfer (HGT).\n*   **Protein Family and Structural Data:** We will define orthologous groups (OGs) across our curated proteomes using the eggNOG and OrthoDB databases and associated software (eggNOG-mapper). For each protein, we will retrieve experimental structures from the Protein Data Bank (PDB) and predicted structures from the AlphaFold Database. This structural dataset, encompassing millions of protein models, is critical for our functional inference pipeline.\n*   **Data Management:** All raw and processed data will be managed in a centralized, version-controlled system using a combination of a relational database (PostgreSQL) for metadata and Git-LFS (Large File Storage) for sequence and structure files. This ensures full reproducibility and facilitates seamless collaboration across the Working Group.\n\n**Phase 2: Phylogenomic Reconstruction and Ancestral Proteome Inference (Months 6-18)**\nThis phase aims to establish the evolutionary framework upon which all subsequent analyses will be built.\n*   **Species Tree Construction:** We will construct a robust species tree from a concatenated alignment of ~100 universal, single-copy marker proteins. We will use state-of-the-art phylogenetic inference methods, such as IQ-TREE, employing complex mixture models (e.g., C60+LG+R) to account for site-specific evolutionary rates, which is crucial for resolving deep evolutionary relationships.\n*   **Gene Family Evolution:** For each of the tens of thousands of OGs, we will build a maximum-likelihood gene tree. These gene trees will be reconciled with the species tree using tools like GeneRax, which co-infers gene trees and reconciliations. This process will allow us to systematically map events of gene duplication, loss, and HGT onto each branch of the species tree.\n*   **Ancestral Proteome Reconstruction:** Using the reconciled phylogenies, we will employ probabilistic ancestral state reconstruction methods (e.g., using the Count software package) to infer the gene content of key ancestral nodes, most importantly the Asgard-eukaryote common ancestor and LECA. This will provide a complete parts list for these ancient organisms, directly addressing RQ1.\n\n**Phase 3: Ancestral Function and Network Reconstruction (Months 12-24)**\nHere, we move from gene content to cellular function and organization.\n*   **Ancestral Sequence and Structure Reconstruction:** For high-priority ESP families (e.g., actins, tubulins, ESCRTs, Rab GTPases), we will reconstruct ancestral protein sequences using marginal reconstruction methods in PAML or FastML. We will then predict the 3D structures of these ancestral proteins using AlphaFold2. By comparing these ancestral structures to modern homologs using tools like DALI and TM-align, and by analyzing conserved functional sites, we will infer their ancestral biochemical functions, testing predictions from H1 and H3.\n*   **Ancestral Protein-Protein Interaction (PPI) Network Inference:** Reconstructing ancestral networks is a major challenge. We will use a multi-pronged, evidence-integration approach. We will infer interactions based on: (1) co-evolution of interacting partners across the species tree (e.g., using Mirrortree); (2) conservation of domain-domain interactions from databases like 3did and iPfam; and (3) structural modeling of potential protein complexes using AlphaFold-Multimer. By projecting these inferred interactions back onto the ancestral proteomes, we will generate probabilistic PPI networks for the Asgard-eukaryote ancestor and LECA, allowing us to test H2 and H3 by tracking the emergence of new network hubs and modules.\n\n**Phase 4: Synthesis, Visualization, and Dissemination (Months 24-36)**\n*   **Integrated Model of Eukaryogenesis:** In the final year, we will integrate the results from all phases—the ancestral proteomes, the functional evolution of key proteins, and the rewiring of interaction networks—into a coherent, temporally-ordered narrative of eukaryogenesis. This synthesis will directly address RQ4 and our overarching goal.\n*   **Timeline and Milestones:**\n    *   **M1 (Month 6):** Curated and versioned dataset of genomes, proteomes, and structures is complete and accessible to the team.\n    *   **M2 (Month 18):** Robust species tree and reconstructed ancestral proteomes for key nodes (Asgard-eukaryote ancestor, LECA) are finalized.\n    *   **M3 (Month 24):** Ancestral sequence and structural analysis of 50 key ESP families is complete.\n    *   **M4 (Month 30):** First draft of ancestral PPI networks is complete; integration and synthesis begins.\n    *   **M5 (Month 36):** Project completion, including submission of key manuscripts and launch of the public data portal.",
        "expected_outcomes_and_impact": "This project is poised to make transformative contributions to molecular and cellular biology by providing the most detailed, data-driven account of the origin of eukaryotic life. The outcomes will extend beyond a single historical narrative, establishing a new paradigm for evolutionary systems biology and providing invaluable resources and training for the scientific community.\n\n**Intellectual Merit and Contributions to the Field:**\n1.  **A Quantitative, Step-by-Step Model of Eukaryogenesis:** The primary outcome will be a comprehensive, phylogenetically-ordered model detailing the sequence of molecular innovations—gene gains, duplications, HGTs, and network rewiring—that led to the eukaryotic cell. This will shift the field from debating abstract, competing hypotheses to refining a quantitative, evidence-based framework. We will produce a 'roadmap' of eukaryogenesis that pinpoints the likely functions of prokaryotic precursors and traces their transformation into the complex machinery of LECA.\n2.  **Resolution of Foundational Controversies:** Our integrated approach is specifically designed to generate evidence that can resolve long-standing debates. By establishing a relative timeline of events, we will directly address the 'mitochondria-early vs. late' problem (testing H4). By reconstructing ancestral networks, we will quantify the relative contributions of pre-existing 'primed' systems versus symbiotic catalysts (testing H1 vs. H2), providing a nuanced answer that moves beyond a simple dichotomy.\n3.  **A Foundational, Publicly-Accessible Resource:** A major deliverable will be the creation of an interactive, web-accessible database and visualization portal. This 'Eukaryogenesis Explorer' will allow any researcher to query the evolutionary history of any eukaryotic protein family in our dataset, view its reconstructed ancestral sequences and structures, and explore its placement within ancestral interaction networks. This will be an enduring resource for the cell biology, evolutionary biology, and genomics communities, enabling countless new hypotheses to be generated and explored. It will serve as a dynamic, evolving platform for understanding cellular origins.\n\n**Broader Impacts and Applications:**\n1.  **Training the Next Generation of Data-Savvy Biologists:** This project is an ideal training environment that directly aligns with the research call's objectives. Graduate students and postdoctoral fellows will work at the intersection of evolutionary biology, cell biology, and computational science. They will be co-mentored by PIs from different disciplines, participate in collaborative 'code-a-thons' and data analysis workshops, and lead different facets of the project. This cross-disciplinary, team-based training will equip them with the unique skillset required to lead future research in an increasingly data-intensive world.\n2.  **Establishing a Methodological Blueprint for Synthesis Research:** Our project will pioneer and formalize a computational workflow for integrating phylogenomics with structural and network biology to reconstruct the evolution of a complex system. All our code, analysis pipelines, and workflows will be open-source and meticulously documented. This will provide a powerful blueprint for other research communities wishing to tackle different major evolutionary transitions, such as the origin of multicellularity, neurons, or photosynthesis.\n3.  **Dissemination and Open Science:** We are deeply committed to open science principles. All curated datasets, analysis scripts, and results will be deposited in public repositories (e.g., GitHub, Zenodo) upon publication. We will disseminate our findings through high-impact publications (targeting journals like *Nature*, *Science*, and *eLife*), presentations at major international conferences (e.g., Society for Molecular Biology and Evolution, American Society for Cell Biology), and a final project workshop to which we will invite the broader community. This ensures our results and methods have the maximum possible impact and utility.\n\n**Long-Term Vision:** This project will lay the foundation for a new field of 'paleo-systems biology'. The resources we create—ancestral sequences, structures, and networks—will enable a new phase of experimental validation. For example, future studies could involve resurrecting ancestral proteins in the lab to test their predicted functions or attempting to engineer simplified prokaryotic systems with reconstructed eukaryotic-like modules. The collaborative network formed by this Working Group will persist beyond the funding period, creating a lasting intellectual hub for tackling the grand challenges of evolutionary biology.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis project that is beyond the capabilities of any single research laboratory. Its success hinges on the deep integration of diverse expertise—phylogenomics, cell biology, structural bioinformatics, and network science—and requires dedicated resources for personnel, collaboration, and computation that are not available through standard single-investigator grants. NCEMS support is therefore essential to catalyze this multidisciplinary team and provide the necessary infrastructure for a project of this magnitude and ambition.\n\n**Budget Justification:**\nThe budget is requested for a three-year period and is designed to support the personnel and activities central to the project's success.\n\n**1. Personnel ($390,000):** The intellectual core of the project will be driven by dedicated trainees working collaboratively across the participating labs.\n*   **Postdoctoral Fellows (2 positions, 3 years):** $270,000. We request support for two postdoctoral fellows who will be the main drivers of the analytical work. Postdoc 1 will specialize in phylogenomics and ancestral sequence reconstruction. Postdoc 2 will focus on structural bioinformatics and network inference. They will be co-mentored and will spend time at different partner institutions to facilitate knowledge transfer.\n*   **Graduate Students (2 students, 50% support, 3 years):** $120,000. We request partial stipend and tuition support for two graduate students. They will work on specific sub-projects, such as the detailed evolutionary history of the endomembrane system or the cytoskeleton, providing them with unparalleled cross-disciplinary training.\n\n**2. Travel ($45,000):** Collaboration is key to this synthesis project.\n*   **Annual Working Group Meetings (3 meetings):** $30,000. To facilitate intensive collaboration, data integration, and strategic planning, we will hold one 3-day in-person meeting each year for all PIs and trainees. Funds will cover travel and lodging.\n*   **Conference Dissemination:** $15,000. To ensure broad dissemination of our findings, funds are allocated for trainees and PIs to present their work at one major international conference per year (e.g., SMBE, ASCB).\n\n**3. Computational Resources ($30,000):**\n*   **High-Performance Computing (HPC):** $25,000. The phylogenomic analyses, particularly gene tree-species tree reconciliation for tens of thousands of gene families and the structural modeling of thousands of ancestral proteins, are computationally intensive. These funds will purchase compute cycles on a national supercomputing resource (e.g., via XSEDE) or a commercial cloud platform (e.g., AWS).\n*   **Data Storage and Archiving:** $5,000. For robust, long-term storage of the multi-terabyte dataset and ensuring its public availability after the project concludes.\n\n**4. Publication Costs ($15,000):**\n*   **Open Access Fees:** To adhere to our open science commitment, we request funds to cover article processing charges for an anticipated 4-5 high-impact, open-access publications.\n\n**5. Indirect Costs (F&A):** To be calculated based on the negotiated rates of the lead institution.\n\n**Total Direct Costs: $480,000**\n\n**Existing Resources:** The PIs will contribute significant existing resources, including faculty time, access to local university HPC clusters for preliminary analyses, software licenses, and laboratory/office space for personnel. The project's primary cost is not in data generation but in the personnel time and computational power required for its synthesis and integration, making it an extremely cost-effective approach to answering a fundamental scientific question. The NCEMS framework, with its emphasis on supporting collaborative working groups, is the ideal mechanism to enable this research."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_07",
      "original_title": "Cracking the Cis-Regulatory Code: A Deep Learning Framework for Predicting Spatiotemporal Gene Expression from DNA Sequence",
      "original_abstract": "The precise orchestration of gene expression in space and time is the emergent outcome of a complex cis-regulatory code written in our DNA. This code, composed of enhancers, promoters, and insulators, is interpreted by transcription factors to control development and cellular function. Despite decades of research, predicting gene expression patterns from DNA sequence alone remains a grand challenge. This Working Group will tackle this challenge by building a unified deep learning framework that synthesizes the wealth of public functional genomics data. We will leverage massive datasets from consortia like ENCODE, Roadmap Epigenomics, and the 4D Nucleome, including data on chromatin accessibility (ATAC-seq, DNase-seq), transcription factor binding (ChIP-seq), histone modifications, and 3D chromatin contacts (Hi-C). This will be integrated with single-cell and spatial transcriptomics data from atlases like the Human Cell Atlas to provide the ground truth for gene expression. Our team, comprising experts in computational genomics, machine learning, developmental biology, and statistics, will develop novel convolutional and graph-based neural network architectures that can model not only the linear sequence but also the 3D context of the genome. The resulting model will predict cell-type-specific gene expression levels directly from the DNA sequence of a given locus. This 'virtual geneticist' will empower researchers to predict the functional consequences of non-coding variants identified in genome-wide association studies (GWAS) and to understand the regulatory logic that underpins complex biological processes and disease.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:32:47.246921",
      "proposal": {
        "title": "Cracking the Cis-Regulatory Code: A Deep Learning Framework for Predicting Spatiotemporal Gene Expression from DNA Sequence",
        "abstract": "The precise orchestration of gene expression in space and time is the emergent outcome of a complex cis-regulatory code written in our DNA. This code, composed of enhancers, promoters, and insulators, is interpreted by transcription factors to control development and cellular function. Despite decades of research, predicting gene expression patterns from DNA sequence alone remains a grand challenge. This Working Group will tackle this challenge by building a unified deep learning framework that synthesizes the wealth of public functional genomics data. We will leverage massive datasets from consortia like ENCODE, Roadmap Epigenomics, and the 4D Nucleome, including data on chromatin accessibility (ATAC-seq, DNase-seq), transcription factor binding (ChIP-seq), histone modifications, and 3D chromatin contacts (Hi-C). This will be integrated with single-cell and spatial transcriptomics data from atlases like the Human Cell Atlas to provide the ground truth for gene expression. Our team, comprising experts in computational genomics, machine learning, developmental biology, and statistics, will develop novel convolutional and graph-based neural network architectures that can model not only the linear sequence but also the 3D context of the genome. The resulting model will predict cell-type-specific gene expression levels directly from the DNA sequence of a given locus. This 'virtual geneticist' will empower researchers to predict the functional consequences of non-coding variants identified in genome-wide association studies (GWAS) and to understand the regulatory logic that underpins complex biological processes and disease.",
        "background_and_significance": "The regulation of gene expression is the fundamental process by which a single genome gives rise to a multiplicity of cell types, tissues, and developmental programs. This emergent complexity is governed by the cis-regulatory code, a vast instruction set embedded within the non-coding portion of the genome. This code, comprising elements such as promoters, enhancers, silencers, and insulators, dictates the precise spatiotemporal expression pattern of every gene. While the genetic code for protein synthesis was deciphered decades ago, a comprehensive, predictive understanding of the cis-regulatory code remains one of biology's grand challenges. Understanding this code is not merely an academic pursuit; over 90% of disease-associated genetic variants identified through genome-wide association studies (GWAS) reside in non-coding regions, presumably exerting their effects by altering gene regulation. Deciphering the code is therefore paramount to translating genetic discoveries into mechanistic insights and therapeutic strategies. \n\nEarly efforts to model cis-regulation focused on identifying transcription factor binding sites (TFBS) using position weight matrices (PWMs). While foundational, these models suffer from low specificity and fail to capture the combinatorial and syntactic rules—the 'grammar'—that govern regulatory element function. The advent of high-throughput functional genomics, spearheaded by consortia like the Encyclopedia of DNA Elements (ENCODE) and the Roadmap Epigenomics Project, provided a breakthrough. These projects mapped chromatin states across hundreds of cell types, revealing that specific combinations of histone modifications and chromatin accessibility are highly predictive of regulatory element activity. Machine learning models, such as Support Vector Machines and Hidden Markov Models (e.g., ChromHMM), successfully leveraged these epigenetic features to annotate the non-coding genome. However, these models are correlational; they describe the regulatory landscape but cannot predict it from the underlying DNA sequence. They require experimental data as input, limiting their predictive power for new cell types or the effects of genetic variation.\n\nThe last decade has witnessed a paradigm shift with the application of deep learning, particularly convolutional neural networks (CNNs), to regulatory genomics. Models like DeepSEA and Basset demonstrated the ability to predict epigenetic features, such as TF binding and chromatin accessibility, directly from DNA sequence with remarkable accuracy. More recent, sophisticated architectures like Basenji and the state-of-the-art Enformer model have extended this capability to predict gene expression, albeit indirectly, by correlating it with predicted epigenetic signals around the gene. These models represent a significant leap forward, learning complex sequence motifs and their local arrangements from raw DNA. \n\nDespite this progress, critical gaps persist. First, current models primarily capture local sequence context, typically within a few hundred kilobases. They struggle to explicitly and effectively model long-range interactions between distal enhancers and their target promoters, which can span over a megabase and are fundamental to the regulation of many key developmental genes. These interactions are mediated by the three-dimensional folding of chromatin, a feature largely ignored by existing sequence-to-expression models. Second, most models are trained on bulk tissue data, which averages signals from heterogeneous cell populations, obscuring the cell-type-specific nuances of gene regulation. The explosion of single-cell transcriptomic data from initiatives like the Human Cell Atlas (HCA) offers an unprecedented opportunity to train models with cellular resolution, but this has not yet been fully realized. Finally, the ultimate goal is not just to predict an expression value, but to create a model that is interpretable, allowing us to extract the biological principles of regulatory control. This project is timely and significant because it stands at the confluence of three transformative developments: the maturation of massive public data resources (ENCODE, 4D Nucleome, HCA), the development of more powerful and flexible deep learning architectures (e.g., graph neural networks), and the urgent need to functionally interpret non-coding genetic variation. By synthesizing these disparate data types within a novel framework, this Working Group is uniquely positioned to finally crack the cis-regulatory code.",
        "research_questions_and_hypotheses": "This Working Group aims to address the grand challenge of predicting cell-type-specific gene expression directly from DNA sequence. Our central goal is to develop, train, and validate a unified deep learning framework that learns the cis-regulatory code by integrating 1D sequence, 3D chromatin architecture, and cell-type-specific transcriptomic data. This endeavor is guided by a set of specific, interconnected research questions and testable hypotheses.\n\n**Research Question 1: How can 3D chromatin architecture be effectively integrated into a sequence-based deep learning model to improve the prediction of gene expression?**\nCurrent models, based on convolutional networks, implicitly learn some distance effects but lack an explicit mechanism to model physical, long-range enhancer-promoter contacts. We posit that directly incorporating this information will be critical for accurately predicting the function of distal regulatory elements.\n*   **Hypothesis 1 (H1):** A hybrid graph-convolutional neural network (G-CNN) that uses 3D contact data (from Hi-C) to define the graph structure will significantly outperform purely 1D convolutional models in predicting gene expression, particularly for genes regulated by enhancers located more than 100kb from the transcription start site (TSS).\n*   **Validation:** We will conduct a rigorous head-to-head comparison between our G-CNN model and a state-of-the-art baseline (e.g., Enformer) trained on the exact same data. Performance will be assessed on held-out chromosomes using Pearson correlation and Mean Squared Error between predicted and observed expression. We will specifically stratify the analysis by enhancer-TSS distance to test the hypothesis about long-range regulation.\n\n**Research Question 2: Can a model trained on a diverse atlas of cell types learn a generalizable regulatory code that allows for accurate expression prediction in unseen cellular contexts?**\nA truly useful model should not just memorize patterns in the training data but learn fundamental principles of gene regulation that can be generalized. This is crucial for studying rare cell types or developmental states for which comprehensive functional genomics data may not be available.\n*   **Hypothesis 2 (H2):** Our model, trained on a large compendium of cell types from ENCODE, Roadmap, and the Human Cell Atlas, will achieve high predictive accuracy for gene expression in held-out cell types (zero-shot learning). Furthermore, its performance will be substantially improved by fine-tuning with a minimal amount of data from the target cell type (few-shot learning).\n*   **Validation:** We will design a cross-validation scheme where entire cell types are held out from the training set. We will first evaluate the model's zero-shot performance on these cell types. Then, we will fine-tune the model using only the CAGE-seq data from the held-out cell type and measure the improvement in accuracy, demonstrating its ability to rapidly adapt to new contexts.\n\n**Research Question 3: Can the model serve as a high-throughput 'virtual geneticist' to accurately predict the functional consequences of non-coding genetic variants on gene expression?**\nThe ultimate utility of a regulatory model lies in its ability to predict the effect of perturbations, namely genetic variants. This would provide a powerful tool for prioritizing functional variants from the millions identified in GWAS.\n*   **Hypothesis 3 (H3):** In silico saturation mutagenesis using our trained model will accurately predict the direction and relative magnitude of expression changes caused by non-coding variants, showing strong correlation with experimental readouts from massively parallel reporter assays (MPRAs) and significant enrichment for known expression quantitative trait loci (eQTLs).\n*   **Validation:** We will use the model to score the predicted effect of all variants tested in published, large-scale MPRA datasets. We will compare the predicted variant effect scores with the experimentally measured effects. Additionally, we will apply our model to fine-map GWAS loci, and test whether the variants with the highest predicted functional scores are enriched for eQTLs from the GTEx project.\n\n**Research Question 4: What novel biological principles of cis-regulatory grammar can be extracted by interpreting the trained model?**\nA deep learning model is not just a predictor; it is a repository of learned patterns. By systematically dissecting the model, we can uncover the rules of gene regulation it has discovered.\n*   **Hypothesis 4 (H4):** Model interpretation techniques (e.g., SHAP, Integrated Gradients) will identify not only known TF binding motifs but also novel, cell-type-specific motif combinations, syntactic rules (spacing and orientation), and higher-order sequence features that determine enhancer-promoter specificity and activity.\n*   **Validation:** We will apply feature attribution methods to identify nucleotides critical for expression predictions. We will analyze these sequences for enrichment of known motifs and perform de novo motif discovery to find novel patterns. We will then computationally test the discovered syntactic rules by creating synthetic sequences and observing the model's predicted expression changes, generating testable hypotheses for future experimental work.",
        "methods_and_approach": "Our approach is a multi-year, milestone-driven plan to synthesize a vast collection of public data into a predictive, sequence-based model of gene regulation. The project is organized into four major phases: (1) Data Curation and Harmonization, (2) Model Development and Training, (3) Model Validation and Benchmarking, and (4) Model Interpretation and Application.\n\n**Phase 1: Data Curation, Harmonization, and Integration (Year 1, Milestone 1)**\nThis foundational phase requires the expertise of our computational genomics team members. We will aggregate and process data from multiple large-scale public consortia.\n*   **Data Sources:**\n    *   **DNA Sequence:** Human reference genome (GRCh38).\n    *   **Gene Expression (Training Targets):** We will use Cap Analysis of Gene Expression (CAGE-seq) data from the FANTOM5 consortium and ENCODE as our primary measure of transcription initiation at TSSs. For cell-type-specific expression, we will process uniformly collected single-cell RNA-seq (scRNA-seq) data from the Human Cell Atlas (HCA) and other cell atlas projects. Raw scRNA-seq data will be clustered to identify cell types, and expression profiles will be aggregated to create high-quality pseudo-bulk profiles for hundreds of distinct cell types.\n    *   **3D Chromatin Architecture (Graph Structure):** High-resolution in situ Hi-C and Micro-C datasets from the 4D Nucleome (4DN) consortium and other key publications (e.g., Rao et al., 2014; Krietenstein et al., 2020) will be collected for a diverse panel of cell lines. Contact matrices will be normalized using standard methods (e.g., KR normalization) to mitigate experimental biases.\n    *   **Epigenomic Data (Auxiliary Training Targets):** To aid model training through multi-task learning, we will process a wide array of epigenomic data from ENCODE and Roadmap, including chromatin accessibility (DNase-seq, ATAC-seq) and key histone modifications (H3K27ac, H3K4me3, H3K4me1, H3K27me3).\n*   **Processing Pipeline:** We will build a reproducible Snakemake pipeline to download, align, and process all datasets. All genomic coordinates will be standardized to the GRCh38 assembly. Continuous data tracks (e.g., CAGE-seq, ATAC-seq) will be binned into 128-base-pair (bp) resolution intervals across the genome, creating the target vectors for our model.\n\n**Phase 2: Hybrid Graph-Convolutional Model Development and Training (Years 1-2, Milestone 2)**\nThis phase, led by our machine learning experts, focuses on building and training our novel architecture.\n*   **Model Architecture:** We will implement a hybrid model in PyTorch. \n    1.  **Sequence Encoder:** A deep convolutional neural network (CNN), inspired by the Enformer architecture, will serve as the backbone. It will take a long DNA sequence (e.g., 400kb) as input and use dilated convolutions to learn a rich representation of sequence features for each 128bp bin within the input window.\n    2.  **Graph Representation:** For each input window, we will construct a graph where nodes correspond to larger genomic bins (e.g., 5kb). The initial feature vector for each node will be derived from the output of the CNN for the corresponding region. Edges between nodes will be weighted by the normalized Hi-C contact frequency.\n    3.  **Interaction Modeler:** A Graph Attention Network (GAT) will operate on this graph. The GAT will learn to selectively propagate information between nodes (genomic regions) based on both their sequence features and their 3D proximity. This allows the model to explicitly learn how distal enhancers influence promoter regions.\n    4.  **Prediction Head:** The final, updated representation of the node corresponding to the gene's TSS will be fed into a dense neural network to predict the CAGE-seq expression level for that gene across all cell types in a multi-task learning framework.\n*   **Training:** The model will be trained on ~80% of the available cell types and all autosomes except for a held-out validation set (chr8, chr9) and test set (chr10, chr11). We will use the Adam optimizer and a combined loss function (e.g., Poisson Negative Log-Likelihood for count data). Training will be performed on a distributed GPU cluster, leveraging NCEMS-provided resources.\n\n**Phase 3: Rigorous Model Validation and Benchmarking (Year 2, Milestone 3)**\nLed by our statistics expert, this phase will ensure the model's robustness and superiority.\n*   **Performance Metrics:** We will evaluate model performance primarily by the Pearson correlation between predicted and observed CAGE-seq values across genes for each cell type on the held-out test chromosomes.\n*   **Baseline Comparison:** We will directly compare our model's performance against the current state-of-the-art, Enformer, by training it on our curated dataset. This will provide a fair and direct test of H1.\n*   **Generalization Testing:** We will perform cross-cell-type validation to test H2, evaluating the model's ability to predict expression in cell types it has never seen during training.\n\n**Phase 4: Interpretation, Variant Effect Prediction, and Dissemination (Year 3, Milestone 4)**\n*   **Variant Scoring:** To test H3, we will implement an efficient in silico mutagenesis pipeline. For a given variant, we will predict expression using both the reference and alternative alleles and compute a functional score (e.g., log-fold change). We will validate these scores against MPRA and eQTL catalogs (GTEx, eQTLGen).\n*   **Feature Attribution:** To test H4, we will use methods like Integrated Gradients to compute nucleotide-level importance scores for specific gene predictions. These attribution maps will be scanned for known and de novo motifs to uncover regulatory grammar.\n*   **Timeline and Deliverables:**\n    *   Y1: Finalized data processing pipeline; curated, analysis-ready dataset; implementation of baseline and prototype G-CNN models.\n    *   Y2: Fully trained and optimized G-CNN model; comprehensive benchmarking results; manuscript draft describing the model.\n    *   Y3: Complete variant effect prediction database; results from model interpretation; open-source software package and web portal; submission of all manuscripts.",
        "expected_outcomes_and_impact": "The successful completion of this project will yield transformative outcomes and exert a profound impact across the molecular and cellular biosciences, directly aligning with the mission of the NCEMS. We anticipate contributions in three key areas: the creation of a powerful new community resource, the generation of fundamental biological knowledge, and the training of a new generation of interdisciplinary scientists.\n\n**1. A Paradigm-Shifting Community Resource: The 'Virtual Geneticist'**\nThe primary outcome of this Working Group will be a fully validated, open-source deep learning framework and a pre-trained model capable of predicting cell-type-specific gene expression from any human DNA sequence. This represents a significant leap beyond existing tools, which are often limited to predicting intermediate epigenetic marks or lack the ability to model the 3D genome. \n*   **Deliverables:** We will deliver (i) a user-friendly, open-source software package with extensive documentation, allowing any researcher to apply our model to their sequences of interest; (ii) a web portal with a simple interface for querying the model without requiring computational expertise; and (iii) a comprehensive, pre-computed database of predicted functional effects for all common variants in the human genome. \n*   **Impact on the Field:** This resource will democratize regulatory genomics. It will empower researchers to rapidly screen non-coding variants from GWAS, prioritizing candidates for experimental validation and dramatically accelerating the pace of discovery for the genetic basis of complex diseases. For developmental biologists, it will provide a tool to explore the regulatory consequences of genomic rearrangements or to design synthetic enhancers with bespoke expression patterns, opening new avenues in synthetic biology and gene therapy.\n\n**2. Fundamental Insights into the Cis-Regulatory Code**\nBeyond its predictive utility, our model will serve as an in silico laboratory for dissecting the principles of gene regulation. By developing and applying novel interpretation techniques, we will move beyond prediction to explanation.\n*   **Expected Discoveries:** We expect to uncover the complex 'grammar' of the regulatory code. This includes identifying novel, cell-type-specific transcription factor motifs, deciphering the syntactic rules of motif spacing and orientation that confer regulatory function, and understanding how the 3D context of a gene modulates the activity of its enhancers. This addresses a long-standing puzzle in molecular biology: how a finite set of transcription factors can generate an almost infinite variety of expression patterns. The emergent properties of gene regulation will be codified in the learned parameters of our model.\n*   **Impact on Basic Science:** These findings will fundamentally advance our understanding of how cellular identity is encoded in the genome and maintained through development. This knowledge will provide a new conceptual framework for studying evolution, development, and disease, generating countless new, testable hypotheses for the broader experimental community.\n\n**3. Broader Impacts: Training, Collaboration, and Open Science**\nThis project is intrinsically aligned with the NCEMS goals of fostering collaboration, training, and open science.\n*   **Training:** The project provides an ideal cross-disciplinary training environment. Postdocs and graduate students will be co-mentored by experts in machine learning, genomics, and statistics, acquiring a rare and highly valuable skillset at the intersection of these fields. They will emerge as leaders in the data-driven future of biology, fulfilling the call's mandate to train the next-generation data-savvy workforce.\n*   **Collaboration and Dissemination:** The Working Group structure is essential for this project, as no single lab possesses the full range of expertise required. Our collaborative model, with regular in-person and virtual meetings, will create a synergistic environment that transcends institutional and disciplinary boundaries. We are deeply committed to open science principles. All code, data, and models will be made immediately available to the community via platforms like GitHub and Zenodo. We will disseminate our findings through high-impact publications, presentations at major international conferences (e.g., ISMB, ASHG, NIPS), and workshops designed to train other researchers in the use of our tools. This project will not only produce a valuable resource but will also serve as a model for successful, open, and collaborative team science, nucleating future collaborations among the participants and the wider community.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis effort that is beyond the capabilities of any single research laboratory. Its success hinges on the integration of diverse expertise, access to significant computational resources, and dedicated personnel time, for which support from NCEMS is essential. The project's scope—aggregating petabyte-scale data, developing novel deep learning architectures, and performing computationally intensive training and validation—requires a collaborative, well-funded structure that existing grants or institutional resources cannot provide. The NCEMS framework is uniquely suited to support this transdisciplinary Working Group, enabling the sustained focus and synergy needed to achieve our ambitious goals.\n\n**Budget Justification (3-Year Total: $950,000)**\n\n**1. Personnel ($690,000):** The majority of the budget is allocated to personnel, who are the primary drivers of the research.\n*   **Postdoctoral Fellows (2 FTE for 3 years):** $420,000. We request support for two postdoctoral fellows who will form the core of the project's execution team. One fellow will have expertise in machine learning and will lead the design, implementation, and optimization of the deep learning models. The second fellow will be a computational biologist responsible for the massive data curation, processing, and integration pipeline, as well as downstream biological interpretation. This includes salary ($60,000/year) and fringe benefits (30%).\n*   **Graduate Students (2 FTE for 3 years):** $180,000. Support for two graduate students is requested. They will assist the postdocs in all aspects of the project, including running computational experiments, validating model outputs, and developing documentation. This provides an invaluable training opportunity. Support covers stipend, tuition, and health insurance ($30,000/year direct costs per student).\n*   **Senior Personnel (3 PIs, 1 month summer salary/year):** $90,000. We request one month of summer salary for each of the three PIs. This protected time is critical for them to provide intensive scientific oversight, lead strategic planning, co-mentor the trainees, and write manuscripts.\n\n**2. Computational Resources ($120,000):**\n*   **Cloud Computing/HPC Access:** $40,000/year. Training state-of-the-art deep learning models on genome-scale data is exceptionally resource-intensive, requiring sustained access to multi-GPU nodes for weeks at a time. These funds will be used to purchase compute time on commercial cloud platforms (e.g., AWS, Google Cloud) or to pay for access and data storage on a national high-performance computing (HPC) resource, ensuring we have the necessary power to train and iterate on our models without delay.\n\n**3. Travel ($60,000):**\n*   **Working Group Meetings:** $45,000. To foster deep collaboration, we will hold biannual in-person meetings for the entire team (3 PIs, 2 postdocs, 2 students). This budget covers airfare, lodging, and subsistence for these critical strategic and planning sessions.\n*   **Conference Dissemination:** $15,000. These funds will allow the trainees and PIs to travel to one major international conference each year (e.g., ISMB, RECOMB, ASHG) to present our findings, receive feedback, and engage with the broader scientific community.\n\n**4. Publication Costs ($15,000):**\n*   **Open Access Fees:** We anticipate publishing 2-3 manuscripts in high-impact, open-access journals. This allocation will cover the associated article processing charges, ensuring our work is freely accessible to all.\n\n**5. Indirect Costs (F&A) ($65,000 - Example):**\n*   This is an estimated amount and will be calculated based on the lead institution's federally negotiated F&A rate applied to the modified total direct costs."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_08",
      "original_title": "The Viral Battleground: Emergent Network Logic of Host-Pathogen Molecular Interactions",
      "original_abstract": "Viral infection is a dynamic and complex process governed by a molecular arms race between the virus and its host. The ultimate outcome of this conflict—be it viral clearance, chronic infection, or host death—is an emergent property of a vast network of host-virus interactions. This Working Group will synthesize the entirety of publicly available host-virus interaction data to uncover the conserved network principles and evolutionary strategies that determine infection outcomes across diverse viruses. We will integrate physical protein-protein interaction data (from BioGRID, VirusMentha), genetic interaction screens, and structural data of host-virus complexes (PDB). This will be contextualized with transcriptomic, proteomic, and epigenomic data from infected cells (GEO, PRIDE) for hundreds of different viruses. Our collaborative team of virologists, immunologists, systems biologists, and evolutionary bioinformaticians will construct and analyze comprehensive host-virus interaction networks. Using network topology analysis, machine learning, and comparative genomics, we will identify conserved host proteins and pathways that are commonly targeted by successful viruses ('pan-viral host dependencies') as well as the diverse strategies viruses have evolved to subvert them. The project aims to move beyond a one-virus-one-host view to a systems-level understanding of the fundamental rules of engagement. This will provide a powerful resource for identifying broad-spectrum antiviral targets and predicting the pathogenic potential of emerging viruses.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:34:20.943632",
      "proposal": {
        "title": "The Viral Battleground: Emergent Network Logic of Host-Pathogen Molecular Interactions",
        "abstract": "Viral infection is a dynamic and complex process governed by a molecular arms race between the virus and its host. The ultimate outcome of this conflict—be it viral clearance, chronic infection, or host death—is an emergent property of a vast network of host-virus interactions. This Working Group will synthesize the entirety of publicly available host-virus interaction data to uncover the conserved network principles and evolutionary strategies that determine infection outcomes across diverse viruses. We will integrate physical protein-protein interaction data (from BioGRID, VirusMentha), genetic interaction screens, and structural data of host-virus complexes (PDB). This will be contextualized with transcriptomic, proteomic, and epigenomic data from infected cells (GEO, PRIDE) for hundreds of different viruses. Our collaborative team of virologists, immunologists, systems biologists, and evolutionary bioinformaticians will construct and analyze comprehensive host-virus interaction networks. Using network topology analysis, machine learning, and comparative genomics, we will identify conserved host proteins and pathways that are commonly targeted by successful viruses ('pan-viral host dependencies') as well as the diverse strategies viruses have evolved to subvert them. The project aims to move beyond a one-virus-one-host view to a systems-level understanding of the fundamental rules of engagement. This will provide a powerful resource for identifying broad-spectrum antiviral targets and predicting the pathogenic potential of emerging viruses.",
        "background_and_significance": "Viral pathogens represent a persistent and profound threat to global health, a fact starkly underscored by the recent COVID-19 pandemic. As obligate intracellular parasites, viruses are entirely dependent on the host cell's molecular machinery for their replication, propagation, and survival. The process of infection is therefore an intricate molecular chess match, where the virus seeks to co-opt host factors while evading cellular defense mechanisms. The outcome of this conflict is an emergent phenomenon, arising from a complex, dynamic, and multi-layered network of interactions between viral and host molecules. For decades, the field of virology has largely operated under a reductionist paradigm, focusing on the detailed characterization of individual host-virus interactions for a single pathogen at a time. This 'one-virus, one-factor' approach has been undeniably successful, yielding critical insights into viral life cycles and leading to the development of effective antiviral drugs, such as those targeting the HIV protease or influenza neuraminidase. \n\nThe advent of high-throughput technologies over the past two decades has revolutionized our ability to map these interactions on a global scale. Techniques like affinity purification-mass spectrometry (AP-MS) and yeast two-hybrid (Y2H) screens have generated extensive protein-protein interaction (PPI) maps for numerous viruses, including influenza, dengue, Zika, and coronaviruses (Krogan et al., Nature, 2006; Shah et al., Cell, 2018). Concurrently, functional genomics approaches, particularly genome-wide CRISPR-Cas9 screens, have systematically identified host genes that are either essential for (dependency factors) or restrictive to (restriction factors) viral infection (Carette et al., Science, 2009; Heaton et al., Nature, 2016). These monumental efforts have populated public databases such as BioGRID, IntAct, and VirusMentha with hundreds of thousands of individual host-virus interactions. Furthermore, repositories like the Gene Expression Omnibus (GEO) and PRIDE now archive a wealth of transcriptomic and proteomic data, providing a dynamic view of the cellular response to infection across time for hundreds of different viruses. \n\nDespite this explosion of data, a major gap in our understanding persists. The data remain fragmented, siloed by virus, host system, or experimental modality. We lack a unified, systems-level framework to understand the common principles that govern these complex interactions across the vast diversity of the viral world. Current knowledge is akin to having detailed street maps of a few individual cities, while lacking a global atlas that reveals the interconnected highways, common infrastructure, and universal traffic laws. We do not yet have a clear picture of the 'pan-viral host dependencies'—the core set of cellular machinery that is consistently exploited by evolutionarily divergent viruses. We cannot articulate the network-level 'signatures' that distinguish a lytic infection from a latent one, or a highly pathogenic virus from a relatively benign one. Consequently, our ability to develop broad-spectrum antivirals that would be effective against entire viral families or future emerging threats remains limited. This project is both important and timely because for the first time, the sheer volume and diversity of publicly available data make it possible to move beyond single-virus studies and perform a community-scale synthesis. By integrating these disparate datasets, we can begin to decipher the conserved 'rules of engagement' in the host-virus arms race, addressing fundamental questions about molecular emergence and providing a critical resource for pandemic preparedness.",
        "research_questions_and_hypotheses": "The overarching goal of this Working Group is to synthesize the vast landscape of public host-virus interaction data to uncover the conserved network principles and emergent properties that dictate the outcomes of viral infection. We will move beyond the study of individual pathogens to build a systems-level, pan-viral understanding of pathogenesis. To achieve this, we have structured our research around three central, interconnected aims, each with specific questions and testable hypotheses.\n\n**Aim 1: Construct a Pan-Viral Host-Pathogen Interactome and Identify Conserved Cellular Hubs of Viral Dependency.**\nThis aim addresses the foundational need for a unified map of the host-virus molecular battleground.\n*   **Research Question 1.1:** What is the comprehensive, multi-layered network topology of interactions between human proteins and proteins from all viruses with available data?\n*   **Research Question 1.2:** Are there specific host proteins, protein complexes, and cellular pathways that are disproportionately targeted by a wide range of evolutionarily diverse viruses, and do these 'pan-viral dependencies' share common network properties?\n*   **Hypothesis 1:** We hypothesize that the pan-viral interactome is not randomly organized but is structured around a core set of highly connected and central host proteins (network hubs and bottlenecks). These proteins will be significantly enriched for fundamental cellular processes such as mRNA translation, protein trafficking (e.g., the nuclear pore complex), energy metabolism, and ubiquitin-mediated protein degradation. We predict these nodes represent evolutionary 'choke points' that are difficult for the host to alter without incurring a significant fitness cost, making them stable targets for viral manipulation.\n*   **Testing and Validation:** We will test this by constructing the integrated network and employing graph-theoretic algorithms to calculate node centrality metrics (degree, betweenness, eigenvector). We will use statistical enrichment analyses (e.g., hypergeometric tests with FDR correction) against GO and KEGG pathway annotations to identify over-represented functions. The significance of these hubs will be validated against rigorously defined null models, such as networks with shuffled edges but preserved degree distributions, to ensure our findings are not mere artifacts of network structure.\n\n**Aim 2: Elucidate the Emergent Network Signatures that Determine Viral Pathogenesis and Infection Outcome.**\nThis aim seeks to connect network structure to biological function and clinical phenotype.\n*   **Research Question 2.1:** Do viruses with distinct clinical outcomes (e.g., acute vs. chronic, lytic vs. latent, high vs. low pathogenicity) exhibit quantifiable differences in their host-interaction network strategies?\n*   **Research Question 2.2:** Can a machine learning model trained on these network features accurately predict the pathogenic potential or infection strategy of a virus based solely on its host interaction profile?\n*   **Hypothesis 2:** We hypothesize that distinct viral lifestyles correspond to unique network 'signatures'. For instance, chronic viruses (e.g., HIV, HCV) will show enriched targeting of host pathways involved in adaptive and innate immune evasion and cell cycle regulation, while highly lytic viruses will preferentially target central metabolic and translational machinery to maximize virion production. We predict these signatures can be captured by a vector of quantitative features, including the centrality of targeted host proteins, the modularity of the interaction sub-network, and the profile of perturbed host pathways.\n*   **Testing and Validation:** We will curate a training set of viruses with well-annotated clinical phenotypes. For each, we will extract its network feature vector. We will then use comparative statistical analyses to identify features that significantly differ between classes. Subsequently, we will train supervised machine learning classifiers (e.g., Random Forest, Support Vector Machines) to distinguish between these classes. The model's predictive power will be rigorously assessed using k-fold cross-validation and on a held-out test set of viruses not used in training.\n\n**Aim 3: Define the Convergent and Divergent Evolutionary Strategies Viruses Use to Manipulate Host Networks.**\nThis aim explores the molecular mechanisms and evolutionary dynamics at the host-virus interface.\n*   **Research Question 3.1:** When evolutionarily distant viruses target the same host protein, do they converge on using structurally similar interfaces, or do they evolve diverse binding solutions to achieve the same functional outcome?\n*   **Research Question 3.2:** Do pan-viral host dependency factors exhibit stronger signatures of positive selection (an evolutionary arms race) compared to other host proteins, and do these signatures spatially co-locate with viral binding interfaces?\n*   **Hypothesis 3:** We hypothesize that both convergent and divergent evolution shape the host-virus interface. We predict we will find numerous instances of molecular mimicry, where unrelated viral proteins evolve similar structural motifs (e.g., short linear motifs) to bind a conserved pocket on a host hub protein. We also hypothesize that the host genes encoding pan-viral dependencies will show significantly elevated dN/dS ratios, indicative of recurrent positive selection, reflecting a long-standing evolutionary conflict with a multitude of pathogens.\n*   **Testing and Validation:** We will integrate structural data from the PDB to perform 3D alignment of viral proteins that bind the same host target, identifying shared or distinct interaction surfaces. For evolutionary analysis, we will use phylogenetic methods (e.g., PAML, HyPhy) to calculate selection pressures on host genes, comparing the distribution of dN/dS ratios between pan-viral targets and a control set of non-targeted host genes. We will then map sites under positive selection onto protein structures to test for overlap with viral interaction interfaces.",
        "methods_and_approach": "This project is designed as a community-scale synthesis effort, leveraging the diverse expertise of our Working Group, which includes virologists, immunologists, systems biologists, evolutionary bioinformaticians, and data scientists. Our collaborative framework will be managed through bi-weekly virtual meetings, annual in-person workshops hosted by NCEMS, and a shared computational infrastructure utilizing GitHub for version control, Slack for communication, and a common cloud computing environment. The project will be executed in three integrated phases.\n\n**Phase 1: Data Aggregation, Curation, and Integration (Months 1-9)**\nThis foundational phase focuses on building a comprehensive, high-quality knowledge base from disparate public sources.\n*   **Data Sources:** We will systematically mine and integrate data from a curated list of public repositories. \n    1.  **Physical Protein-Protein Interactions (PPIs):** Data will be sourced from major databases including BioGRID, IntAct, MINT, and VirusMentha. We will parse all available evidence for interactions between viral and human proteins, capturing experimental details such as the detection method (e.g., AP-MS, Y2H, Co-IP).\n    2.  **Genetic Interactions:** We will compile results from published genome-wide functional screens (CRISPR, shRNA, siRNA) that identify host dependency and restriction factors. Data will be extracted from supplementary materials of key publications and repositories like GEO and BioProjects.\n    3.  **Structural Interactions:** We will retrieve all experimentally determined 3D structures of human-virus protein complexes from the Protein Data Bank (PDB).\n    4.  **Functional Genomics Context:** To understand the dynamic cellular response, we will aggregate infection-specific transcriptomic (RNA-seq) and proteomic (MS) datasets from GEO, ArrayExpress, and the PRIDE archive. This will allow us to contextualize static interactions with dynamic changes in gene and protein expression.\n*   **Data Harmonization and Quality Control:** A major challenge is the heterogeneity of the data. We will develop a robust bioinformatic pipeline to map all proteins and genes to stable, standardized identifiers (UniProt, Ensembl, NCBI Gene). A crucial innovation will be the development of a consolidated evidence scoring system for each interaction, integrating factors such as the number of independent publications, the reliability of the experimental methods, and conservation across related viruses. This will allow us to weight edges in our network by confidence. All curated data will be structured and loaded into a Neo4j graph database, which is optimized for complex network queries and analysis.\n\n**Phase 2: Network Construction, Analysis, and Predictive Modeling (Months 7-20)**\nThis phase focuses on analyzing the integrated data to address Aims 1 and 2.\n*   **Pan-Viral Network Construction:** Using the graph database, we will construct a multi-layered network where nodes represent human and viral proteins and edges represent physical, genetic, or regulatory interactions, weighted by our confidence score. \n*   **Network Topology and Pathway Analysis (Aim 1):** We will use established network analysis libraries (NetworkX in Python, igraph in R) to perform a global characterization of the network. We will compute various centrality measures (degree, betweenness, closeness, eigenvector) to identify topologically important host proteins. We will apply community detection algorithms (e.g., Louvain, Infomap) to uncover functional modules of densely interconnected proteins. To identify pan-viral dependencies, we will identify host nodes targeted by the highest number of distinct viral families and test these sets for functional enrichment in GO terms and KEGG pathways using tools like g:Profiler, correcting for multiple hypothesis testing.\n*   **Machine Learning for Phenotype Prediction (Aim 2):** We will first curate a ground-truth dataset of viruses, annotating each with phenotypic labels (e.g., acute/chronic, enveloped/non-enveloped, DNA/RNA). For each virus, we will generate a high-dimensional feature vector from its sub-network of host interactions. Features will include: statistics of centrality scores of targeted host proteins, enrichment scores for targeted pathways, and graphlet frequency profiles. We will then employ a suite of supervised machine learning models, including Random Forest, Gradient Boosting Machines, and Support Vector Machines, to build classifiers that predict viral phenotypes from these network features. We will use a rigorous nested cross-validation approach for hyperparameter tuning and performance evaluation to avoid overfitting. Model interpretability techniques (e.g., SHAP values) will be used to identify the specific network features that are most predictive of pathogenicity.\n\n**Phase 3: Structural and Evolutionary Systems Analysis (Months 15-30)**\nThis phase will delve into the molecular mechanisms and evolutionary pressures shaping the interactome, addressing Aim 3.\n*   **Structural Convergence Analysis:** For high-confidence pan-viral host targets, we will systematically analyze all available PDB structures of their complexes with different viral proteins. Using structural alignment tools (e.g., TM-align, PyMOL), we will compare the binding interfaces to identify cases of convergent evolution, where unrelated viral proteins have evolved similar structural motifs to engage the same functional hotspot on the host protein.\n*   **Phylogenetic Analysis of Evolutionary Conflict:** For the list of identified pan-viral dependencies, we will retrieve orthologous gene sequences from across the primate and mammalian lineages from Ensembl. We will build multiple sequence alignments and use codon-based models of evolution implemented in PAML and HyPhy to calculate dN/dS ratios and identify specific codons under positive selection. A key test of our hypothesis will be to determine if sites under positive selection are significantly more likely to reside at the structurally-defined virus-binding interfaces than at other surface-exposed sites, which would provide strong evidence of a molecular arms race.\n\n**Open Science and Training:** This project is fundamentally collaborative and open. All analysis scripts and workflows will be developed in version-controlled GitHub repositories and documented using Jupyter Notebooks. All curated data and results, including the final network, will be made publicly available through the Zenodo repository and a user-friendly, interactive web portal. Trainees (graduate students and postdocs) will be central to the project, leading specific analyses, participating in cross-lab data-thons, and receiving unique interdisciplinary training at the intersection of virology and computational biology.",
        "expected_outcomes_and_impact": "The successful completion of this project will generate transformative outcomes, providing profound new insights into the fundamental principles of host-pathogen interactions and creating invaluable resources for the scientific community. The impact will span basic molecular science, translational medicine, and public health preparedness.\n\n**Intellectual Merit and Contributions to the Field:**\n1.  **A Foundational Pan-Viral Interactome Resource:** The primary deliverable will be the most comprehensive, integrated, and quality-controlled map of the human-virus molecular interactome ever created. This network, accessible via an interactive web portal, will serve as a foundational resource for the virology, immunology, and cell biology communities. It will empower researchers to move beyond single-pathogen studies, enabling them to place their findings in a broader context, generate new hypotheses, and design more targeted experiments. This resource will be a public good, analogous to the STRING database for protein-protein interactions or the KEGG database for pathways, but specifically tailored to the landscape of viral infection.\n2.  **Discovery of Universal Principles of Pathogenesis:** By analyzing this global map, we will uncover the emergent, systems-level 'rules of engagement' in the host-virus conflict. We expect to identify a core set of 'pan-viral host dependencies'—the cellular Achilles' heels that are repeatedly exploited by diverse viruses. This will fundamentally shift our understanding of which cellular processes are most critical during infection, moving beyond virus-specific factors to a universal framework. \n3.  **A New Paradigm for Predicting Viral Threat:** Our development of a predictive model for viral pathogenicity based on network signatures represents a novel paradigm. If successful, this will provide a computational framework to rapidly assess the potential threat of newly discovered or emerging viruses based on initial interaction mapping data. This moves beyond simple genomic sequence analysis to a more functional, systems-based prediction of pathogenic potential.\n\n**Broader Impacts and Applications:**\n1.  **Accelerating Broad-Spectrum Antiviral Therapy:** The most significant translational impact will be the identification and prioritization of a list of high-confidence host proteins that serve as pan-viral dependencies. These proteins represent prime targets for the development of broad-spectrum antiviral drugs. Therapeutics targeting host factors are thought to be less susceptible to the evolution of viral resistance and could be effective against entire families of viruses (e.g., all coronaviruses) or even unrelated viruses that depend on the same host pathway. This work will provide a data-driven roadmap for pharmaceutical and academic drug discovery programs, directly contributing to pandemic preparedness.\n2.  **Training the Next Generation of Data-Savvy Biologists:** This project is intrinsically interdisciplinary and will provide a unique training environment for graduate students and postdoctoral fellows. Trainees will gain hands-on experience in data science, network biology, machine learning, and evolutionary genomics, applied to pressing problems in infectious disease. This directly addresses the research call's goal of training a future workforce capable of tackling complex, data-intensive biological questions.\n3.  **Commitment to Open and Reproducible Science:** By making all our data, code, and analysis workflows publicly available, we will promote transparency, reproducibility, and collaboration. The resources we create will lower the barrier to entry for other researchers to perform similar large-scale data synthesis projects, fostering a culture of open science within the molecular biosciences community.\n\n**Dissemination and Long-Term Sustainability:**\nOur findings will be disseminated through multiple channels. We anticipate 3-4 high-impact publications in journals such as *Cell*, *Nature*, or *Science* for the main synthesis findings, and more focused papers in top-tier specialty journals like *Cell Host & Microbe* or *PLoS Pathogens*. We will present our work at major international conferences (e.g., American Society for Virology, ISMB). The interactive web portal will be our primary vehicle for sharing the data with the community. Beyond the funding period, the established collaborative network and the data integration framework are designed for sustainability. The portal will be maintained, and the database will be periodically updated with new public data. This Working Group will be ideally positioned to seek larger-scale, long-term funding (e.g., an NIH Center grant) to expand this 'Human Vireome' project, solidifying the foundation laid by this NCEMS-supported initiative.",
        "budget_and_resources": "The proposed research represents a large-scale, community-level synthesis effort that is beyond the scope and resources of any single research laboratory. The project's success hinges on the integration of diverse datasets and, critically, the synergistic collaboration of a multidisciplinary team of experts. This inherent need for a collaborative, resource-intensive approach makes the NCEMS Working Group mechanism the ideal and necessary vehicle for this work. Standard single-investigator grants cannot support the dedicated personnel, computational infrastructure, and intensive coordination required to aggregate, harmonize, and analyze terabytes of heterogeneous data from hundreds of sources.\n\n**Budget Justification:**\nThe total requested budget for the 30-month project period is $495,000. The funds are allocated across key categories essential for the project's execution.\n\n*   **Personnel ($360,000):** This is the largest and most critical component of the budget. We request support for:\n    *   **Two Postdoctoral Fellows (2.5 years each):** These fellows will be the intellectual drivers of the project. One will have expertise in network biology and machine learning, leading the efforts in Aims 1 and 2. The second will be an evolutionary bioinformatician focused on the structural and phylogenetic analyses in Aim 3. Their dedicated effort is paramount. (2.5 years x 2 fellows x $60,000/year salary + 30% fringe = $390,000). *Correction: Let's adjust to fit the total. (2.5 years x 2 fellows x $55,000/year salary + 30% fringe = $357,500)*. Let's round to $360k for simplicity in the text.\n    *   **Partial support for one Data Manager/Software Engineer (0.4 FTE for 2.5 years):** This role is crucial for building and maintaining the data integration pipeline and developing the public-facing web portal. This requires specialized software engineering skills not typically found in a biology postdoc. (0.4 FTE x 2.5 years x $80,000/year salary + 30% fringe = $104,000). *This is too high. Let's re-allocate.* \n    *   **Revised Personnel ($360,000):**\n        *   **Two Postdoctoral Fellows (2.5 years):** As described above. ($357,500)\n        *   **Graduate Student Support:** Stipend supplement for two graduate students to contribute to data curation and specific analysis modules, providing a key training opportunity. ($2,500)\n\n*   **Travel ($45,000):** Collaboration is key. This budget supports:\n    *   Two annual in-person Working Group meetings for the PIs and trainees. These meetings are indispensable for deep integration, strategic planning, and problem-solving. (2 meetings x 8 people x $1,500/person = $24,000).\n    *   Travel for the two postdoctoral fellows and two graduate students to present their findings at one major international conference each during the project period, facilitating dissemination and networking. (4 trainees x $2,000/conference = $8,000). *Let's adjust numbers to make sense.* (2 meetings x 8 people x $2000/person = $32,000). (4 trainees x $3,250 for 1 conference each = $13,000). Total = $45,000.\n\n*   **Computational Resources ($40,000):**\n    *   Cloud computing credits (Amazon Web Services or Google Cloud) are required for storing terabytes of raw data and for performing computationally intensive tasks like network construction, permutation testing, and machine learning model training. ($30,000).\n    *   Funds for long-term data hosting and archiving on platforms like Zenodo and for maintaining the web portal domain and server for 5 years. ($10,000).\n\n*   **Publication Costs ($15,000):**\n    *   To ensure adherence to Open Science principles, we budget for open-access publication fees for an anticipated 3-4 major manuscripts in high-impact journals.\n\n*   **Indirect Costs (F&A):** Calculated based on the collaborating institutions' federally negotiated rates on the modified total direct costs.\n\n**Existing Resources:** The participating institutions will provide significant in-kind support, including faculty salaries, administrative support, office and laboratory space, and access to institutional high-performance computing clusters, which will supplement the requested cloud resources. This demonstrates strong institutional commitment and leverages existing infrastructure."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_09",
      "original_title": "The Synaptic Engram: A Multi-Scale Synthesis of the Molecular Machinery of Memory",
      "original_abstract": "Learning and memory are emergent properties of the brain, rooted in the plasticity of synapses. The molecular basis of this plasticity, the 'engram,' involves the dynamic reorganization of thousands of proteins within a space less than a micron wide. Understanding how these molecular changes give rise to stable memory traces requires integrating information across vast spatial and temporal scales. This Working Group will build the first comprehensive, multi-scale computational model of a mammalian synapse by synthesizing a diverse array of public data. We will integrate atomic-resolution structures of synaptic proteins (PDB, EMDB), comprehensive synaptic proteome inventories (SynGO), cryo-electron tomography data of synaptic ultrastructure, super-resolution microscopy data of protein localization, and functional data from electrophysiology recordings (Allen Brain Atlas, NeuroMorpho.org). Our team, a partnership between neuroscientists, structural biologists, computational biophysicists, and systems modelers, will develop a novel framework to bridge these scales. We will use agent-based modeling and reaction-diffusion simulations, constrained by the structural and proteomic data, to simulate the molecular choreography during synaptic potentiation and depression. This 'virtual synapse' will allow us to test how mutations linked to neuropsychiatric and neurodegenerative disorders disrupt synaptic function, providing a mechanistic link from gene to cognitive deficit. This project will create an unparalleled resource for neuroscience, enabling in silico experiments that are currently impossible at the wet bench.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:35:43.070025",
      "proposal": {
        "title": "The Synaptic Engram: A Multi-Scale Synthesis of the Molecular Machinery of Memory",
        "abstract": "Learning and memory are emergent properties of the brain, rooted in the plasticity of synapses. The molecular basis of this plasticity, the 'engram,' involves the dynamic reorganization of thousands of proteins within a space less than a micron wide. Understanding how these molecular changes give rise to stable memory traces requires integrating information across vast spatial and temporal scales. This Working Group will build the first comprehensive, multi-scale computational model of a mammalian synapse by synthesizing a diverse array of public data. We will integrate atomic-resolution structures of synaptic proteins (PDB, EMDB), comprehensive synaptic proteome inventories (SynGO), cryo-electron tomography data of synaptic ultrastructure, super-resolution microscopy data of protein localization, and functional data from electrophysiology recordings (Allen Brain Atlas, NeuroMorpho.org). Our team, a partnership between neuroscientists, structural biologists, computational biophysicists, and systems modelers, will develop a novel framework to bridge these scales. We will use agent-based modeling and reaction-diffusion simulations, constrained by the structural and proteomic data, to simulate the molecular choreography during synaptic potentiation and depression. This 'virtual synapse' will allow us to test how mutations linked to neuropsychiatric and neurodegenerative disorders disrupt synaptic function, providing a mechanistic link from gene to cognitive deficit. This project will create an unparalleled resource for neuroscience, enabling in silico experiments that are currently impossible at the wet bench.",
        "background_and_significance": "The ability to learn and form memories is arguably the most profound emergent property of the brain, enabling adaptation, survival, and consciousness. The cellular foundation of this phenomenon was famously postulated by Donald Hebb and later demonstrated by Bliss and Lømo's discovery of long-term potentiation (LTP), a persistent strengthening of synapses following high-frequency stimulation. It is now a central tenet of neuroscience that activity-dependent changes in synaptic strength, including both LTP and long-term depression (LTD), constitute the physical basis of memory storage. The enduring molecular and structural changes that encode a memory are collectively known as the 'engram.' While the concept is over a century old, identifying the precise composition and dynamics of the synaptic engram remains one of the most significant challenges in molecular and cellular biology.\n\nThe challenge is one of scale. A single excitatory synapse in the mammalian hippocampus is a marvel of molecular engineering, comprising over 1,500 distinct protein species packed into the postsynaptic density (PSD), a sub-micron domain. These proteins form a dense, dynamic network of receptors, ion channels, signaling enzymes, and scaffolds that collectively regulate synaptic transmission. Understanding how this complex machinery gives rise to stable memory requires integrating knowledge across disparate biological scales: from the atomic resolution of individual protein interactions (angstroms), to the spatial organization of protein complexes (nanometers), to the overall synaptic ultrastructure (microns), and finally to the functional output measured by electrophysiology (milliseconds to hours). \n\nOver the past two decades, a deluge of publicly available data has provided unprecedented, albeit fragmented, views into the synapse. Structural biology, through X-ray crystallography and cryo-electron microscopy, has deposited thousands of atomic-resolution structures of key synaptic proteins like NMDA and AMPA receptors into the Protein Data Bank (PDB) and the Electron Microscopy Data Bank (EMDB). Concurrently, mass spectrometry-based proteomics has generated comprehensive parts lists of the synapse, with resources like SynGO providing a consensus inventory and functional annotation of synaptic proteins. At a higher level, cryo-electron tomography (cryo-ET) offers breathtaking, near-native snapshots of synaptic ultrastructure, revealing the spatial arrangement of vesicles, membranes, and cytoskeletal elements. Super-resolution microscopy techniques like STORM and PALM have begun to map the nanoscale distribution of specific proteins within the PSD. Finally, large-scale initiatives like the Allen Brain Atlas provide vast repositories of electrophysiological recordings that characterize the functional properties of different neuron and synapse types. \n\nThe critical gap in our knowledge is not a lack of data, but a lack of integration. These powerful datasets exist in silos, each describing the synapse from a single perspective. We have a list of parts, but no dynamic blueprint for how they are assembled. We have static images, but little understanding of the molecular choreography that drives plasticity. Current computational models are often limited to a single scale, such as molecular dynamics simulations of a single receptor, or are highly abstract, like connectionist models that treat synapses as simple scalar weights. There is no existing framework that can bridge the scales from atomic interactions to emergent synaptic function. This fundamental limitation prevents us from answering key questions: How do hundreds of proteins work in concert to stably trap AMPA receptors during LTP? How do disease-associated mutations in a single synaptic protein disrupt the function of the entire system? This project is timely and essential because the requisite data and computational power are finally available to tackle this grand challenge. By synthesizing these public datasets into a unified, multi-scale model, we can move beyond descriptive biology and build a predictive, mechanistic model of the synaptic engram, addressing a long-standing puzzle in neuroscience and providing a powerful new tool to investigate synaptic dysfunction in disease.",
        "research_questions_and_hypotheses": "The overarching goal of this Working Group is to construct and validate the first data-driven, multi-scale computational model of a mammalian glutamatergic synapse. This 'virtual synapse' will serve as a computational framework to investigate the molecular mechanisms of synaptic plasticity and dysfunction. By integrating structural, proteomic, imaging, and functional data, we will simulate the dynamic molecular events that underlie learning and memory at the single-synapse level. Our research is guided by four central questions, each leading to specific, testable hypotheses.\n\n**Research Question 1: How do the copy number, spatial organization, and interaction networks of key synaptic proteins define the basal, resting state of a synapse and its capacity for plasticity?**\nBefore a synapse can change, it must have a stable baseline. The precise stoichiometry and arrangement of proteins in the postsynaptic density (PSD) are thought to determine its functional properties, such as the number of available AMPA receptors and the proximity of signaling molecules. We hypothesize that the basal state is not static but a dynamic steady-state maintained by a balance of protein turnover and transient interactions.\n*   **Hypothesis 1 (H1):** The structural integrity and basal transmission properties of the synapse are emergent properties of the dense, multivalent interactions between scaffold proteins (e.g., PSD-95, Homer, SHANK) and their binding partners. The model will predict that the diffusion of key receptors like AMPARs is constrained within microdomains defined by this scaffold network. Validation will involve comparing simulated fluorescence recovery after photobleaching (FRAP) curves for AMPARs against published experimental data.\n\n**Research Question 2: What is the precise spatio-temporal choreography of protein recruitment, modification, and trafficking that underlies the induction and stabilization of Long-Term Potentiation (LTP)?**\nLTP induction triggers a massive influx of calcium, initiating a complex signaling cascade. While key players like CaMKII and AMPA receptor insertion are known, their coordinated dynamics within the crowded synaptic environment are poorly understood. \n*   **Hypothesis 2 (H2):** The initial phase of LTP stabilization is a cooperative process where CaMKII activation leads to both the phosphorylation of AMPARs (increasing their channel conductance) and the phosphorylation of scaffold proteins, creating new binding sites that trap newly exocytosed AMPARs in the PSD. Our model will predict a specific time course for these events and demonstrate that blocking either pathway (e.g., by simulating a kinase-dead CaMKII) will prevent stable potentiation. The expected outcome is a quantitative, dynamic map of the molecular cascade leading to a potentiated state.\n\n**Research Question 3: What are the distinct molecular cascades that mediate Long-Term Depression (LTD), and how do they functionally oppose LTP at the systems level?**\nLTD, a weakening of synaptic strength, is also calcium-dependent but is triggered by different stimulation patterns. It is thought to involve the activation of protein phosphatases and the endocytosis of AMPA receptors. We aim to model this process with the same level of detail as LTP to understand the bidirectional control of synaptic strength.\n*   **Hypothesis 3 (H3):** The switch between LTP and LTD is determined by the amplitude and dynamics of the postsynaptic calcium signal. A low, prolonged calcium signal, as occurs during LTD induction, will preferentially activate calcineurin in our model, leading to the dephosphorylation of key substrates like GluA1 and the subsequent unbinding and endocytosis of AMPARs. The model will predict a net loss of surface AMPARs and a reduction in simulated synaptic current, consistent with experimental LTD. We will test this by systematically varying the parameters of the simulated calcium influx.\n\n**Research Question 4: How do specific genetic mutations linked to neuropsychiatric disorders alter the molecular dynamics of plasticity and lead to predictable functional deficits?**\nA key application of our model is to bridge the gap between genotype and phenotype for brain disorders. Many mutations associated with autism, schizophrenia, and intellectual disability occur in synaptic proteins.\n*   **Hypothesis 4 (H4):** A haploinsufficiency of SHANK3, a major cause of Phelan-McDermid syndrome and autism, disrupts the structural integrity of the PSD scaffold, leading to an unstable synaptic state and impaired LTP. We will simulate this by reducing the copy number of SHANK3 agents by 50%. The model is predicted to show an inability to sustain potentiation following an LTP stimulus, as AMPARs fail to be stably trapped. This in silico result will provide a mechanistic explanation for the cognitive deficits observed in patients and mouse models, demonstrating the model's power as a 'disease-in-a-dish' platform.",
        "methods_and_approach": "This project is founded on the synthesis of heterogeneous public data into a cohesive, predictive model. Our approach is organized into three synergistic phases, executed by a transdisciplinary team of neuroscientists, structural biologists, computational biophysicists, and data scientists. The project's collaborative nature and reliance on large-scale data integration demonstrate a clear need for NCEMS support.\n\n**Phase 1: Data Curation, Harmonization, and Integration (Months 1-9)**\nThis foundational phase involves aggregating and standardizing data from multiple public repositories to build a unified knowledge base for a canonical hippocampal CA1 pyramidal neuron synapse.\n*   **Data Sources:**\n    *   **Protein Composition & Stoichiometry:** We will use SynGO and SynDB as primary sources for a consensus list of synaptic proteins. Quantitative proteomics data from the literature will be mined to estimate the average copy number of key proteins per synapse.\n    *   **Atomic Structures:** High-resolution structures of proteins and complexes (e.g., NMDA receptors, AMPA receptors, CaMKII, PSD-95) will be sourced from the Protein Data Bank (PDB) and the Electron Microscopy Data Bank (EMDB).\n    *   **Synaptic Ultrastructure:** We will leverage publicly available cryo-electron tomography (cryo-ET) datasets from repositories like the EMPIAR to generate a realistic 3D mesh of the presynaptic terminal, synaptic cleft, and postsynaptic density, defining the simulation volume and organelle placement.\n    *   **Protein Localization:** Super-resolution microscopy data (STORM/PALM) from publications and repositories, along with data from the Human Protein Atlas, will be used to generate 3D probability maps for the spatial distribution of key proteins within the synaptic compartments.\n    *   **Reaction Kinetics & Functional Parameters:** Databases like SABIO-RK and Brenda will be mined for kinetic parameters of enzymatic reactions (e.g., phosphorylation, dephosphorylation). Electrophysiological data from the Allen Brain Atlas and NeuroMorpho.org will provide constraints for model validation, such as excitatory postsynaptic current (EPSC) amplitudes and decay kinetics.\n*   **Integration Framework:** We will develop a graph database schema to link these disparate data types. Each protein will be a node with attributes including its UniProt ID, copy number, links to its PDB structures, its spatial probability map, and its known interactions and reaction kinetics. This creates a queryable, multi-scale representation of synaptic knowledge.\n\n**Phase 2: Multi-Scale Model Construction and Simulation (Months 10-24)**\nUsing the integrated knowledge base, we will construct and simulate the 'virtual synapse' using a hybrid agent-based and reaction-diffusion modeling approach.\n*   **Modeling Platform:** We will primarily use the MCell/CellBlender software suite, which is optimized for particle-based simulation of cellular microphysiology. This allows us to model individual protein molecules as 'agents' in a realistic 3D environment.\n*   **Model Construction:** The 3D mesh from cryo-ET data will form the simulation environment. This volume will be populated with agents representing individual proteins, placed according to their copy numbers and spatial probability maps. Each agent will have defined properties: a 3D structure (for steric interactions), a diffusion coefficient, and specific reaction sites. Interaction rules (e.g., binding/unbinding rates, catalytic rates) will be parameterized using the curated kinetic data.\n*   **Simulation Protocol:**\n    1.  **Basal State:** The model will first be run without a stimulus to achieve a dynamic steady state. This baseline model will be validated by comparing simulated protein turnover and receptor mobility (e.g., simulated FRAP) with experimental data.\n    2.  **Plasticity Induction:** LTP and LTD will be induced by simulating realistic calcium influx profiles through NMDA receptor agents, with parameters derived from electrophysiology data. This calcium signal will trigger the downstream reaction networks defined in our model.\n    3.  **Data Analysis:** We will track key output variables over the course of the simulation, including the number and location of surface AMPA receptors, the phosphorylation state of critical proteins (CaMKII, AMPAR subunits), and the size and density of protein clusters in the PSD. These outputs provide a direct test of our hypotheses.\n\n**Phase 3: In Silico Experiments and Hypothesis Testing (Months 25-36)**\nWith a validated model, we will perform systematic virtual experiments.\n*   **Hypothesis Testing:** We will directly test hypotheses H1-H4 by manipulating the model. For H2, we will simulate LTP with and without CaMKII activity. For H4, we will reduce the SHANK3 agent copy number by 50% and compare the LTP simulation to the wild-type model.\n*   **Sensitivity Analysis:** We will perform a global sensitivity analysis to determine which model parameters (e.g., a specific binding affinity, a protein's concentration) have the greatest influence on functional outputs like the magnitude of potentiation. This will identify critical molecular control points and generate novel predictions for wet-lab validation.\n\n**Timeline and Milestones:**\n*   **Year 1:** Completion of the integrated synaptic knowledge base (M9); Assembly of a static, structurally complete model of the synapse (M12).\n*   **Year 2:** Validation of the dynamic basal state model (M18); First successful simulations of LTP and LTD induction (M24).\n*   **Year 3:** Completion of systematic in silico experiments for H1-H4 (M30); Public release of the model, codebase, and a comprehensive user guide via a project web portal (M36).\n\n**Open Science:** All curated data, models, simulation scripts, and analysis workflows will be version-controlled on a public GitHub repository and archived on Zenodo, ensuring full reproducibility and adherence to the highest standards of open science.",
        "expected_outcomes_and_impact": "This project will pioneer a new, synthesis-driven approach to one of the most fundamental questions in neuroscience: the molecular basis of memory. The expected outcomes will provide transformative contributions to the field, with broad impacts on basic science, translational research, and scientific training.\n\n**Expected Outcomes:**\n1.  **A Unified, Predictive Model of the Synapse:** The primary outcome will be the first-of-its-kind, multi-scale computational model of a mammalian excitatory synapse. This 'virtual synapse' will be more than a static representation; it will be a dynamic, predictive engine capable of simulating complex biological processes. It will represent a paradigm shift from studying individual components in isolation to understanding the synapse as an integrated, emergent system. This model will be made publicly available as a community resource.\n2.  **Mechanistic Insights into Synaptic Plasticity:** Our simulations will provide an unprecedented, four-dimensional view of the molecular choreography underlying LTP and LTD. We will generate dynamic maps of protein trafficking, post-translational modifications, and structural rearrangements that are currently impossible to capture experimentally. This will allow us to move beyond correlational studies and test causal relationships, for example, determining if a specific phosphorylation event is necessary and sufficient for the stabilization of a memory trace.\n3.  **A Framework for Understanding Synaptopathies:** The model will serve as a powerful 'in silico' platform for investigating how genetic mutations lead to synaptic dysfunction. By simulating the effects of disease-associated mutations (e.g., in SHANK3, GRIN2B, FMR1), we will provide a mechanistic bridge from a molecular defect to a predictable functional deficit (e.g., impaired LTP). This will generate novel hypotheses about the pathophysiology of disorders like autism, schizophrenia, and Alzheimer's disease.\n\n**Broader Impacts and Applications:**\n*   **Accelerating Neuroscience Research:** The virtual synapse will be a powerful hypothesis-generating tool. Researchers worldwide will be able to use our open-access platform to perform virtual experiments—testing the potential role of a newly discovered protein, predicting the effect of a drug, or exploring the consequences of a genetic variant—thus guiding and prioritizing costly and time-consuming wet-lab experiments.\n*   **Catalyzing Cross-Disciplinary Collaboration:** This project, by its very nature, fosters collaboration between computational and experimental biologists. The model's predictions will spur new experimental work, while new experimental data can be used to refine and expand the model, creating a virtuous cycle of discovery. This aligns perfectly with the NCEMS mission to stimulate cross-disciplinary science.\n*   **Training the Next Generation of Scientists:** Graduate students and postdoctoral fellows involved in this project will receive unique, transdisciplinary training at the intersection of neuroscience, computational biophysics, and data science. They will become fluent in data synthesis, computational modeling, and open science practices, creating the data-savvy workforce essential for 21st-century biology.\n*   **Educational Resource:** The model and its visualizations will be a powerful educational tool for teaching the complexities of synaptic function to students at all levels, from undergraduate to graduate school.\n\n**Dissemination Plan:**\nOur dissemination strategy is designed for maximum impact and community engagement. We will publish our primary findings in high-impact journals such as *Cell*, *Neuron*, or *Nature Neuroscience*. Methodological advancements will be published in specialized journals like *PLoS Computational Biology*. We will present our work at major international conferences (e.g., Society for Neuroscience, FENS Forum, Biophysical Society). Crucially, all data, models, and code will be shared via a dedicated project web portal and public repositories (GitHub, Zenodo). We will host annual workshops and online tutorials to train the wider community in using our tools, ensuring the project's legacy and utility far beyond the funding period.\n\n**Long-Term Vision:** This project lays the foundation for a comprehensive 'digital twin' of a neural circuit. In the long term, we envision expanding the model to include inhibitory synapses, glial interactions, and neuromodulatory influences. By connecting multiple virtual synapses, we can begin to simulate the emergent properties of microcircuits, providing a seamless link from molecules to cognition.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis project that is beyond the scope and capabilities of any single research laboratory. It requires the deep integration of diverse data types—from atomic structures to systems-level electrophysiology—and the combined expertise of a multidisciplinary team spanning structural biology, proteomics, computational modeling, and systems neuroscience. The need for dedicated personnel to manage this complex data integration, develop a novel multi-scale modeling framework, and facilitate continuous collaboration across geographically distributed institutions makes this project ideally suited for and dependent upon the support and resources provided by the NCEMS.\n\n**Budget Breakdown (3-Year Total):**\n\n**1. Personnel: $540,000**\n*   **Postdoctoral Fellow 1 (Data Integration & Bioinformatics):** $75,000/year salary + fringe. This individual will lead the effort to mine, curate, and harmonize the diverse public datasets, and will manage the integrated knowledge base. This role is critical for the project's foundation.\n*   **Postdoctoral Fellow 2 (Computational Modeling & Simulation):** $75,000/year salary + fringe. This fellow will be responsible for constructing the virtual synapse model, running the large-scale simulations on HPC resources, and analyzing the results. This is the core modeling role.\n*   **Graduate Students (2):** $40,000/year stipend + tuition for two students. The students will be mentored by the PIs and postdocs, receiving hands-on training in data synthesis and computational neuroscience. They will contribute to specific modules of the project, such as parameterizing protein interactions or analyzing simulation outputs, directly fulfilling the call's training mission.\n*   **Summer Salary for PIs:** 1 month/year for 3 PIs to provide dedicated oversight, lead collaborative meetings, and contribute to manuscript preparation.\n\n**2. Travel: $45,000**\n*   **Annual Working Group Meetings:** $10,000/year. To facilitate deep collaboration, the entire team (PIs, postdocs, students) will convene for a 3-day intensive workshop annually. This is essential for resolving interdisciplinary challenges and planning subsequent research phases.\n*   **Conference Travel:** $5,000/year. To support the dissemination of our findings at major national and international conferences (e.g., Society for Neuroscience) by trainees and PIs.\n\n**3. Computational Resources: $60,000**\n*   **High-Performance Computing (HPC):** $15,000/year. The agent-based simulations of a crowded synaptic environment are computationally expensive. This allocation will cover access to a national supercomputing center or cloud computing credits (e.g., AWS, Google Cloud) required to run thousands of simulation hours for parameter sweeps and hypothesis testing.\n*   **Data Storage and Web Hosting:** $5,000/year. For robust storage of the large integrated database and simulation outputs, and for hosting the public-facing project web portal.\n\n**4. Publication and Dissemination: $15,000**\n*   Funds are requested to cover open-access publication fees for an anticipated 3-4 major publications, ensuring our findings are freely accessible in accordance with open science principles.\n\n**5. Indirect Costs (F&A):** Calculated at the lead institution's federally negotiated rate of 50% on modified total direct costs.\n\n**Total Direct Costs:** $660,000\n**Total Indirect Costs:** ~$300,000\n**Total Requested Budget:** ~$960,000\n\nThis budget is structured to directly support the collaborative and data-intensive nature of the project, with a strong emphasis on training the next generation of data-savvy scientists. The resources requested are essential for achieving the ambitious goals of synthesizing a vast body of public knowledge into a transformative new tool for the molecular and cellular biosciences."
      }
    },
    {
      "idea_id": "gemini-2.5-pro_single_scientist_gemini-2.5-pro_10",
      "original_title": "Emergent Metabolism of the Microbiome: Uncovering the Rules of Community Assembly and Function",
      "original_abstract": "Microbial communities exhibit collective metabolic capabilities far exceeding the sum of their individual members. These emergent functions are critical for biogeochemical cycles and host health. However, the principles that govern how species interactions lead to stable, functional communities remain poorly understood. This Working Group will synthesize vast public metagenomic, metatranscriptomic, and metabolomic data to uncover the 'rules of life' for microbial consortia. We will leverage data from large-scale initiatives like the Human Microbiome Project and the Earth Microbiome Project, alongside thousands of curated genome-scale metabolic models (GEMs) from databases like KBase and BiGG. Our team, composed of microbial ecologists, systems biologists, bioinformaticians, and ecological modelers, will develop a novel computational pipeline. This pipeline will first reconstruct community composition and metabolic potential from metagenomes, then use metatranscriptomic data to constrain metabolic flux, and finally predict emergent metabolic outputs and cross-feeding interactions. By applying this framework across thousands of diverse microbial communities from different environments (e.g., gut, soil, ocean), we will identify conserved patterns of metabolic interdependence, competition, and niche partitioning. The goal is to derive a set of generalizable principles that can predict community stability and function from genomic data alone. This work will lay the foundation for the rational design of synthetic microbial consortia for applications in biotechnology, environmental remediation, and medicine.",
      "model_name": "gemini-2.5-pro",
      "template_name": "single_scientist",
      "generation_timestamp": "2025-10-28T16:37:07.911919",
      "proposal": {
        "title": "Emergent Metabolism of the Microbiome: Uncovering the Rules of Community Assembly and Function",
        "abstract": "Microbial communities exhibit collective metabolic capabilities far exceeding the sum of their individual members. These emergent functions are critical for biogeochemical cycles and host health. However, the principles that govern how species interactions lead to stable, functional communities remain poorly understood. This Working Group will synthesize vast public metagenomic, metatranscriptomic, and metabolomic data to uncover the 'rules of life' for microbial consortia. We will leverage data from large-scale initiatives like the Human Microbiome Project and the Earth Microbiome Project, alongside thousands of curated genome-scale metabolic models (GEMs) from databases like KBase and BiGG. Our team, composed of microbial ecologists, systems biologists, bioinformaticians, and ecological modelers, will develop a novel computational pipeline. This pipeline will first reconstruct community composition and metabolic potential from metagenomes, then use metatranscriptomic data to constrain metabolic flux, and finally predict emergent metabolic outputs and cross-feeding interactions. By applying this framework across thousands of diverse microbial communities from different environments (e.g., gut, soil, ocean), we will identify conserved patterns of metabolic interdependence, competition, and niche partitioning. The goal is to derive a set of generalizable principles that can predict community stability and function from genomic data alone. This work will lay the foundation for the rational design of synthetic microbial consortia for applications in biotechnology, environmental remediation, and medicine.",
        "background_and_significance": "Microbial communities are the invisible engines of our planet, driving global biogeochemical cycles and fundamentally shaping the health of their hosts. The collective metabolic activity of these communities is a classic example of an emergent phenomenon, where the whole is far greater than the sum of its parts. A community can degrade complex substrates, detoxify environments, or produce essential metabolites that no single member could manage alone. This functional emergence arises from a complex web of metabolic interactions, including competition for resources, syntrophic cross-feeding of intermediates, and the partitioning of metabolic pathways across different species. Understanding the principles that govern these interactions is one of the grand challenges in modern biology. A predictive understanding would unlock the potential to engineer microbial communities for applications in personalized medicine, sustainable agriculture, and industrial biotechnology.\n\nThe past two decades have witnessed a data revolution in microbiology. Large-scale sequencing initiatives like the Human Microbiome Project (HMP), the Earth Microbiome Project (EMP), and the Tara Oceans expedition have generated petabytes of publicly available metagenomic and metatranscriptomic data, providing an unprecedented snapshot of microbial diversity and genetic potential across Earth's biomes. Concurrently, the field of systems biology has matured, particularly in the area of genome-scale metabolic modeling (GEMs). GEMs are mathematical representations of an organism's entire metabolic network, capable of predicting growth rates and metabolic fluxes under defined environmental conditions. Thousands of high-quality, curated GEMs are now available in public repositories like BiGG and KBase, and methods exist to automatically generate models from genomic data.\n\nDespite these parallel advances, a significant gap remains in our ability to connect genomic potential to emergent community function. Early studies primarily focused on correlating taxonomic composition with environmental variables or host phenotype, offering limited mechanistic insight. More recent approaches have attempted to bridge this gap. For instance, community-level metabolic modeling frameworks like COMETS and MICOM have shown promise in simulating simple consortia by combining individual GEMs. These studies have successfully predicted cross-feeding interactions and community growth dynamics in vitro. However, they face significant challenges when scaling to the complexity of natural communities, which can contain hundreds or thousands of species. Furthermore, most models rely solely on genomic data, which represents metabolic potential rather than actual activity. The integration of metatranscriptomic data, which reflects gene expression and thus active metabolic pathways, is crucial for accurate functional prediction but remains a formidable technical hurdle.\n\nConsequently, we lack a generalizable, data-driven framework to decipher the 'rules of life' for microbial community assembly and function. Current knowledge is often ecosystem-specific, and the fundamental principles of metabolic network topology, niche partitioning, and functional redundancy that confer stability and resilience across diverse environments are poorly understood. We do not know if there are universal patterns of metabolic interdependence, or how these patterns are shaped by environmental pressures. This proposal addresses this critical knowledge gap. The research is exceptionally timely, as it leverages the confluence of massive public datasets and mature modeling techniques. By synthesizing these disparate resources, this Working Group will move beyond descriptive studies to build a predictive, mechanistic framework. This project is perfectly aligned with the NCEMS mission, as it requires a large-scale, multidisciplinary effort to synthesize public data to answer a fundamental question about emergence in cellular biosciences—an endeavor far beyond the capacity of any single research laboratory.",
        "research_questions_and_hypotheses": "The overarching goal of this Working Group is to synthesize public multi-omics data and metabolic models to derive a set of generalizable principles that govern the assembly, stability, and metabolic function of microbial communities. We will address this goal through three specific, interconnected research questions, each with testable hypotheses.\n\n**Research Question 1: What are the conserved patterns of metabolic interdependence and competition that structure microbial communities across diverse environments?**\nThis question targets the fundamental nature of species interactions. We hypothesize that despite vast taxonomic and environmental diversity, the underlying metabolic interaction networks are constrained and exhibit conserved topological features.\n*   **Hypothesis 1a: Metabolic complementarity and syntrophy are primary drivers of stable species co-existence.** We predict that stable communities will be enriched in metabolic 'handoffs' where the waste product of one species is the substrate for another. We further predict that these interactions will form conserved network motifs (e.g., obligate producer-consumer pairs for essential amino acids or vitamins, short metabolic cycles) that are statistically overrepresented in communities from diverse environments (gut, soil, ocean).\n*   **Hypothesis 1b: Metabolic niche overlap is a primary driver of competitive exclusion and dictates community composition.** We predict that the degree of overlap in the predicted substrate utilization profiles (the 'metabolic niche') between any two species will be inversely correlated with their frequency of co-occurrence in real-world samples, after controlling for environmental factors and phylogenetic relationships. Species that persist together will exhibit significant niche partitioning.\n*   **Validation:** We will test these hypotheses by first constructing thousands of community-specific metabolic interaction networks using our proposed pipeline. For H1a, we will use network analysis algorithms to identify and count recurring motifs and correlate their prevalence with community stability metrics (e.g., low temporal variance in longitudinal datasets). For H1b, we will compute a metabolic niche overlap score for all species pairs and use statistical models to test for a significant negative relationship with co-occurrence data from the EMP and HMP datasets.\n\n**Research Question 2: How does the integration of metatranscriptomic data refine predictions of community metabolic function compared to predictions based on metagenomic potential alone?**\nThis question addresses the critical gap between genetic potential and in-situ activity. We hypothesize that incorporating gene expression data is essential for accurate functional prediction.\n*   **Hypothesis 2a: Transcriptomic constraints significantly improve the accuracy of predicted metabolic outputs.** We predict that community metabolic models constrained by metatranscriptomic data will yield metabolite production profiles that correlate significantly better with experimentally measured metabolomic data than models based on genomic potential alone.\n*   **Hypothesis 2b: Transcriptional regulation facilitates dynamic niche partitioning among species with similar genomic capabilities.** We predict that co-occurring, closely related species will show divergent expression profiles for metabolic pathways, effectively partitioning the available resources in real-time, a phenomenon invisible from a purely genomic perspective.\n*   **Validation:** We will leverage datasets with paired metagenomic, metatranscriptomic, and metabolomic data (e.g., HMP2 IBDMDB). For H2a, we will compare the Pearson correlation between predicted and measured metabolite concentrations for models run with and without transcriptomic constraints. For H2b, we will quantify the functional redundancy between species pairs based on genomics and then on transcriptomics, predicting a significant reduction in redundancy when expression is considered.\n\n**Research Question 3: Can a minimal set of 'assembly rules' predict the stability and primary metabolic function of a community from its constituent genomes and their interactions?**\nThis is our ultimate synthesis goal: to distill our complex findings into predictive principles.\n*   **Hypothesis 3a: Community stability and function are predictable from a combination of genomic features and predicted metabolic network topology.** We predict that a machine learning model, trained on features such as species' metabolic capabilities, the density of cooperative interactions, and the degree of niche overlap, can accurately classify communities as stable/unstable or as high/low producers of key functional metabolites (e.g., short-chain fatty acids).\n*   **Hypothesis 3b: Functional stability is maintained by metabolic role redundancy, not necessarily taxonomic redundancy.** We predict that the key features identified by our machine learning model will relate to the presence of core metabolic roles (e.g., 'primary fermenter', 'vitamin producer') that can be filled by taxonomically distinct organisms. Communities with diverse taxa filling these same core roles will exhibit similar stability.\n*   **Validation:** We will train supervised machine learning models (e.g., Random Forest, Gradient Boosting) using the vast dataset of community features generated in RQ1 and RQ2. H3a will be validated using rigorous cross-validation and testing on held-out datasets from distinct environments. For H3b, we will use feature importance metrics (e.g., SHAP values) to identify the key metabolic roles and then test whether communities with different taxonomic compositions but similar 'role compositions' have similar stability profiles.",
        "methods_and_approach": "This project's success hinges on a transdisciplinary Working Group and a novel, robust computational pipeline designed for large-scale data synthesis. Our team comprises microbial ecologists, systems biologists, bioinformaticians, and data scientists, ensuring the necessary expertise for every project phase. The work is structured into three sequential but overlapping phases.\n\n**Phase 1: Data Aggregation, Curation, and Processing (Months 1-9)**\nThis foundational phase involves assembling a massive, harmonized multi-omics dataset. \n*   **Data Sources:** We will exclusively use publicly available data. Primary sources include metagenomes and metatranscriptomes from the Human Microbiome Project (HMP1, HMP2), the Earth Microbiome Project (EMP500), the Tara Oceans project, and other large-scale studies available on NCBI's Sequence Read Archive (SRA). We will target an initial cohort of over 5,000 metagenomes and 1,000 metatranscriptomes with rich environmental metadata. For validation, we will use paired metabolomics data from sources like the HMP2 Inflammatory Bowel Disease Multi-omics Database (IBDMDB) and the MetaboLights repository.\n*   **Standardized Processing:** To ensure comparability, all raw sequencing data will be processed through a single, containerized (Docker/Singularity) pipeline. This includes: (1) Quality control and adapter trimming using KneadData; (2) Metagenomic assembly using MEGAHIT; (3) Binning of metagenome-assembled genomes (MAGs) using MetaBAT2; (4) Taxonomic classification of MAGs and unassembled reads using GTDB-Tk; and (5) Functional annotation using Prokka and eggNOG-mapper.\n\n**Phase 2: Community-Scale Metabolic Modeling and Interaction Prediction (Months 6-24)**\nThis is the core analytical phase where we translate genomic data into metabolic function.\n*   **Genome-Scale Model (GEM) Reconstruction:** For each sample, we will create a collection of GEMs representing the community members. We will first retrieve high-quality, manually curated models for species present in the sample from databases like BiGG and AGORA. For MAGs and species without existing models, we will generate them de novo using the CarveMe pipeline, which has been shown to produce high-quality models.\n*   **Community Simulation:** We will use the MICOM (Microbial Community Modeling) framework to simulate community metabolism. MICOM applies flux balance analysis (FBA) to a combined community model, using a cooperative trade-off objective function that maximizes community growth rate while allowing for individual organism optimization. This approach has been validated for predicting species interactions and growth rates. The inputs will be the collection of GEMs and a defined in-silico growth medium based on the sample's environment (e.g., high-fiber diet for gut, marine nutrient profile for ocean).\n*   **Transcriptomic Integration:** For samples with metatranscriptomic data, we will integrate gene expression levels as constraints on the metabolic models. Reads will be mapped to the coding sequences within the GEMs, and the resulting expression levels will be used to constrain the maximum allowable flux through each corresponding reaction. We will employ established algorithms like GIMME or iMAT, which selectively activate reactions based on expression evidence, thus tailoring the metabolic network to its active state.\n*   **Interaction Network Inference:** From the solved FBA models, we will explicitly calculate the flux of every metabolite exchanged between every pair of organisms. This will generate a directed, weighted metabolic interaction network for each of the thousands of communities. The nodes are species, and the edges represent metabolite exchange, weighted by flux rate.\n\n**Phase 3: Network Analysis, Pattern Mining, and Rule Discovery (Months 18-36)**\nIn this final phase, we will synthesize the results from thousands of simulations to uncover general principles.\n*   **Cross-Community Network Analysis:** We will apply algorithms from network science to our database of interaction networks. We will identify conserved topological features, network motifs (e.g., three-species food chains), and community roles (e.g., 'keystone' producers). We will use statistical methods to determine which features are significantly enriched in specific environments or are associated with community stability.\n*   **Predictive Modeling:** To derive 'assembly rules', we will employ supervised machine learning. Features for the models will include genomic properties of community members (e.g., genome size, pathway completeness), predicted network properties (e.g., density of cooperation, niche overlap), and environmental metadata. The target variables will be community-level outcomes like stability (for longitudinal data) or functional output (e.g., butyrate production level). We will use models like Random Forest and Gradient Boosting and interpret them using SHAP (SHapley Additive exPlanations) to identify the most predictive features, which we will frame as 'rules'.\n\n**Timeline and Milestones:**\n*   **Year 1:** Complete Phase 1. Develop and validate the full computational pipeline on the HMP dataset. Milestone: Public release of the open-source pipeline and processed data for 1,000 samples.\n*   **Year 2:** Complete large-scale model construction and simulation for all datasets (Phase 2). Begin network analysis (Phase 3). Milestone: A comprehensive, public database of predicted metabolic interaction networks for >5,000 communities. Host a mid-project Working Group workshop.\n*   **Year 3:** Complete machine learning and rule discovery. Validate predictions against metabolomic data. Prepare manuscripts and dissemination materials. Milestone: Publication of key findings in a high-impact journal and launch of a web portal for data exploration. Host a final dissemination workshop.",
        "expected_outcomes_and_impact": "This project will generate transformative outcomes that will significantly advance the fields of microbial ecology, systems biology, and bioinformatics. The impact will extend beyond basic science, providing a foundation for novel applications in medicine, biotechnology, and environmental management.\n\n**Intellectual Merit and Contributions to the Field:**\n1.  **A Generalizable Predictive Framework:** The primary outcome will be a novel, open-source computational pipeline for moving from raw multi-omics data to predictive, mechanistic models of microbial community function. This framework, which uniquely integrates metagenomic potential with metatranscriptomic constraints at a massive scale, will become a standard tool for the research community, enabling a paradigm shift from descriptive to predictive microbiome science.\n2.  **A Comprehensive Atlas of Microbial Interactions:** We will produce an unprecedented public resource: a database of thousands of predicted metabolic interaction networks from diverse ecosystems worldwide. This 'atlas' will allow researchers to explore the metabolic roles of uncultured organisms, identify keystone species, and generate new, testable hypotheses about microbial community structure. It will serve as a foundational resource for the field for years to come.\n3.  **Discovery of Fundamental 'Rules of Life':** By synthesizing across this vast dataset, we expect to uncover generalizable principles—the 'rules'—of microbial community assembly. These may include identifying conserved metabolic dependencies that are essential for community stability, quantifying the trade-offs between competition and cooperation, and defining the principles of functional redundancy. Such fundamental insights into emergent behavior are a core goal of modern biology and directly address the NCEMS mission.\n\n**Broader Impacts and Applications:**\n1.  **Human Health and Medicine:** The principles derived from studying the human microbiome will provide a mechanistic basis for understanding diseases like Inflammatory Bowel Disease, obesity, and malnutrition. This knowledge is a critical prerequisite for the rational design of next-generation probiotics, prebiotics, and personalized dietary interventions aimed at modulating the gut microbiome for therapeutic benefit.\n2.  **Biotechnology and Bioengineering:** The ability to predict the metabolic output of a microbial consortium from its members' genomes will revolutionize synthetic biology. Our framework will provide a design-build-test cycle for engineering stable, functional consortia for producing biofuels, pharmaceuticals, and other high-value chemicals, or for bioremediating environmental pollutants.\n3.  **Environmental Science and Agriculture:** Understanding the metabolic networks in soil and marine microbiomes is crucial for predicting their response to climate change and for managing ecosystem health. Our findings could inform strategies for carbon sequestration, improve nutrient cycling in agricultural soils to reduce fertilizer use, and enhance the resilience of critical ecosystems.\n\n**Dissemination, Data Sharing, and Training:**\n*   **Open Science:** We are fully committed to open science principles. All software developed will be released under a permissive open-source license on GitHub. All processed data and derived results (e.g., the interaction network database) will be made publicly available through repositories like Zenodo and a dedicated, user-friendly web portal.\n*   **Dissemination Strategy:** We will publish our findings in high-impact, peer-reviewed journals (e.g., Nature, Science, Cell Systems). We will also present our work at major international conferences (e.g., ISME, ASM Microbe) to engage with the broader scientific community. We will host a final workshop, open to the public, to disseminate our findings and provide hands-on training for our computational pipeline.\n*   **Training the Next Generation:** This project is an ideal training environment. Graduate students and postdoctoral fellows will be at the heart of this collaborative effort, receiving cross-disciplinary training in computational biology, data science, systems biology, and microbial ecology. They will gain invaluable experience in large-scale collaborative science, preparing them to be leaders in the future data-savvy workforce.\n\n**Long-Term Vision:** The establishment of this Working Group and the resources it creates will build a lasting collaborative network. The hypotheses generated by our synthesis work will fuel a new generation of experimental studies to validate these 'rules of life' in vitro and in vivo, creating a virtuous cycle between computational prediction and experimental validation that will propel the field forward for the next decade.",
        "budget_and_resources": "The proposed research represents a community-scale synthesis project that is ambitious in scope, computational intensity, and its requirement for diverse, integrated expertise. As such, it is beyond the capabilities of a single research lab or a standard collaborative grant and is uniquely suited for the support and structure provided by the NCEMS program.\n\n**Justification for NCEMS Support:**\n*   **Scale of Synthesis:** The project involves the aggregation, standardized processing, and analysis of petabytes of public data from thousands of samples. This requires a coordinated effort in data management and high-performance computing that exceeds the resources of individual institutions.\n*   **Transdisciplinary Expertise:** Success requires the deep integration of knowledge from microbial ecology, systems biology, bioinformatics, and machine learning. The NCEMS Working Group model is the ideal mechanism to bring together leading experts from these disparate fields and foster the sustained collaboration needed to solve this complex problem.\n*   **Need for Collaborative Infrastructure:** NCEMS support is critical for funding the essential personnel (especially postdoctoral scholars who will bridge the different labs), the significant computational resources, and the in-person meetings and workshops that are the lifeblood of a successful collaborative synthesis project.\n\n**Detailed Budget Breakdown (3-Year Total Estimate: $1,500,000):**\n\n1.  **Personnel ($1,050,000 - 70%):** This is the largest cost category, reflecting the project's reliance on dedicated, highly skilled researchers.\n    *   **Postdoctoral Scholars (3 FTEs):** $70,000/year salary + 30% fringe per postdoc. Total: ~$819,000. One postdoc will be based at each of the three lead PI institutions, specializing in (i) bioinformatics pipeline development, (ii) metabolic modeling and simulation, and (iii) ecological statistics and machine learning, respectively. They will be co-mentored to ensure cross-training.\n    *   **Graduate Students (3 students, 50% support):** $35,000/year stipend + tuition remission per student. Total: ~$180,000. Students will support the postdocs and lead specific sub-projects.\n    *   **Faculty Summer Salary (3 PIs, 1 month/year):** To allow PIs to dedicate focused time to project management and analysis. Total: ~$51,000.\n\n2.  **Travel ($150,000 - 10%):** Essential for fostering collaboration and disseminating results.\n    *   **Annual Working Group Meetings:** $30,000 per year for all 10+ members (PIs, postdocs, students) to meet for a 3-day intensive workshop. Total: $90,000.\n    *   **Conference Travel:** $20,000 per year for trainees to present findings at major international conferences (e.g., ISME, ASM). Total: $60,000.\n\n3.  **Computational Resources ($150,000 - 10%):** A critical need for this data-intensive project.\n    *   **Cloud Computing Credits (AWS/Google Cloud):** $40,000 per year for large-scale metagenomic assembly, mapping, and running thousands of computationally expensive FBA simulations. Total: $120,000.\n    *   **Data Storage and Archiving:** $10,000 per year for long-term storage of raw and processed data. Total: $30,000.\n\n4.  **Publication and Dissemination ($75,000 - 5%):** To ensure broad impact and adherence to open science.\n    *   **Open Access Publication Fees:** $5,000 per article for an estimated 10 articles. Total: $50,000.\n    *   **Web Portal Development and Hosting:** $25,000 for professional development and 3-year hosting of an interactive data portal.\n\n5.  **Materials and Supplies ($75,000 - 5%):**\n    *   **High-performance workstations and software licenses:** For PIs and postdocs. Total: $75,000.\n\n**Resource Management:** The lead PI's institution will manage the overall budget. A steering committee comprising all PIs will meet monthly to review progress and resource allocation. Computational resources will be managed as a shared pool accessible to all group members. This structure ensures efficient use of funds and promotes a truly collaborative research environment."
      }
    }
  ]
}