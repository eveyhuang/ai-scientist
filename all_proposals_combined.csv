proposal_id,who,role,model,title,abstract,authors,full_draft,proposal_status,ranking
human_1,human,human,human,CYTOKINETIC BOTTLENECKS OF HEAT WAVES,"The lack of motility makes plants highly vulnerable to environmental anomalies such as periods of extreme high temperature known as heat waves. Plants can search for better habitats by exploiting seed mobility, which relies on completing the life cycle and producing seeds. Formation of reproductive organs and seed development requires the production of cells through cell division. The last stage of cell division, cytokinesis, encompasses many key cellular processes, including vesicle trafficking, cell wall biosynthesis, cytoskeletal dynamics, and signaling. While cytokinesis is highly sensitive to heat stress in plants adapted to temperate climates, cytokinesis is more resilient in species adapted to tropical zones. However, how heat stress affects cytokinesis and the mechanisms of cytokinetic robustness across different plant species remains unknown. This project aims to compare changes in cytokinetic components under heat stress in susceptible and hardy species to characterize differential responses of genetic circuits responsible for specific cellular events during cytokinesis. The long term goal is to harness this data for identifying the bottlenecks of cytokinetic robustness. The experimental approach here will be systematic analysis of publicly available data from single-cell and whole-organ omics data collected under control and high-temperature conditions including transcriptomics, translatomics, and proteomics. Predicting cytokinetic bottlenecks will inform future experimental design for their functional characterization leading to better understanding of cellular adaptation mechanisms to heat stress. This knowledge may provide insight into evolutionary strategies of plant adaptation to different climates and improve predictions for heat resiliency at the level of individual plants and populations.",Andrei Smertenko; Carolyn Rasmussen; Dawn Nagel; Georgia Drakakaki; Stephen Ficklin,"Introduction 4.1. Problem overview. 4.1.1. Cytokinesis is critical for plant life. The majority of planetary plant biomass derives from cytokinesis. Plant cytokinesis occurs via the formation of a specialized membrane compartment, the cell plate, between daughter cells. The cell plate gradually matures into a cell wall after cytokinesis is complete. One of the key processes accompanying this transition is replacing callose, the main polysaccharide in the cell plate lumen, with cellulose. A plant-specific polarized secretory module, the phragmoplast, is responsible for constructing the cell plate. The phragmoplast consists of microtubules and actin cytoskeleton, vesicles, and associated proteins. As the phragmoplast diameter is commonly smaller than the cell width, completing the cell plate construction requires phragmoplast expansion. The expansion is driven by microtubules and requires coordination between microtubule dynamics, membrane and cell-wall-component trafficking, exocytosis, vesicle fusion, and endocytosis. Phragmoplast expansion encompasses four steps: establishment of microtubule overlap, vesicle recruitment, vesicle fusion and remodeling, and microtubule depolymerization. Each step requires a dedicated genetic network comprising structural proteins, enzymes, and signaling components. 4.1.2. Cytokinesis is sensitive to heat stress. Exposing tobacco tissue culture cells to 38°C for 2 h causes cytokinetic arrest and elongation of phragmoplast microtubules while cells remain viable; subsequently, microtubule organization does not recover even by 2 hours at normal temperature, indicating that the arrest is long-term. In meiotic Arabidopsis cells treated at 36–38°C, chromosome segregation is disrupted and microtubule density in the phragmoplast is reduced. In the flowering sea grass Cymodocea nodosa, treatment at 38°C for 2 h depolymerizes some phragmoplast microtubules but does not block cytokinesis. The graded heat stress responses among species will allow us to identify both common and distinct resilience mechanisms of cytokinesis. 4.1.3. Outcomes of prior studies 4.1.3.1. Cytokinesis is differentially sensitive to heat stress within the plant lineage. Arabidopsis plants are killed at about 35°C whereas maize plants are killed by temperatures at least 10 degrees higher. Consequently, maize cytokinesis could be resilient to heat stress. We compared the impact of heat stress on cytokinesis in these species. On the basis of published data 38°C was used to induce stress. Exposure of 6-day-old Arabidopsis seedlings to 38°C for 1 hour caused cell plate tilting and disorganization, as indicated by the cytokinetic vesicle marker RABA2a. Cell plate assembly failed resulting in formation of cell wall stubs and phragmoplasts were aberrantly long. We examined microtubule stability in the elongated phragmoplasts using fluorescence recovery after photobleaching and found very slow recovery of the fluorescent signal. As the signal recovery requires microtubule depolymerization, this outcome suggests that heat stress suppressed depolymerization of phragmoplast microtubules. Oppositely, in 3-week-old maize seedlings the phragmoplast morphology appeared unaltered and no cell wall stubs were detected even after exposure to 42°C or 45°C. Thus, cytokinesis is heat-resilient in maize. This outcome is consistent with the report that the 2021 heat wave caused yield losses to all dryland farming crops except maize. Therefore, comparing the impact of 38°C on cytokinetic components in Arabidopsis and tobacco with those in maize will likely inform on mechanisms of cytokinetic robustness. 4.1.3.2. Impact of heat stress on transcription and translation of cytokinetic genes. While analyzing the impact of heat stress on circadian clock-regulated genes, we discovered changes in both the abundance of mRNAs and in the ribosome-binding fraction encoding key cytokinetic genes. For example, the phragmoplast-localized kinesin HINKEL was reduced after heat treatment on both transcriptome and translatome levels. AIR9 transcripts were not significantly affected by heat treatment. Heat stress resulted in lower transcription and translation of multiple cytokinetic genes with the exception of the midzone localized formin homology protein AtFH8 whose translation was specifically upregulated. AtFH8 might contribute to the stabilization of microtubule plus-ends at the phragmoplast midzone. Up-regulation of this protein could be a mechanism to sustain phragmoplast organization under heat stress. Therefore, comparison of transcriptome and translatome data captures the complexity of cytokinetic gene regulation in responses to stresses. 4.1.4. Conceptual framework. According to the theory of biological robustness, genetic networks for individual traits contain specific robustness alleles instead of a master robustness allele acting across multiple networks. Consequently, known robustness alleles for other traits are unlikely to contribute to heat resiliency of cytokinesis. Instead, cytokinetic robustness must be achieved by temporally controlling and genetically buffering distinct cytokinetic processes in a modular fashion. When a genetic network experiences an interruption, both the central hubs and less connected nodes could either limit or maintain resiliency. Consequently, identifying cytokinetic bottlenecks requires generating cytokinetic protein networks and comparing their behaviour under normal and stress conditions. Construction of cytokinetic networks is hindered by limited information about their components. Despite genetic screens for cytokinetic mutants having already identified multiple players, many cytokinetic genes remain unidentified either due to genetic redundancy or lethality. Furthermore, despite biophysical modeling indicating that callose provides the spreading force for membrane remodeling during cell plate expansion, there are numerous knowledge gaps about other carbohydrates and corresponding enzymes during cytokinesis. 4.2. Goals. The long-term goal of our research is to determine the rules and limits of cytokinetic robustness under heat stress and harness this knowledge for improving plant resiliency. Although it has been shown that high temperatures perturb membrane assembly and cytoskeletal dynamics during cytokinesis the underlying molecular mechanisms of such changes remain unknown. The objective of this project is to exploit available data to predict bottlenecks of cytokinesis. A bottleneck is defined as a gene whose product becomes dysfunctional or less functional in response to heat stress. The rationale for this project is based on the following elements: heat stress induces transcriptional changes of genes encoding cytokinetic proteins; plants adapted to hotter environments likely evolved mechanisms to buffer the effects of heat stress on cell division; analysis of available data can inform on differences of cytokinetic responses in heat resilient and heat tolerant species. We plan to achieve the main objective through three specific aims below. 5. Proposed Activities. 5.1. Aim 1. Construct cytokinetic networks using single-cell omics data. Our knowledge about cytokinetic components is limited to several dozens of cytokinesis-defective mutants, a few protein-protein interaction maps, and information about mitotic transcripts. Many proteins required for cytokinesis remain unknown. This aim will take a holistic approach to identify the cytokinetic gene networks using single-cell transcriptomics data from plant species including Arabidopsis thaliana, Zea mays, Oryza sativa, Sorghum bicolor, and Nicotiana tabacum to identify conserved cytokinetic transcripts as well as species-specific cytokinetic transcripts. Raw data will be reprocessed following a standardized pipeline for all omics datasets to allow for effective comparative analysis and interpretation of the data. For single-cell transcriptomics, the analysis for count normalization, dimensionality reduction, clustering visualization, trajectory analysis, and differential gene expression analysis may vary depending on the lab. Therefore, data reprocessing will account for variation and enable statistically significant perturbations to be compared. Then the following pipeline will be used. First, a list of mitotic-related genes including paralogs will be compiled for Arabidopsis from published data. Second, a standardized pipeline such as an orthology inference method will identify orthologous genes from maize, sorghum, rice, and tobacco. Third, the single-cell transcriptome data will be either analyzed or reanalyzed to identify co-expressed transcripts. Fourth, transcripts that are only expressed in cells expressing known mitotic-specific transcripts will be selected as benchmark transcripts to identify coexpressing transcripts. Then, transcripts that appear in fewer than half of the datasets per organism will be more closely assessed for removal from the list. The expected outcome would be a table of Arabidopsis mitosis and cytokinesis proteins and their homologs in other plant species. 5.2. Aim 2. Compare cytokinetic responses to heat stress in heat susceptible and resilient species. Our preliminary experiments demonstrate that cytokinesis is heat-sensitive in tobacco and Arabidopsis thaliana, and heat-tolerant in Zea mays. This aim will compare omics data of Zea mays and other heat-tolerant species with those of heat-sensitive species. As in Aim 1, the raw data will be reprocessed. Analysis will focus on identifying responses that are different between tolerant and susceptible species for each type of omics analyses. We will also identify the transcription factors and cis-regulatory components of these genes that act specifically in heat stress response using available datasets. Completing this aim will inform on which pathways are affected by heat stress in heat tolerant and susceptible species. 5.3. Aim 3. Predict cytokinetic bottlenecks through integration of available datasets. The activities in this aim will construct multi-layered cytokinetic-specific networks and compare their changes under heat stress conditions in susceptible and tolerant species. The first step will be single-layer analysis within a single type of omics data. Analyses for each layer will include differential analysis, correlation network analysis, time series clustering, and association networks. The second stage will be construction of a multiplex network that combines single-layer networks from different omics datasets into a multilayer model. Connections, or edges, between the layers indicate inter-layer relationships. Analysis of networks is expected to generate candidate molecular features with potential influence on differences in cytokinetic responses. These comparisons are predicted to identify cytokinetic bottlenecks under heat stress. We will write a manuscript on this topic and submit it towards the end of year 2. 5.4. Potential problems could arise from the datasets being generated using plants grown under different conditions as well as due to the differences in how the normalization and further analysis were performed. To address this problem we will reanalyze all datasets as outlined in Specific Aim 1, and also including a large number of datasets will help to average the noise caused by variable growth conditions. 6. Outcomes. We anticipate this project will predict cytokinetic bottlenecks under heat stress. This information will be used to formulate the theory of cytokinetic responses to heat stress. The theory will aim to answer the following questions. Which changes to cytokinetic processes in response to heat stress measured either at the transcript or the protein level are common to all species or species-specific? Are there specific subsets of cytokinesis genes or paralogs that have a differential response to heat stress? Do plants eliminate proteins or transcripts or stabilize them to adapt to heat stress? Would these changes promote or block cytokinesis? We will publish this theory as a review article. The theory will inform on the experiments that will lead to the identification of novel genetic, biochemical, and cellular markers of plant heat stress response. Such markers would enable predicting the performance of individual species and stability of entire ecosystems under elevated temperatures, identifying resilient genotypes among the available germplasm for urban ecosystems and afforestation projects, breeding crops with higher yield in hotter environments, developing new approaches for engineering heat-resilient landscaping plant species to contribute to sustainable habitats for humans and animals. 7. Rationale for a working group approach. Our team possesses expertise in cell biology of plant cytokinesis, membrane trafficking, heat stress impact on plant physiology, and protein biochemistry. We collectively have a deep mechanistic understanding of cytokinesis. Further, we collected preliminary data showing that heat stress does not break cytokinetic processes in susceptible plant species, but prevents cytokinetic progression through the arrest of cellular dynamics. The next logical step is to identify cytokinetic pathways affected by the heat stress, and compare them in tolerant and susceptible species using a bioinformatics approach. We want to team up with a synthesis center for the next important stage in understanding the impact of heat stress on cytokinesis. 8. Rationale for support. We hope to gain access to professional expertise in bioinformatics and complex data analysis. Publicly available single-cell and whole-tissue heat stress omics data offers rich material for predicting cytokinetic bottlenecks. Collaboration will lower the barriers of analyzing publicly available omics data for the investigators. Our cell biological data suggests that there will be differentially regulated cytokinetic pathways in heat tolerant and susceptible organisms and we can predict them using bioinformatics analyses. There is currently no alternative funding mechanism to support the transition from an initial concept to an experimental analysis. This project will contribute to developing the theory of cytokinetic robustness and help us to design experiments to test it. 9. Requested resources. To successfully perform the work described in the aims above, a staff scientist with bioinformatics expertise is requested for 4 months per year and an assistant staff scientist for 6 months per year for 2 years. Computational resources for storage and data processing are also needed to successfully implement and achieve the above aims. To accommodate the download and storage of the publicly available datasets listed in the data sources table and support the analyses we estimate and are requesting 50 terabytes of storage and 25,000 CPU hours. ",Accepted,1
human_2,human,human,human,Elucidating genome structure-function relationships in human brain cells,"Single-cell analysis has profoundly enhanced our understanding of brain tissue by facilitating the examination of gene expression and epigenetic features. However, investigating these modalities independently offers a fragmented view of gene regulation and its relationship to transcriptional activity. To overcome this limitation, we propose developing a multimodal generative AI model that integrates single-cell genomics datasets. This framework seeks to illuminate the interplay between three-dimensional genomic architecture and gene expression, uncovering the physical principles underlying the emergence of cell-type-specific gene expression programs, despite the inherent stochasticity of gene regulation at the single-cell level. Applied to datasets from Alzheimer's patients, this approach has the potential to identify epigenetic drivers of disease progression, thereby contributing to a deeper understanding of neurodegenerative processes.",Bin Zhang; Longzhi Tan; Tamar Schlick; Dave Thirumalai; Justin Whalley,"Introduction and Goals The human brain is a highly complex biological system that contains a wide array of distinct cell types, each characterized by specific functions and properties. Understanding this intricate neural architecture requires an investigation into the molecular mechanisms that regulate the brain's cellular diversity, development, and function. A key aspect of this research is the classification of all cell types and the elucidation of their origins from a shared genetic framework. All brain cells share the same genome, yet their identities and functions are shaped by their epigenomes, which consist of covalent modifications to histones and DNA. Chromatin, the structure formed by DNA wrapping around histone proteins, regulates gene expression in eukaryotic cells through a complex network of chemical modifications. These modifications determine the transcriptional activity of the genome within each cell, thus defining their distinct characteristics and roles. To fully characterize a cell, it is essential to identify specific chemical modifications in DNA regions, assess DNA accessibility, analyze chromatin spatial organization, and measure RNA transcription. Single-cell analysis provides a powerful framework for uncovering details and variability often obscured in population-averaged studies. By profiling RNA expression, chromatin accessibility, chromatin organization, and DNA methylation, such analyses have advanced our understanding of the cellular diversity within human brain tissue, shedding light on the intricate epigenetic elements that may constitute regulatory networks. Nonetheless, examining individual modalities at the single-cell level offers an incomplete perspective on the complexity of gene regulatory networks and is insufficient for inferring causal relationships between these networks and gene expression. We propose methodologies to integrate single-cell genomics data for constructing multimodal datasets. With these datasets, we will develop a multimodal generative AI model to capture the distribution across three genomic domains: chromatin accessibility, three-dimensional chromatin organization, and gene expression. This model will define genomic states, offering molecular insights into neuron characteristics, and facilitate a mechanistic understanding of the structure-function relationships within the genome. Further applying the model to single-cell data collected from Alzheimer’s patients will provide insight into molecular determinants that drive disease progression. Proposed Activities Develop a multimodal model for human brain cells. A primary objective of the working group is to develop a multimodal model that integrates chromatin accessibility and three-dimensional chromatin organization with gene expression. This initiative is inspired by recent advances in deep learning techniques, particularly in AI models capable of synthesizing text and image data to provide comprehensive contextual insights. These models have shown considerable effectiveness across various domains, including multimodal data analysis and predictive modeling. The development of this model will enhance the understanding of chromatin organization’s functional role in regulating gene expression under normal conditions and its potential contribution to aberrant gene expression during disease progression. Model Design The proposed model is based on the transformer architecture linking DNA sequences and chromatin accessibility to gene expression. The design builds upon the framework of EPCOT introduced by Zhang et al., which generates transferable embeddings for DNA sequence and chromatin accessibility data that can be fine-tuned for diverse downstream tasks. Notably, this framework demonstrated that these embeddings enable reliable predictions of gene expression from bulk RNA-seq data. Our approach introduces two key advances: a focus on single-cell RNA-seq prediction and the incorporation of structural information from single-cell Hi-C data to improve predictive accuracy. The three-dimensional chromatin organization captured by single-cell Hi-C is represented as a contact matrix and processed using the Vision Transformer architecture, originally designed for image data but successfully applied to Hi-C data. The Vision Transformer encoder transforms the three-dimensional chromatin data into numerical embeddings, which are integrated into the expression prediction module through cross-attention layers. Specifically, the three-dimensional organization is represented by a contact matrix centered on the target gene. This matrix encapsulates the structural context surrounding the gene, including chromatin loops, topologically associating domains, and compartments. The spatial range of the proposed model substantially exceeds that of existing approaches, enabling it to capture a diverse array of gene regulatory mechanisms. Model Parameterization and Validation Parameterizing this model requires co-assay data obtained from experiments that concurrently measure chromatin organization and gene expression. Due to limited availability of such datasets, we propose adopting a pretraining and fine-tuning approach to enhance model transferability. The encoder for single-cell Hi-C will be pretrained using data sources that provide chromatin organization and gene expression information at single-cell resolution for various human brain tissues. These datasets will be consolidated into a consensus list of cell types, facilitating the learning of cell-type-specific chromatin organization features that underpin the corresponding gene expression programs. Beyond generating embeddings for gene expression prediction, the proposed model, termed single-cell chromatin organization embedding (scCOE), is expected to serve as a robust tool for extracting interpretable features of chromatin organization. The pretraining strategy is inspired by the CLIP framework developed by OpenAI that achieves co-embedding of text and image data, thereby linking two distinct modalities. Specifically, we will utilize contrastive learning to align embeddings for chromatin organization with embeddings for gene expression derived from scGPT. scGPT, a large language model trained on single-cell RNA-seq data from millions of cells, has demonstrated significant utility in tasks including cell type annotation, perturbation response prediction, and gene network inference. By aligning chromatin organization embeddings with those from scGPT, our approach aims to uncover structurally relevant features linked to gene function. During this phase, gene expression embeddings will be precomputed and held constant, while only the parameters of the vision transformer encoding chromatin organization will be optimized. A key advantage of using contrastive learning lies in its ability to optimize the model using single-cell data derived from separate experiments. Although chromatin organization and gene expression are not measured in the same cells, categorization into a consensus set of cell types allows optimization. Within this framework, embeddings are optimized so that chromatin organization and gene expression from the same cell type share higher similarity than those from different cell types. Following pretraining, the complete model, named single-cell gene expression prediction (scGEP), will be fine-tuned using additional data. Co-assay data for human brain cells are unavailable; nevertheless, assuming that different cell types share similar gene regulatory mechanisms, we anticipate that the model fine-tuned with available cell types will be transferable to brain cells. Chromatin accessibility will be assessed at the bulk level via ATAC-seq. The chromatin organization and DNA sequence plus ATAC-seq encoders will be adopted from the pretrained EPCOT model, with their parameters frozen. Training will focus on predicting single-cell gene expression based on single-cell chromatin organization contextualized by bulk chromatin accessibility. Tensor Decomposition After preparing the data for the AI model, we can enhance analysis by performing tensor decomposition, a technique successfully applied in projects like the COVID-19 Multi-Omics Blood Atlas and Genomic Advances in Sepsis. This supplementary analysis offers two key benefits: first, dimensionality reduction and outlier detection; second, handling missing data through imputation. By decomposing multimodal data into components, we can reduce its dimensionality and identify or remove outliers that could adversely affect analysis. The tensor decomposition method can accommodate missing data, making it ideal for samples lacking certain modalities. By reconstructing data from decomposed tensors, we can impute missing values, expanding and enriching the dataset for further analysis. Potential Caveats Data will be divided into training, validation, and testing subsets to assess model performance. Limited co-assay data may impact model transferability. If performance is unsatisfactory, additional training will be carried out using co-assay data from mouse cells, which, while not as deep as human datasets, may still serve for pretraining. Hypothesis Variation in genome organization drives gene expression changes Despite extensive research, the mechanisms governing gene expression remain poorly understood. Prior studies have yielded conflicting conclusions regarding chromatin organization’s significance in gene regulation, often due to limited gene coverage or reliance on population-averaged data. The availability of genome-wide datasets encompassing both expression and three-dimensional organization from the same cell provides an opportunity for systematic exploration of chromatin’s role in gene regulation. The computational tool scCOE offers a framework to associate changes in chromatin organization with variations in gene expression. Traditional analyses emphasize structural motifs such as loops, domains, and compartments, but their functional relevance remains uncertain. scCOE generates embeddings aligned with gene expression, capturing functionally significant structural characteristics. Investigating the variation of these embeddings across brain cell types can provide insight into how chromatin organization contributes to neuronal diversity from the same DNA sequence. This approach also enables examination of how heterogeneity in gene expression within a cell type correlates with variations in chromatin architecture. While scCOE offers qualitative insights into chromatin organization’s influence, scGEP quantitatively assesses whether chromatin organization suffices for accurate expression prediction. The multimodal model’s performance will be compared to models using only DNA sequence, chromatin accessibility, or chromatin organization. We hypothesize that a model based solely on DNA sequence will perform moderately well but lack precision, while adding chromatin organization as a contextual feature will substantially improve predictions of cell type-specific expression patterns. Hypothesis Functionally relevant embedding of genome organization offers insight into Alzheimer’s disease Genome organization is hierarchical, encoding vast amounts of information. Interpreting changes within this organization, particularly between normal and diseased cells, poses challenges. Profiling the chromatin conformation of Alzheimer’s disease patients and controls has revealed variability, but identifying functionally relevant variations remains difficult. To address this, we propose two complementary analyses to interrogate genome organization alterations associated with disease. We will first use scCOE to derive embeddings of three-dimensional chromatin organization at the single-cell level. These embeddings, optimized for predicting gene expression, are designed to capture structural features linked to regulatory mechanisms. Dimensionality reduction visualization will be used to compare cells from Alzheimer’s patients and controls. Observing significant structural differences may reveal disease-relevant architectural changes. Downstream modeling will aim to uncover molecular determinants—potentially epigenetic or nucleosome-related—that underlie structural disruptions. Structural analysis of folding mechanisms involving looping motifs and nucleosome interactions can elucidate biophysical rules that dictate genome folding and its relationship with gene expression. Next, scGEP will be used to predict gene expression changes to directly compare patients and controls. Elevated expression of specific genes in microglia has been implicated in pathology. By using scGEP to predict alterations in gene expression, we aim to determine whether such changes result from modifications in chromatin organization. Together, these approaches may provide mechanistic insight into causal relationships between chromatin architecture and expression, identifying potential therapeutic strategies to restore normal gene regulation. Outcomes Technological advancement Two computational tools, scCOE and scGEP, will be produced. scCOE will facilitate analysis and interpretation of single-cell genome organization data as such data become increasingly available. scGEP will enable integration of single-cell multimodal data to better define cell types and characterize cellular diversity. Scientific advancement The proposed research will transform understanding of brain function, potentially identifying molecular markers for diseases such as Alzheimer’s. It will provide insight into the connection between DNA sequence, three-dimensional genome architecture, and gene expression, addressing fundamental questions regarding genome organization’s role in regulation. Publications Two manuscripts are expected, one focusing on technological innovation in multimodal modeling of gene regulatory mechanisms and another focusing on biological insights into genome organization in normal brain and diseased cells. Rationale for a working group approach The proposed approach is interdisciplinary, requiring bioinformatics expertise to organize genomics datasets, deep learning skills to build mechanistic models, and understanding of brain biology to interpret AI findings. The project’s complexity necessitates collaboration across multiple research groups. Rationale for NCEMS support The NCEMS team will play a key role in collecting single-cell genomics data and integrating assays into a unified database. The center’s computational resources will also facilitate deep learning model training. Requested resources Model training will require substantial GPU and CPU computing resources and data storage capacity. We request partial support of a staff scientist with expertise in single-cell genomics and deep learning.",Accepted,7
human_3,human,human,human,MaiTool - LLM-powered bioinformatics tools for microbiome analysis,"Large language models (LLMs) like ChatGPT and BioGPT have revolutionized how researchers engage with scientific literature, offering capabilities such as extracting insights, summarizing methodologies, and identifying relevant publications. These models have applications in microbiome research, where they can assist in identifying datasets in repositories like MGnify and MG-RAST. However, LLMs lack the ability to directly analyze datasets, necessitating integration with bioinformatics tools to enable robust computational analysis. This project aims to develop MaiTool, a graphical interface that combines LLM capabilities with bioinformatics workflows for microbiome and ancient DNA research. MaiTool will enable users to explore publications, access public datasets via APIs, and perform dynamic analyses, integrating quality control measures to enhance reliability. Existing GPT systems (e.g., BioGPT, BioMedLM, and GeneGPT) will be evaluated and fine-tuned for microbiome applications. We will assess repositories such as HMP, IMG/M, and SRA for their suitability and develop APIs for seamless data access and integration. By addressing queries such as studies involving specific taxa, MaiTool will allow LLMs to retrieve literature while bioinformatics pipelines analyze and visualize dataset profiles. This approach aims to advance interdisciplinary research workflows and establish a new standard for AI-driven bioinformatics.",Daniel Huson; Laura Weyrich; Maik Pietzner,"Introduction and Goals The rise of large language models (LLMs) like ChatGPT and BioGPT has revolutionized how researchers engage with scientific publications. These models excel at understanding complex queries, enabling intelligent data retrieval tailored to project-specific needs. For instance, they can extract insights from publications, summarize methodologies, and, in the area of microbiome research, say, identify relevant datasets in repositories like MGnify and MG-RAST. However, the direct analysis of datasets remains beyond the core capabilities of such systems. Integrating LLMs with traditional bioinformatics tools offers a promising solution, bridging the gap between natural language understanding and robust computational analysis. This integration could enable on-the-fly analyses of microbiome data while incorporating essential components like quality control to flag potential issues with datasets early in the pipeline. By combining LLMs’ contextual understanding with the computational power of bioinformatics tools, researchers could streamline workflows, ensuring efficient access to both published findings and actionable data. This project aims to develop a graphical interface “MaiTool” that facilitates the exploration of microbiome-related publications, provides seamless access to publicly available data, integrates quality control measures, and enables dynamic lightweight analyses. Such a system will empower researchers to make informed decisions efficiently, fostering innovation and collaboration in microbiome research and beyond. While the primary focus is on microbiome science, we also plan to explore extensions to computational challenges in analyzing ancient DNA sequences, including the microbiome analysis of ancient DNA samples. Proposed Activities: Data Collection, Curation, and Analysis Framework We propose to evaluate various GPT systems (BioGPT, BioMedLM and GeneGPT) to determine their effectiveness in assisting researchers with engaging and extracting insights from the scientific literature. Building on this foundation, we will identify 5–10 key use cases where the integration of bioinformatics tools—focused on quality assurance, data visualization, and lightweight computational analysis—can enhance the relevance and utility of the results. This approach aims to create a synergistic system that combines the contextual understanding of GPT models with the analytical power of bioinformatics to deliver targeted, actionable insights for microbiome research. In addition to evaluating existing GPT systems for their applicability in microbiome research, we will explore the feasibility of enhancing their performance through two approaches: either setting up our own large language model (LLM) tailored to specific microbiome research needs or fine-tuning existing open-source models. This exploration will help determine the optimal balance between leveraging pre-trained models and creating customized solutions to support advanced analysis and research workflows. In microbiome research, several public repositories provide access to microbiome data. Key repositories include MGnify, MG-RAST, the Human Microbiome Project (HMP), the Microbiome Atlas of Mothers and Infants (MAMI), the Integrated Microbial Genomes and Microbiomes system (IMG/M) and the Sequence Read Archive (SRA) from NCBI (further please see Data Sources Table), which is the most comprehensive resource for raw sequencing data. We will evaluate each of these repositories to assess their suitability as sources of analysis results, focusing on their data accessibility, quality, and relevance to specific research needs. Additionally, we plan to develop software tools that leverage the APIs provided by these repositories to facilitate seamless access, integration, and utilization of their datasets within our system. This will enable efficient retrieval and analysis of microbiome data, supporting advanced research workflows. As an example, consider a user interested in determining whether there are studies that include a specific microbiome species of interest, such as the keystone gut species Christensenella minuta or oral species Methanobrevibacter oralis. Using the proposed system, a GPT model would first identify relevant publications that mention this species, extracting key information such as study descriptions, methodologies, and findings and data accessions from the literature. Simultaneously, using the reported accessions, a bioinformatics pipeline could access taxonomic profiles for the associated studies by accessing public microbiome datasets (along with metadata) through APIs. The pipeline may perform additional analyses, such as generating visualizations or conducting statistical analyses to explore patterns and relationships involving C. minuta. In addition, the GPT can provide a pipeline that the user can run themselves. Preliminary Data, Tools and Analytical Approaches We will conduct a systematic investigation of several GPT models, including BioGPT, BioMedLM, and GeneGPT, to evaluate their suitability for applications in microbiome analysis. For each model, we will assess its capabilities and explore the potential for fine-tuning to enhance support for data analysis tasks. Simultaneously, we will examine key microbiome data resources, including MG-RAST, HMP, IMG/M, MAMI, and datasets hosted at NCBI. This evaluation will focus on methods for extracting data, identifying analyses that can be performed dynamically, and establishing effective communication with the APIs provided by each resource. A third focus of our activities will be determining the best approach for implementing the interactive exploration tool MaiTool. MaiTool will serve as the primary interface for users, connecting seamlessly to an API for tasks such as LLM usage, data retrieval, and lightweight microbiome analysis. We intend to develop a standalone application using Java, JavaFX and Gluon, which would provide a highly responsive and feature-rich user experience, running on all platforms. The development of MaiTool will include a dedicated phase for comprehensive testing and usability studies to ensure the system meets the needs of researchers effectively. Outcomes This project will deliver several key outcomes: 1. Enhanced GPT for Microbiome and Ancient DNA Analysis: We will identify or fine-tune a GPT model specifically tailored to support research in microbiome and ancient DNA analysis. 2. Unified Microbiome Data API: A unified API will be developed to facilitate seamless access to microbiome data from multiple resources, streamlining data retrieval and integration. 3. Lightweight Bioinformatics Analysis Framework: We will create a bioinformatics framework to perform lightweight analyses on data obtained through the API, ensuring efficient and accessible computational workflows. (While a GPT may provide the description of a pipeline, the user will have to run it themselves, if they are interested in starting from scratch or raw data .) 4. Integrated AI-Bioinformatics System: A GPT system will be integrated with the developed tools to enable interactive access and analysis of microbiome data, bridging AI capabilities with bioinformatics functionality. 5. MaiTool Software for Analysis: We will develop a user-friendly software tool called MaiTool, designed to support both microbiome and ancient DNA analysis. MaiTool will provide an interactive interface for researchers, connecting seamlessly to APIs and integrating some analysis capabilities. 6. Insights into AI-Bioinformatics Integration: This project will provide valuable insights into the challenges and opportunities of integrating AI-driven systems with traditional bioinformatics software and algorithms, setting a foundation for future advancements in computational biology. These outcomes aim to advance microbiome research by enhancing data accessibility, analysis capabilities, and the synergy between AI and bioinformatics. Rationale for a working group approach This project focuses on integrating large language models (LLMs) with traditional data retrieval and bioinformatics analysis in the fields of microbiome and ancient DNA research. Achieving this goal requires expertise spanning multiple disciplines. • Daniel Huson specializes in developing algorithms and software for metagenomics and ancient DNA analysis, bringing essential computational and methodological expertise to the project. • Laura Weyrich contributes her extensive experience in the analysis of ancient DNA and ancient microbiomes, providing critical domain knowledge and practical insights into data interpretation and validation. • Maik Pietzner has a substantial track record in molecular epidemiology, spearheading the computational integration of multimodal ‘omics data sets in population-based studies with a particular focus on molecular quantitative trait loci. Together, this collaboration combines cutting-edge AI technology with expertise in microbiome and ancient DNA research, ensuring a robust and innovative approach to tackling these complex scientific challenges. Rationale for NCEMS support This project will require substantial computational resources to support the exploration of LLMs and microbiome resources. These resources will also be critical for hosting the GPT model used by MaiTool. Robust computational capacity is essential for running experiments, managing data workflows, and ensuring the scalability and responsiveness of the system. In addition, staff support will be vital for setting up computational workflows and implementing a REST API, ensuring seamless integration between components and efficient operation of the system. To foster innovation and collaboration, the project will also require support for short-term visitations between the labs of Working Group members. These visits will facilitate brainstorming, knowledge exchange, and the development of strategies to address the project’s interdisciplinary challenges. Requested resources The project requires 10–20 TB of storage for the enhancement and optimization of LLMs. Computational resources include 200–500 CPU cores and 10–30 GPUs, each with at least 100 GB of memory, to facilitate model development and optimization. RAM demands may increase up to 1–3 TB when models are combined. This will require approximately 200,000 CPU hours and 25000 GPU hours. We estimate that running the final model on a NCEMS host for users will require about 100GB of runtime memory, based on a estimated model size of 10 billion parameters. A staff scientist (up to 33% FTE) with expertise in LLMs will assist in model optimization, complementing the efforts of our existing experts. In addition, they can help ensure optimal utilization of NCEMS resources, including computational infrastructure, project management, and team science training. Proposed Timeline Year 1 Months 1–3: Conduct initial exploration of GPT models to assess suitability for microbiome and ancient DNA analysis. Begin initial exploration of microbiome resources (MG-RAST, HMP, IMG/M, MAMI, SRA) to evaluate data accessibility and API capabilities. Months 4–9: Design the framework for integrating LLMs with microbiome resources, including methods for extracting sample accessions and structuring data workflows. Months 6–12: Develop and implement APIs for seamless access to various microbiome resources, ensuring compatibility and efficient data retrieval. Months 10–12: Refine the accuracy of LLM-driven dataset selection. Improve presentation of analysis results, focusing on clarity and relevance for microbiome researchers. Year 2 Months 1–6: Develop and deploy a REST API to provide access to an LLM, supporting data access and bioinformatics analysis. Months 4–6: Build and implement the front-end interface for MaiTool, providing an alpha release for initial feedback. Months 7–9: Conduct comprehensive testing of the integrated system. Finalize documentation and address feedback to prepare for a beta release. Months 10–12: Publish results, describing the system’s design, capabilities, and applications. Release the fully functional system to the public.",Rejected,10
human_4,human,human,human,Synthesizing gene expression data to create an atlas of bacterial cell states,"Bacterial cells have evolved to adapt their molecular composition to maximize growth and survival. Our current knowledge of bacterial cell states is fragmented. Researchers often only analyze data they generate themselves—usually for just one strain under a limited number of conditions—because it is difficult to harmonize omics datasets collected with different technologies in different labs. Our NCEMS working group will synthesize bacterial gene expression data using machine learning to create a first-of-its-kind atlas of bacterial cell states. To manageably scale data collection and analysis, we will build from examining model Escherichia coli strains in lab conditions, to adding wild E. coli strains in natural environments such as the human gut, to including diverse bacterial species with additional cell states. We expect to identify distinct foci corresponding to bacterial growth states and lifestyles (e.g., replicating, starving, motile, biofilm, spore, viable but nonculturable) by mapping the organization and occupancy of cell state space with dimensional reduction approaches. We will examine to what extent the gene expression signatures of different cell states and the relationships among them are conserved during bacterial evolution. Finally, we will use this framework to create a classifier that predicts the cell state composition of a bacterial population from metatranscriptomic data. By following open-science practices, we will make our pipelines and datasets reusable and maintainable by the research community. Our project may reveal new bacterial cell states and ways that we can non-genetically reprogram bacteria to prevent them from adopting lifestyles that damage the environment and cause disease.",Jeffrey Barrick; Jeremy Schmit,"Introduction and Goals Bacterial cells modulate gene expression to adapt their physiology to changes in their environment, for example transitioning from actively replicating to quiescent states in response to nutrient exhaustion. Much like cells within multicellular organisms, clonal bacterial populations can diversify and differentiate into cell types with distinct phenotypes such as biofilm or planktonic growth, motile or sessile forms, and specialized nitrogen-fixing cells. However, it remains unclear to what extent bacterial cells have evolved to occupy a small number of discrete gene expression states that they abruptly transition between, or if they exist along a continuum of states that gradually shift in composition. Less is known about how these gene expression states are related and conserved across species during evolution, and it is possible that some bacterial cell states remain undiscovered because existing data are limited to specific laboratory conditions and strains. The goal of this project is to test key hypotheses about the organization of bacterial gene expression space and to synthesize this information into a unified atlas of bacterial cell states. Specifically, we aim to determine whether bacterial cells occupy and transition between a limited number of distinct states, whether key principles such as the growth–quiescence axis are conserved across species, whether specialized lifestyles such as biofilm formation evolved from subfunctionalization of core states, and whether a comprehensive atlas of these states can enable prediction of population composition from transcriptomic data. We will synthesize existing bacterial gene expression datasets collected using microarray and RNA-seq technologies, beginning with model Escherichia coli strains, then expanding to wild isolates and ultimately to diverse bacterial species, to examine the organization and occupancy of bacterial gene expression state space. Existing Datasets Thousands of bacterial gene expression datasets have been generated and deposited in databases such as OKgeneExpDB, E. coli Expression 2, Ecomics, PRECISE, COLUMBOS, and iModulonDB. These repositories encompass tens of thousands of gene expression profiles representing variations in environmental conditions, genetic backgrounds, and experimental perturbations. Although many datasets include curated metadata and quality control, most existing resources are not fully open or community-maintained, limiting reuse and integration. Prior Analyses Machine learning approaches have previously been applied to predict bacterial growth rates and conditions from gene expression profiles and to identify modules of co-regulated genes. However, differences in technologies, protocols, and experimental designs introduce systematic variation that can confound these analyses. Robust harmonization of datasets and advanced quality control are necessary to enable meaningful synthesis and discovery. Project Significance Previous studies have not focused on systematically identifying bacterial cell states, which we define as distinct growth or lifestyle conditions characterized by coordinated shifts in gene expression across many genes. For example, aerobic planktonic growth represents one state, distinct from anaerobic growth, starvation, or biofilm formation. While single-cell RNA-seq could eventually define these states with precision, technical challenges currently limit its applicability to bacteria. Instead, we will integrate existing bulk gene expression datasets, reasoning that many of them capture nearly pure cell states or mixtures of a few dominant ones. Using dimensional reduction and clustering methods, we aim to infer an atlas of bacterial cell states and the relationships among them. Proposed Activities Develop the open-source inGEST pipeline for harmonizing bacterial gene expression datasets Our analysis requires assembling and processing large numbers of high-quality gene expression datasets. We will process tens of thousands of RNA-seq and microarray datasets from public repositories in a uniform way, applying standardized filtering and normalization while identifying and correcting for technical variation. The Integrative Gene Expression Synthesis Tool (InGEST) will include conventional preprocessing steps for both microarray and RNA-seq data as well as advanced quality control functions that detect anomalies such as DNA contamination or inconsistent replicates. Using the scale of the aggregated data, we will train algorithms to identify and adjust for systematic biases, such as those introduced by differing read lengths or protocols. InGEST will be released as an open, community-maintainable software package with full documentation, tutorials, and containerized deployment options to ensure accessibility and long-term sustainability. Apply dimensional reduction methods to create the Atlas of Bacterial Cell States To delineate distinct cell states and understand their relationships, we will apply unsupervised learning and dimensional reduction techniques commonly used in eukaryotic single-cell RNA-seq analysis. These approaches can reveal structure in high-dimensional data and help identify clusters corresponding to biological states. Our initial focus will be on curated Escherichia coli datasets, where we expect to identify primary attractors corresponding to exponential growth and stationary phase, forming a foundational growth–quiescence axis. Additional variation will likely correspond to sub-states such as stress responses or biofilm-associated growth. As InGEST matures, we will extend the analysis to non-laboratory E. coli strains and to natural environments such as the gut, identifying subfunctionalized states that build on the conserved axes. To account for variation in gene content between strains, we will use ortholog mapping tools such as Orthofinder to establish a consistent feature space across datasets. Finally, we will expand the analysis to include more than twenty bacterial species with sufficient datasets, encompassing model organisms and pathogens that exhibit additional states such as sporulation or quorum sensing. This broader atlas will allow us to identify conserved and specialized expression patterns and explore evolutionary relationships among cell states. Create a classifier that predicts the distribution of cell states in bacterial populations Using the atlas as a foundation, we will train classifiers to estimate the proportions of cells occupying different states within bulk transcriptomic or metatranscriptomic samples. Initially, we will train and validate these models on synthetic mixtures generated from pure-state datasets and later test them on available single-cell bacterial transcriptomes. Ultimately, we will apply these classifiers to environmental and host-associated microbial communities to predict functional composition and state distribution, providing a new lens for interpreting community-level gene expression. Outcomes This project will generate conceptual, methodological, and practical outcomes. Scientifically, it will yield the first comprehensive Atlas of Bacterial Cell States, revealing how gene expression landscapes are structured and conserved across species and environments. Methodologically, it will produce three key open-source products: the InGEST pipeline for harmonizing bacterial gene expression data, the Atlas of Bacterial Cell States as a community resource with visualization tools, and the Bacterial Cell Population Classifier that predicts cell state composition from expression data. Collectively, these resources will enable new analyses of bacterial physiology, evolution, and ecology. By integrating and harmonizing unprecedented amounts of data, we expect to uncover organizing principles of bacterial behavior and potentially identify novel cell states. Future work can extend this framework to other microbial domains and data types, including proteomics and metabolomics, enabling multi-omics integration of microbial states. Rationale for NCEMS support This project exemplifies mesoscale data synthesis: it leverages machine learning to integrate and analyze large volumes of bacterial gene expression data. Achieving this requires reproducible, scalable pipelines and community infrastructure that individual groups cannot maintain alone. NCEMS support is essential for implementing best practices in data management and sustainability, ensuring that the resulting resources are open, reusable, and extensible. In addition, NCEMS computational expertise will help in developing and applying advanced analytical methods for visualizing and quantifying relationships between bacterial cell states. Requested resources The primary computational demands involve downloading, processing, and analyzing thousands of RNA-seq datasets. Each dataset ranges from a few hundred megabytes to several gigabytes and can be processed within minutes on modern CPUs. Microarray data require negligible storage and compute time. Dimensional reduction and machine learning analyses can initially be run on modest hardware but may benefit from scalable computing for cross-species analyses. The final resource will consist of metadata, gene expression matrices, and annotated reference genomes, small enough to be hosted publicly and versioned for community access. Proposed Timeline The project will span two years. In the first year, we will focus on Escherichia coli, developing and validating the InGEST pipeline, harmonizing thousands of datasets, and identifying major cell states and marker genes. The second year will expand to additional bacterial species, refine the atlas to include conserved and specialized states, and develop the classifier for predicting state composition in mixed samples. Publications describing both the pipeline and the atlas will disseminate the findings and ensure long-term visibility and reuse.",Accepted,1
human_5,human,human,human,Energetic Origins of Connectivity within Protein Interaction Networks,"At least 10% of cellular proteins are intrinsically unstable and fold with marginal efficiency. In the cell, these proteins are preferentially engaged by chaperones and quality control proteins that refold or degrade them. Similarly, intrinsically disordered proteins (IDPs) preferentially interact with specific partners that they require for folding and function. Such proteins must exhibit bias in the types of protein-protein interactions they form within the cell. We therefore hypothesize that the emergent scaling properties of protein interaction networks arise, in part, from the promiscuous interactions formed by unstable proteins and the obligate interactions formed by IDPs. To test this hypothesis, we will analyze existing large-scale interactome datasets to explore the context of unstable and disordered proteins within interaction networks. By extending this approach to evaluate how proteins with various conformational properties are distributed across several unicellular and multicellular networks, we aim to uncover generalizable insights into how protein biophysics shapes network architecture. Additionally, by mapping the centrality of proteins involved in various supramolecular organization regimes, we will identify topological niches associated with the occurrence of liquid-liquid phase separation and quinary structure. Finally, to determine the extent to which these biases constrain network evolution, we will utilize genetic algorithms to explore how changes in the stability of the proteome restrict the topologies that are accessible through evolution. Together, these investigations will provide insights into the role of protein stability and binding efficiency in the evolution of PPI networks and how this relates to the molecular mechanisms of evolution.",Jonathan Schlebach; Shahid Mukhtar; Adrian Serohijos,"Introduction and Goals Over the past 20 years, emergent evidence has suggested that the architecture, scaling, and topologies of protein-protein interaction networks are similar across prokaryotes and eukaryotes. One apparent property of these networks is a scale-free topology in which interactions among proteins follow a power law distribution. While true scaling may be obscured by experimental biases, it is clear that most proteins form only a few discrete interactions, whereas a small number of hub proteins engage in numerous promiscuous interactions. It has been proposed that selection pressures have refined resilient network topologies in which key metabolic pivots can be regulated through a handful of connecting nodes. To ensure robustness, these high-degree hubs often have functionally redundant paralogs that minimize the fitness impact of deleterious mutations. In biological systems, “party hubs” are large proteins that form continuous interactions within functional complexes and exhibit co-expression, while “date hubs” are smaller proteins that form transient interactions with different partners and bridge network modules. Intrinsically disordered regions are a common feature of eukaryotic hub proteins, and disordered proteins in both hub types can introduce new specificities that differentiate single- and multiple-interface hubs. The position of a node within a network strongly influences its function. This can be described by its betweenness centrality, which measures how often a node lies on the shortest paths between pairs of other nodes. Nodes with high betweenness act as bottlenecks and need not have high degree. Centrality descriptors can generally be categorized into neighborhood-based, path-based, and iterative refinement-based types. The magnitude of connections ultimately depends on the number of complexes a protein forms, which is determined by its biophysical properties and cellular abundance. Integrating multiscale biophysical and cellular network descriptors may reveal how intrinsic protein properties give rise to the topology of interaction networks and their biological roles. Numerous interactome datasets are available for constructing such networks, including for yeast and higher organisms like Arabidopsis and humans. These data have already provided insight into how pathogens target hubs and bottlenecks. Despite this progress, it remains unclear how cells achieve and maintain preferred network architectures given that their nodes—proteins—continuously evolve new interactions. While the evolutionary dynamics of interaction networks are poorly understood, much is known about evolutionary constraints on protein structure and function. Most mutations destabilize protein structure and reduce stability, meaning individual proteins can only tolerate certain combinations of stabilizing and destabilizing mutations before selection removes them. Based on this, we propose to develop a physical framework that describes how the properties of protein interaction networks emerge from the physical properties of their constituent proteins. For example, increases in abundance could compensate for low stability or weak binding, while weakened binding may promote promiscuous interactions. Unstable proteins might also form new chaperone interactions that facilitate transitions between clusters. Purifying selection could be weaker for low-degree or redundant high-degree proteins. We suspect that networks contain niches capable of accommodating variations in protein stability in ways that preserve overall topology. Such models could clarify how changes in protein stability, binding, and abundance contribute to network robustness. Our investigations will address how modular protein clusters maintain connectivity despite genetic drift, whether chaperone hubs act as anchors for topology, whether unstable clients engaging multiple chaperones move between clusters, and whether intrinsically disordered proteins become anchored within specific modules. We will repurpose interactome data to model networks across organisms and evaluate the topological context of proteins with divergent biophysical properties to reveal how protein physics gives rise to biological network organization. Goal Determine how protein stability biases interaction networks. Over time, most proteins traverse low-stability regimes that predispose them to chaperone binding. Shared chaperone hubs can create indirect proteostatic couplings that affect solubility and function. However, the influence of these client-chaperone interactions on network topology is unclear. We hypothesize that unstable proteins exhibit enhanced connectivity due to their association with the proteostasis network. To test this, we will construct a high-resolution yeast network using quality interactome data and combine it with computational protein stability estimates to assess the connectivity of unstable proteins. These analyses will be validated using proteolysis-based stability data, proteomic half-life measurements, and abundance data. Finally, we will repeat these analyses in organelle-specific subnetworks to test whether such patterns differ by compartment. Goal Survey interactions formed by unstable proteins and intrinsically disordered proteins within divergent networks. Higher organisms have evolved complex proteostasis networks that support structurally diverse proteomes and increase overall buffering capacity. It is unknown whether changes in proteostasis composition alter the interactions of unstable or disordered proteins or whether these proteins remain bound to specific chaperones across long evolutionary timescales. We hypothesize that unstable proteins form promiscuous chaperone interactions and more interactions overall than disordered proteins. To test this, we will extend the mapping approach to multiple organisms, characterize the topological positions of unstable and disordered proteins, and compare chaperone associations among homologous clients across species. By comparing metrics across stability and disorder classes, we will determine whether unstable proteins and their ancestors show preferential attachment to chaperone hubs that maintains their positions and preserves network scaling. Goal Determine how marginally stable and unstable proteins constrain network topology. Many proteins develop a dependence on specific chaperones as part of their assembly pathways. We hypothesize that persistent chaperone interactions restrain the topology of interaction networks. To test this, we will use genetic algorithms to simulate evolutionary trajectories of networks that integrate stability, binding efficiency, and abundance with tunable constraints on chaperone hub number and interaction persistence. These simulations will reveal whether degeneracy among client-chaperone interactions emerges across evolutionary timescales and how proteostasis interactions constrain network architecture. Proposed Activities Generation of network models The Mukhtar group will curate interaction data and use it to build models for yeast, E. coli, and C. elegans based on in vivo and high-throughput datasets. For each protein, we will compute multiple centrality measures—including degree, betweenness, eigenvector, closeness, load, PageRank, and weighted k-shell decomposition—using Python and Cytoscape. Node prioritization algorithms will identify influential proteins through composite metrics, and clustering will be performed with MCODE and Cytoscape visualization. We will also integrate single-cell and condition-specific data from GEO to parameterize context-dependent networks with the aid of contextual AI models. Functional enrichment analyses using GO, WikiPathways, and related databases will contextualize biological meaning. Generation of biophysical descriptors To determine whether proteins with particular structural properties cluster in networks, we will compare each protein’s centrality to predicted stability and disorder features. Using disorder prediction resources, we will distinguish structured from disordered proteins and estimate absolute stability for structured proteins through generative modeling and simulation-based validation. These stability descriptors will categorize proteins as stable, unstable, or disordered. Enrichment analyses will then test whether high-centrality proteins are overrepresented in these categories, and thresholds will be varied to assess robustness. Additional node annotations such as chaperones or phase-separating proteins will be incorporated to identify biochemical phenomena correlated with centrality. Data from proteolysis, turnover, and expression experiments will be integrated to evaluate associations between network position and proteostasis dynamics. Generation of evolutionary models The Serohijos and Shakhnovich groups have developed integrative models of protein evolution combining biophysics, cellular properties, and population genetics. These models, expanded to proteome scale, will incorporate chaperone and disorder effects. The model includes a fitness function linking native folding probability to free energy and chaperone efficiency, a description of random mutation effects, and an evolutionary algorithm implementing selection. The fitness of complexes will depend on protein stabilities, binding affinities, and abundances, satisfying the law of mass action across the proteome. Random mutations will alter folding and binding based on established datasets, and their selective advantages will be evaluated by fitness comparisons and fixation probabilities. This framework will allow exploration of how mutations affecting stability or binding influence network connectivity and evolution. Outcomes This work will clarify how variations in protein stability and disorder shape the architecture of protein interaction networks. We expect that unstable proteins will display higher degree than stable ones and cluster around chaperone hubs, while disordered proteins dependent on specific partners will maintain their positions across networks. Simulations will demonstrate how these interactions impose constraints on network evolution. Collectively, these results will establish a biophysical foundation for understanding how protein properties give rise to network connectivity and robustness. Working Group Rationale The project merges computational biophysics, systems biology, and evolutionary modeling. The Schlebach lab contributes expertise in protein biophysics and structural informatics; the Mukhtar lab contributes network biology and graph analysis capabilities; and the Serohijos lab contributes evolutionary modeling. The Shakhnovich lab provides complementary theoretical expertise linking these approaches to metabolic organization. This collaboration will enable the integration of data, methods, and trainees across disciplines. Rationale for NCEMS Support The investigations represent a new direction requiring computational infrastructure and collaborative coordination. NCEMS support will facilitate data integration, provide computing resources, and enable administrative and logistical support for efficient collaboration. Requested Resources The project requests a staff scientist and assistant, cyberinfrastructure including storage and computing hours, travel and publication support, and resources for personnel exchange. Proposed Timeline Major milestones include construction of interaction networks for multiple species, computation of centrality metrics, generation of protein stability estimates, annotation of chaperones and disordered proteins, enrichment analyses, and development of genetic algorithms for evolutionary simulation.",Accepted,10
human_6,human,human,human,Elucidating emergent structures in cellular RNA-protein interaction networks,"Messenger RNA molecules are constantly bound by proteins that regulate all aspects of their function, including their processing, localization, translation, and degradation1. These mRNA-protein complexes (mRNPs) can also interact with each other and assemble into higher-order structures that play critical roles in cellular organization and response to environmental stimuli1-3. Despite their importance, we lack a comprehensive map of these cellular RNA-protein interaction networks at both the nano and micro scales. Years of efforts by researchers across the globe have yielded a large, but disjointed, collection of datasets that map different aspects of RNA-protein interactions. Currently, we lack tools to integrate these datasets, preventing their synthesis into a coherent and interpretable interaction map. Here, we propose to develop tools to combine disparate data types to build transcriptome-wide maps of mRNPs, and their higher-order interactions within cells. We will then apply these tools to gain insights into long-standing biological mysteries, including the impacts of synonymous mutations and cell-type specific effects of widely expressed RNAs.",Kalli Kappel; Steven Boeynaems,"Introduction and Goals RNA is Life’s prime messenger molecule of genetic information. While for all organisms—minus some viral exceptions—DNA provides long-term storage, RNA is its direct output, relaying this genetic info into molecular actions. Messenger RNA instructs the ribosomes to fabricate the plethora of proteins that carry out most cellular and biochemical functions. There are also non-coding RNAs that serve mostly structural, enzymatic or regulatory functions. For example, long non-coding RNAs like NEAT1 RNA recruit specific sticky proteins to form biochemically distinct subcellular compartments called paraspeckles—a type of biomolecular condensate. Ribosomal RNA is a prime example of a catalytic RNA or ribozyme, and small RNAs like micro RNAs can regulate the abundance of other RNA species. While it is clear that RNAs carry out essential functions in cells, they do not act alone. It is well understood that RNAs are never naked in the cellular environment. Cells harbor hundreds of RNA-binding proteins that can decorate RNA molecules depending on their specific nucleotide sequence and secondary structure or simply based on the inherent negative charge of the RNA backbone. So, for every RNA molecule, from the moment it emerges from the polymerase transcription bubble, there will be a complex, spatially and temporally dynamic set of protein interaction partners that dictate its processing and structural remodeling, its transport through the cell, its biochemical function and eventual decay. Given that most of these RNA-protein complexes are considered to form their own single entity, they each present as a wildly complex and dynamic microcosm of different proteins interacting with a single RNA. Moreover, all of these proteins can have specific or non-specific interactions with each other—based on their biochemical composition, structure and disorder—that affect each other’s behavior and the eventual mRNP composition. Given such complexity and the breadth of potential RNA-RNA, RNA-protein and protein-protein interactions, it is highly unlikely that the sum of each of these individual components is sufficient to explain the outcome. On the contrary, new unexpected behaviors are bound to emerge from this complexity. We argue that mRNPs are one of the best examples to study emergent behavior in an essential cellular process for life on this planet. The goal of this project is to create a roadmap to understand the composition of the cell’s many thousands of RNPs, focusing specifically on mRNA-protein complexes and their higher order interactions, by integrating available experimental RNA-RNA, RNA-protein and protein-protein interaction data using recent state-of-the-art computational tools for predicting protein-protein and protein-RNA interactions. We postulate that this will allow us to come up with a predictive framework to understand the emergent behaviors that arise from mRNP composition. We will validate the predictive power of our framework through the lens of human sequence variation dictating RNA abundance and splicing, the robustness or sensitivity of splicing to RNA-binding protein loss, and the composition of higher-order biomolecular structures that form when individual mRNPs form environmentally-triggered condensates. Proposed Activities Integrate publicly available datasets to build a transcriptome-wide map of mRNPs. In cells, mRNA molecules are constantly bound by proteins, which regulate nearly every aspect of their processing, localization, translation, and degradation. As such, there have been numerous efforts to systematically map mRNA-protein interactions in cells, resulting in a long list of large-scale and transcriptome-wide datasets. These datasets have been generated through many different experimental techniques, each measuring a different aspect of these interactions, making it nontrivial to compare and combine them. Without the ability to integrate these measurements, we lack a transcriptome-wide mRNP map—we have a fragmented and incomplete understanding of the composition and architecture of mRNPs, despite an abundance of data. We hypothesize that a transcriptome-wide mRNP map would help mechanistically explain several long-standing biological mysteries, including how synonymous mutations and mutations in non-coding regions of RNA molecules impact cellular function, and why mutations in RNA molecules that are widely expressed have highly cell-type specific effects. Here, to address this challenge, we will develop a method to integrate disparate datasets, which report on multiple aspects of RNA-protein interactions, to build complete mRNP maps; apply this method to build a transcriptome-wide mRNP map for K562 cells; and use this map to predict and develop mechanistic hypotheses for RNA fate upon genetic perturbation. Develop a method to integrate disparate datasets to build complete mRNP maps. Our goal is to develop a computational method that takes a set of disparate RNA-protein interaction datasets as input and outputs an mRNP map consisting of mRNA molecules and bound proteins including specific nucleotides at which they are bound. We will first collect all publicly available datasets that report on different aspects of RNA-protein interactions in cells. To integrate the datasets, we will develop a set of linked features that connect each pair of data modalities. A linked feature is a shared characteristic or metric that is either directly measured by both datasets or can be reliably inferred from their respective measurements. These linked features will then serve as anchors for data normalization and integration. We will test multiple linked features and normalization approaches and assess their accuracy using the β-actin mRNP, which has been extensively characterized at high resolution. By linking all data modalities to transcriptome-wide mapping of protein-binding sites, this will result in a transcriptome-wide mRNP map. Build a transcriptome-wide mRNP map for K562 cells. We will extend the tool developed above to identify additional proteins that may be part of each mRNP. We will identify protein interaction partners for each protein member of each mRNP by utilizing data from existing protein-protein interaction databases and by predicting protein-protein interactions with AlphaFold for structured domains and with FINCHES for interactions between intrinsically disordered regions. Input to this tool will be a list of mRNP components and a parameter specifying the number of layers of connections to predict. We will apply this tool using data collected for K562 cells to build a complete mRNP map for this cell type. We will also assess the signal-to-noise for each dataset by comparing to corresponding high-resolution measurements for the extensively characterized β-actin mRNP, so we can assign confidence scores to each interaction within the mRNP map. Predict RNA fate upon genetic perturbation. To test the predictive power of our framework regarding regulation of RNAs upon perturbation, we will follow two approaches. First, we will determine whether our framework, which takes into account the different proteins and regulatory RNAs that bind a transcript, can explain the effect of a point mutation in the sequence on its eventual abundance or splicing. For example, a mutation may affect an RNA-binding protein or miRNA binding site. Second, we will ask whether our framework reproduces effects seen in RNA-binding protein knockout data. We will then use our model to make mechanistic predictions as to why mutations in RNA molecules that are widely expressed have highly cell-type specific effects. Map out interaction networks of mRNPs. This aim seeks to identify the network of interactions between mRNPs by addressing the following questions: What is the mesoscale interaction network that a given mRNP belongs to? Can we predict what subcellular compartments, such as biomolecular condensates, that a given mRNP would localize to? Can we predict the compositional variability of condensates such as stress granules based on mRNP networks? To this end, our team will pioneer new approaches to map out higher-order interaction networks of mRNPs within cells. We hypothesize that the physicochemical characteristics of mRNPs encode their spatiotemporal properties within living cells. Deploy our mRNP map tool to identify emergent structures of mRNPs. Our goal is to predict higher-order interaction networks of mRNPs. Using the mRNP map tool generated previously, we will determine the network of proteins associated with each mRNP. Protein interaction networks will be analyzed using clustering or community detection algorithms to group proteins into mesoscale networks based on shared physical properties, such as interaction motifs or binding affinities. Our identified clusters will be validated using mass spectrometry crosslinking data and proximity labeling datasets. Finally, we will compare the identified clusters to known protein compositions of individual condensates to assess their alignment with specific biomolecular compartments. Predict mRNP partitioning into biomolecular condensates. Leveraging databases such as CD-Code and RNA Granule Database, we will curate a subset of proteins and RNAs known to localize to stress granules and P bodies, gathering their physicochemical properties, including sequence composition, charge distribution, hydrophobicity, structural disorder, RNA-binding domains, and low-complexity domains. In parallel, we will use the tool developed earlier to identify mRNPs associated with both types of condensates. Based on the curated data, we will design model condensates that incorporate averaged physiological features representative of stress granules and P bodies. Using the Mpipi model, we will perform residue-resolution molecular dynamics simulations to measure the partitioning coefficients of approximately 2,500 mRNPs across both model condensates. This data will then be used to train a neural network capable of predicting the partition coefficient of any given mRNP into either stress granules or P bodies. The model will be validated against existing transcriptome datasets that profile RNAs enriched in stress granules and P bodies under specific conditions. Predict stress granule composition across cell types. Stress granules are some of the most widely studied biomolecular condensates with many datasets on their protein and RNA composition being readily available. Based on the RNA and protein abundance of specific cells, we will use our framework to predict the variance and similarity in stress granule composition. Addressing this question will help us uncover the molecular rules that dictate condensate formation and composition in both a cell-specific and non-specific manner. Outcomes Our work will result in open-source tools for integrating disparate data sources to map RNA-protein interactions at the nano-scale and the meso-scale, as well as specific applications of these tools to explain emergent structures and functional data. We will produce open-source tools for predicting transcriptome-wide mRNP maps from disparate datasets reporting on different aspects of RNA-protein interactions; a transcriptome-wide mRNP map/database for K562 cells containing detailed molecular annotations; an assessment of the predictive power of our mRNP map to explain the functional consequences of genetic perturbations; a database of microscale clusters composed of mRNPs and their interaction partners; an open-source tool to predict the relative localization of any mRNP to stress granules and P bodies; and a model that predicts cell type specific stress granule composition. Rationale for working group approach This project requires several distinct areas of expertise that go far beyond the skill sets of any individual lab. In addition to general subject-matter expertise in RNA-protein interactions, which all participants have, the work will require expertise in machine learning, statistics, molecular biophysics, and bioinformatics to build tools that integrate disparate data sources and create predictive models; molecular simulations to augment existing experimental data and ground our models in biophysics; and deep biological expertise in both RNA-protein interactions and condensates to guide development of useful tools and apply them to discover new biology. Additionally, each of our aims has a large data science component that would benefit from NCEMS Staff Scientist Support. Rationale for NCEMS support This work aligns closely with NCEMS goals: we will develop and apply tools that utilize only publicly available data; it requires close collaboration between three labs with distinct yet complementary skills; and by combining our diverse perspectives, we will address long-standing mysteries including how synonymous and non-coding RNA mutations impact cellular function and why widely expressed RNAs have cell-type specific effects. Additionally, this project will require NCEMS support for collaboration logistics, virtual leadership and team science training, centralized computing and storage through CyVerse, and staff scientist assistance for data science. Requested resources Our project requires 60 TB of data storage to store all datasets and simulation results, 1.35 million CPU hours for simulations, access to approximately 8 GPUs for machine learning, an NCEMS Staff Scientist at 33% FTE for 2 years with expertise in bioinformatics, machine learning, and statistics, an NCEMS Staff Scientist Assistant at 50% FTE for 2 years with expertise in RNP biology and data science, $5,000 for publication costs, and travel funds for in-person kickoff meetings and events. Proposed timeline We expect that this project can be completed with 2 years of NCEMS support. We plan for intermediate outputs, including publications describing tools and databases from the early aims and biological findings from later aims. The project will engage undergraduate teams across participating labs, with staff scientists providing mentorship and coordination. Regular meetings and collaborative writing will ensure integration across all components of the project.",Rejected,4
human_7,human,human,human,Oceans of Disorder: Elucidating the Role of Disordered Proteins in Cellular Adaptation,"Intrinsically disordered proteins and protein regions (collectively IDRs) are key regulators of environmental sensing, owing to their ability to undergo conformational changes in response to physicochemical fluctuations. Despite evidence from individual studies, a comprehensive understanding of IDR-driven adaptations across diverse environments remains elusive. The deep sea, characterized by extreme and variable conditions such as temperature, pressure, and salinity, offers a unique opportunity to address this knowledge gap. Over the past decade, large-scale ocean sampling expeditions using shotgun metagenomics have revolutionized our understanding of marine biodiversity, providing access to the genetic diversity of organisms, from microbes to fish, thriving in these extreme habitats. These datasets offer an unprecedented opportunity to explore how IDRs have evolved as molecular sensors to support survival across varied environmental conditions. Here, we will, for the same time, use these metagenomic datasets to systematically characterize IDRs as environmental sensors. Our multidisciplinary team, with expertise in metagenomics, protein biophysics, cell biology, and deep learning, will integrate bioinformatics, simulations, machine learning, and theoretical approaches to uncover sequence-level and functional adaptations of IDRs across marine environments. The outcomes of this research will produce a comprehensive map of IDR adaptations in marine systems, revealing their emergent properties as molecular sensors and actuators. In addition to advancing fundamental insights into stress adaptation and evolutionary biology, this work will inform biotechnological innovations, including engineering robust systems for extreme environments.",Keren Lasker; Jerelle A. Joseph; Alex Holehouse,"Introduction & goals IDRs are emerging as critical components in environmental sensing due to their unique ability to adopt dynamic conformations in response to external stimuli. Unlike structured proteins, IDRs lack a fixed three-dimensional structure, allowing them to function as versatile molecular sensors capable of adapting to physicochemical changes. While several studies have highlighted the role of IDRs in stress adaptation, a systematic understanding of how IDRs enable organisms to thrive under extreme and fluctuating conditions remains a major knowledge gap. The marine environment serves as a unique natural laboratory for investigating how life adapts to variable conditions, including temperature, salinity, and pressure. The Tara Oceans project, launched in 2009, has been instrumental in advancing this understanding. Over four years, using standardized protocols, Tara Oceans systematically collected over 30,000 samples from more than 200 ocean stations, including depths reaching 2,000 meters. These samples encompass various environmental conditions and yielded a vast metagenomic dataset with millions of novel sequences from viruses, prokaryotes, and eukaryotes. Analyses of this dataset have revealed temperature as a key driver of microbial community structure and uncovered previously unknown gene families. Leveraging Tara's dataset, we will, for the first time, focus on the biophysical properties of individual proteins to uncover how they enable organisms to thrive in distinct marine environments. To achieve this, we are developing a multidisciplinary approach that integrates metagenomics, biophysical simulations, and machine learning with environmental metadata. Our project is organized around three goals: catalog and annotate IDRs by identifying them in marine protein sequences and correlating their biophysical properties with environmental parameters, simulate IDR dynamics by performing biophysical simulations of IDR dynamics under varying environmental conditions to extract key structural features, and elucidate IDR adaptation mechanisms by linking sequence-level features to functional biophysical properties and environmental stimuli. The outcomes of this work will address critical gaps in our understanding of IDRs functionality across diverse environments and provide a foundation for broader applications. Identifying universal principles of IDR adaptation will reveal protein plasticity mechanisms and enable the engineering of adaptable systems. Proposed activities Activity 1: Building a catalog of IDRs in marine proteins across environmental parameters We will build a comprehensive catalog of IDRs in marine proteins, correlating their biophysical properties with key environmental parameters, including temperature, salinity, and ocean depth. This work leverages the Ocean Gene Atlas v2.0, a publicly available database derived from the Tara Oceans expedition, which provides extensive marine protein data. This expedition standardized sampling methodologies, collecting 12,543 samples across diverse marine ecosystems, encompassing 472 unique sampling events. These samples span a broad range of environmental conditions, including temperatures from 0 to 30 °C, ocean depths from 0 to 1000 m, and salinities between 30 to 40 PSU. Linking marine proteins to environmental and genomic parameters. We will curate the OGA2 database, which contains approximately 45 million prokaryotic and 10 million eukaryotic protein sequences, to connect protein sequences with their environmental and genomic contexts. OGA2 includes protein abundance data and environmental metadata for each sampling location. We hypothesize that protein orthologs span diverse environments, allowing us to associate protein features with specific environmental conditions. Expanding on this analysis, we will systematically annotate each protein with the range of environmental conditions where it is detected. Additionally, we will link each protein to metagenome-associated genomes in the OceanDNA database to incorporate genomic metadata such as genome size and GC content. Annotating and structurally characterizing marine proteins. We will predict the structure of each protein using ESMFold and AlphaFold3. Domains will be identified using an integrative approach combining UniProt annotations, Chainsaw (a convolutional neural network for detection of structural domains), and Metapredict (a deep learning-based consensus predictor of IDRs). For ordered domains, we will extract detailed structural features, including secondary structures, dimensions, surface chemistry, and functional annotations where available. For disordered regions, we will extract sequence-derived features using CIDER. Labeling IDRs based on biophysical properties. IDRs will be labeled according to their intrinsic biophysical properties using ALBATROSS, a deep learning-based approach for predicting IDR ensembles under physiological conditions. ALBATROSS enables assignment of properties such as radius of gyration, end-to-end distance, asphericity, Flory scaling exponent, and prefactor. Since ALBATROSS was trained for physiological conditions, its reliability may decrease under extreme conditions. To address this, we will incorporate additional biophysical labels derived from molecular dynamics simulations, as outlined in Activity 2. Activity 2: Coarse-grained simulations to decipher IDR properties under different environmental conditions The functional versatility of IDRs derives from their 3D conformations, which cannot be directly inferred from their sequences. These conformations are expected to adapt dynamically to environmental changes, potentially encoding mechanisms for adaptation. To investigate these hypotheses, we will study IDR conformations using coarse-grained simulations under a range of environmental conditions. Specifically, we will simulate IDRs across gradients of temperature and salinity to capture their biophysical properties and structural responses. These simulations will provide detailed 3D conformational data for IDRs in marine proteins, offering insights into their adaptive mechanisms. Our approach leverages residue-level coarse-grained models that enable the efficient exploration of long timescales and larger molecular systems. To analyze the resulting high-dimensional data, we will integrate advanced deep learning techniques, including graph neural networks and variational autoencoders, to extract structural features and annotate IDR properties. Simulating IDRs under different environmental conditions. Building on insights from Activity 1, we will group homologous IDRs and select representative IDRs per protein homolog to create a diverse yet focused dataset for analysis. Coarse-grained one-bead-per-residue molecular dynamics simulations will be conducted for these representative IDRs. To enable extended timescale simulations and comprehensive trajectory sampling, we will employ chemically specific, residue-level coarse-grained models to simulate IDRs under specific environmental conditions: salt concentration gradients using the Mpipi-Recharge model and temperature gradients using a custom in-house model developed by our team. Pressure effects will not be simulated due to the absence of a validated coarse-grained model. Simulations will be performed using the open-source LAMMPS software. We will run each IDR at three temperatures and three salt concentrations, totaling eight simulated ensembles, excluding the case near physiological conditions, which will be annotated in Activity 1. Annotating biophysical properties that arise in simulations under a range of environmental conditions. We will employ deep learning-based methods to extract critical information from high-dimensional simulation data, focusing on uncovering conformational representations and structural propensities of IDRs. By leveraging advanced clustering and neural network techniques, we will learn nonlinear representations that disentangle key variation factors, capture invariant structural features, and enable the transfer of insights across diverse IDR populations. Specifically, we will integrate variational autoencoders, graph neural networks, and generative adversarial networks to analyze molecular dynamics trajectories under diverse environmental conditions. This approach will result in an ensemble decoder that outputs multi-vector conformational matrices, highlighting key features such as global conformations, pairwise contacts, and secondary structures, which collectively annotate IDR biophysical responses. Activity 3: Mapping the mechanisms of IDR adaptation and Engineering Environmentally Resilient IDRs We will uncover the molecular principles governing the adaptation of IDRs to diverse environmental conditions. By identifying chemical features of IDRs that correlate with environmental variables, we will elucidate universal properties that can be fine-tuned to enable IDR adaptability. Comparative analyses of IDRs from orthologous proteins across different environmental contexts will reveal chemical adaptations required to maintain essential protein functions. These insights will culminate in the development of a framework for designing IDRs with enhanced adaptability and functionality tailored to specific conditions. Identification of essential molecular features for IDR adaptation. To understand how IDRs adapt to environmental conditions, we will employ machine learning approaches to distill high-dimensional sequence and biophysical data into a low-dimensional latent space. This space will encapsulate critical features, such as compositional biases and environmentally dependent biophysical properties. Key steps include data distillation, where supervised or semi-supervised machine learning models encode sequence and property data into a compact latent representation; feature mapping, where environmental parameters are overlaid onto the latent space to identify how IDRs modulate their features to adapt; and validation, where comparative analysis with conventional dimensionality reduction techniques ensures robustness while enabling mutational and functional predictions that extend beyond conventional methods. Predicting evolutionary trajectories and designing novel IDR sequences. Building on these insights, we will model evolutionary trajectories and design IDRs optimized for specific biophysical and environmental properties. Adaptation trajectory prediction will use generative modeling frameworks to simulate IDR adaptation under progressive environmental conditions, allowing us to predict how sequence features evolve to optimize functionality. We will validate predicted trajectories by comparing them to adaptation trajectories in our dataset and cross-referencing experimental data. Adaptive rule discovery will identify how variations in sequence features influence adaptation, enabling formulation of design rules for engineering environmentally resilient IDRs. Using the latent space as a generative tool, we will design novel IDR sequences with desired biophysical properties. These sequences will be tested computationally and experimentally through collaborations with biophysics experts. This framework will enable creation of IDPs tailored for synthetic biology and environmental resilience. Expected outcomes This research will provide transformative insights into the adaptive mechanisms of IDRs in marine proteins, elucidating their role in survival across environmental gradients such as depth, salinity, and temperature. By cataloging IDRs alongside their biophysical properties and environmental metadata, we will establish the first comprehensive database linking IDR biophysical features to environmental contexts. This database will serve as a foundational resource for understanding the evolutionary adaptations of IDRs. Through large-scale simulations and machine learning models, we expect to uncover universal chemical features that govern IDR adaptability. These findings will address critical knowledge gaps and provide a foundation for engineering environmentally adaptable proteins. The resulting tools and frameworks will have broad applicability, extending beyond marine systems to other biological contexts, and paving the way for advances in synthetic biology, biotechnology, and evolutionary studies. We anticipate multiple publishable outcomes from this work, including a database of marine IDRs cataloging sequence features and environmental metadata, large-scale simulations and biophysical analyses revealing key adaptive features, a machine learning-based predictor of IDR adaptation pathways, and a computational tool for the de novo design of IDRs tailored to specific environmental niches. Rationale for the working group collaborative approach The scope and interdisciplinary demands of this project are beyond what a single lab can address alone. Understanding the interactions between IDR sequence, structure, and function across diverse environments requires expertise in multiple disciplines, including metagenomics, biophysics, and machine learning. By combining these skill sets, our working group provides a unique platform to tackle the project's challenges effectively. Metagenomics and bioinformatics experts will curate and analyze large datasets of marine proteins, annotate IDRs with environmental and genomic parameters, and uncover adaptation trends. This work establishes a foundation for identifying patterns of IDR adaptation across ecosystems. Biophysics and modeling experts will utilize advanced simulation tools developed by members of the working group for modeling IDR behavior under diverse environmental conditions, offering unparalleled insights into conformational dynamics and adaptation mechanisms. Machine learning experts will apply advanced algorithms to analyze high-dimensional molecular dynamics data, extracting structural and functional insights and enabling predictive modeling and design of novel, environmentally adaptable IDRs. By combining these areas of expertise, the working group fosters innovation and ensures the creation of broadly applicable tools and frameworks. Rationale for NCEMS support This project requires NCEMS support and resources because its scope and complexity extend beyond the capabilities of a single lab or existing collaborations. NCEMS's infrastructure and expertise make it an ideal partner to address the following critical needs: simulation management for approximately 440,000 molecular dynamics trajectories; database development to catalog IDRs and integrate biophysical properties with environmental metadata; collaborative infrastructure to support interdisciplinary synergy; and AI expertise to extract connections between IDR sequences, environmental factors, and adaptation mechanisms. Requested resources We request a 33% FTE Staff Scientist to help design and build the IDR catalog, develop data analysis strategies, and identify relationships between IDR behaviors and environmental conditions; a 50% FTE Staff Scientist Assistant to manage and curate molecular dynamics simulations and support infrastructure; approximately 26 million CPU hours for large-scale simulations; and 52 TB of data storage. Proposed Timeline The project spans 24 months. The first 6 months will focus on designing and building the IDR catalog. Months 5–16 will involve running and curating simulations, processing approximately 440,000 trajectories, and publishing preliminary findings. Year 2 will leverage results from simulations to develop predictive models and design de novo IDRs, culminating in final publications and public release of the tools and datasets.",Accepted,8
human_9,human,human,human,From image data and biophysical simulations to principles of mesoscale organization through integrated transfer learning,"Mesoscale cellular organization emerges from complex interactions among chemical, structural, and physical components. Understanding how mesoscale structures influence local and collective cellular properties remains a significant challenge, often beyond the scope of individual experimental studies. While vast image datasets for cellular components exist, their full potential for deeper secondary analysis and synthesis remains untapped. We propose to synthesize morphological and physico-chemical measurements from cell image, in vitro, and in silico data to unravel the principles of mesoscale cellular organization. Our approach will develop a pipeline using transfer learning and mutual prediction, trained on complete datasets (all components of interest present) and applied to incomplete datasets (missing one or more components). This methodology will uncover relationships between cellular components and enable holistic systems-level predictions. To demonstrate the generalizability and biological relevance of this approach, we will test it on three increasingly complex mesoscale systems centered on the nucleus: (1) nucleus-nucleolus, (2) nucleus-centrosome-microtubules, and (3) nucleus-mitochondria-mitochondrial condensates. These test cases will showcase how our method integrates and predicts complex systems relationships, advancing our understanding of mesoscale cellular organization.",Mary Mirvis; Adriana Dawes,"Background Understanding mesoscale cellular organization requires investigating the interplay of physical forces, spatial constraints, and interactions between cellular components. Advances in morphological profiling, spatial and organellar proteomics, and generative whole-cell modeling have laid the foundation for mapping organizational patterns on scales ranging from molecules to organelles. However, achieving a holistic view of what drives mesoscale organization necessitates integrating these components across each other and with biophysical principles. This proposal shifts focus from traditional gene/protein-centric views to explore how physical principles and geometric constraints give rise to cellular architecture. By combining existing cell imaging data, in vitro reconstitution of mesoscale structures, and published outcomes of molecular simulations, we aim to develop a framework to predict how cellular architectures emerge from the localization, morphology, and interactions of components. This integrative approach will uncover emergent behaviors and previously unidentified features in high-dimensional datasets. Quantitative comparisons across high-dimensional datasets are challenging due to differences in data space. Traditional methods like clustering require data to reside in a single space, but advances in metric geometry, particularly Gromov-Wasserstein distances, enable comparisons across distinct spaces. Recent applications of GW distances have demonstrated their utility in aligning single-cell data and comparing biological networks and time-series data. Here, we propose leveraging GW distances to develop transfer learning algorithms to integrate biological datasets of mesoscale structures. This approach addresses critical gaps in cross-space learning, enabling rigorous integrative analyses of multimodal data. Driving Questions How much of mesoscale cell organization arises from intrinsic physical properties of structures versus their interactions? What role do mesoscale interactions play in determining the morphological and physico-chemical properties of cytoskeletal, organelle, and condensate components? Can a transfer learning approach infer morphological and physical properties of mesoscale structures from joint and incomplete datasets to address these questions and beyond? Proposed Activities Generalizable computational analysis and modeling approach Our approach integrates multicomponent image data to predict unseen properties in incomplete datasets. We extract high-dimensional morphological feature sets describing component size, shape, number, position, and inter-component interactions. Dynamic metrics, such as positional and shape changes, are included for live-cell data. Dimensional reduction generates interpretable feature spaces, which can be tuned for exploratory or hypothesis-driven studies, including granular dissection of cross-component interdependencies and contextual comparisons such as drug treatments, disease states, and cell types. The feature space is sampled along statistical dimensions to reveal feature covariance and specific feature evolutions. Using GW distance analysis to go beyond by-eye interpretations, we quantitatively describe feature covariance and predictive relationships between components. This two-step approach integrates and learns from data across cellular contexts: Step 1: Integrate data across cells. Multicomponent images are aligned via GW distances, generating pairwise similarity matrices for features of interest. This enables comparisons across distinct data spaces, leveraging complex features such as organelle shape. Step 2: Infer feature relationships. Deep learning on alignment matrices reveals predictive relationships between features. Generative adversarial networks under GW distance and transfer learning algorithms facilitate inference, validated using open-source datasets. To tackle the complexity of cellular data, we will incorporate data from minimal systems to compare intrinsic properties in isolated components versus cellular contexts and test whether morphology predicts physical or chemical properties. These data reveal the intrinsic properties of the component in isolation, providing a baseline for comparison with the cellular context. Morphological and physical feature spaces extracted from in vitro and in silico data will be compared with cell image features. New simulations will be developed to ask whether inter-component interactions could close the gap between minimal system and cellular feature distributions. If not, this could indicate that other interactors or environmental factors need to be incorporated. Translating images, which are high-dimensional representations of reality, into physical models requires a combination of approaches. First, we analyze images to extract the underlying physics using low-dimensional models. Second, GW distance analysis can form the basis for standard granular agent-based models. We aim to develop physical models alongside molecular and cellular descriptions to identify mechanisms governing complex processes. Application of workflow to test cases We will refine our workflow by applying it to three mesoscale interaction systems: Nucleus and Nucleolus. Integration of image and minimal system data will explore universal principles governing nucleolar morphology, dynamics, and chromatin interactions. In vitro and simulation data for nucleoli revealing biophysical properties such as surface tension and fluidity will be incorporated. Nucleus, centrosome, and microtubules. This system spans membrane-bound, condensate, and cytoskeletal components. Using multicomponent imaging datasets, we will reconstruct three-part feature relationships from two-component feature relationships and validate findings with in vitro data for microtubules and centrosome. The visibility of all three components in tubulin staining patterns will be leveraged to predict missing components from single-component images. Mitochondria, mitochondrial nucleoids, and nucleus. We examine how morphological properties of mitochondrial nucleoids, which behave as biomolecular condensates, are related to mitochondrial and nuclear structure and function. High-throughput imaging of fibroblasts from healthy and HGPS patients, where mitochondrial nucleoid morphology is altered, will reveal links between structural changes and cellular aging and enable prediction of mitochondrial nucleoid properties in other images. Integration and Synthesis The nucleus serves as an anchor point across test cases due to its extensive biological characterization and rich representation across multicomponent and minimal system data types. After initial workflow implementation, findings will be integrated into a larger model incorporating all components across test cases with the nucleus at the nexus, revealing new mesoscale interactions. Limitations in dataset availability, findability, and quality will likely necessitate further data scouting, particularly for in vitro and in silico data. Systematic literature synthesis methods adapted from the interdisciplinary community will provide complementary insights, building on novel applications for fundamental cell biology currently under development by Mary Mirvis. This combined approach will give insight into variability and reproducibility of findings, supporting robust statistical analyses and modeling efforts. Outcomes The project will yield key outcomes aimed at advancing our understanding of mesoscale cellular organization. These include: a generalizable analytical framework, a computational pipeline leveraging GW distances and transfer learning to analyze multicomponent imaging datasets; predictive models validated for inferring unseen properties from incomplete datasets; integrated datasets combining cell image, in vitro, and in silico sources; and new biophysical insights into mesoscale structure-function relationships spanning sub-nucleus dynamics, organelle interactions, and cytoskeletal organization. Broader Impacts By enabling the integration of diverse datasets, the project will facilitate systems-level analyses of cellular organization and dynamic processes. The generalizable methods will benefit disciplines ranging from biophysics to computational biology. Additionally, by providing new tools and datasets, the work will empower researchers to explore previously intractable mesoscale and multiscale cellular organization questions. Rationale for a Working Group Approach The proposed work requires expertise in computational image analysis, statistical and mathematical modeling including machine learning and simulations, biological and biophysical insight into specific test cases, and deep subject matter expertise for meaningful interpretation of results. No single lab possesses all of these qualities, but our group includes field leaders in all relevant fields. A multi-lab collaboration also expands access to additional complete and well-documented datasets, enhancing the robustness and impact of our analyses. Rationale for NCEMS Support Our proposal builds on several open-source data and methods resources and aims to produce an open-source, highly generalizable data integration workflow to benefit the scientific community, strongly in line with NCEMS’ mission. We will require IT support from NCEMS due to the computational demands of GW distance analyses, which are computationally expensive. Additionally, high-resolution image data necessitates substantial data storage resources, another area where NCEMS can provide critical support. Furthermore, this project involves extensive data and knowledge synthesis, including the integration of image datasets and feature data from dynamic and static cell images, in vitro studies, and simulations, as well as comprehensively synthesized published findings. NCEMS support can enhance these efforts by facilitating systematic curation and pooling of smaller datasets, assembling a large, diverse corpus. These efforts address gaps in publicly available resources, enabling broader applications of the pipeline. Requested Resources We expect to use TensorFlow and PyTorch to train deep learning models and the POT Python library to compute GW distances. For each instance of model training, we expect approximately two hours of training time per GPU, with total time depending on the need to train additional models. Initially, we estimate requiring at least ten thousand GPU hours, with this demand scaling proportionally as additional datasets are integrated into the analysis. High-resolution imaging datasets and intermediate data products, including simulation outputs, will require substantial storage capacity, estimated at approximately fifty terabytes for the duration of the project. Assistance from NCEMS and CYVERSE IT specialists will be critical for optimizing workflows, managing data transfers, and ensuring compatibility between datasets and computational tools. As part of the project’s data synthesis efforts, we also require support for systematic dataset curation and integration, including scouting and pooling smaller datasets from literature and open-source repositories. Proposed Timeline A two-year timeline is appropriate for the proposed work. Development of the core workflow for cell image feature space comparison for two-component test cases is expected to take at least six months. Further refinement, expansion to three-component systems, incorporation of additional data types, and integration across test cases will extend through the second year, depending on availability of data, resources, and personnel.",Rejected,12
human_10,human,human,human,Transposable elements and the emergence of genomic innovation,"Our working group aims to comprehensively investigate how transposable elements (TEs) contribute to genomic innovation and species diversification in vertebrates through the generation of novel functional elements. While TEs comprise nearly half of the human genome and are increasingly recognized as sources of regulatory innovation, their repetitive nature has complicated their analysis using short-read sequencing technologies. We propose a two-phase approach: First, we will reprocess tens of thousands of publicly available genomic datasets (including ChIP-seq, ATAC-seq, RNA-seq, and Ribo-seq) using repeat-aware analysis pipelines and new telomere-to-telomere and pangenome assemblies. Second, we will synthesize this reprocessed data to understand how TEs generate new transcription factor binding sites, cell-specific and species-specific enhancers, novel transcripts, and protein-coding sequences. The project combines multidisciplinary expertise from multiple labs and will leverage advanced computational approaches to uncover complex relationships between TEs and regulatory networks. Our investigation represents a quintessential mesoscale emergence phenomenon, bridging molecular-level TE insertions with emergent regulatory networks and phenotypic innovations. The project will produce both a comprehensive public data resource of repeat-aware genomic analyses and new insights into how TEs drive biological innovation through regulatory network evolution.",Shaun Mahony,"Introduction Transposable elements are mobile genetic elements that can insert themselves into new genomic locations, resulting in duplication of DNA. Nearly half of the human genome originates from transposon activity. Since their discovery by Barbara McClintock, their biological and evolutionary roles have been debated. Once dismissed as “junk DNA,” they are now recognized as key drivers of genomic innovation, creating new regulatory elements and contributing to evolutionary diversification. For instance, a large fraction of primate-specific transcription factor binding sites are derived from transposable elements. This means TEs have played a fundamental role in shaping species-specific gene regulation. However, their repetitive nature complicates functional characterization with sequencing-based assays. High-throughput methods like ChIP-seq, ATAC-seq, and RNA-seq rely on short reads that often map ambiguously to repetitive regions, producing multi-mapped reads that most pipelines discard. As a result, much of the genome’s repetitive and TE-rich regions remain underexplored in existing analyses. Recent advances in both computational and sequencing technologies now make it possible to revisit these regions with improved accuracy. Telomere-to-telomere and pangenome assemblies fill previous gaps in reference genomes, providing a more complete representation of repetitive DNA. New probabilistic read allocation methods, such as Allo, allow recovery of signals from multi-mapped reads, substantially increasing the detection of functional genomic elements associated with TEs. Preliminary analyses already show large gains in identified binding sites when accounting for such reads. Together, these technological advances provide an unprecedented opportunity to systematically characterize the functional roles of TEs across genomes and species. Goals This project aims to determine how transposable elements generate novel functional elements that drive phenotypic diversification across vertebrates. We will reprocess large-scale genomic, transcriptomic, and ribosome profiling datasets using repeat-aware pipelines and integrate them across data modalities, cell types, and species to answer fundamental biological questions. The key questions include: How do TEs generate new transcription factor binding sites, and which transcription factors preferentially bind to specific TE families? How do TEs give rise to cell-type- or species-specific enhancers? How do they contribute to novel transcripts and protein-coding sequences? Addressing these questions will illuminate how molecular-level TE insertions give rise to new regulatory networks and biological functions. Proposed Activities Phase 1: Reprocessing regulatory genomics data with repeat-aware pipelines We will work with computational staff to reprocess tens of thousands of publicly available genomic datasets using repeat-aware methods. This includes ATAC-seq, DNase-seq, ChIP-seq (for transcription factors and histone marks), RNA-seq, and Ribo-seq from major consortia such as ENCODE, Roadmap Epigenomics, FAANG, and GTEx, along with vertebrate comparative data. Both human and mouse datasets will be aligned to telomere-to-telomere assemblies for maximum genome coverage, and human data will also be aligned to pangenome assemblies to capture population-level structural variation. Reads will be mapped using standard aligners configured to retain multiple mapping locations, followed by probabilistic allocation of multi-mapped reads using tools such as Allo and Rcount. Peak calling and quantification will be performed on these reprocessed data to generate consistent repeat-aware maps of regulatory activity, transcription, and translation. This phase will produce a comprehensive collection of regulatory and expression datasets that accurately capture activity within repetitive regions of the genome. Phase 2: Data synthesis to understand the role of TEs in generating functional elements Building on Phase 1, we will integrate and analyze the repeat-aware datasets to uncover patterns of TE-derived functional innovation. We will identify TE-derived transcription factor binding sites, enhancers, transcripts, and translated sequences by intersecting data with RepeatMasker annotations and evaluating their conservation across species using multi-species alignments. Chromatin states will be annotated using hidden Markov models, enabling classification of TE-derived regions as promoters, enhancers, or other regulatory states across different cell types. Statistical analyses will assess associations between TE families, transcription factors, gene expression, and tissue specificity. Beyond correlation-based approaches, we will apply integrative methods such as tensor decomposition to identify higher-order dependencies between data modalities—for example, linking specific transcription factors with TE families that activate enhancer programs leading to tissue-specific gene expression. This integrative analysis will reveal how TEs collectively reshape regulatory landscapes. Outcomes The project will yield a comprehensive, publicly accessible collection of reprocessed datasets capturing regulatory and transcriptional activity in repetitive regions, along with reproducible analysis pipelines for repeat-aware genomics. The data and methods will enable researchers to study regions of the genome that were previously inaccessible and to explore how TEs contribute to genomic and phenotypic innovation. Phase 2 will result in research publications describing biological discoveries as well as review papers summarizing best practices for repeat-aware data processing and analysis. Rationale for a Working Group Approach This project brings together complementary expertise in transposon biology, regulatory genomics, evolutionary biology, and computational data integration. Members specialize in distinct but interconnected areas—TE functionalization, TE evolution, enhancer and network evolution, species-specific regulation, and multi-omic data modeling—making collaboration essential. The scale of data reprocessing and integration required exceeds the capabilities of any individual laboratory, underscoring the need for coordinated efforts and shared computational infrastructure. Rationale for NCEMS Support The scope of this project aligns directly with the mission of NCEMS: synthesizing large-scale data to uncover mesoscale emergence phenomena. TE-driven regulatory innovation exemplifies such emergence, linking molecular events like TE insertions to higher-order regulatory networks and organismal diversity. NCEMS support will provide the computational power, storage, and expert personnel required to reprocess over 75,000 datasets, implement integrative analyses, and coordinate multi-lab collaboration. Requested Resources The project requires approximately ten million CPU hours for large-scale data reprocessing and analysis, staff scientist support for pipeline execution and data management, and storage for intermediate and processed datasets. Additional resources will support collaborative meetings, data publication, and personnel exchanges between participating labs. Proposed Timeline The project will span two years, with Phase 1 dedicated to reprocessing datasets and Phase 2 focusing on data integration and biological interpretation. The first year will include an initial meeting to finalize analysis plans and pipelines, followed by repeat-aware reprocessing across data modalities. During the second year, integrated analyses will be conducted, and results will be synthesized into collaborative publications and data resources.",Accepted,1
human_11,human,human,human,Searching the crosslinking mass spectrometry universe for new protein-protein interactions,"Crosslinking mass spectrometry (XL-MS) is a powerful structural proteomics method that can provide high-resolution structural information about complex mixtures of proteins in their native physiological context.1,2,3 The method works by chemically crosslinking two reactive amino acid residues that are close to one another in 3-D space, thereby freezing this spatial information into a covalent bond, and then retrieving this information by sequencing crosslinked peptides following enzymatic digest of the constituent proteins (like Hi-C,4 but for proteins). Initially developed in the 2000s to interrogate purified proteins, by the late-2010s, methods became available to interrogate protein-protein interactions (PPIs) on the proteome-scale and in the cellular environment.5 This project seeks to standardize, combine, and integrate existing publicly available crosslinking datasets from PRIDE to make these data more re-usable and useful for integrative and hybrid modeling. We will also create a meta-dataset to catalogue PPIs in human and other model organisms. Because XL-MS encodes information about both protein identity and interacting residues, these findings can be cross-validated to structural predictions of protein complexes using AlphaFold3, thereby providing stronger evidence for their existence compared to huMAP3.0, which predicts protein complexes without structural evidence. We will integrate these findings into the EBI Complex Portal to make it accessible to the life science community.",Stephen D. Fried; Yasset Perez-Riverol; Henning Hermjakob,"Introduction Every protein is connected. The physical interactions made by proteins often define their functions; hence, to understand the whole proteome, we must also have a complete compendium of protein complexes. Several proteomics approaches can be used to infer the existence of a pairwise protein-protein interaction, the fundamental unit of data used to build a model of a protein complex, such as affinity purification mass spectrometry, proximity labeling, co-fractionation mass spectrometry, and photoactivated RNA crosslinking coupled to mass spectrometry. Large-scale efforts such as BioPlex have conducted tens of thousands of affinity purification experiments to catalog human protein complexes. huMAP integrates BioPlex data with co-fractionation and proximity labeling datasets, using machine learning to cluster protein pairs into a global network. This network estimates that most human proteins exist in some complex, and its predictions have been incorporated into the EBI Complex Portal, greatly expanding the manually curated human complexes. While these approaches are powerful, none provide structural information or evidence of which residues or interfaces are involved in a protein-protein interaction. Furthermore, these assays require that complexes survive lysis and remain intact in vitro, meaning transient interactions are often missed. Crosslinking mass spectrometry is an orthogonal method that encodes pairwise interactions by sequencing the two peptides derived from two proteins covalently connected through a chemical crosslinker. Originally applied to purified proteins, advances in crosslinker chemistry, mass spectrometry instrumentation, and search software have enabled proteome-wide applications. A contact between any reactive residue on one protein and any reactive residue on another can now be identified and validated. For many years, confidently identifying inter-protein crosslinks in complex samples was challenging because search software could not properly estimate false discovery rates. This limitation has been overcome by new algorithms capable of accurate validation. The XL-MS community has generally made raw data available on PRIDE, a major proteomics repository, but reusability remains limited. Different search algorithms, customized databases, and inconsistent metadata have created barriers to data reuse. Until recently, there was no unified format for reporting processed crosslinking data, and sample metadata were often poorly annotated. Proposed Activities This project has three main activities. The first aims to select large-scale XL-MS datasets from PRIDE, re-annotate them with standardized metadata, and reanalyze them using modern search algorithms to improve data quality and reusability. The second will mine reprocessed data to discover potentially new protein-protein interactions and provide additional validation for existing ones, integrating these validated interactions into the EBI Complex Portal. The third goal, if time permits, is to use the interaction data to suggest potential functions for uncharacterized proteins based on their interaction partners. Reanalysis of XL-MS data on PRIDE There are currently over a thousand PRIDE datasets mentioning crosslinking, though few include complete processed data. Many contain only partial or technical studies without biological context. We will compile a list of biologically meaningful, large-scale XL-MS datasets focused on protein complex and interaction analysis. Each legacy dataset will be reannotated using standardized metadata formats and reprocessed using modern algorithms such as Scout. The results will be redeposited to PRIDE under new accessions as complete standardized submissions, linked to the originals. These will populate a dedicated PRIDE Crosslinking database, providing consistent and accessible data for reuse. Conglomerate Pairwise Residue Interactions to Validate Predicted Complexes A curated subset of large-scale reprocessed datasets across multiple model organisms will be analyzed in depth. Criteria for inclusion will include proteome-wide design, use of cleavable crosslinkers, studies in model organisms such as yeast or human cells, and sufficient experimental scale. Inter-protein crosslinks from these datasets will be pooled and analyzed to validate predicted complexes. For human data, each detected interaction will be compared against huMAP predictions. If the proteins are linked in huMAP, structural modeling with AlphaFold will assess whether the crosslinked residues are spatially proximal in the predicted structure. Verified interactions will increase the evidence level for those complexes in the Complex Portal, annotated with structural and dataset references. Crosslinks not already in huMAP will also be structurally tested, and passing cases will be added to the portal with provisional evidence labels. Similar analyses will be extended to yeast datasets. Additional analyses may include reprocessing affinity purification data from other organisms using quantitative pipelines, clustering them into complexes, and comparing them to XL-MS data using the same validation framework. Proposing Functions for Unannotated Proteins Many human proteins still lack assigned functions. While most functional proteins contain recognizable catalytic domains, uncharacterized proteins may act as scaffolds that mediate specific interactions. To begin annotating these, we will examine XL-MS-derived interaction networks. If an uncharacterized protein interacts with one or more proteins of known function, we will compile examples to identify emerging patterns or network features that may guide hypothesis generation for future studies. Outcomes Reanalysis of PRIDE XL-MS data will yield roughly a hundred large-scale standardized datasets, greatly expanding the number of accessible, biologically relevant crosslinking experiments. These data will facilitate reuse, allow non-specialists to access residue-pair information directly, and ensure compatibility with hybrid modeling databases. Conglomerating residue-level interactions will increase confidence in predicted complexes, refine evidence levels in the Complex Portal, and identify new interactions supported by structural modeling. Preliminary results will support high-impact publications and public data resources. Analyses of uncharacterized proteins may yield hypotheses about their roles and guide future functional studies. Working Group Approach The project requires combined expertise in mass spectrometry, data standards, database engineering, and structural biology. Dr. Fried’s team brings experience in XL-MS and structural biology, while collaborators at the EBI provide critical infrastructure for PRIDE and the Complex Portal, enabling large-scale reanalysis and integration. Manual curation and automation tasks will be supported by dedicated data scientists. This collaboration ensures both scientific depth and technical feasibility. Rationale for NCEMS Support XL-MS data provide unique residue-level evidence of protein-protein interactions but remain underutilized due to inconsistent formats and metadata. NCEMS support is essential to harmonize, reprocess, and integrate these data, linking them to existing structural predictions and interaction networks. This integration aligns with ongoing developments in hybrid structural biology, providing valuable resources for the community. Requested Resources The project requires a data scientist for dataset curation, annotation, and reanalysis, with travel support for collaboration between NCEMS and EBI teams. Computational resources are needed for large-scale reprocessing and structural predictions, estimated at tens of thousands of CPU hours and thousands of AlphaFold runs. Personnel and computational support will ensure timely and reproducible execution. Proposed Timeline The project will span approximately fifteen months. The first six months will focus on dataset reannotation and reprocessing, led by a data scientist with oversight from EBI collaborators. The next six months will focus on structural validation and integration of results into the Complex Portal. The final phase will explore uncharacterized proteins and synthesize results for publication and public release.",Accepted,4
human_12,human,human,human,Intelligent metadata compilation to enhance the reusability and discoverability of mass spectrometry-based proteomics data,"Mass spectrometry-based proteomics generates vast amounts of data, yet the effective reuse and discovery of these datasets remain challenging due to incomplete and inconsistent metadata. Metadata, which provides critical contextual information such as experimental conditions, sample characteristics, and data processing details, is essential for making data FAIR (findable, accessible, interoperable, and reusable). To address these challenges, our working group aims to develop intelligent, automated workflows for comprehensive metadata extraction, harmonization, and integration into PRIDE, the largest public proteomics data repository. We will develop powerful bioinformatics and natural language processing tools to extract metadata directly from raw mass spectrometry files and scientific publications. By leveraging structured data standards, metadata will be enriched with controlled vocabulary terms, enhancing dataset transparency and usability. Community engagement through a machine-learning challenge will further drive innovation in metadata extraction techniques. The outcomes of the working group's activities will include automated metadata compilation workflows integrated into user-friendly tools, advanced dataset querying capabilities in PRIDE, and the creation of educational resources for the scientific community. These efforts will enable researchers to efficiently locate and reuse proteomics datasets, facilitating secondary analyses such as AI model training and large-scale reanalysis.",Wout Bittremieux; Iddo Friedberg,"Introduction and Goals In line with the FAIR principles—findable, accessible, interoperable, and reusable—omics data is increasingly accessible through public repositories tailored to diverse research domains. This surge in data availability has immense potential to accelerate scientific discovery and foster cross-disciplinary innovation. However, the effective findability and reusability of these datasets remain limited, primarily due to a lack of comprehensive metadata. Metadata, which includes details like experimental conditions, sample characteristics, data processing methods, and instrument settings, is critical for transforming raw data into scientifically valuable resources. Unfortunately, metadata is often incomplete, inconsistently structured, or buried in unstructured formats such as free-form text in publications. These shortcomings restrict automated processing, systematic querying, and the discovery of relevant datasets. To bridge this gap, the working group will develop automated methods to extract, refine, and associate high-quality metadata with omics data in public repositories. This initiative will lay the groundwork for improved data discoverability and expanded reusability. While metadata enhancement is relevant across all omics fields, the initial focus is on mass spectrometry-based proteomics due to its complex metadata needs and established annotation frameworks. Specifically, the group will leverage the SDRF-Proteomics format, a structured tab-delimited standard adopted from transcriptomics (MAGE-TAB) by the Proteomics Standards Initiative. SDRF-Proteomics improves data transparency and usability by specifying relationships between samples, assays, and data files. Despite its growing adoption, many datasets remain inconsistently annotated. The group proposes to address this by developing intelligent, automated workflows to systematically gather and associate metadata for proteomics experiments and integrate it into PRIDE, the largest proteomics data repository. The tools will extract technical and biological metadata directly from experimental data, open-access publications, and existing annotations. Extracted metadata will be harmonized using controlled vocabulary and ontology terms and linked to datasets in PRIDE, creating an enriched metadata layer accessible for secondary analyses. To ensure accessibility, a user-friendly interface will be implemented in PRIDE’s web portal, alongside an API for metadata-based queries. Enhanced metadata infrastructure will unlock powerful capabilities for proteomics researchers, such as identifying suitable training datasets for AI tools, enabling automated large-scale reanalyses like differential protein expression profiling, and facilitating re-interpretation of public data in light of new hypotheses. Proposed Activities The project will initially focus on standard bottom-up discovery proteomics workflows, encompassing data-dependent and data-independent acquisition with both label-free and label-based quantitation, as well as post-translational modification-enriched datasets. Time permitting, it will expand to specialized approaches such as crosslinking and hydrogen-deuterium exchange proteomics. This stepwise approach allows controlled validation and extension to other omics disciplines in the future. Metadata retrieval will combine extraction from raw mass spectrometry data with mining of complementary metadata from scientific literature. From raw data, proprietary formats will be converted to mzML, and essential metadata such as instrument type, parameters, and search settings (mass tolerances, digestion enzymes, isobaric labeling strategies) will be derived using existing tools such as Param-Medic. Organism identification will be obtained by matching against reference data, and digestion enzymes and modifications will be determined via open modification searching. These components will be integrated into an automated workflow using Nextflow to streamline extraction and organization. For literature-based metadata, critical contextual information not found in raw data (biological conditions, case/control groupings, disease relevance) will be gathered using a natural language processing pipeline built on large language models fine-tuned for scientific text mining. To ensure accuracy and transparency, the models will indicate unavailable elements, perform self-verification, and use ensemble cross-checking among LLMs. To engage the community, a machine-learning challenge will be launched to drive innovation in metadata extraction. Winning open-source algorithms will be integrated into workflows. Tutorial datasets and documentation will be shared through ProteomicsML, fostering education for both data scientists and proteomics practitioners. To develop and validate the approach, the group will use a curated set of public datasets with SDRF metadata as a gold standard, along with a manually curated private test set as an independent benchmark for evaluating metadata accuracy. Scoring will emphasize precision to ensure high-confidence results. Finally, the workflows will be integrated into user-friendly tools such as lesSDRF, enabling automated metadata compilation during PRIDE submissions. Retrospective metadata generation will also be applied to existing public datasets with support from PRIDE’s data curation team. Enhanced PRIDE functionality will include metadata-driven dataset querying via the website and API. Outcomes The project will deliver multiple key outcomes: automated workflows for metadata extraction from raw data and open-access publications using NLP; a community challenge to produce open-source metadata extraction solutions; educational resources and tutorials through ProteomicsML; enhanced metadata tools integrated with PRIDE submission workflows; advanced metadata-based querying and API retrieval in PRIDE; and several open-access publications describing methodologies and technical innovations. All workflows and outputs will follow open-science principles, with code released on GitHub and manuscripts preprinted with accompanying data. While initially focused on proteomics, this work will create a scalable template for addressing metadata challenges across omics disciplines, beginning with metabolomics and beyond. Rationale for a Working Group Approach The project combines expertise across bioinformatics, machine learning, NLP, proteomics, and data repository management, exceeding the capacity of any single research group. Machine-learning and NLP specialists will design algorithms to extract metadata from raw files and publications, while bioinformaticians will align metadata with structured data standards. Proteomics experts will ensure biological relevance, and repository teams will oversee integration with PRIDE. This coordinated, interdisciplinary effort enables robust, scalable, and biologically meaningful metadata solutions. Rationale for NCEMS Support The project aligns with NCEMS’s mission to leverage public data for large-scale synthesis and develop novel integrative strategies. NCEMS staff scientists will assist with SDRF metadata curation, community challenge support, and workflow integration. CyVerse computational resources will provide scalability for NLP and large-scale metadata extraction. Team science training, project management support, and coordination of tool integration across geographically distributed members will further enhance outcomes. NCEMS’s contribution ensures both technical infrastructure and sustainability for this collaborative initiative. Requested Resources The project requires staff scientist and assistant support specializing in metadata harmonization and proteomics experimental design. These roles will include curation of metadata, support for NLP corpus creation, management of the community challenge, technical guidance, and workflow integration. Computational resources include approximately 15 TB of storage for public data mirroring, 2,500 CPU hours for metadata extraction pipelines, and 500 GPU hours for local fine-tuning of LLMs. Proposed Timeline The project is planned for two years. The first year will focus on metadata compilation, workflow development, and a community challenge launch. The second year will include evaluation of challenge outcomes, integration of workflows into lesSDRF, retrospective metadata generation for PRIDE datasets, and implementation of metadata-based querying functionality. Final deliverables will include completed tools, datasets, and open-access publications disseminated to the research community.",Accepted,4
ai_single_gemini_01,ai,single,gemini-2.5-pro,The Chromatin Compiler: Emergence of Cellular Identity from Genome Architecture,"A fundamental question in biology is how a single genome can give rise to hundreds of distinct, stable cell types. This emergent property, cellular identity, is encoded not just in the DNA sequence but in its dynamic, three-dimensional folding within the nucleus. We propose to create a 'Chromatin Compiler,' a predictive framework that synthesizes vast, publicly available datasets to understand how chromatin architecture dictates cell fate. This Working Group will integrate multi-modal data including Hi-C, ChIA-PET, and Micro-C from the 4D Nucleome portal to map genome topology; single-cell ATAC-seq and RNA-seq from cell atlases (e.g., Human Cell Atlas) to link chromatin state to gene expression at cellular resolution; and ChIP-seq data from ENCODE and Roadmap Epigenomics for histone modifications and transcription factor binding. Our transdisciplinary team, comprising polymer physicists, computational biologists, developmental biologists, and machine learning experts, will develop novel graph-based neural networks and dynamical systems models. These models will aim to uncover the 'grammatical rules' governing how interactions between regulatory elements, topological associating domains (TADs), and nuclear compartments orchestrate complex gene expression programs during differentiation. The project's success will yield a powerful in silico tool to predict how perturbations to genome architecture—such as those caused by structural variants in cancer or developmental disorders—lead to aberrant cell identities. This work will provide unprecedented insight into the emergent logic of cellular programming and establish a new paradigm for studying genome function.",,"Background And Significance

The establishment and maintenance of cellular identity is a cornerstone of multicellular life and a classic example of an emergent biological phenomenon. How hundreds of distinct, stable cell types arise from a single zygotic genome remains one of the most fundamental questions in biology. While the central dogma describes the flow of information from DNA to RNA to protein, it is the complex regulatory landscape controlling this flow that specifies cell type. This landscape is not merely linear; it is profoundly shaped by the three-dimensional (3D) architecture of the genome within the nucleus. The past decade has revolutionized our understanding of this architecture, revealing a hierarchical organization from nucleosomes to chromatin loops, Topologically Associating Domains (TADs), and large-scale compartments (Lieberman-Aiden et al., 2009; Dixon et al., 2012). This structure is not random; it is intimately linked to function. TADs, for instance, act as insulated neighborhoods that constrain enhancer-promoter interactions, ensuring genes are regulated appropriately. The disruption of these boundaries by structural variants is now a known mechanism in developmental disorders and cancer (Lupiáñez et al., 2015). Concurrently, large-scale international consortia such as the Encyclopedia of DNA Elements (ENCODE), the Roadmap Epigenomics Project, the 4D Nucleome (4DN) Program, and the Human Cell Atlas (HCA) have generated an unprecedented wealth of publicly available data. We now have access to multi-modal datasets spanning 3D genome conformation (Hi-C, Micro-C), chromatin accessibility (ATAC-seq), histone modifications and transcription factor binding (ChIP-seq), and gene expression (RNA-seq) across a vast array of human cell types, often at single-cell resolution. This data explosion presents a monumental opportunity for synthesis. Despite these advances, critical gaps in our knowledge persist, preventing a truly predictive understanding of genome function. First, our view of genome architecture is largely static. We have snapshots of different cell types but lack a comprehensive, dynamic model of how architecture reconfigures during cell fate transitions. Second, existing computational models are often siloed, focusing on predicting one aspect of genome organization (e.g., TAD boundaries from sequence) but failing to integrate multi-modal data to predict a holistic, functional outcome like a cell-type-specific gene expression program. Third, and most critically, our understanding remains descriptive rather than predictive. We can characterize the architectural differences between a neuron and a lymphocyte, but we cannot yet formulate a set of generalizable 'rules' or a computational model that can predict, *ab initio*, the functional consequences of a specific architectural perturbation. There is no 'compiler' to translate the language of genome structure into the language of cellular function. This project is both important and timely because it directly addresses these gaps. It is important because a predictive model of genome function would transform our ability to interpret genetic variation, understand disease mechanisms, and engineer cell fates for therapeutic purposes. It is timely because the necessary ingredients are finally in place: the requisite large-scale public data is available, and advances in machine learning, particularly deep learning on graphs, provide the powerful analytical tools needed to tackle this complexity. This proposal outlines a community-scale synthesis project to build this 'Chromatin Compiler,' a framework that will learn the grammatical rules of genome architecture and provide a quantitative, predictive model of how cellular identity emerges from the dynamic folding of the genome.

Research Questions And Hypotheses

Our overarching goal is to develop and validate a predictive computational framework, the 'Chromatin Compiler,' that formalizes the relationship between 3D genome architecture and the emergent property of cellular identity. This ambitious objective is broken down into three specific, interconnected research questions, each with testable hypotheses.

**Research Question 1 (RQ1): What are the fundamental architectural principles and regulatory 'grammar' that define a stable cellular identity?**
We posit that the complex, multi-modal data describing a cell's chromatin state can be distilled into a core set of predictive features and interaction rules. 
*   **Hypothesis 1a:** A stable cellular identity is encoded by a unique, multi-scale 'signature' of chromatin interactions. This signature is not defined by any single feature but by the synergistic combination of specific long-range enhancer-promoter contacts, the precise positioning and insulation strength of TAD boundaries, and the global pattern of genomic compartmentalization. 
*   **Hypothesis 1b:** A finite set of 'grammatical rules' governs these interactions. We hypothesize that cell-type-specific transcription factors (TFs) and epigenetic marks act as a regulatory 'syntax,' determining which genomic loci can interact and modulating the probability and stability of these interactions. For example, the presence of H3K27ac and pioneer TFs at distal elements may be a rule that 'permits' looping to a promoter marked by H3K4me3.
*   **Testing and Validation:** We will test these hypotheses by building a graph neural network (GNN) that integrates multi-modal data (Hi-C, ATAC-seq, ChIP-seq) to classify cell types. Success will be defined by high classification accuracy (>95%) on held-out test data. We will then use model interpretability techniques (e.g., GNNExplainer) to extract the features and interactions (the 'grammar') most salient for each cell-type classification, thereby validating our hypotheses.

**Research Question 2 (RQ2): How do dynamic changes in genome architecture orchestrate cell fate transitions during differentiation?**
We propose that differentiation is not a random walk but a directed, programmatic process of architectural remodeling.
*   **Hypothesis 2a:** Cell differentiation follows a stereotyped trajectory through a high-dimensional 'chromatin state space.' This trajectory is characterized by the ordered and sequential formation, dissolution, and rewiring of specific chromatin loops and TADs, which precedes and directs changes in gene expression.
*   **Hypothesis 2b:** Master developmental transcription factors (e.g., PAX6 in neurogenesis) act as primary drivers of these architectural changes. They function by binding to key nodes in the chromatin network, initiating local changes in accessibility and recruiting chromatin remodeling complexes, which then propagate through the network to establish a new, stable architectural state.
*   **Testing and Validation:** We will use publicly available time-series multi-omic data from well-characterized *in vitro* differentiation systems (e.g., embryonic stem cells to cardiomyocytes). We will develop a spatio-temporal GNN or a neural ODE model to learn the transition rules between successive time points. The model will be validated by its ability to accurately predict future chromatin and expression states from earlier ones. We will perform *in silico* 'knockout' experiments by removing the signal of a master TF from the input data and predicting the resulting deviation from the normal differentiation trajectory, comparing these predictions to published experimental results.

**Research Question 3 (RQ3): Can the Chromatin Compiler predict the functional consequences of architectural perturbations, such as pathogenic structural variants (SVs), on cellular identity?**
We aim to move from descriptive correlation to causal prediction, using our model as a tool for *in silico* genetics.
*   **Hypothesis 3a:** Pathogenic non-coding SVs exert their effects primarily by altering the 'grammatical rules' of chromatin folding. Deletions of TAD boundaries, for example, lead to 'enhancer hijacking' by allowing enhancers from one domain to ectopically activate oncogenes in an adjacent domain. Inversions and translocations can create novel, disease-driving enhancer-promoter contacts.
*   **Hypothesis 3b:** Our trained Chromatin Compiler can accurately predict the aberrant gene expression patterns resulting from a given SV. By inputting the altered genomic sequence and structure, the model will predict the new 3D contact map and, consequently, which genes will be misregulated.
*   **Testing and Validation:** We will curate a validation set of well-characterized pathogenic SVs from literature and databases (e.g., COSMIC, ClinVar). We will implement these SVs *in silico* within our model's framework and predict their impact on gene expression in the relevant cell type. Predictions will be quantitatively compared against ground-truth experimental data (e.g., RNA-seq, 4C-seq) from patient-derived cells or engineered cell lines harboring these SVs. The model's predictive power will be assessed by the correlation between predicted and observed changes in gene expression.

Methods And Approach

This project is founded on the synthesis of public data and the development of novel computational methods, executed by a transdisciplinary Working Group. Our approach is organized into four synergistic aims that directly address our research questions.

**Working Group Structure and Collaboration:** The project's success hinges on the deep integration of diverse expertise. The team comprises four PIs and their trainees: **PI 1 (Computational Biology)** will lead data acquisition, harmonization, and the development of reproducible analysis pipelines. **PI 2 (Machine Learning)** will spearhead the design, implementation, and training of the core deep learning models. **PI 3 (Polymer Physics)** will provide theoretical guidance on chromatin biophysics, ensuring our models are physically plausible and aiding in the interpretation of architectural changes. **PI 4 (Developmental Biology)** will provide crucial biological context, guiding the selection of cellular systems and validating the biological relevance of model predictions. The group will convene for biannual, intensive in-person workshops, supplemented by weekly virtual meetings, to foster continuous collaboration. Trainees will be co-mentored and will spend time in partner labs to gain cross-disciplinary skills.

**Aim 1: Unified Multi-modal Chromatin Data Compendium.** The foundation of our project is a comprehensive, harmonized database of publicly available human genomics data. We will systematically mine repositories including 4D Nucleome, ENCODE, Roadmap Epigenomics, GEO, and the Human Cell Atlas. Data types will include: 1) **3D Architecture:** Hi-C, Micro-C, ChIA-PET; 2) **1D Epigenomics:** ChIP-seq for key histone marks (H3K27ac, H3K4me3, H3K27me3) and architectural proteins (CTCF, RAD21), ATAC-seq, and DNase-seq; and 3) **Transcriptomics:** Bulk and single-cell RNA-seq. We will focus on curating data from at least 100 distinct cell types and several well-documented differentiation time courses (e.g., ESCs to neurons, HSCs to hematopoietic lineages). All data will be processed through a single, containerized (Docker/Singularity) and version-controlled (Nextflow) pipeline to ensure maximal reproducibility and eliminate batch effects from variable processing. The final output will be a unified data resource where all modalities are mapped to a common genomic coordinate system, ready for machine learning integration.

**Aim 2: Modeling Static Cellular Identity with Graph Neural Networks (GNNs).** To address RQ1, we will represent the genome as a graph. Genomic regions (e.g., 10kb bins) will serve as nodes. Node features will be vectors of 1D epigenomic signals from Aim 1. Edges between nodes will be weighted by their 3D contact frequency from Hi-C data. We will develop a Graph Attention Network (GAT), a GNN architecture adept at learning the importance of different neighbors in a graph. The model will be trained on our compendium of cell types with the objective of predicting the cell type label from the input chromatin graph. The trained model will serve as our initial 'Chromatin Compiler.' We will employ model interpretation tools (e.g., saliency maps, attention weight analysis) to dissect the trained model and extract the predictive genomic features and interactions that constitute the 'grammatical rules' of each cell identity.

**Aim 3: Modeling Dynamic Cell Fate Transitions.** To address RQ2, we will extend our static model to capture dynamics. Using time-series data from differentiation, we will build a spatio-temporal GNN. This model will incorporate a recurrent component (like an LSTM or GRU) or be framed as a Neural Ordinary Differential Equation (Neural ODE) to learn the continuous-time evolution of the chromatin state. The model will learn a function `f` that maps the chromatin graph at time `t` to the graph at time `t+1`. Validation will involve predicting later time points from earlier ones and assessing the model's ability to reconstruct the known differentiation trajectory. This dynamic model will allow us to identify critical 'bifurcation points' in the chromatin state space where cell fate decisions are made and to pinpoint the key TF and regulatory events that drive these transitions through *in silico* perturbation experiments.

**Aim 4: Predictive Modeling of Architectural Perturbations.** To address RQ3, we will build a predictive module on top of our trained models. This module will take a structural variant (SV) as input (e.g., coordinates of a deletion, inversion, or translocation). It will algorithmically modify the genomic graph representation to reflect the SV. The perturbed graph will then be fed into the trained 'Chromatin Compiler' from Aim 2. We will add a regression head to the GNN, trained to predict gene expression levels from the surrounding chromatin graph state. The model's output will be a prediction of the new 3D contact landscape and the resulting changes in gene expression for all genes near the SV. We will validate these predictions against a curated set of known pathogenic SVs, comparing our *in silico* results with published experimental data.

**Timeline:** **Year 1:** Completion of Aim 1 data compendium; development and initial training of the static GNN model (Aim 2). **Year 2:** Refinement and interpretation of the static model; development and training of the dynamic model (Aim 3); first major publication on the static compiler. **Year 3:** Validation of the dynamic model; development and validation of the SV prediction module (Aim 4); release of the open-source 'Chromatin Compiler' software package and web portal; final publications.

Expected Outcomes And Impact

The 'Chromatin Compiler' project is a high-risk, high-reward endeavor poised to fundamentally shift the paradigm of genome biology from a descriptive to a predictive science. Its success will yield profound intellectual contributions, significant broader impacts on human health, and a powerful new platform for the scientific community, directly aligning with the goals of the NCEMS research call.

**Intellectual Merit and Contribution to the Field:** The primary outcome of this work will be a validated, predictive computational framework that quantitatively links 3D genome architecture to cellular identity. This represents a significant leap beyond current correlative studies. We will deliver: 1) A comprehensive, harmonized multi-modal atlas of the human epigenome and 3D genome. 2) A novel class of machine learning models (spatio-temporal GNNs) tailored for integrative genomics. 3) A catalogue of the architectural 'grammatical rules' that define and maintain specific cell types. 4) The first dynamic models that map the architectural trajectories of cell differentiation. By formalizing how a complex biological property—cellular identity—emerges from the collective interactions of molecular components, this project provides a concrete solution to a central question in emergence phenomena, a core focus of this call. It will establish a new field of 'predictive 4D genomics.'

**Broader Impacts and Biomedical Applications:** The long-term impact of the Chromatin Compiler on biomedical research will be substantial. The ability to predict the functional consequences of genomic variants is a central goal of personalized medicine. Our framework will provide a powerful tool for: 1) **Interpreting Disease Genomics:** Over 90% of disease-associated variants from GWAS lie in non-coding regions. Our tool will help prioritize and mechanistically explain how these variants, particularly structural variants, contribute to disease by altering genome architecture. 2) **Understanding Developmental Disorders:** Many congenital diseases are caused by mutations that disrupt long-range gene regulation. The Compiler will provide a means to diagnose and understand these 'chromatinopathies.' 3) **Cancer Biology:** We can use the model to understand how somatic SVs in cancer genomes lead to oncogene activation or tumor suppressor inactivation through mechanisms like enhancer hijacking, providing insights for novel therapeutic strategies. 4) **Synthetic Biology:** A deep understanding of the genome's operating principles is a prerequisite for rational cell engineering. The 'rules' we uncover could guide the design of synthetic chromosomes and customized cell types for regenerative medicine.

**Alignment with NCEMS Goals:** This project is perfectly aligned with the NCEMS mission. It is a community-scale synthesis project that is impossible for a single lab to undertake due to the immense data scale and the need for tightly integrated, diverse expertise (ML, physics, computational and developmental biology). It leverages exclusively public data to answer a fundamental question. We are committed to **Open Science**; all code will be open-source (GitHub), data will be shared via public repositories (Zenodo), and models will be accessible through a user-friendly web portal. Our dissemination plan includes high-impact, open-access publications and presentations at major international conferences. Furthermore, the project is designed to **Train the Next Generation** of data-savvy scientists. Trainees will be at the heart of the collaboration, gaining unique interdisciplinary skills that are in high demand. We will host an annual open hackathon to disseminate our tools and train the broader community, tapping into diverse new talent. The collaborative partnership we have assembled spans multiple institutions, career stages, and scientific disciplines, ensuring a rich and innovative research environment.

Budget And Resources

The proposed research requires a level of coordinated effort, computational power, and diverse expertise that extends far beyond the capacity of a single research lab or a traditional multi-PI grant. The NCEMS Working Group mechanism is uniquely suited to provide the necessary support and collaborative infrastructure for this community-scale synthesis project.

**Justification for NCEMS Support:** The primary challenge of this project is the integration of two distinct elements: massive, heterogeneous datasets and deep, diverse intellectual expertise. A single lab may possess expertise in one area but not all (e.g., machine learning but not developmental biology). NCEMS support is critical for assembling our geographically dispersed team of experts in computational biology, machine learning, polymer physics, and developmental biology, and providing the dedicated resources for them to function as a single, cohesive unit. Furthermore, the computational costs associated with training deep learning models on petabyte-scale genomic data are substantial and often exceed the budget of standard research grants. NCEMS funding will provide the necessary cloud computing resources and support for data management, which are central to the project's success. Finally, the NCEMS framework for regular in-person meetings and workshops is essential for fostering the intense, cross-disciplinary brainstorming required to develop truly innovative models and interpret their biological meaning.

**Budget Breakdown (3-Year Total):**
*   **Personnel ($480,000):** This constitutes the largest portion of the budget, reflecting our focus on training and dedicated effort. This includes salary and benefits for two full-time Postdoctoral Fellows who will be the primary technical leads, and stipend support for two Graduate Students. It also includes one month of summer salary for each of the four PIs to ensure dedicated time for management, supervision, and intellectual leadership.
*   **Computational Resources ($150,000):** This is a critical component for a project of this scale. Funds are requested for cloud computing credits (e.g., AWS/GCP) for GPU/TPU access required for training large-scale graph neural networks. This also includes costs for long-term cloud storage of the harmonized data compendium.
*   **Travel ($90,000):** To facilitate deep collaboration, we request funds for biannual, three-day in-person meetings of the entire Working Group (4 PIs, 4 trainees). Funds are also allocated for each trainee and PI to attend one major international conference per year to present their findings and network with the broader community.
*   **Training and Dissemination ($60,000):** This includes costs to organize and host an annual two-day workshop/hackathon open to the wider scientific community, providing training on our tools and methods. It also covers costs for developing and maintaining a public web portal for the Chromatin Compiler and open-access publication fees for an anticipated 4-5 manuscripts.
*   **Indirect Costs (IDC):** Calculated based on the respective institutional rates, applied to the direct costs.

**Total Direct Costs:** $780,000

**Institutional Resources:** The PIs' home institutions will provide significant in-kind support, including faculty and administrative salaries, office and lab space, and access to institutional high-performance computing clusters for model development and data preprocessing, thereby leveraging existing infrastructure and demonstrating strong institutional commitment to the project.",,
ai_single_gemini_02,ai,single,gemini-2.5-pro,Deconstructing the Condensatome: A Predictive Atlas of Liquid-Liquid Phase Separation in Cellular Function and Disease,"The discovery of biomolecular condensates, membraneless organelles formed via liquid-liquid phase separation (LLPS), has revolutionized our understanding of cellular organization. These dynamic compartments emerge from multivalent interactions among proteins and nucleic acids, concentrating components to regulate key processes like transcription and stress response. However, the sequence-level rules governing which molecules enter which condensate, and how condensate properties yield specific functions, remain elusive. This Working Group will build a predictive atlas of the 'condensatome' by synthesizing disparate public data. We will integrate protein sequence and domain information (UniProt, Pfam), experimentally determined and predicted structures (PDB, AlphaFold DB), protein-protein interaction networks (STRING, BioGRID), and LLPS-specific databases (PhaSepDB, DrLLPS). Our team of biophysicists, protein biochemists, cell biologists, and AI specialists will employ advanced machine learning models, including large language models trained on protein sequences and graph neural networks on interaction data, to predict LLPS propensity and composition. We will correlate these predictions with cell-type-specific transcriptomic and proteomic data to understand how the cellular context modulates condensate formation. A key goal is to identify the 'emergent grammar' of LLPS and predict how mutations, suchas those found in neurodegenerative diseases like ALS and Alzheimer's, disrupt condensate dynamics and lead to pathological aggregation. This project will produce a publicly accessible, predictive platform that will transform our ability to understand and therapeutically target the emergent biology of cellular compartmentalization.",,"Background And Significance

The paradigm of cellular organization has been fundamentally reshaped by the discovery of biomolecular condensates, membraneless compartments formed through liquid-liquid phase separation (LLPS). These dynamic assemblies, such as the nucleolus, stress granules, and processing bodies, emerge from multivalent interactions among proteins and nucleic acids, creating distinct biochemical environments that regulate a vast array of cellular processes. This emergent phenomenon of self-organization challenges the classical view of a cell governed solely by membrane-bound organelles. The formation of condensates is primarily driven by proteins containing intrinsically disordered regions (IDRs) and low-complexity domains (LCDs), which engage in a network of transient, specific interactions. The 'sticker-and-spacer' model provides a powerful conceptual framework, where 'stickers' (e.g., aromatic residues, charged motifs) mediate interactions and 'spacers' dictate the dynamics and material properties of the resulting condensate. The functional consequences are profound; by concentrating specific molecules and excluding others, condensates can enhance reaction rates, sequester components, and act as hubs for signal transduction and gene regulation. The significance of LLPS extends to pathophysiology. A growing body of evidence links aberrant phase transitions, particularly the conversion of dynamic liquid condensates into solid, pathological aggregates, to the etiology of numerous diseases. In neurodegenerative disorders like amyotrophic lateral sclerosis (ALS) and frontotemporal dementia (FTD), mutations in RNA-binding proteins such as FUS and TDP-43 promote their aggregation within stress granules. Similarly, the pathological aggregation of Tau protein in Alzheimer's disease is now understood through the lens of LLPS. Despite this rapid progress, the field faces critical knowledge gaps that hinder our ability to predict and control condensate behavior. Current understanding of the 'condensatome'—the complete set of condensates and their components—is fragmented and largely descriptive. We lack a comprehensive, predictive model that can answer fundamental questions: What are the precise sequence and structural features that determine a protein's partitioning into a specific condensate? How does the cellular context, including protein concentrations and post-translational modifications, modulate the composition and function of the condensatome? And how do disease-associated mutations disrupt this delicate equilibrium? Existing computational tools have made initial strides but are limited. Predictors like PScore, catGRANULE, and FuzDrop primarily rely on amino acid sequence features to predict a generic propensity for LLPS. While useful, they often fail to capture the specificity of interactions that dictate which proteins co-assemble. Furthermore, they do not typically integrate structural information or the broader context of protein-protein interaction networks. Databases such as PhaSepDB and DrLLPS are invaluable repositories of experimentally validated phase-separating proteins, but they represent a sparse sampling of the proteome and lack the integrative framework needed for system-wide prediction. This project is both important and timely due to a confluence of factors. First, the explosion of publicly available biological data—from comprehensive protein sequences (UniProt) and interaction maps (BioGRID, STRING) to the revolutionary availability of accurate predicted structures for entire proteomes (AlphaFold DB)—provides the raw material for an unprecedented data synthesis effort. Second, recent breakthroughs in artificial intelligence, particularly the development of protein large language models (pLMs) and graph neural networks (GNNs), offer powerful new methodologies to learn complex patterns from multi-modal biological data. This Working Group will address these gaps by synthesizing these disparate data streams within a unified machine learning framework. By building a predictive atlas of the condensatome, we will move beyond describing individual components to understanding the emergent, system-level logic of cellular organization, providing a transformative resource for basic science and therapeutic discovery.

Research Questions And Hypotheses

This Working Group is organized around a central overarching question: Can we decipher the multi-modal 'grammar'—encoded in protein sequence, structure, and interaction networks—that governs the assembly, composition, and function of the cellular condensatome? To address this, we have formulated three specific, interconnected research aims, each with testable hypotheses and clear validation strategies.

**Aim 1: Develop a unified, multi-modal machine learning framework to predict protein LLPS propensity and condensate-specific partitioning.**
This aim addresses the fundamental limitation of current predictors by integrating diverse data types to achieve a more holistic and accurate model of LLPS. 
*   **Research Question 1.1:** Can a model that integrates protein sequence, 3D structure, and network context significantly outperform single-modality approaches in predicting a protein's intrinsic ability to undergo LLPS?
*   **Hypothesis 1.1:** We hypothesize that an integrative machine learning model, combining embeddings from a protein language model (pLM) for sequence, a geometric deep learning model for structure (from AlphaFold DB), and a graph neural network (GNN) for protein-protein interaction (PPI) network topology, will yield superior predictive performance for LLPS propensity. The synergy of these modalities will capture complementary information: pLMs excel at context-rich sequence patterns, structural models reveal surface properties and multivalency, and GNNs encode the system-level context of potential binding partners.
*   **Validation 1.1:** The model will be trained on a curated set of known phase-separating proteins (positives) from PhaSepDB and LLPSDB and a rigorously selected set of negatives. Performance will be evaluated using receiver operating characteristic (ROC) and precision-recall (PR) curves under strict cross-validation and on held-out test sets. We will benchmark our multi-modal approach against a suite of state-of-the-art, single-modality predictors.
*   **Research Question 1.2:** Beyond general propensity, can we predict the specific condensate(s) a protein is likely to partition into?
*   **Hypothesis 1.2:** We hypothesize that the learned multi-modal representations contain signatures specific to different condensate types (e.g., nucleolus, stress granule, P-body). By training a multi-label classifier on these representations using proteins with known subcellular locations from high-quality, condensate-specific proteomic datasets, we can accurately predict a protein's 'condensate fingerprint'.
*   **Validation 1.2:** We will use proteins with experimentally verified, exclusive localization to specific condensates as a gold-standard test set. We will assess classification accuracy and use model interpretability techniques (e.g., SHAP) to identify the key sequence, structural, and network features that distinguish different condensate families.

**Aim 2: Elucidate how cellular context modulates the composition and dynamics of the condensatome.**
LLPS is not a fixed property but an emergent behavior sensitive to the cellular environment. This aim seeks to model that context-dependency.
*   **Research Question 2.1:** How do variations in protein and RNA expression levels across different cell types and physiological states alter the predicted landscape of biomolecular condensates?
*   **Hypothesis 2.1:** We hypothesize that the probability of a condensate forming and its final composition are critically dependent on the stoichiometry of its components. By integrating cell-type-specific transcriptomic and proteomic data (from GTEx, Human Protein Atlas) into our GNN framework, we can create 'context-aware' predictions. We predict that condensates will be more stable in cell types where their core scaffold proteins are highly expressed, and their client composition will reflect the abundance of available binding partners.
*   **Validation 2.1:** We will generate predicted condensatomes for well-characterized cell lines (e.g., U2OS, HeLa, HEK293). These predictions will be systematically compared against published, experimentally determined condensate proteomes for these same cell lines, assessing the overlap and correlation of component enrichment.

**Aim 3: Predict the functional impact of genetic variants on condensate integrity and link disruptions to disease pathology.**
This aim leverages our predictive framework to bridge the gap between genotype and phenotype in condensate-related diseases.
*   **Research Question 3.1:** Can our model accurately quantify how disease-associated mutations alter a protein's LLPS behavior and its interactions within the condensatome?
*   **Hypothesis 3.1:** We hypothesize that pathogenic mutations found in neurodegenerative diseases (e.g., in FUS, TDP-43, Tau from ClinVar) will receive high 'disruption scores' from our model. These scores will reflect predicted changes in LLPS propensity, interaction partner affinity, or a shift towards aggregation-prone states. In contrast, benign polymorphisms from gnomAD will have minimal predicted impact.
*   **Validation 3.1:** We will perform a large-scale in silico saturation mutagenesis analysis on key condensate proteins. Our predicted disruption scores will be benchmarked against published experimental data (e.g., changes in saturation concentration, droplet fusion dynamics, or fibrillization rates) for a subset of these mutations. We will also assess the ability of our score to discriminate between known pathogenic and benign variants from clinical databases, a critical test of its translational potential.

Methods And Approach

This project will be executed by a multidisciplinary Working Group composed of three synergistic teams, ensuring that diverse expertise is leveraged to tackle this complex data synthesis challenge. The teams are: Team A (Data Curation & Integration), comprising computational biologists; Team B (Machine Learning & Model Development), led by AI specialists; and Team C (Biological Interpretation & Validation), consisting of cell biologists and biophysicists. Trainees (graduate students and postdocs) will be embedded within each team and will participate in regular cross-team meetings and annual in-person workshops to foster collaboration and provide comprehensive training, directly aligning with the NCEMS mission.

**Phase 1: Unified Data Curation and Integration (Months 1-6)**
The foundation of this project is the creation of a comprehensive, harmonized knowledge graph. Team A will be responsible for this critical first step.
*   **Data Sources:** We will aggregate data from a wide array of public repositories. 
    *   **Sequence & Function:** UniProtKB/Swiss-Prot (canonical human sequences, PTMs, functional annotations), Pfam and InterPro (protein domain definitions).
    *   **Structure:** The Protein Data Bank (PDB) for experimentally determined structures and the AlphaFold DB for high-quality, comprehensive predicted structures of the human proteome.
    *   **Interactions:** BioGRID and the Human Reference Interactome (HuRI) for curated physical protein-protein interactions (PPIs). STRING will be used for functional associations, filtered for high-confidence evidence channels.
    *   **LLPS Ground Truth:** PhaSepDB, LLPSDB, and DrLLPS will be integrated to form a gold-standard set of known phase-separating proteins, their interacting partners, and experimental conditions.
    *   **Cellular Context:** Gene- and protein-level expression data from the Genotype-Tissue Expression (GTEx) project and the Human Protein Atlas will provide tissue- and cell-type-specific context.
    *   **Genetic Variation:** ClinVar and gnomAD will be our primary sources for disease-associated and population variants, respectively.
*   **Integration Pipeline:** Data will be programmatically downloaded, parsed, and cleaned. We will implement a rigorous entity resolution protocol to map identifiers across databases. The integrated data will be loaded into a Neo4j graph database, where proteins are nodes and their diverse relationships (e.g., physical interaction, domain co-occurrence, functional association) are represented as typed edges with associated properties (e.g., confidence scores, experimental evidence).

**Phase 2: Multi-Modal Model Development and Training (Months 7-18)**
Team B, in close consultation with Team C, will develop and validate our core predictive models.
*   **Feature Engineering:** We will generate rich, multi-modal feature representations for every human protein.
    *   **Sequence Embeddings:** We will use a state-of-the-art, pre-trained protein large language model (pLM), such as ESM-2, to generate fixed-length vector embeddings from protein sequences. These embeddings capture latent evolutionary and biophysical information.
    *   **Structural Embeddings:** Using the AlphaFold structures, we will employ a geometric deep learning model (e.g., a graph convolutional network operating on the protein structure graph) to learn features describing surface charge distribution, hydrophobicity, and the spatial arrangement of potential 'sticker' residues.
    *   **Network Embeddings:** A graph neural network (GNN), such as GraphSAGE, will be applied to our integrated PPI network to generate embeddings that encode a protein's network topology and local neighborhood.
*   **Aim 1 Model (Propensity & Partitioning):** The sequence, structural, and network embeddings will be concatenated and passed through a deep neural network classifier. This model will first be trained as a binary classifier for general LLPS propensity. Subsequently, using condensate-specific proteomic data as labels, it will be adapted for multi-label classification to predict partitioning into specific condensates.
*   **Aim 2 Model (Context-Awareness):** To model cellular context, we will create tissue-specific PPI graphs by weighting the edges based on the co-expression levels of the interacting proteins in that tissue. The GNN will be re-trained on these weighted graphs to produce context-specific network embeddings, which will then be used to make context-dependent predictions of condensate stability and composition.
*   **Aim 3 Model (Mutation Impact):** We will build an in silico mutagenesis pipeline. For a given variant, the mutated protein sequence will be generated. Its 3D structure will be re-predicted using efficient tools like ColabFold. The new sequence and structure will be passed through our feature extractors and trained models. The difference between the output scores of the mutant and wild-type proteins will yield a 'disruption score,' quantifying the mutation's predicted impact on LLPS.

**Phase 3: Atlas Construction, Validation, and Dissemination (Months 19-24)**
Team C will lead the biological interpretation of model outputs and the development of the public-facing resource.
*   **The Condensatome Atlas:** We will build a user-friendly web portal with an intuitive interface. Users will be able to search for their protein of interest and view its predicted LLPS score, its predicted condensate partners, the features driving the prediction, and the predicted impact of known clinical variants. The portal will feature network visualizations of predicted condensate compositions across different cell types.
*   **Open Science Commitment:** In adherence with NCEMS principles, all software will be open-sourced on GitHub with permissive licenses. All curated datasets, trained model weights, and genome-wide predictions will be deposited in Zenodo. A comprehensive API will be developed to allow programmatic access to the atlas data, facilitating its integration into other analysis pipelines.

**Timeline and Milestones:**
*   **Month 6:** Completion of the integrated data warehouse and feature extraction pipeline.
*   **Month 12:** Version 1.0 of the multi-modal LLPS propensity and partitioning predictor is trained, benchmarked, and internally validated.
*   **Month 18:** Context-aware and mutation-impact prediction models are finalized. Beta version of the web portal is deployed for internal testing.
*   **Month 24:** Public launch of the Condensatome Atlas web portal and API. Submission of primary manuscripts describing the resource and key biological findings.

Expected Outcomes And Impact

The successful completion of this project will yield transformative outcomes and exert a significant, long-term impact on the molecular and cellular biosciences. Our work will not only generate a powerful new resource for the scientific community but will also fundamentally advance our understanding of the principles of cellular organization.

**Intellectual Merit and Contributions to the Field:**
1.  **A Predictive, Mechanistic Atlas of the Condensatome:** The primary outcome will be the 'Condensatome Atlas,' a first-of-its-kind, publicly accessible platform. Unlike existing static databases, our atlas will be predictive and dynamic. It will provide the community with system-wide predictions of LLPS propensity, condensate composition, and context-dependent modulation for the entire human proteome. This resource will empower researchers to move from observation to hypothesis, enabling them to query how their protein of interest behaves within the complex landscape of cellular compartmentalization.
2.  **Deciphering the 'Grammar' of LLPS:** By applying interpretable AI techniques to our multi-modal models, we will deconstruct the complex interplay of factors that govern condensate assembly. This will allow us to move beyond simple correlations (e.g., IDR content) to a more nuanced understanding of the emergent 'grammar' of LLPS. We expect to identify novel sequence motifs, define the specific structural contexts that confer multivalency, and uncover the network topologies that stabilize distinct condensates. This represents a major step towards a comprehensive, first-principles understanding of biological self-assembly.
3.  **A New Paradigm for Data Synthesis in Cell Biology:** This project will serve as a blueprint for applying integrative, multi-modal machine learning to other complex problems in biology. Our methodology for harmonizing sequence, structure, network, and expression data to predict an emergent cellular phenotype will be broadly applicable to areas such as signal transduction, protein complex formation, and metabolic network analysis.

**Broader Impacts and Applications:**
1.  **Accelerating Disease Research and Therapeutic Development:** The atlas will have immediate translational relevance. By providing a tool to systematically predict the impact of mutations on condensate integrity, we will help researchers prioritize variants of unknown significance and elucidate disease mechanisms. For neurodegenerative diseases like ALS and Alzheimer's, our platform can identify critical nodes in the network of pathological phase transitions, revealing novel targets for therapeutic intervention aimed at restoring condensate homeostasis or preventing aggregation.
2.  **Enabling Rational Design in Synthetic Biology:** A deep, predictive understanding of the rules of LLPS will empower the field of synthetic biology. Engineers will be able to use our atlas and models to design synthetic proteins and RNA molecules that form artificial condensates with bespoke properties. These synthetic organelles could be used to create novel bioreactors, sequester toxic metabolites, or control complex metabolic pathways within engineered cells.
3.  **Training a New Generation of Data-Savvy Biologists:** In line with the NCEMS mission, our Working Group is structured to provide exceptional training opportunities. Graduate students and postdocs will gain hands-on experience at the cutting edge of computational biology, machine learning, and data science, all while being deeply embedded in a collaborative, transdisciplinary environment. Through our open-source tools and public workshops, we will disseminate these skills to the broader community, helping to build the future data-savvy workforce.

**Dissemination and Open Science:**
Our commitment to open science is unwavering. All outcomes will be made immediately and broadly available. We plan to publish our findings in high-impact, open-access journals (e.g., Cell, Nature Methods). The Condensatome Atlas web portal and its underlying API will be freely accessible without restriction. All source code will be maintained in a public GitHub repository, and all curated data and model weights will be deposited in Zenodo with detailed documentation to ensure full reproducibility. We will actively promote the resource through presentations at major international conferences (e.g., ASCB, ISMB) and by hosting virtual tutorials and workshops.

**Long-Term Vision:**
The NCEMS support is critical to catalyze this ambitious synthesis effort, which is far beyond the scope of a single research lab. The collaborative network and computational infrastructure established by this project will create a durable hub for condensate research. We envision the atlas as a living resource, sustained long-term by the lead institution and updated with new data from the community. This foundational work will seed numerous follow-up projects, including experimental validation of novel predictions and the pursuit of large-scale center grants to further explore the therapeutic potential of targeting the condensatome.

Budget And Resources

The proposed budget is designed to support a highly collaborative, computationally intensive, three-PI Working Group for a 24-month period. The resources requested are essential for the project's success and reflect the community-scale nature of the research, which surpasses the capabilities of any single lab or standard research grant.

**1. Personnel (Approximately 65% of total budget):**
The primary investment is in dedicated personnel who will drive the project's research and development activities.
*   **Postdoctoral Fellows (3.0 FTEs):** We request support for three full-time postdoctoral fellows, one based in each collaborating PI's lab. Each postdoc will lead one of the core teams: Data Integration, ML Model Development, and Biological Interpretation/Validation. Their focused effort is critical for executing the ambitious data synthesis and modeling tasks.
*   **Graduate Students (3 x 0.5 FTEs):** To fulfill the NCEMS training mission, we request partial support (stipend and tuition) for three graduate students. These trainees will be mentored by the PIs and postdocs, contributing directly to the project's aims while receiving unparalleled cross-disciplinary training in data science and molecular biology.
*   **Principal Investigator Effort (3 x 0.5 summer months/year):** We request modest summer salary support for the three PIs to ensure they have dedicated time for project management, intensive mentoring, cross-team coordination, and manuscript preparation.

**2. Computational Resources (Approximately 15%):**
Training state-of-the-art machine learning models on proteome-scale datasets is a significant computational expense.
*   **Cloud Computing Credits / HPC Access:** We request a substantial allocation for GPU-enabled cloud computing (e.g., AWS or Google Cloud) or for purchasing dedicated GPU nodes for an institutional high-performance computing (HPC) cluster. This is essential for training protein language models and large-scale graph neural networks.
*   **Data Storage and Servers:** Funds are requested for a dedicated server to host the integrated Neo4j database and the public-facing web portal, including costs for long-term, robust data storage and backup (estimated 5-10 TB).

**3. Travel (Approximately 10%):**
Fostering genuine collaboration is paramount.
*   **Working Group Meetings:** We request funds to hold in-person meetings for the entire team (PIs, postdocs, students) twice per year. These intensive, multi-day workshops are indispensable for brainstorming, resolving technical challenges, and strengthening the collaborative fabric of the group.
*   **Conference Travel:** Support is requested for all trainees and PIs to attend one major international conference annually to present their findings, disseminate the project's outcomes, and receive feedback from the scientific community.

**4. Other Direct Costs (Approximately 5%):**
*   **Publication Fees:** Funds are allocated to cover open-access publication charges for the anticipated 3-4 high-impact manuscripts resulting from this work, ensuring adherence to open science principles.
*   **Software Licenses:** Costs for any necessary commercial software licenses (e.g., for data visualization or specialized analysis tools).

**5. Indirect Costs (F&A):**
Indirect costs are calculated based on the federally negotiated rates for each of the three participating institutions.

**Justification for NCEMS Support:**
This project is uniquely suited for the NCEMS program. The synthesis of vast, disparate public datasets to address a fundamental question of emergent biological organization requires a collaborative, multi-lab effort with diverse expertise that cannot be assembled or funded through traditional mechanisms like an NIH R01. The need for dedicated computational resources (HPC/cloud) and specialized personnel (postdocs with hybrid expertise) goes beyond the scope of a single lab. NCEMS support is the catalyst required to bring together this specific team, provide them with the necessary resources, and enable a transformative project that will produce a lasting, open resource for the entire biosciences community.",,
ai_single_gemini_03,ai,single,gemini-2.5-pro,The Metabolic Symphony of the Tumor Microenvironment: Emergence of Drug Resistance and Immunosuppression,"The tumor microenvironment (TME) is a complex ecosystem where cancer cells, immune cells, fibroblasts, and other stromal cells engage in intricate metabolic cross-talk. Emergent properties of this system, such as profound immunosuppression and therapeutic resistance, are major barriers to effective cancer treatment. This Working Group aims to deconstruct this 'metabolic symphony' by building a spatially-resolved, multi-cellular model of TME metabolism. We will synthesize publicly available single-cell and spatial transcriptomics data (from TCGA, GEO, and specialized atlases), proteomics data, and public metabolomics datasets. Our collaborative team, uniting cancer biologists, immunologists, computational systems biologists, and spatial data scientists, will develop novel computational methods to integrate these data layers. We will use this integrated data to parameterize community-scale, genome-scale metabolic models (GEMs) that simulate nutrient competition and metabolite exchange between every cell type within a spatially-defined TME. The goal is to identify critical metabolic dependencies and feedback loops that drive emergent system-level behaviors. For example, how does lactate produced by glycolytic cancer cells reprogram macrophages to an immunosuppressive M2 phenotype? How do cancer-associated fibroblasts fuel tumor growth? By simulating metabolic perturbations, we will predict novel therapeutic strategies that disrupt these symbiotic interactions, aiming to break drug resistance and reinvigorate anti-tumor immunity. This synthesis project will provide a systems-level blueprint of TME metabolism, offering a powerful new lens through which to view and treat cancer.",,"Background And Significance

The treatment of cancer has been revolutionized by immunotherapies, yet a significant fraction of patients fails to respond, largely due to the complex and immunosuppressive nature of the tumor microenvironment (TME). The TME is not merely a passive scaffold for malignant cells but a dynamic, multicellular ecosystem where cancer cells, immune cells, fibroblasts, and endothelial cells engage in a complex network of interactions. A growing body of evidence indicates that metabolism is a central organizing principle of this ecosystem. The concept of metabolic reprogramming in cancer, first described by Otto Warburg nearly a century ago, has evolved from a cancer-cell-centric view to one that encompasses the entire TME. It is now clear that the metabolic activities of all cellular constituents are deeply intertwined, creating a unique metabolic landscape that promotes tumor growth, angiogenesis, and immune evasion. Key examples of this metabolic cross-talk are well-documented. Cancer cells, through aerobic glycolysis, consume vast amounts of glucose and secrete lactate, leading to a nutrient-poor, acidic milieu that cripples the function of effector T cells, which also rely on glucose for their anti-tumor activity. This lactate is not merely a waste product; it acts as a signaling molecule that polarizes macrophages towards an immunosuppressive, pro-tumorigenic M2 phenotype. Similarly, the enzyme indoleamine 2,3-dioxygenase (IDO), often expressed by cancer or stromal cells, depletes local tryptophan, an amino acid essential for T cell proliferation, thereby inducing immune tolerance. Beyond competition, metabolic symbiosis is also a critical feature. The 'reverse Warburg effect' posits that cancer-associated fibroblasts (CAFs) undergo glycolysis and export lactate and other nutrients, which are then taken up and utilized by adjacent cancer cells for oxidative phosphorylation, effectively 'feeding' the tumor. These individual examples, while insightful, represent only single threads in a much larger, more intricate tapestry. The fundamental limitation of current research is its reductionist approach, which typically focuses on a single pathway or the interaction between two cell types. This fails to capture the emergent properties of the TME—system-level behaviors like robust immunosuppression and therapeutic resistance that arise from the collective, nonlinear interactions of all components. The advent of high-throughput omics technologies, particularly single-cell and spatial transcriptomics, has provided unprecedented, high-resolution snapshots of the TME's cellular composition and spatial organization. We can now identify dozens of cell subtypes and map their locations relative to one another. However, these data are largely descriptive. A critical gap exists in our ability to translate these static maps into a dynamic, mechanistic understanding of the metabolic fluxes and interactions that govern TME function. We lack integrated, predictive models that can simulate how the spatial arrangement of cells dictates local metabolic niches and how perturbations to one component ripple through the entire system. This proposal is both important and timely because it directly addresses this gap. The explosion of publicly available, multi-modal TME datasets from consortia like TCGA, CPTAC, and the Human Tumor Atlas Network provides a rich, untapped resource for data synthesis. Concurrently, computational systems biology approaches, such as genome-scale metabolic modeling, have matured to a point where they can be adapted to tackle multi-cellular systems. By convening a multidisciplinary Working Group of cancer immunologists, computational biologists, and spatial data scientists, we are uniquely positioned to synthesize these disparate data streams into the first spatially-resolved, multi-cellular metabolic model of the TME. This project will move the field beyond correlative observations to a predictive, systems-level understanding of TME metabolism, uncovering novel vulnerabilities and paving the way for next-generation metabolic therapies to overcome immunotherapy resistance.

Research Questions And Hypotheses

The overarching goal of this Working Group is to develop and apply a novel computational framework that synthesizes public multi-omics data to build a predictive, spatially-resolved model of tumor microenvironment (TME) metabolism. This model will serve as an in silico laboratory to dissect the emergent properties of the TME, specifically immunosuppression and therapeutic resistance, and to identify novel metabolic targets to reverse these states. To achieve this goal, we will address three central research questions, each associated with a specific, testable hypothesis. 

**Research Question 1: How does the spatial organization of distinct cell types within the TME orchestrate local metabolic niches and give rise to emergent, system-level immunosuppressive functions?**
The arrangement of cells is not random; it creates micro-domains with unique metabolic properties. A glycolytic tumor cell cluster will create a different metabolic environment than a region rich in oxidative CAFs. We hypothesize that the functional state of immune cells is critically dependent on their spatial 'metabolic zip code'.
*   **Hypothesis 1:** The emergence of functionally distinct, immunosuppressive TME domains is a spatially-determined phenomenon driven by localized gradients of key immunomodulatory metabolites (e.g., lactate, kynurenine, adenosine, protons). We predict that our spatially-resolved model will reveal specific, recurring spatial motifs, such as the co-localization of lactate-secreting cancer cells with M2-polarized, arginase-expressing macrophages. We will test this by simulating metabolite concentrations across the spatial map and correlating these predictions with the observed functional states of immune cells inferred from transcriptomic data. We further predict that in silico disruption of these spatial motifs (e.g., by moving T-cells away from lactate 'hotspots') will revert their simulated metabolic and functional state towards an anti-tumor phenotype.

**Research Question 2: What are the critical metabolic dependencies and symbiotic cross-feeding interactions between cancer cells and the diverse stromal and immune cell populations that are essential for sustained tumor growth and immune evasion?**
Tumors are complex ecosystems that thrive on metabolic cooperation. Identifying the keystone interactions that support the entire system is crucial for developing effective therapies. We aim to move beyond known interactions to create a comprehensive, unbiased map of these metabolic exchanges.
*   **Hypothesis 2:** Cancer-associated fibroblasts (CAFs) and immunosuppressive myeloid cells function as metabolic 'hubs,' reprogramming their own metabolism to supply limiting nutrients and anabolic precursors to cancer cells, thereby sustaining proliferation under nutrient stress. We hypothesize that our multi-cellular model will identify specific, high-flux metabolic exchange pathways (e.g., transfer of specific amino acids, lipids, or TCA cycle intermediates) from CAFs or M2 macrophages to cancer cells. We will test this by performing in silico 'knockout' experiments. We predict that blocking the efflux of a key metabolite from CAFs in our model will have a more profound inhibitory effect on cancer cell biomass production than blocking the synthesis of that same metabolite within the cancer cell itself, thus revealing a critical, non-cell-autonomous dependency.

**Research Question 3: Can we computationally identify and validate novel metabolic checkpoints that, when targeted, synergize with existing immunotherapies to overcome resistance by remodeling the TME?**
The ultimate goal is to translate our systems-level understanding into therapeutic strategies. The metabolic state of the TME is a key determinant of response to immune checkpoint inhibitors (ICIs).
*   **Hypothesis 3:** The metabolic signature of the TME is a robust predictor of response to immunotherapy, and targeted metabolic interventions can sensitize non-responsive tumors to ICIs. We will parameterize our models using public datasets from patients treated with ICIs, separating them into responder and non-responder cohorts. We predict the non-responder models will exhibit distinct metabolic features, such as higher lactate production and greater nutrient competition between tumor and T-cells. We will test this by simulating the effects of various metabolic inhibitors in our 'non-responder' models. We predict that in silico inhibition of specific enzymes (e.g., lactate dehydrogenase A in cancer cells, arginase 1 in myeloid cells, or fatty acid oxidation pathways in regulatory T-cells) will not only directly impact tumor cells but will also remodel the simulated TME to be more permissive to T-cell function (e.g., increased glucose/glutamine availability for T-cells), thereby creating a synergistic anti-tumor effect when combined with simulated anti-PD-1 therapy. The deliverable will be a ranked list of metabolic targets predicted to have the highest synergistic potential.

Methods And Approach

This project is founded on the synthesis of publicly available data and the collaborative expertise of our multidisciplinary Working Group, which unites computational systems biologists, cancer immunologists, and spatial data scientists. Our approach is organized into three sequential but interconnected aims, designed to build a robust, predictive model of TME metabolism from the ground up. The entire workflow will be developed as an open-source, reproducible pipeline.

**Data Sources and Curation:**
We will exclusively use publicly available data, obviating the need for new data generation. Our primary data sources include:
1.  **Single-Cell RNA-seq (scRNA-seq):** Datasets from repositories like the Gene Expression Omnibus (GEO), the Human Tumor Atlas Network (HTAN), and The Cancer Genome Atlas (TCGA). We will focus on major cancer types with rich data availability, such as melanoma, non-small cell lung cancer, and breast cancer, ensuring we have data from both primary tumors and metastases.
2.  **Spatial Transcriptomics (ST):** We will leverage publicly available datasets generated using platforms like 10x Genomics Visium, which provide gene expression data overlaid on a tissue histology image. These datasets are critical for providing the spatial scaffold for our models.
3.  **Proteomics:** Data from the Clinical Proteomic Tumor Analysis Consortium (CPTAC) will be used to constrain our metabolic models, as enzyme abundance is often more directly related to metabolic flux than mRNA levels.
4.  **Metabolomics:** Publicly available mass spectrometry imaging (MSI) and bulk metabolomics data from resources like the Metabolomics Workbench will be used not for model construction, but for validation of our model's predictions of spatial metabolite distributions.
5.  **Knowledge Bases:** Our models will be built upon the foundation of human metabolic reconstructions like Recon3D and HMR2.0, with pathway information from KEGG and Reactome.

**Aim 1: Data Harmonization and Multi-Modal Integration (Months 1-9)**
The first major challenge is to process and integrate these diverse data types. We will develop a standardized computational pipeline to:
*   Process raw scRNA-seq data using tools like Seurat or Scanpy for quality control, normalization, and cell type annotation based on canonical marker genes. This will yield cell-type-specific expression profiles for all major TME constituents (e.g., malignant cells, T-cell subtypes, macrophages, CAFs, endothelial cells).
*   Process ST data using tools like Squidpy and Giotto. This involves aligning ST spots to the histology image and performing cell-type deconvolution, an analytical process that estimates the proportion of each cell type within each spatial spot, by integrating the scRNA-seq data.
*   Integrate proteomics data by mapping proteins to their corresponding genes and using these abundance levels to further refine expression estimates.
The output of Aim 1 will be a unified data object for each tumor sample, containing cell type definitions, their spatial locations, and their associated transcriptomic and proteomic profiles.

**Aim 2: Construction of Spatially-Resolved, Multi-Cellular Genome-Scale Metabolic Models (GEMs) (Months 7-24)**
This aim forms the core of our proposal. We will build our TME model in three steps:
1.  **Cell-Type-Specific GEMs:** For each cell type identified in Aim 1, we will generate a context-specific metabolic model. Using its unique expression profile, we will 'prune' a generic human GEM (e.g., Recon3D) to retain only the reactions and pathways active in that cell type, using established algorithms like iMAT or GIMME. This results in a unique metabolic network for each cell type.
2.  **Spatial Scaffolding:** We will use the processed ST data to create a 2D grid that represents the tissue slice. Each location (spot) on this grid will be populated with the specific GEMs corresponding to the cell types found there, weighted by their predicted abundance from the deconvolution step.
3.  **Community Simulation:** We will implement a community metabolic modeling framework, such as COMETS (Computation of Microbial Ecosystems in Time and Space), adapted for a mammalian system. This framework simulates the metabolic activity of all cells simultaneously. All cells in the grid share a common extracellular environment, allowing them to compete for nutrients (e.g., glucose, oxygen, amino acids) and exchange metabolites (e.g., lactate, acetate). We will use Flux Balance Analysis (FBA) to predict metabolic fluxes for each cell, defining biologically relevant objective functions (e.g., maximizing biomass for cancer cells, maximizing cytokine production for T-cells). This integrated model will allow us to simulate the emergent metabolic state of the entire TME.

**Aim 3: In Silico Simulation, Hypothesis Testing, and Target Identification (Months 19-36)**
With the model built, we will use it as a virtual laboratory:
*   **Model Validation:** We will first validate the model by comparing its predictions (e.g., spatial distribution of lactate) against withheld metabolomics data and established experimental findings from the literature.
*   **Hypothesis Testing:** We will systematically address our research questions. To test Hypothesis 1, we will analyze the predicted spatial metabolite gradients and their correlation with immune cell states. For Hypothesis 2, we will perform in silico gene knockouts for hundreds of metabolic enzymes and transporters in each cell type to map the network of metabolic dependencies. For Hypothesis 3, we will build models of immunotherapy responders and non-responders and simulate the systemic effects of combining metabolic inhibitors with anti-PD-1 therapy, quantifying synergy.

**Timeline and Milestones:**
*   **Year 1:** Completion of the data integration pipeline (M9); generation of first-generation, non-spatial multi-cellular models (M12).
*   **Year 2:** First fully spatially-resolved TME model for melanoma completed and validated (M18); comprehensive dependency mapping and initial therapeutic target list generated (M24).
*   **Year 3:** Models for lung and breast cancer completed (M30); synergistic combination therapies predicted and prioritized (M32); public release of open-source software, models, and web portal (M36).

Expected Outcomes And Impact

This project, through its large-scale synthesis of public data, is poised to deliver transformative outcomes and have a significant impact on both basic cancer biology and translational oncology. Our contributions will be methodological, conceptual, and therapeutic.

**Expected Outcomes:**
1.  **A Novel, Open-Source Computational Platform:** A primary deliverable will be our complete, documented, and containerized (e.g., Docker) computational pipeline for integrating spatial and single-cell multi-omics data to build spatially-resolved metabolic models. This platform will be a powerful resource for the broader research community, adaptable to other cancer types and even other complex tissue environments like those in fibrosis or neurodegeneration.
2.  **A Spatially-Resolved Metabolic Atlas of the TME:** We will generate the first comprehensive, dynamic maps of metabolic activity across different tumor types. These atlases will visualize nutrient consumption, metabolite exchange, and pathway activity with cellular and spatial resolution. This will be made accessible through an interactive web portal, allowing researchers to explore metabolic heterogeneity in the TME without requiring computational expertise.
3.  **New Mechanistic Insights into TME Biology:** Our systems-level approach will uncover emergent properties of TME metabolism that are invisible to reductionist methods. We expect to identify novel metabolic symbioses, unappreciated nutrient dependencies, and the precise mechanisms by which spatial organization dictates immune function. For example, we may discover that a specific metabolic exchange between endothelial cells and regulatory T-cells is a key, previously unknown driver of immunosuppression.
4.  **A Prioritized List of Novel Therapeutic Targets:** The ultimate translational outcome will be a high-confidence, computationally validated list of metabolic enzymes and transporters predicted to be critical nodes in the TME network. Crucially, these will not just be targets in cancer cells, but also in stromal and immune cells. We will also provide predictions for which of these targets are most likely to synergize with existing immunotherapies, providing a strong rationale for future preclinical and clinical investigation.

**Broader Impact and Applications:**
*   **Advancing Cancer Research:** This project will shift the paradigm of cancer metabolism research from a cell-centric to an ecosystem-level perspective. Our findings and tools will enable countless new research directions for cancer biologists and immunologists, providing a framework to interpret their own experimental data in a richer, systemic context.
*   **Accelerating Therapeutic Development:** The in silico platform can serve as a screening tool to rapidly test and prioritize novel metabolic drug targets and combination strategies, reducing the time and cost associated with preclinical research. This can help de-risk the development of new cancer therapies for both academic labs and pharmaceutical partners.
*   **Training the Next Generation of Scientists:** This project is intrinsically multidisciplinary and collaborative, providing an ideal training environment. Graduate students and postdoctoral fellows will gain unique, highly sought-after skills at the intersection of big data analysis, computational modeling, and cancer biology. In line with the research call's mission, we will actively train a new generation of data-savvy scientists through hands-on research, workshops, and the development of open-source educational materials.
*   **Commitment to Open and Reproducible Science:** By adhering strictly to open science principles—making all code, data, and models publicly available—we will ensure our work is transparent, reproducible, and a lasting resource for the entire scientific community. This fosters a culture of collaboration and accelerates the pace of discovery.

**Dissemination and Long-Term Vision:**
Our dissemination strategy includes high-impact publications, presentations at major international conferences (AACR, SITC, ISMB), and annual workshops to train users on our platform. Our long-term vision is to establish this Working Group as a durable hub for TME systems biology. The developed framework will be continuously expanded to incorporate new data types (e.g., epigenomics, proteomics) and new cancer types. The collaborative network forged by this project will catalyze future research, ensuring that the impact of this NCEMS-supported initiative extends far beyond the initial funding period, creating a self-sustaining community focused on solving complex biological problems through data synthesis.

Budget And Resources

The proposed research represents a community-scale synthesis project that is beyond the scope and capabilities of a single research laboratory or a typical multi-PI R01 grant. The project's success hinges on the deep integration of diverse expertise—cancer immunology, computational systems biology, and spatial data science—and the dedicated effort required to synthesize vast, heterogeneous public datasets. This aligns perfectly with the mission of the NCEMS to catalyze multidisciplinary teams to address fundamental questions through data synthesis. Traditional funding mechanisms prioritize new data generation, whereas our project exclusively leverages existing data, requiring significant person-hours for data curation, methods development, and large-scale computational analysis. NCEMS support is therefore essential to provide the protected time, collaborative infrastructure, and specialized personnel needed for a project of this magnitude.

**Budget Justification (Total Request over 3 Years):**

**1. Personnel (~75% of total budget):** This is the largest and most critical component of the budget, reflecting the project's focus on analysis and integration rather than experimental costs.
*   **Principal Investigators (3):** 1.0 month of summer salary per year for each PI. This provides protected time for project leadership, intellectual direction, data interpretation, and manuscript preparation.
*   **Postdoctoral Fellows (2):** We request support for two full-time postdocs for three years. Postdoc 1 will specialize in genome-scale metabolic modeling and simulation. Postdoc 2 will focus on the upstream analysis of single-cell and spatial omics data and the biological interpretation of model outputs from an immunological perspective. Their salaries are budgeted with full fringe benefits.
*   **Graduate Students (2):** Support for two graduate students for three years, including stipend, tuition, and health insurance. The students will work collaboratively across the PIs' labs, receiving unique cross-disciplinary training.
*   **Data Scientist/Software Engineer (0.5 FTE):** Support for a part-time professional staff member dedicated to building and maintaining the public-facing web portal, ensuring the project's deliverables are robust, user-friendly, and sustainable.

**2. Equipment (~5%):**
*   **High-Performance Computing:** We request funds to purchase a dedicated high-performance computing server with multiple CPUs and high-memory GPUs. This is essential for the computationally intensive tasks of model construction, parameterization, and running thousands of in silico perturbation simulations.

**3. Travel (~5%):**
*   **Working Group Meetings:** Funds to allow the entire team (PIs, postdocs, students) to meet in person twice per year for intensive workshops and strategic planning sessions.
*   **Scientific Dissemination:** Funds for each trainee and PI to attend and present at one major international conference per year (e.g., AACR, SITC, ISMB) to share our findings with the community.

**4. Other Direct Costs (~5%):**
*   **Publication Fees:** To cover open-access fees for an anticipated 4-5 peer-reviewed publications in high-impact journals.
*   **Software and Cloud Computing:** Costs for specialized software licenses (e.g., Gurobi optimization solver, MATLAB) and cloud computing credits (AWS/Google Cloud) for scalable data storage and on-demand computational bursts.
*   **Workshop Organization:** Modest funds to support the organization of an annual training workshop for the broader community.

**5. Indirect Costs (F&A):** Calculated based on the federally negotiated rates for each participating institution.

This budget is designed to support a highly collaborative and productive team, ensuring we can achieve our ambitious goals and deliver high-impact, open-source resources to the scientific community.",,
ai_single_gemini_04,ai,single,gemini-2.5-pro,Allostery Across the Proteome: Uncovering the Hidden Communication Network of Cellular Regulation,"Allostery, the process by which binding at one site on a protein affects a distant functional site, is a fundamental emergent property that enables complex biological regulation. While well-studied in individual proteins, a proteome-wide understanding of allosteric communication pathways is lacking. This Working Group will create a comprehensive, predictive map of allostery across the human proteome. We will synthesize a massive corpus of public data, including static protein structures (PDB), structural ensembles (NMR, cryo-EM), and predicted structures (AlphaFold DB); molecular dynamics simulation trajectories from repositories like MDTraj and MDsrv; and large-scale sequence data with evolutionary couplings (Pfam, EVcouplings). We will also integrate data on disease- and population-level mutations (ClinVar, gnomAD) to identify variants that likely function by disrupting allosteric regulation. Our team, composed of structural biologists, computational chemists, bioinformaticians, and machine learning experts, will develop a novel framework based on graph theory and geometric deep learning. This framework will model proteins as dynamic networks, identifying conserved pathways of communication ('allosteric wires') that are invisible to static structural analysis. The ultimate goal is to create a publicly accessible 'Allosterome Atlas' that allows researchers to query any protein and visualize its potential allosteric sites and communication pathways. This resource will revolutionize drug discovery by revealing novel druggable sites on challenging targets and provide a new framework for interpreting the functional impact of genetic variation.",,"Background And Significance

Allostery is a fundamental emergent property of biological macromolecules, enabling the regulation of protein function through ligand binding or covalent modification at sites distal to the primary functional site. This 'action at a distance' is the linchpin of cellular signaling, metabolic feedback, and genetic regulation, allowing proteins to act as sophisticated information processors. The classical models of Monod-Wyman-Changeux (MWC) and Koshland-Nemethy-Filmer (KNF) first conceptualized allostery in terms of discrete conformational state transitions. However, our modern understanding, informed by decades of biophysical research, has evolved to view proteins as dynamic conformational ensembles (Frauenfelder et al., 1991). In this paradigm, allostery arises from a perturbation of the protein's free energy landscape, where a binding event at one site shifts the equilibrium distribution of conformational states, thereby altering the activity at a distant site (Cooper & Dryden, 1984). This dynamic view is critical, as it implies that allosteric communication pathways are not necessarily encoded in a single static structure but are emergent properties of the protein's collective motions.

Despite its central importance, our knowledge of allostery remains fragmented and largely anecdotal, confined to a few well-studied protein systems like hemoglobin, lac repressor, and protein kinases. The methods developed to study these systems have provided invaluable insights but are not scalable to the proteome level. Experimental techniques such as NMR spectroscopy, hydrogen-deuterium exchange mass spectrometry (HDX-MS), and double-mutant cycles are low-throughput and resource-intensive. Computationally, several approaches have been developed to probe allosteric mechanisms. Molecular Dynamics (MD) simulations can, in principle, map the complete conformational landscape, but are computationally expensive. Network-based models, such as Protein Structure Networks (PSNs), represent proteins as graphs of interacting residues and use network theory metrics to identify communication pathways (Bahar et al., 2010). Sequence-based methods, like Statistical Coupling Analysis (SCA) and Direct Coupling Analysis (DCA), leverage the vast amount of sequence data to identify co-evolving residues, which are often functionally and allosterically linked (Lockless & Ranganathan, 1999; Morcos et al., 2011). More recently, machine learning models have shown promise in predicting allosteric sites from sequence and structural features.

A critical gap in the field is the absence of a unified, systematic framework to map allosteric communication networks across an entire proteome. Current approaches suffer from several key limitations. First, they typically rely on a single data modality—either static structure, sequence co-evolution, or limited dynamics—failing to integrate these complementary sources of information. Allostery is a multi-faceted phenomenon, and a holistic understanding requires synthesizing structural, dynamic, and evolutionary data. Second, most studies are based on single, static crystal structures, which represent only one snapshot of a dynamic ensemble and may completely obscure the pathways of communication. Third, the connection between genetic variation and allosteric disruption is profoundly under-explored. A vast number of disease-associated mutations, particularly Variants of Uncertain Significance (VUS), are located far from active sites, and it is highly probable that many of them exert their pathogenic effects by perturbing allosteric regulation. 

This research is exceptionally timely. We are at a unique confluence of data availability and methodological advancement. The AlphaFold database provides high-quality structural models for the entire human proteome, solving the structural coverage problem. Public repositories of MD simulations are growing, and genomic databases like ClinVar and gnomAD contain millions of annotated human variants. Concurrently, advances in geometric deep learning provide a powerful new toolkit for learning complex patterns from 3D structural and graph-based data. This project directly addresses the research call's mandate by proposing to synthesize these massive, publicly available datasets using a transdisciplinary team to answer a fundamental question about an emergent cellular phenomenon. By creating a proteome-wide map of allostery, we will provide a new layer of functional annotation, revolutionize our ability to target proteins therapeutically, and establish a new paradigm for interpreting the impact of genetic variation.

Research Questions And Hypotheses

The overarching goal of this Working Group is to transition the study of allostery from a case-by-case endeavor to a systematic, proteome-wide science. We aim to develop a novel computational framework to synthesize multi-modal public data and generate a predictive, comprehensive map of allosteric communication networks across the human proteome—the 'Allosterome Atlas.' This endeavor is guided by a set of specific, interconnected research questions and testable hypotheses.

**Research Question 1: Can a unified computational framework that integrates static structure, conformational dynamics, and evolutionary information systematically and accurately identify allosteric communication pathways on a proteome-wide scale?**
Currently, methods for predicting allostery are fragmented, each leveraging a different data type. We propose that a holistic approach that learns from these disparate but complementary data streams will yield a more accurate and robust model of allosteric communication than any single method alone.
*   **Hypothesis 1 (H1):** A multi-modal geometric deep learning model, trained on integrated features from experimental structures (PDB), predicted structures (AlphaFold), molecular dynamics (MD) simulations, and evolutionary couplings (EVcouplings), will significantly outperform existing single-modality methods in predicting experimentally validated allosteric sites and pathways.
*   **Testing and Validation:** We will benchmark our model's performance against established tools (e.g., AlloSite-Pro, SPACER) using curated datasets like ASBench. The primary metric will be the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) for identifying known allosteric sites. We will further validate the model by performing retrospective analyses on well-characterized allosteric drugs, predicting their binding sites and the communication pathways to the active site, and comparing these predictions with extensive experimental literature.

**Research Question 2: Do conserved architectural principles and recurring motifs ('design patterns') govern the wiring of allosteric networks across diverse protein families?**
Just as certain folds are reused throughout the proteome, it is plausible that nature has evolved common solutions for transmitting allosteric signals. Identifying these patterns would reveal fundamental principles of protein design and evolution.
*   **Hypothesis 2 (H2):** Allosteric communication pathways are not random walks but are enriched in specific structural motifs (e.g., chains of beta-strands, alpha-helical interfaces) and are composed of residues that are evolutionarily conserved and dynamically coupled. These pathways will exhibit topological similarities across unrelated proteins that share similar regulatory functions.
*   **Testing and Validation:** After computing allosteric pathways for the entire proteome, we will perform a large-scale statistical analysis to identify over-represented secondary structure elements, interface types, and residue properties within these pathways. We will use graph clustering algorithms to group proteins based on the topology of their allosteric networks and test whether these clusters correlate with functional classifications (e.g., GO terms, KEGG pathways) more strongly than simple fold-based classifications.

**Research Question 3: To what extent do disease-associated genetic variants, particularly those distal to functional sites, exert their pathogenic effects by disrupting allosteric communication?**
A significant challenge in clinical genetics is interpreting Variants of Uncertain Significance (VUS). We posit that a large fraction of these variants are 'allosteric mutations' that cause disease by subtly rewiring the internal communication of a protein.
*   **Hypothesis 3 (H3):** Pathogenic missense variants cataloged in ClinVar, especially those classified as VUS, are significantly more likely to map onto predicted allosteric pathways and disrupt network connectivity than benign population variants from gnomAD.
*   **Testing and Validation:** We will develop a quantitative 'Allosteric Disruption Score' (ADS) that measures the predicted change in pathway integrity upon in silico mutation. We will compute this score for all variants in ClinVar and gnomAD. Our hypothesis will be confirmed if the distribution of ADS for pathogenic variants is statistically significantly higher than for benign variants. We will validate this by correlating high ADS scores with known loss-of-function or gain-of-function effects for specific disease-causing mutations documented in the literature (e.g., mutations in glucokinase causing diabetes or in FGFRs causing craniosynostosis).

**Expected Outcomes and Deliverables:** The successful completion of this project will yield: (1) A novel, open-source computational framework for multi-modal allostery prediction. (2) The 'Allosterome Atlas,' a public web portal for visualizing allosteric networks. (3) A comprehensive catalog of predicted allosteric sites and pathways for the human proteome. (4) A prioritized list of VUS reclassified based on their predicted impact on allosteric regulation. (5) Several high-impact publications detailing our methodology, findings, and the utility of the Atlas.

Methods And Approach

This project is a large-scale data synthesis effort that requires a multidisciplinary team and a phased, systematic approach. Our Working Group comprises experts in structural biology, computational chemistry, bioinformatics, and machine learning, ensuring all facets of the project are handled with rigor. The project is organized into three primary Aims.

**Aim 1: Curation and Synthesis of a Multi-Modal Proteome-Scale Dataset.**
The foundation of our project is the aggregation and harmonization of diverse, publicly available datasets into a unified data structure suitable for machine learning. 
*   **Data Sources:** We will leverage a comprehensive set of databases. (1) **Structural Data:** All human protein structures from the Protein Data Bank (PDB), supplemented by the complete human proteome from the AlphaFold Database (v4). (2) **Dynamic Data:** We will collate structural ensembles from the Biological Magnetic Resonance Bank (BMRB) and cryo-EM maps from the Electron Microscopy Data Bank (EMDB). Crucially, we will mine public MD simulation repositories such as MDsrv, BioSimGrid, and the MoDEL database. For key protein families lacking dynamic data, we will perform new, standardized all-atom MD simulations (using GROMACS/AMBER with the CHARMM36m force field) on high-performance computing (HPC) resources. (3) **Evolutionary Data:** We will generate Multiple Sequence Alignments (MSAs) for each protein family in Pfam and compute evolutionary couplings using established tools like EVcouplings and GREMLIN. (4) **Genetic and Functional Data:** We will use variant data from ClinVar and gnomAD, and functional site annotations (active sites, binding sites, PTMs) from UniProt and FireDB.
*   **Data Integration Pipeline:** We will develop an automated Snakemake workflow to process these data. For each protein, the pipeline will generate a unified graph representation. Residues will be nodes, and edges will represent spatial proximity, covalent bonds, and dynamic correlations. Each node and edge will be decorated with a rich feature vector containing information from all data modalities: static structural properties (secondary structure, solvent accessibility), dynamic properties (B-factors, RMSF from MD), evolutionary properties (conservation, co-evolutionary scores), and functional annotations.

**Aim 2: Development and Validation of a Geometric Deep Learning Framework for Allostery Prediction.**
We will develop a novel machine learning model to learn the principles of allosteric communication from our integrated dataset.
*   **Model Architecture:** We will employ a Geometric Graph Neural Network (GNN), specifically an equivariant message-passing network (e.g., E(n)-GNN). This architecture is ideal as it naturally operates on 3D graph data and respects the rotational and translational symmetries inherent to protein structures. The model will take our multi-modal protein graphs as input and learn to predict an 'allosteric potential' score for each residue.
*   **Training:** The model will be trained in a supervised manner on a gold-standard set of ~500 proteins with experimentally validated allosteric sites curated from databases like ASBench and AlloReg. We will use a combination of binary cross-entropy loss for site prediction and self-supervised learning objectives to enforce biophysical realism.
*   **Pathway Identification:** Once trained, the model's learned edge weights will represent the strength of allosteric communication. We will apply graph traversal algorithms (e.g., Dijkstra's algorithm) to this learned graph to identify the optimal, highest-probability communication pathways ('allosteric wires') between any two residues, such as a predicted allosteric site and a known active site.
*   **Validation:** The framework will be rigorously validated on a held-out test set. We will compare its performance to existing methods and conduct in-depth case studies on well-understood allosteric systems (e.g., GPCRs, kinases, nuclear hormone receptors) to ensure our predicted pathways align with experimental evidence from NMR, HDX-MS, and mutational studies.

**Aim 3: Proteome-Scale Deployment, Variant Impact Analysis, and Creation of the Allosterome Atlas.**
With a validated framework, we will perform the first-ever allosteric analysis of an entire proteome.
*   **High-Throughput Analysis:** The analysis pipeline will be deployed on HPC resources to predict allosteric sites and pathways for every protein in the human proteome.
*   **Variant Impact Scoring:** We will develop an in silico mutagenesis protocol within our framework. For each missense variant from ClinVar/gnomAD, we will compute an Allosteric Disruption Score (ADS) by quantifying the difference in network properties (e.g., pathway probabilities, centrality measures) between the wild-type and mutant protein graphs. This will allow us to systematically test Hypothesis 3.
*   **The Allosterome Atlas:** The final output will be a publicly accessible web portal. Built with modern frameworks (e.g., React, D3.js, NGL viewer), the Atlas will allow users to search for any human protein, visualize its structure annotated with predicted allosteric sites and pathways, and query the predicted allosteric impact of known genetic variants.

**Timeline:**
*   **Year 1:** Complete data aggregation pipeline (M6). Develop and train initial GNN model prototype (M12).
*   **Year 2:** Finalize and validate the predictive framework (M18). Begin proteome-scale deployment (M20). Launch internal prototype of the Allosterome Atlas (M24).
*   **Year 3:** Complete proteome-wide analysis and variant scoring (M30). Populate and publicly launch the Allosterome Atlas (M32). Submit primary manuscripts for publication (M36).

Expected Outcomes And Impact

This project will generate transformative outcomes that will significantly advance the molecular and cellular biosciences, with profound impacts on basic research, therapeutic development, and personalized medicine. Our contributions will extend far beyond the specific scientific questions addressed, creating a lasting resource and a new conceptual framework for the entire research community.

**Intellectual Merit and Contributions to the Field:**
The primary outcome will be the **Allosterome Atlas**, the first-ever comprehensive map of allosteric communication networks across the human proteome. This represents a paradigm shift, moving the study of allostery from a qualitative, case-by-case analysis to a quantitative, systems-level science. This Atlas will serve as a new functional annotation layer for the human genome, providing mechanistic insights that are orthogonal to existing annotations like gene ontology or pathway databases. We will uncover the prevalence and diversity of allosteric regulation, potentially revealing that it is a far more ubiquitous mechanism than currently appreciated. Our large-scale analysis will identify conserved 'design principles' of allosteric communication—common structural motifs and network topologies that nature has evolved to transmit information within proteins. This will provide fundamental insights into protein evolution and biophysics.

Methodologically, we will deliver a novel, open-source **multi-modal deep learning framework**. This framework, which integrates structural, dynamic, and evolutionary data, will be a significant contribution to computational biology. Its success will demonstrate the power of data synthesis and will be adaptable to other challenging problems in protein science, such as predicting the effects of post-translational modifications or designing protein-protein interactions.

**Broader Impacts and Applications:**
The societal and economic impacts of this research will be substantial, particularly in medicine and biotechnology.
*   **Revolutionizing Drug Discovery:** The Allosterome Atlas will be a powerful engine for identifying novel therapeutic targets. Allosteric drugs offer significant advantages over traditional active-site inhibitors, including higher specificity and the ability to modulate, rather than simply block, protein function. Our Atlas will systematically reveal previously unknown allosteric sites on thousands of proteins, including high-value targets that have been deemed 'undruggable' due to flat, featureless active sites (e.g., transcription factors, scaffolding proteins). This will open up vast new therapeutic opportunities for cancer, neurodegenerative disorders, and metabolic diseases.
*   **Advancing Personalized Medicine:** Our framework for calculating an 'Allosteric Disruption Score' for genetic variants will have a direct clinical impact. It will provide a powerful tool for interpreting the functional consequences of Variants of Uncertain Significance (VUS) identified in patient genomes. By providing a mechanistic hypothesis—disruption of allosteric regulation—we can help reclassify VUS, improve diagnostic accuracy, and guide the development of personalized therapies.
*   **Enabling Rational Protein Engineering:** A detailed map of a protein's internal 'wiring' is invaluable for protein engineering. Researchers in synthetic biology and biotechnology can use the Atlas to rationally design mutations that fine-tune protein activity, stability, or substrate specificity for applications ranging from industrial enzyme production to the development of novel biosensors.

**Dissemination, Data Sharing, and Training:**
In line with the research call's emphasis on open science, all outcomes will be made broadly and freely available. The Allosterome Atlas will be a public, user-friendly web portal. All software will be released as open-source code on GitHub, and all generated data will be deposited in public repositories (e.g., Zenodo). We plan to publish our findings in high-impact journals (e.g., Nature, Cell) and present at major international conferences. Furthermore, this project is an ideal training vehicle. The graduate students and postdocs in the Working Group will receive unique cross-disciplinary training at the interface of biophysics, data science, and genomics, preparing them to be leaders in the future data-savvy workforce. We will host annual workshops to train the broader community on using our tools and resources, ensuring maximum impact and fostering a collaborative ecosystem around the study of the allosterome.

Budget And Resources

The proposed research represents a community-scale synthesis project that is beyond the capabilities of any single research laboratory. Its successful execution requires the deep integration of expertise from four distinct scientific domains—structural biology, computational chemistry, bioinformatics, and machine learning—and the synthesis of petabyte-scale public datasets. The computational demands for large-scale molecular dynamics simulations, deep learning model training on 3D graph data, and the analysis of the entire human proteome far exceed the resources typically available to individual PIs. Therefore, the support and collaborative framework provided by NCEMS are essential for the project's feasibility and success.

**Budget Justification (3-Year Project Total: $1,250,000)**

*   **A. Personnel ($780,000):** The intellectual core of the project will be driven by dedicated trainees. We request support for two Postdoctoral Scholars for three years ($100,000/yr each, salary + fringe). One postdoc will have expertise in molecular dynamics and computational biophysics, leading the dynamic data generation and analysis. The second postdoc will be a machine learning expert responsible for developing and implementing the geometric GNN framework. We also request support for two Graduate Student Research Assistants for three years ($60,000/yr each, stipend, tuition, fees). These students will focus on the data integration pipeline and the development of the Allosterome Atlas web portal, respectively. Finally, we request one month of summer salary per year for each of the four collaborating PIs ($15,000/PI/yr) to support their dedicated effort in project oversight, management, and trainee mentorship.

*   **B. Computational Resources ($210,000):** This project is computationally intensive. We request $70,000 per year to cover costs for high-performance computing. This includes an allocation on a national supercomputing resource (e.g., ACCESS) for large-scale simulations and model training on GPU clusters. Funds will also be used for cloud computing credits (AWS/GCP) for flexible development and prototyping, and for purchasing dedicated high-capacity data storage solutions to manage the massive integrated dataset.

*   **C. Travel ($90,000):** Collaboration is key. We budget for the full Working Group (4 PIs + 4 trainees) to meet in person twice annually ($15,000/meeting) to facilitate intensive collaboration, strategic planning, and cross-training. We also allocate funds for each trainee to travel to one major international conference per year (e.g., ISMB, BPS) to present their findings and network with the broader community.

*   **D. Publication and Dissemination ($30,000):** We request funds to cover open-access publication fees for an anticipated 4-5 major manuscripts ($5,000/publication). An additional $5,000 is budgeted for costs associated with the long-term hosting and maintenance of the Allosterome Atlas web portal.

*   **E. Indirect Costs (F&A) ($140,000):** This is an estimated amount based on a blended rate across the collaborating institutions and is subject to negotiation based on the final direct cost base. This support is critical for the institutional infrastructure that enables this research.",,
ai_single_gemini_05,ai,single,gemini-2.5-pro,Emergence of Robustness and Plasticity in Cellular Signaling Networks,"Cellular signaling networks face a fundamental design challenge: they must be robust enough to buffer against noise and perturbations, yet plastic enough to adapt to new environmental cues. How these dual, seemingly contradictory, emergent properties arise from the underlying molecular interactions is a central puzzle in systems biology. This Working Group will address this question by synthesizing diverse, large-scale public datasets to uncover the general design principles of robust and plastic networks. We will integrate phosphoproteomic and transcriptomic time-series data following various perturbations (e.g., from LINCS, GEO), comprehensive protein-protein interaction maps (BioGRID, IntAct), and kinetic parameters curated from the literature (SABIO-RK). Our multidisciplinary team of systems biologists, mathematicians specializing in dynamical systems, control theory engineers, and computational scientists will develop and apply novel analytical strategies. We will use network topology analysis, information theory, and machine learning-based parameter inference to build predictive dynamical models of key signaling pathways (e.g., MAPK, NF-κB). By comparing network architectures and dynamics across different pathways and organisms, we will identify recurring motifs, feedback/feedforward loop structures, and parameter regimes that confer robustness versus plasticity. The project will deliver a set of generalizable principles that explain how cells achieve this critical balance, with profound implications for understanding diseases where signaling is dysregulated (e.g., cancer, autoimmune disorders) and for the rational design of synthetic biological circuits.",,"Background And Significance

Cellular life depends on the faithful transmission and processing of information. Signaling networks are the intricate communication systems that allow cells to sense their environment and internal state, and to execute appropriate responses, such as proliferation, differentiation, or apoptosis. A central challenge in the design of these networks is the need to balance two opposing emergent properties: robustness and plasticity. Robustness is the ability to maintain stable function and produce a reliable output despite perturbations, such as thermal noise, stochastic fluctuations in component concentrations, or genetic mutations. This property ensures the fidelity of critical cellular processes. Conversely, plasticity, or adaptability, is the capacity to alter signaling responses in the face of new or persistent environmental cues, enabling cells to learn from experience and adapt to changing conditions. How cells achieve this delicate and seemingly paradoxical balance is a fundamental, unanswered question in molecular and cellular biology.

Early work in systems biology identified key network motifs that contribute to specific dynamic behaviors. Negative feedback loops (NFLs) were shown to promote homeostasis and accelerate response times, key features of robust systems. Positive feedback loops (PFLs) were found to generate bistability and ultrasensitivity, enabling switch-like, decisive cell fate decisions. Incoherent feedforward loops (FFLs) can act as pulse generators or fold-change detectors, buffering against fluctuations in input signal amplitude. These foundational studies, pioneered by researchers like Uri Alon and others, provided a 'parts list' of network motifs, but a comprehensive understanding of how these parts are assembled into systems that are simultaneously robust and plastic remains elusive. Seminal studies have provided deep insights into individual pathways. For instance, the work of Barkai and Leibler on yeast chemotaxis demonstrated perfect adaptation, a powerful form of robustness, achieved through integral feedback control. Similarly, the oscillatory dynamics of the NF-κB transcription factor, elucidated by Hoffmann and colleagues, revealed how temporal coding can regulate gene expression, a mechanism that blends robust signal transmission with plastic, context-dependent interpretation.

Despite these advances, the field faces significant limitations that this Working Group is uniquely positioned to address. First, most research has been siloed, focusing intensely on a single pathway in a specific model organism. This has yielded deep but narrow insights, leaving a critical gap in our understanding of the generalizable principles that govern signaling network design across diverse biological contexts. Second, many analyses have relied on static protein-protein interaction maps, which ignore the highly dynamic and context-dependent nature of signaling. Third, the construction of predictive dynamical models, which are essential for understanding emergent properties, has been severely hampered by the 'curse of dimensionality' and the scarcity of well-constrained kinetic parameters. Finally, the explosion of publicly available high-throughput data—including transcriptomic and proteomic time-series—has created an unprecedented opportunity for a synthesis-based approach, yet these datasets are often heterogeneous and noisy, requiring sophisticated integration strategies beyond the scope of a single research lab.

This research is therefore both important and timely. It is important because a failure to properly balance robustness and plasticity is a hallmark of numerous human diseases. In cancer, signaling pathways become pathologically robust, locked into a proliferative state and resistant to apoptotic signals. In autoimmune diseases, immune cell signaling can become hyper-plastic, overreacting to self-antigens. A principled understanding of how this balance is achieved could unveil novel therapeutic strategies aimed at re-tuning network dynamics rather than simply inhibiting a single protein. The research is timely because we are at a confluence of data availability and methodological innovation. The maturation of public data repositories like GEO, LINCS, and BioGRID, combined with advances in machine learning for parameter inference and network science, makes it possible, for the first time, to systematically compare network architectures and dynamics across dozens of pathways and cell types. This project will synthesize these disparate resources to extract fundamental design principles, moving the field from a descriptive to a predictive science of cellular signaling.

Research Questions And Hypotheses

The overarching goal of this Working Group is to elucidate the generalizable design principles that enable cellular signaling networks to simultaneously achieve robustness and plasticity. To address this complex challenge, we have formulated three specific, interconnected research questions, each associated with a set of testable hypotheses that will guide our data synthesis and modeling efforts.

**Research Question 1 (RQ1): How do network topology and modular organization quantitatively contribute to the balance between robustness and plasticity?**
While specific motifs like feedback loops are known to influence network behavior, a systematic, cross-pathway understanding of how entire network architectures are tuned for robustness versus plasticity is lacking. We hypothesize that these two properties are encoded in distinct, quantifiable features of the network graph.
*   **Hypothesis 1a:** Robustness to intrinsic noise and parameter variation is primarily conferred by a high density of fast-acting negative feedback loops and incoherent feedforward loops. We predict that pathways known for homeostatic function (e.g., metabolic regulation) will show significant enrichment for these motifs compared to pathways involved in developmental decision-making.
*   **Hypothesis 1b:** Plasticity is enabled by a modular network structure. We hypothesize that networks are organized into a robust 'core' module responsible for signal propagation, which is peripherally regulated by 'tuning' modules. These tuning modules, often containing positive feedback loops and slower transcriptional components, can re-wire or re-parameterize the core response without compromising its fundamental integrity. We predict that the degree of modularity will correlate with the known adaptive capacity of a given pathway.
*   **Validation:** We will test these hypotheses by performing a large-scale comparative analysis of network topologies reconstructed from integrated public data. We will quantify motif enrichment and modularity scores for dozens of signaling pathways and correlate these topological metrics with functional measures of robustness (e.g., low cell-to-cell response variability from single-cell data) and plasticity (e.g., the degree of adaptive desensitization from time-series data).

**Research Question 2 (RQ2): What are the dynamic and information-theoretic principles that govern the trade-off between reliable signal transmission and adaptive capacity?**
Signaling networks are not just wires, but information channels. How they process information dynamically is key to their function. We propose that the robustness-plasticity trade-off can be rigorously framed using the language of information theory.
*   **Hypothesis 2a:** Robust signaling pathways operate as high-fidelity communication channels optimized to maximize mutual information between a specific input and its downstream output. Plasticity, in this framework, represents a mechanism for dynamically reallocating the channel's bandwidth, for instance, by changing its sensitivity or dynamic range in response to a secondary, contextual cue.
*   **Hypothesis 2b:** The temporal coding of signals is a key mechanism for separating robust and plastic responses. We hypothesize that robust, essential information is encoded in simple, easily decodable features (e.g., signal amplitude), while plastic, adaptive information is encoded in more complex dynamic features (e.g., frequency, pulse duration), as exemplified by the MAPK and NF-κB systems, respectively.
*   **Validation:** We will apply information-theoretic tools to the curated time-series datasets to quantify the channel capacity of different pathways. We will build dynamical models to simulate how network modifications affect information flow. By comparing pathways with different known functions, we will test whether their dynamic encoding strategies align with our predictions.

**Research Question 3 (RQ3): How do kinetic parameter landscapes and multi-scale feedback determine whether a network operates in a robust or plastic regime?**
The structure of a network defines its potential behaviors, but the specific kinetic parameters determine its actual function. We hypothesize that robustness and plasticity are emergent properties of the system's location in a high-dimensional parameter space.
*   **Hypothesis 3a:** Robustness arises when a system operates in a 'flat' region of its parameter landscape, where the system's output is insensitive to large variations in most individual kinetic parameters. This property, known as structural robustness, is often achieved through mechanisms like enzyme saturation and integral feedback.
*   **Hypothesis 3b:** Plasticity is enabled by the existence of nearby bifurcations in the parameter space. We predict that slow-acting feedback loops (e.g., transcriptional regulation) can push the system across these bifurcations, causing qualitative shifts in its dynamic behavior (e.g., from a stable steady state to an oscillation or a bistable switch), which underlies long-term adaptation.
*   **Validation:** Using machine learning-based inference on time-series data, we will estimate posterior distributions for the parameters of our dynamical models. We will then perform global sensitivity analysis (e.g., Sobol indices) to identify sloppy vs. stiff parameter directions, testing Hypothesis 3a. We will use bifurcation analysis to map the dynamic regimes of our models and identify the parameters that act as 'levers' for plasticity, testing Hypothesis 3b.

Methods And Approach

Our research plan is a multi-pronged, integrative strategy designed to systematically address our research questions. The project is organized into four sequential but interconnected aims, forming a comprehensive workflow from data aggregation to the generation of generalizable principles. This project will exclusively use publicly available data, in full compliance with the research call.

**Aim 1: Data Curation, Integration, and Construction of a Pan-Pathway Signaling Atlas.**
The foundation of this project is the synthesis of diverse, large-scale datasets. We will develop a reproducible computational pipeline using Nextflow to automate the retrieval, processing, and normalization of data from multiple public repositories.
*   **Data Sources:** We will target three main classes of data. (1) **Dynamic Response Data:** Time-series transcriptomic (from GEO and ArrayExpress) and phosphoproteomic (from PRIDE, CPTAC, and the LINCS L1000/P100 projects) data capturing cellular responses to a wide range of perturbations (e.g., growth factors, cytokines, small molecule inhibitors) in well-characterized human cell lines (e.g., MCF7, HeLa, A549). (2) **Network Scaffolds:** Comprehensive protein-protein interaction data from BioGRID and IntAct, kinase-substrate interactions from PhosphoSitePlus and SIGNOR, and transcription factor-target interactions from ENCODE and TRRUST. (3) **Kinetic Parameters:** Experimentally measured enzyme kinetic parameters (Km, kcat, Ki) from SABIO-RK and BRENDA will be curated to serve as priors for our dynamic models.
*   **Integration:** Raw data will be processed into a standardized format. Interaction data will be integrated using a weighted scheme, where edge weights reflect the amount and type of evidence. This will result in a comprehensive, multi-layered 'Signaling Atlas' that will serve as the foundational data structure for all subsequent analyses. All processing steps will be containerized (e.g., using Docker) to ensure full reproducibility.

**Aim 2: Comparative Topological and Information-Theoretic Analysis.**
Using the Signaling Atlas, we will extract context-specific networks for canonical signaling pathways (e.g., MAPK, NF-κB, PI3K/Akt, Wnt, TGF-β) and perform systematic analyses to test Hypotheses 1a, 1b, 2a, and 2b.
*   **Topological Analysis:** For each pathway, we will use established algorithms implemented in Python libraries (NetworkX) and Cytoscape to quantify global network properties (e.g., degree distribution, clustering coefficient) and local features. We will employ motif-finding algorithms (e.g., mfinder) to detect the enrichment of feedback and feedforward loops. Community detection algorithms (e.g., Louvain) will be used to identify modular structures and test Hypothesis 1b.
*   **Information-Theoretic Analysis:** We will apply methods from information theory to the curated time-series data. Mutual information will be calculated between stimulus and response time-series to estimate the channel capacity of each pathway. Transfer entropy will be used to infer the direction and magnitude of information flow between network components, providing a dynamic, data-driven view of network connectivity.

**Aim 3: Development, Parameterization, and Analysis of Predictive Dynamical Models.**
To move beyond static correlations and understand mechanism, we will build and analyze mechanistic models of core signaling modules.
*   **Model Formulation:** We will use Ordinary Differential Equations (ODEs) as our primary modeling formalism, leveraging tools like PySB for programmatic model construction. This allows us to explicitly represent biochemical reactions and their kinetics. For each pathway, we will construct a series of models of increasing complexity, starting with a core topology and adding regulatory loops.
*   **Parameter Inference:** This is a critical challenge we will address with state-of-the-art methods. We will employ Bayesian inference, specifically Markov Chain Monte Carlo (MCMC) methods (e.g., via the PyMC library), to fit our ODE models to the relevant time-series data. This approach has two key advantages: it can integrate prior knowledge (from SABIO-RK), and it yields full posterior distributions for each parameter, naturally capturing our uncertainty. This probabilistic approach is essential for robustly testing our hypotheses about parameter landscapes.
*   **Model Analysis:** Once parameterized, the models will be our primary tool for in silico experimentation. We will perform global sensitivity analysis to identify stiff (plasticity-conferring) and sloppy (robustness-conferring) parameter combinations. Bifurcation analysis will be used to map the different dynamic regimes (e.g., stable, bistable, oscillatory) accessible to the network, revealing the mechanisms of plasticity. We will simulate the models under noisy conditions to quantify robustness and directly test its relationship with topological and parametric features.

**Timeline and Milestones:**
*   **Year 1:** Complete the data processing pipeline and release version 1.0 of the Signaling Atlas. Perform initial topological and information-theoretic analyses across 5-10 pathways. Publish a data descriptor paper.
*   **Year 2:** Develop and parameterize robust dynamical models for 3-4 core pathways (e.g., ERK, NF-κB). Perform initial sensitivity and bifurcation analyses. Hold the first in-person Working Group meeting to synthesize results. Present preliminary findings at a major conference.
*   **Year 3:** Expand modeling to a larger set of pathways. Conduct the final cross-pathway comparative analysis to synthesize general principles. Prepare and submit manuscripts to high-impact journals. Release all models, code, and analysis workflows as a comprehensive, open-source package. Hold the final Working Group meeting.

Expected Outcomes And Impact

This project is designed to produce a series of high-impact outcomes that will significantly advance the field of molecular and cellular biology, while also providing valuable resources and training opportunities for the broader scientific community.

**Intellectual Merit and Contributions to the Field:**
The primary intellectual contribution will be the elucidation of a set of generalizable design principles that explain how cellular signaling networks resolve the fundamental trade-off between robustness and plasticity. This will represent a significant step towards a predictive understanding of cellular regulation. By moving beyond the study of single pathways, our comparative, synthesis-based approach will uncover common strategies that cells use to manage information, providing a unifying framework for systems biology. We expect to deliver:
1.  **A Quantitative Catalog of Design Principles:** A set of rules linking specific network topologies (e.g., motif combinations, modularity), dynamic strategies (e.g., temporal coding), and parameter landscapes (e.g., sloppiness, proximity to bifurcations) to the emergent properties of robustness and plasticity.
2.  **A Novel, Integrative Analytical Framework:** A fully documented, open-source computational pipeline for integrating multi-modal 'omics data to construct, parameterize, and analyze dynamical models of signaling networks. This will lower the barrier for other researchers to conduct similar synthesis projects.
3.  **A Rich Community Resource:** The curated Pan-Pathway Signaling Atlas and the library of validated, parameterized models will be made publicly available through a user-friendly web portal. This will serve as a foundational resource for hypothesis generation and in silico experimentation by the wider cell biology community.

**Broader Impacts and Applications:**
The implications of this research extend far beyond basic science. Understanding the principles of robust and plastic design has profound relevance for human health and biotechnology.
*   **Translational Medicine:** The dysregulation of signaling robustness and plasticity is a root cause of many diseases. In cancer, pathways become rigidly robust to death signals; in autoimmune disorders, they are excessively plastic and hyper-responsive. Our findings will provide a 'network-level' perspective on disease, identifying novel therapeutic targets and strategies. For example, instead of simply inhibiting a kinase, one might design drugs that push a network away from a pathologically robust state, re-sensitizing it to other treatments. This aligns with the growing field of network medicine.
*   **Synthetic Biology:** The rational design of synthetic biological circuits is often hampered by their fragility and lack of adaptability. Our work will provide a 'design manual' for engineers to build more sophisticated and reliable circuits. By incorporating the principles of robustness and plasticity we uncover, synthetic biologists can create cells with novel functions, such as smart therapeutics that can adapt their behavior to the state of a patient's disease or robust biosensors for environmental monitoring.
*   **Training and Education:** This project is an ideal training vehicle for the next generation of data-savvy scientists. Trainees involved will gain a unique, interdisciplinary skillset spanning data science, bioinformatics, mathematical modeling, and systems biology. The collaborative Working Group structure will foster team science and provide mentorship opportunities for graduate students and postdocs from PIs with diverse expertise.

**Dissemination and Open Science:**
We are deeply committed to the principles of open and reproducible science. All products of this research will be made rapidly and broadly available.
*   **Publications:** We will publish our findings in high-impact, peer-reviewed journals (e.g., Cell Systems, Nature Communications). We will budget for open-access fees to ensure maximum accessibility.
*   **Public Repositories:** All code will be developed in a version-controlled GitHub repository with a permissive open-source license. All curated data and models will be deposited in community-standard repositories like Zenodo, GEO, and the BioModels Database, with rich metadata to ensure they are FAIR (Findable, Accessible, Interoperable, and Reusable).
*   **Community Engagement:** We will present our work at national and international conferences (e.g., ISMB, Q-Bio) and will organize workshops and tutorials to train the community on how to use our software and data resources. This will ensure the long-term impact and sustainability of our work.

Budget And Resources

The proposed research represents a large-scale synthesis effort that is beyond the capabilities of any single research laboratory. It requires a unique convergence of expertise in systems biology, dynamical systems theory, control engineering, and data science, as well as significant dedicated personnel time and computational resources for data integration and modeling. The NCEMS Working Group mechanism is therefore the ideal framework to support this project, as it is specifically designed to catalyze the kind of deep, multidisciplinary collaboration required for success. The requested budget reflects the personnel- and computation-intensive nature of this synthesis project.

**Budget Justification (Total Request over 3 Years):**

*   **A. Personnel ($390,000):** The bulk of the budget is allocated to personnel who will perform the day-to-day research.
    *   **Postdoctoral Scholars (2 FTE for 3 years):** We request support for two postdoctoral fellows who will be the primary drivers of the project. One will specialize in bioinformatics and data integration, managing the data pipeline and topological analyses. The second will specialize in computational modeling and dynamical systems, leading the development and analysis of ODE models. (Approx. $65,000/year salary + benefits per scholar).
    *   **Graduate Students (Partial support for 2 students):** We request stipend support for two graduate students. Their involvement is critical for training the next generation of scientists and for exploring specific sub-projects in depth. (Approx. $20,000/year per student).

*   **B. Travel ($45,000):** Collaboration and dissemination are key to the project's success.
    *   **Working Group Meetings ($25,000):** Funds to support two in-person, multi-day meetings for the entire team (4 PIs, 2 postdocs, 2 students). These intensive workshops are essential for integrating the different project arms, resolving challenges, and strategic planning.
    *   **Conference Travel ($20,000):** Support for trainees and PIs to present findings at one major international conference per year (e.g., ISMB, ICSB), facilitating dissemination of results and fostering new collaborations.

*   **C. Computational Resources ($30,000):**
    *   **Cloud Computing ($20,000):** Credits for a commercial cloud provider (e.g., AWS or Google Cloud) are required for storing the large integrated datasets and for performing computationally demanding tasks, particularly the Bayesian parameter inference (MCMC), which can require thousands of CPU hours per model.
    *   **Data Storage and Servers ($10,000):** Funds for long-term data archiving and hosting of the public web portal for our Signaling Atlas and model repository.

*   **D. Publication Costs ($15,000):**
    *   Funds to cover open-access publication fees for an anticipated 3-4 major manuscripts in high-quality journals, ensuring our findings are freely accessible to all.

*   **E. Indirect Costs (IDC):** Calculated based on the federally negotiated rate for the lead institution and applied to the modified total direct costs.

**Existing Resources:** The collaborating PIs will contribute their existing laboratory space, equipment, and access to institutional high-performance computing clusters. The institutions provide significant support through library access, administrative support, and IT services. The unique value provided by NCEMS is the dedicated funding and framework to unite these distributed resources and expertise into a cohesive and highly productive collaborative unit.",,
ai_single_gemini_06,ai,single,gemini-2.5-pro,Reconstructing Eukaryogenesis: A Data-Driven Synthesis of the Great Evolutionary Transition,"The origin of the eukaryotic cell, with its nucleus, mitochondria, and complex endomembrane system, represents one of the most profound and enigmatic emergent events in the history of life. While hypotheses abound, a comprehensive, data-driven reconstruction of this transition is lacking. This Working Group will synthesize the explosion of genomic, proteomic, and structural data from across the tree of life to build a detailed, step-by-step model of eukaryogenesis. We will integrate complete genomes and proteomes from diverse eukaryotes, archaea (with a focus on the recently discovered Asgardarchaeota, our closest prokaryotic relatives), and bacteria from NCBI and UniProt. This will be combined with protein family phylogenies (OrthoDB), and a vast repository of protein structures (PDB, AlphaFold DB) to trace the origin and evolution of key eukaryotic signature proteins (ESPs) involved in membrane trafficking, cytoskeletal dynamics, and nuclear organization. Our team, a unique collaboration of evolutionary biologists, cell biologists, structural bioinformaticians, and computational phylogenomicists, will use sophisticated computational methods to reconstruct ancestral protein functions and interaction networks. We aim to pinpoint the specific molecular innovations and horizontal gene transfers that enabled the emergence of eukaryotic complexity, resolving long-standing debates about the roles of endosymbiosis and autogenous evolution. This project will produce the most detailed data-synthesized narrative of our own cellular origins, providing a foundational resource for understanding the principles of biological innovation.",,"Background And Significance

The emergence of the eukaryotic cell from prokaryotic ancestors stands as one of the most significant and complex events in evolutionary history, a true singularity that paved the way for all macroscopic life. This transition involved a radical increase in cellular complexity, including the origin of the nucleus, mitochondria, a dynamic endomembrane system, a versatile cytoskeleton, and linear chromosomes. Understanding this 'great evolutionary transition' is not merely a historical curiosity; it is fundamental to comprehending the principles that govern biological innovation and the emergence of complexity from simpler components. For decades, the field has been dominated by conceptual models, often based on limited, fragmentary evidence. The classic endosymbiotic theory, championed by Lynn Margulis, correctly identified the bacterial origin of mitochondria, but the nature of the host cell and the sequence of events that led to its complex architecture have remained fiercely debated. Broadly, hypotheses fall into two camps: autogenous models, which propose that complexity arose gradually within a single prokaryotic lineage before any major symbiotic event, and symbiogenetic models, which posit that the symbiotic merger itself was the catalyst for complexification. The 'inside-out' model, for instance, is a sophisticated autogenous theory suggesting that the nucleus and cytoplasm evolved from extracellular blebs formed by an archaeal ancestor. Conversely, symbiogenetic models like the 'hydrogen hypothesis' argue that the metabolic dependency between an archaeal host and a hydrogen-producing alphaproteobacterial symbiont drove the engulfment that initiated eukaryogenesis. A central point of contention has been the 'mitochondria-early' versus 'mitochondria-late' debate: did a complex 'proto-eukaryote' engulf the mitochondrion, or did the energy and genetic material from the endosymbiont fuel the evolution of complexity? The discovery of the Asgardarchaeota superphylum has revolutionized this field. Phylogenomic analyses by Zaremba-Niedzwiedzka et al. (2017) and others have robustly placed the eukaryotic lineage as a sister group to, or branching from within, the Asgardarchaeota. This discovery effectively ended the three-domain debate in favor of a two-domain tree of life, confirming the archaeal nature of the host cell. Crucially, Asgardarchaeal genomes contain an unprecedented number of 'eukaryotic signature proteins' (ESPs)—genes previously thought to be unique to eukaryotes. These include homologs of actin, tubulin, ESCRT proteins involved in membrane remodeling, and components of the ubiquitin system. This finding suggests the host was not a simple, passive partner but was 'primed' with a toolkit of proteins that could be co-opted for eukaryotic functions. However, this discovery has also raised new, more nuanced questions. What were these proteins doing in a prokaryotic context? How were their functions repurposed during eukaryogenesis? Did the Asgard ancestor possess a rudimentary cytoskeleton or endomembrane system? Answering these questions requires moving beyond simply cataloging genes. The key gap in our current knowledge is the lack of a holistic, integrated model that connects the evolution of genes to the evolution of cellular systems and functions. Previous studies have often focused on individual protein families or specific cellular processes in isolation. A comprehensive synthesis that integrates genomic, phylogenetic, structural, and functional data is required to reconstruct the step-by-step emergence of the eukaryotic cell. This project is exceptionally timely due to a confluence of factors. First, the torrent of publicly available genomic data from diverse microbial eukaryotes and newly discovered prokaryotic lineages provides an unprecedented dataset for comparative analysis. Second, the advent of highly accurate protein structure prediction through AlphaFold has opened the door to inferring the function of ancestral proteins for which no experimental data exists. Third, advances in computational phylogenetics and network biology provide the analytical tools necessary to synthesize these heterogeneous datasets. This project will leverage these advances to build the first data-driven, systems-level reconstruction of our own cellular origins, addressing a foundational question in molecular and cellular biosciences.

Research Questions And Hypotheses

The overarching goal of this Working Group is to synthesize publicly available data to construct a high-resolution, temporally-ordered model of the molecular events that transpired during the transition from a prokaryotic ancestor to the Last Eukaryotic Common Ancestor (LECA). We will move beyond qualitative models to a quantitative, evidence-based reconstruction of this emergent phenomenon. To achieve this, we have formulated four central research questions (RQs), each associated with specific, testable hypotheses.

**RQ1: What was the molecular toolkit of the Asgard-eukaryote common ancestor, and how did it function in a prokaryotic context?** While we know Asgardarchaeota possess many ESPs, their ancestral functions and interactions remain unknown. We aim to reconstruct the ancestral proteome and protein-protein interaction (PPI) network to understand the cellular capabilities of the eukaryotic host just before the mitochondrial endosymbiosis.
*   **Hypothesis 1 (The 'Primed Host' Hypothesis):** The Asgardarchaeal ancestor of eukaryotes already possessed a rudimentary, dynamic cytoskeleton and a basic membrane-remodeling system, which were essential prerequisites for engulfing the proto-mitochondrion. This system was likely involved in generating cell surface complexity and interacting with the environment.
    *   *Prediction:* Ancestral state reconstruction will place the origin of key membrane-remodeling proteins (e.g., ESCRTs, BAR domains) and profilin/gelsolin-regulated actin homologs *before* the massive influx of genes from the alphaproteobacterial endosymbiont. Ancestral protein structure modeling will show that these proteins had the core biophysical properties (e.g., membrane curvature sensing, filament polymerization) necessary for these functions.

**RQ2: How did the acquisition and integration of the alphaproteobacterial endosymbiont reshape the host's proteome and cellular architecture?** The endosymbiosis was not just a metabolic merger but also a massive genetic event, involving the transfer of hundreds of genes from the endosymbiont to the host genome (Endosymbiotic Gene Transfer, EGT). We will trace the impact of this event on the host's genetic and network evolution.
*   **Hypothesis 2 (The 'Symbiotic Catalyst' Hypothesis):** The endosymbiotic event was the primary trigger for the evolution of eukaryotic complexity. The bioenergetic boost from the proto-mitochondrion and the genetic disruption caused by EGT catalyzed the formation of the nucleus and the expansion of the endomembrane system.
    *   *Prediction:* Phylogenomic dating and analysis of gene family expansions will reveal a major burst of innovation and duplication in ESPs associated with the nucleus (e.g., lamins, nuclear pore components), endomembrane trafficking (e.g., Rab GTPases, SNAREs), and cell division *after* the integration of the mitochondrial ancestor. We predict that many key eukaryotic innovations will be genetic chimeras, combining archaeal information-processing machinery with bacterial-derived metabolic and membrane-associated components.

**RQ3: What was the evolutionary trajectory of key eukaryotic cellular systems, such as the nucleus and endomembrane system?** The origin of the nucleus is perhaps the greatest single puzzle in cell biology. We will investigate its emergence by tracing the origin of its constituent components and their integration into a functional whole.
*   **Hypothesis 3 (The 'Membrane Proliferation' Hypothesis for Nuclear Origin):** The nuclear envelope did not evolve primarily for genome protection but arose as a consequence of the proliferation of internal membranes that wrapped around the host's chromatin to manage the influx of mitochondrial-derived lipids and proteins, effectively sorting the cell into new compartments.
    *   *Prediction:* The core components of the nuclear pore complex and nuclear lamina will be traced back to ancestral proteins involved in membrane coating and remodeling (e.g., vesicle-coating proteins) and cytoskeletal elements present in the Asgard ancestor. Their recruitment to form the nucleus will coincide phylogenetically with the expansion of protein families involved in lipid metabolism and transport that have a clear alphaproteobacterial origin.

**RQ4: Can we resolve the 'mitochondria-early' vs. 'mitochondria-late' debate through data synthesis?** By creating a relative timeline of molecular innovations, we can determine if significant host complexity preceded or followed the endosymbiotic event.
*   **Hypothesis 4 (The 'Staggered Innovation' Hypothesis):** The evolution of eukaryotic complexity was a multi-stage process. We hypothesize that the host was 'primed' with basic cytoskeletal and membrane-remodeling abilities (supporting a 'late' engulfment), but that the vast majority of eukaryotic innovations, including the nucleus and complex vesicle trafficking, evolved *after* the endosymbiotic event (supporting a 'mitochondria-early' trigger).
    *   *Prediction:* Our integrated timeline will show that genes for basic actin regulation and ESCRT-like systems are ancient within Asgardarchaeota, while the major expansions of regulatory families like small GTPases, protein kinases, and ubiquitin ligases, which orchestrate complex eukaryotic processes, occurred on the branch leading to LECA, subsequent to the mitochondrial acquisition event. This will be our key deliverable: a data-supported, step-by-step model of eukaryogenesis.

Methods And Approach

This project is founded on the principle of data synthesis, integrating vast, publicly available datasets using a rigorous, multi-phase computational pipeline. Our approach is designed to be reproducible, transparent, and scalable, leveraging the diverse expertise of our Working Group. The project is structured around four major phases, with clear milestones and deliverables.

**Phase 1: Comprehensive Data Acquisition and Curation (Months 1-6)**
This foundational phase involves assembling a high-quality, consistent, and comprehensive dataset. This task is non-trivial and requires significant bioinformatic effort, representing a key contribution in itself.
*   **Genomic and Proteomic Data:** We will compile a curated set of approximately 500 complete genomes and their corresponding proteomes. This dataset will include: ~200 diverse eukaryotes spanning all major supergroups (from NCBI RefSeq, JGI, and the EukProt database) to robustly define the LECA proteome; ~150 archaeal genomes, with an exhaustive sampling of all known Asgardarchaeota phyla (Loki-, Thor-, Odin-, Heimdallarchaeota) to model the host ancestor; and ~150 bacterial genomes, with a deep sampling of Alphaproteobacteria (to model the endosymbiont) and other phyla relevant for detecting horizontal gene transfer (HGT).
*   **Protein Family and Structural Data:** We will define orthologous groups (OGs) across our curated proteomes using the eggNOG and OrthoDB databases and associated software (eggNOG-mapper). For each protein, we will retrieve experimental structures from the Protein Data Bank (PDB) and predicted structures from the AlphaFold Database. This structural dataset, encompassing millions of protein models, is critical for our functional inference pipeline.
*   **Data Management:** All raw and processed data will be managed in a centralized, version-controlled system using a combination of a relational database (PostgreSQL) for metadata and Git-LFS (Large File Storage) for sequence and structure files. This ensures full reproducibility and facilitates seamless collaboration across the Working Group.

**Phase 2: Phylogenomic Reconstruction and Ancestral Proteome Inference (Months 6-18)**
This phase aims to establish the evolutionary framework upon which all subsequent analyses will be built.
*   **Species Tree Construction:** We will construct a robust species tree from a concatenated alignment of ~100 universal, single-copy marker proteins. We will use state-of-the-art phylogenetic inference methods, such as IQ-TREE, employing complex mixture models (e.g., C60+LG+R) to account for site-specific evolutionary rates, which is crucial for resolving deep evolutionary relationships.
*   **Gene Family Evolution:** For each of the tens of thousands of OGs, we will build a maximum-likelihood gene tree. These gene trees will be reconciled with the species tree using tools like GeneRax, which co-infers gene trees and reconciliations. This process will allow us to systematically map events of gene duplication, loss, and HGT onto each branch of the species tree.
*   **Ancestral Proteome Reconstruction:** Using the reconciled phylogenies, we will employ probabilistic ancestral state reconstruction methods (e.g., using the Count software package) to infer the gene content of key ancestral nodes, most importantly the Asgard-eukaryote common ancestor and LECA. This will provide a complete parts list for these ancient organisms, directly addressing RQ1.

**Phase 3: Ancestral Function and Network Reconstruction (Months 12-24)**
Here, we move from gene content to cellular function and organization.
*   **Ancestral Sequence and Structure Reconstruction:** For high-priority ESP families (e.g., actins, tubulins, ESCRTs, Rab GTPases), we will reconstruct ancestral protein sequences using marginal reconstruction methods in PAML or FastML. We will then predict the 3D structures of these ancestral proteins using AlphaFold2. By comparing these ancestral structures to modern homologs using tools like DALI and TM-align, and by analyzing conserved functional sites, we will infer their ancestral biochemical functions, testing predictions from H1 and H3.
*   **Ancestral Protein-Protein Interaction (PPI) Network Inference:** Reconstructing ancestral networks is a major challenge. We will use a multi-pronged, evidence-integration approach. We will infer interactions based on: (1) co-evolution of interacting partners across the species tree (e.g., using Mirrortree); (2) conservation of domain-domain interactions from databases like 3did and iPfam; and (3) structural modeling of potential protein complexes using AlphaFold-Multimer. By projecting these inferred interactions back onto the ancestral proteomes, we will generate probabilistic PPI networks for the Asgard-eukaryote ancestor and LECA, allowing us to test H2 and H3 by tracking the emergence of new network hubs and modules.

**Phase 4: Synthesis, Visualization, and Dissemination (Months 24-36)**
*   **Integrated Model of Eukaryogenesis:** In the final year, we will integrate the results from all phases—the ancestral proteomes, the functional evolution of key proteins, and the rewiring of interaction networks—into a coherent, temporally-ordered narrative of eukaryogenesis. This synthesis will directly address RQ4 and our overarching goal.
*   **Timeline and Milestones:**
    *   **M1 (Month 6):** Curated and versioned dataset of genomes, proteomes, and structures is complete and accessible to the team.
    *   **M2 (Month 18):** Robust species tree and reconstructed ancestral proteomes for key nodes (Asgard-eukaryote ancestor, LECA) are finalized.
    *   **M3 (Month 24):** Ancestral sequence and structural analysis of 50 key ESP families is complete.
    *   **M4 (Month 30):** First draft of ancestral PPI networks is complete; integration and synthesis begins.
    *   **M5 (Month 36):** Project completion, including submission of key manuscripts and launch of the public data portal.

Expected Outcomes And Impact

This project is poised to make transformative contributions to molecular and cellular biology by providing the most detailed, data-driven account of the origin of eukaryotic life. The outcomes will extend beyond a single historical narrative, establishing a new paradigm for evolutionary systems biology and providing invaluable resources and training for the scientific community.

**Intellectual Merit and Contributions to the Field:**
1.  **A Quantitative, Step-by-Step Model of Eukaryogenesis:** The primary outcome will be a comprehensive, phylogenetically-ordered model detailing the sequence of molecular innovations—gene gains, duplications, HGTs, and network rewiring—that led to the eukaryotic cell. This will shift the field from debating abstract, competing hypotheses to refining a quantitative, evidence-based framework. We will produce a 'roadmap' of eukaryogenesis that pinpoints the likely functions of prokaryotic precursors and traces their transformation into the complex machinery of LECA.
2.  **Resolution of Foundational Controversies:** Our integrated approach is specifically designed to generate evidence that can resolve long-standing debates. By establishing a relative timeline of events, we will directly address the 'mitochondria-early vs. late' problem (testing H4). By reconstructing ancestral networks, we will quantify the relative contributions of pre-existing 'primed' systems versus symbiotic catalysts (testing H1 vs. H2), providing a nuanced answer that moves beyond a simple dichotomy.
3.  **A Foundational, Publicly-Accessible Resource:** A major deliverable will be the creation of an interactive, web-accessible database and visualization portal. This 'Eukaryogenesis Explorer' will allow any researcher to query the evolutionary history of any eukaryotic protein family in our dataset, view its reconstructed ancestral sequences and structures, and explore its placement within ancestral interaction networks. This will be an enduring resource for the cell biology, evolutionary biology, and genomics communities, enabling countless new hypotheses to be generated and explored. It will serve as a dynamic, evolving platform for understanding cellular origins.

**Broader Impacts and Applications:**
1.  **Training the Next Generation of Data-Savvy Biologists:** This project is an ideal training environment that directly aligns with the research call's objectives. Graduate students and postdoctoral fellows will work at the intersection of evolutionary biology, cell biology, and computational science. They will be co-mentored by PIs from different disciplines, participate in collaborative 'code-a-thons' and data analysis workshops, and lead different facets of the project. This cross-disciplinary, team-based training will equip them with the unique skillset required to lead future research in an increasingly data-intensive world.
2.  **Establishing a Methodological Blueprint for Synthesis Research:** Our project will pioneer and formalize a computational workflow for integrating phylogenomics with structural and network biology to reconstruct the evolution of a complex system. All our code, analysis pipelines, and workflows will be open-source and meticulously documented. This will provide a powerful blueprint for other research communities wishing to tackle different major evolutionary transitions, such as the origin of multicellularity, neurons, or photosynthesis.
3.  **Dissemination and Open Science:** We are deeply committed to open science principles. All curated datasets, analysis scripts, and results will be deposited in public repositories (e.g., GitHub, Zenodo) upon publication. We will disseminate our findings through high-impact publications (targeting journals like *Nature*, *Science*, and *eLife*), presentations at major international conferences (e.g., Society for Molecular Biology and Evolution, American Society for Cell Biology), and a final project workshop to which we will invite the broader community. This ensures our results and methods have the maximum possible impact and utility.

**Long-Term Vision:** This project will lay the foundation for a new field of 'paleo-systems biology'. The resources we create—ancestral sequences, structures, and networks—will enable a new phase of experimental validation. For example, future studies could involve resurrecting ancestral proteins in the lab to test their predicted functions or attempting to engineer simplified prokaryotic systems with reconstructed eukaryotic-like modules. The collaborative network formed by this Working Group will persist beyond the funding period, creating a lasting intellectual hub for tackling the grand challenges of evolutionary biology.

Budget And Resources

The proposed research represents a community-scale synthesis project that is beyond the capabilities of any single research laboratory. Its success hinges on the deep integration of diverse expertise—phylogenomics, cell biology, structural bioinformatics, and network science—and requires dedicated resources for personnel, collaboration, and computation that are not available through standard single-investigator grants. NCEMS support is therefore essential to catalyze this multidisciplinary team and provide the necessary infrastructure for a project of this magnitude and ambition.

**Budget Justification:**
The budget is requested for a three-year period and is designed to support the personnel and activities central to the project's success.

**1. Personnel ($390,000):** The intellectual core of the project will be driven by dedicated trainees working collaboratively across the participating labs.
*   **Postdoctoral Fellows (2 positions, 3 years):** $270,000. We request support for two postdoctoral fellows who will be the main drivers of the analytical work. Postdoc 1 will specialize in phylogenomics and ancestral sequence reconstruction. Postdoc 2 will focus on structural bioinformatics and network inference. They will be co-mentored and will spend time at different partner institutions to facilitate knowledge transfer.
*   **Graduate Students (2 students, 50% support, 3 years):** $120,000. We request partial stipend and tuition support for two graduate students. They will work on specific sub-projects, such as the detailed evolutionary history of the endomembrane system or the cytoskeleton, providing them with unparalleled cross-disciplinary training.

**2. Travel ($45,000):** Collaboration is key to this synthesis project.
*   **Annual Working Group Meetings (3 meetings):** $30,000. To facilitate intensive collaboration, data integration, and strategic planning, we will hold one 3-day in-person meeting each year for all PIs and trainees. Funds will cover travel and lodging.
*   **Conference Dissemination:** $15,000. To ensure broad dissemination of our findings, funds are allocated for trainees and PIs to present their work at one major international conference per year (e.g., SMBE, ASCB).

**3. Computational Resources ($30,000):**
*   **High-Performance Computing (HPC):** $25,000. The phylogenomic analyses, particularly gene tree-species tree reconciliation for tens of thousands of gene families and the structural modeling of thousands of ancestral proteins, are computationally intensive. These funds will purchase compute cycles on a national supercomputing resource (e.g., via XSEDE) or a commercial cloud platform (e.g., AWS).
*   **Data Storage and Archiving:** $5,000. For robust, long-term storage of the multi-terabyte dataset and ensuring its public availability after the project concludes.

**4. Publication Costs ($15,000):**
*   **Open Access Fees:** To adhere to our open science commitment, we request funds to cover article processing charges for an anticipated 4-5 high-impact, open-access publications.

**5. Indirect Costs (F&A):** To be calculated based on the negotiated rates of the lead institution.

**Total Direct Costs: $480,000**

**Existing Resources:** The PIs will contribute significant existing resources, including faculty time, access to local university HPC clusters for preliminary analyses, software licenses, and laboratory/office space for personnel. The project's primary cost is not in data generation but in the personnel time and computational power required for its synthesis and integration, making it an extremely cost-effective approach to answering a fundamental scientific question. The NCEMS framework, with its emphasis on supporting collaborative working groups, is the ideal mechanism to enable this research.",,
ai_single_gemini_07,ai,single,gemini-2.5-pro,Cracking the Cis-Regulatory Code: A Deep Learning Framework for Predicting Spatiotemporal Gene Expression from DNA Sequence,"The precise orchestration of gene expression in space and time is the emergent outcome of a complex cis-regulatory code written in our DNA. This code, composed of enhancers, promoters, and insulators, is interpreted by transcription factors to control development and cellular function. Despite decades of research, predicting gene expression patterns from DNA sequence alone remains a grand challenge. This Working Group will tackle this challenge by building a unified deep learning framework that synthesizes the wealth of public functional genomics data. We will leverage massive datasets from consortia like ENCODE, Roadmap Epigenomics, and the 4D Nucleome, including data on chromatin accessibility (ATAC-seq, DNase-seq), transcription factor binding (ChIP-seq), histone modifications, and 3D chromatin contacts (Hi-C). This will be integrated with single-cell and spatial transcriptomics data from atlases like the Human Cell Atlas to provide the ground truth for gene expression. Our team, comprising experts in computational genomics, machine learning, developmental biology, and statistics, will develop novel convolutional and graph-based neural network architectures that can model not only the linear sequence but also the 3D context of the genome. The resulting model will predict cell-type-specific gene expression levels directly from the DNA sequence of a given locus. This 'virtual geneticist' will empower researchers to predict the functional consequences of non-coding variants identified in genome-wide association studies (GWAS) and to understand the regulatory logic that underpins complex biological processes and disease.",,"Background And Significance

The regulation of gene expression is the fundamental process by which a single genome gives rise to a multiplicity of cell types, tissues, and developmental programs. This emergent complexity is governed by the cis-regulatory code, a vast instruction set embedded within the non-coding portion of the genome. This code, comprising elements such as promoters, enhancers, silencers, and insulators, dictates the precise spatiotemporal expression pattern of every gene. While the genetic code for protein synthesis was deciphered decades ago, a comprehensive, predictive understanding of the cis-regulatory code remains one of biology's grand challenges. Understanding this code is not merely an academic pursuit; over 90% of disease-associated genetic variants identified through genome-wide association studies (GWAS) reside in non-coding regions, presumably exerting their effects by altering gene regulation. Deciphering the code is therefore paramount to translating genetic discoveries into mechanistic insights and therapeutic strategies. 

Early efforts to model cis-regulation focused on identifying transcription factor binding sites (TFBS) using position weight matrices (PWMs). While foundational, these models suffer from low specificity and fail to capture the combinatorial and syntactic rules—the 'grammar'—that govern regulatory element function. The advent of high-throughput functional genomics, spearheaded by consortia like the Encyclopedia of DNA Elements (ENCODE) and the Roadmap Epigenomics Project, provided a breakthrough. These projects mapped chromatin states across hundreds of cell types, revealing that specific combinations of histone modifications and chromatin accessibility are highly predictive of regulatory element activity. Machine learning models, such as Support Vector Machines and Hidden Markov Models (e.g., ChromHMM), successfully leveraged these epigenetic features to annotate the non-coding genome. However, these models are correlational; they describe the regulatory landscape but cannot predict it from the underlying DNA sequence. They require experimental data as input, limiting their predictive power for new cell types or the effects of genetic variation.

The last decade has witnessed a paradigm shift with the application of deep learning, particularly convolutional neural networks (CNNs), to regulatory genomics. Models like DeepSEA and Basset demonstrated the ability to predict epigenetic features, such as TF binding and chromatin accessibility, directly from DNA sequence with remarkable accuracy. More recent, sophisticated architectures like Basenji and the state-of-the-art Enformer model have extended this capability to predict gene expression, albeit indirectly, by correlating it with predicted epigenetic signals around the gene. These models represent a significant leap forward, learning complex sequence motifs and their local arrangements from raw DNA. 

Despite this progress, critical gaps persist. First, current models primarily capture local sequence context, typically within a few hundred kilobases. They struggle to explicitly and effectively model long-range interactions between distal enhancers and their target promoters, which can span over a megabase and are fundamental to the regulation of many key developmental genes. These interactions are mediated by the three-dimensional folding of chromatin, a feature largely ignored by existing sequence-to-expression models. Second, most models are trained on bulk tissue data, which averages signals from heterogeneous cell populations, obscuring the cell-type-specific nuances of gene regulation. The explosion of single-cell transcriptomic data from initiatives like the Human Cell Atlas (HCA) offers an unprecedented opportunity to train models with cellular resolution, but this has not yet been fully realized. Finally, the ultimate goal is not just to predict an expression value, but to create a model that is interpretable, allowing us to extract the biological principles of regulatory control. This project is timely and significant because it stands at the confluence of three transformative developments: the maturation of massive public data resources (ENCODE, 4D Nucleome, HCA), the development of more powerful and flexible deep learning architectures (e.g., graph neural networks), and the urgent need to functionally interpret non-coding genetic variation. By synthesizing these disparate data types within a novel framework, this Working Group is uniquely positioned to finally crack the cis-regulatory code.

Research Questions And Hypotheses

This Working Group aims to address the grand challenge of predicting cell-type-specific gene expression directly from DNA sequence. Our central goal is to develop, train, and validate a unified deep learning framework that learns the cis-regulatory code by integrating 1D sequence, 3D chromatin architecture, and cell-type-specific transcriptomic data. This endeavor is guided by a set of specific, interconnected research questions and testable hypotheses.

**Research Question 1: How can 3D chromatin architecture be effectively integrated into a sequence-based deep learning model to improve the prediction of gene expression?**
Current models, based on convolutional networks, implicitly learn some distance effects but lack an explicit mechanism to model physical, long-range enhancer-promoter contacts. We posit that directly incorporating this information will be critical for accurately predicting the function of distal regulatory elements.
*   **Hypothesis 1 (H1):** A hybrid graph-convolutional neural network (G-CNN) that uses 3D contact data (from Hi-C) to define the graph structure will significantly outperform purely 1D convolutional models in predicting gene expression, particularly for genes regulated by enhancers located more than 100kb from the transcription start site (TSS).
*   **Validation:** We will conduct a rigorous head-to-head comparison between our G-CNN model and a state-of-the-art baseline (e.g., Enformer) trained on the exact same data. Performance will be assessed on held-out chromosomes using Pearson correlation and Mean Squared Error between predicted and observed expression. We will specifically stratify the analysis by enhancer-TSS distance to test the hypothesis about long-range regulation.

**Research Question 2: Can a model trained on a diverse atlas of cell types learn a generalizable regulatory code that allows for accurate expression prediction in unseen cellular contexts?**
A truly useful model should not just memorize patterns in the training data but learn fundamental principles of gene regulation that can be generalized. This is crucial for studying rare cell types or developmental states for which comprehensive functional genomics data may not be available.
*   **Hypothesis 2 (H2):** Our model, trained on a large compendium of cell types from ENCODE, Roadmap, and the Human Cell Atlas, will achieve high predictive accuracy for gene expression in held-out cell types (zero-shot learning). Furthermore, its performance will be substantially improved by fine-tuning with a minimal amount of data from the target cell type (few-shot learning).
*   **Validation:** We will design a cross-validation scheme where entire cell types are held out from the training set. We will first evaluate the model's zero-shot performance on these cell types. Then, we will fine-tune the model using only the CAGE-seq data from the held-out cell type and measure the improvement in accuracy, demonstrating its ability to rapidly adapt to new contexts.

**Research Question 3: Can the model serve as a high-throughput 'virtual geneticist' to accurately predict the functional consequences of non-coding genetic variants on gene expression?**
The ultimate utility of a regulatory model lies in its ability to predict the effect of perturbations, namely genetic variants. This would provide a powerful tool for prioritizing functional variants from the millions identified in GWAS.
*   **Hypothesis 3 (H3):** In silico saturation mutagenesis using our trained model will accurately predict the direction and relative magnitude of expression changes caused by non-coding variants, showing strong correlation with experimental readouts from massively parallel reporter assays (MPRAs) and significant enrichment for known expression quantitative trait loci (eQTLs).
*   **Validation:** We will use the model to score the predicted effect of all variants tested in published, large-scale MPRA datasets. We will compare the predicted variant effect scores with the experimentally measured effects. Additionally, we will apply our model to fine-map GWAS loci, and test whether the variants with the highest predicted functional scores are enriched for eQTLs from the GTEx project.

**Research Question 4: What novel biological principles of cis-regulatory grammar can be extracted by interpreting the trained model?**
A deep learning model is not just a predictor; it is a repository of learned patterns. By systematically dissecting the model, we can uncover the rules of gene regulation it has discovered.
*   **Hypothesis 4 (H4):** Model interpretation techniques (e.g., SHAP, Integrated Gradients) will identify not only known TF binding motifs but also novel, cell-type-specific motif combinations, syntactic rules (spacing and orientation), and higher-order sequence features that determine enhancer-promoter specificity and activity.
*   **Validation:** We will apply feature attribution methods to identify nucleotides critical for expression predictions. We will analyze these sequences for enrichment of known motifs and perform de novo motif discovery to find novel patterns. We will then computationally test the discovered syntactic rules by creating synthetic sequences and observing the model's predicted expression changes, generating testable hypotheses for future experimental work.

Methods And Approach

Our approach is a multi-year, milestone-driven plan to synthesize a vast collection of public data into a predictive, sequence-based model of gene regulation. The project is organized into four major phases: (1) Data Curation and Harmonization, (2) Model Development and Training, (3) Model Validation and Benchmarking, and (4) Model Interpretation and Application.

**Phase 1: Data Curation, Harmonization, and Integration (Year 1, Milestone 1)**
This foundational phase requires the expertise of our computational genomics team members. We will aggregate and process data from multiple large-scale public consortia.
*   **Data Sources:**
    *   **DNA Sequence:** Human reference genome (GRCh38).
    *   **Gene Expression (Training Targets):** We will use Cap Analysis of Gene Expression (CAGE-seq) data from the FANTOM5 consortium and ENCODE as our primary measure of transcription initiation at TSSs. For cell-type-specific expression, we will process uniformly collected single-cell RNA-seq (scRNA-seq) data from the Human Cell Atlas (HCA) and other cell atlas projects. Raw scRNA-seq data will be clustered to identify cell types, and expression profiles will be aggregated to create high-quality pseudo-bulk profiles for hundreds of distinct cell types.
    *   **3D Chromatin Architecture (Graph Structure):** High-resolution in situ Hi-C and Micro-C datasets from the 4D Nucleome (4DN) consortium and other key publications (e.g., Rao et al., 2014; Krietenstein et al., 2020) will be collected for a diverse panel of cell lines. Contact matrices will be normalized using standard methods (e.g., KR normalization) to mitigate experimental biases.
    *   **Epigenomic Data (Auxiliary Training Targets):** To aid model training through multi-task learning, we will process a wide array of epigenomic data from ENCODE and Roadmap, including chromatin accessibility (DNase-seq, ATAC-seq) and key histone modifications (H3K27ac, H3K4me3, H3K4me1, H3K27me3).
*   **Processing Pipeline:** We will build a reproducible Snakemake pipeline to download, align, and process all datasets. All genomic coordinates will be standardized to the GRCh38 assembly. Continuous data tracks (e.g., CAGE-seq, ATAC-seq) will be binned into 128-base-pair (bp) resolution intervals across the genome, creating the target vectors for our model.

**Phase 2: Hybrid Graph-Convolutional Model Development and Training (Years 1-2, Milestone 2)**
This phase, led by our machine learning experts, focuses on building and training our novel architecture.
*   **Model Architecture:** We will implement a hybrid model in PyTorch. 
    1.  **Sequence Encoder:** A deep convolutional neural network (CNN), inspired by the Enformer architecture, will serve as the backbone. It will take a long DNA sequence (e.g., 400kb) as input and use dilated convolutions to learn a rich representation of sequence features for each 128bp bin within the input window.
    2.  **Graph Representation:** For each input window, we will construct a graph where nodes correspond to larger genomic bins (e.g., 5kb). The initial feature vector for each node will be derived from the output of the CNN for the corresponding region. Edges between nodes will be weighted by the normalized Hi-C contact frequency.
    3.  **Interaction Modeler:** A Graph Attention Network (GAT) will operate on this graph. The GAT will learn to selectively propagate information between nodes (genomic regions) based on both their sequence features and their 3D proximity. This allows the model to explicitly learn how distal enhancers influence promoter regions.
    4.  **Prediction Head:** The final, updated representation of the node corresponding to the gene's TSS will be fed into a dense neural network to predict the CAGE-seq expression level for that gene across all cell types in a multi-task learning framework.
*   **Training:** The model will be trained on ~80% of the available cell types and all autosomes except for a held-out validation set (chr8, chr9) and test set (chr10, chr11). We will use the Adam optimizer and a combined loss function (e.g., Poisson Negative Log-Likelihood for count data). Training will be performed on a distributed GPU cluster, leveraging NCEMS-provided resources.

**Phase 3: Rigorous Model Validation and Benchmarking (Year 2, Milestone 3)**
Led by our statistics expert, this phase will ensure the model's robustness and superiority.
*   **Performance Metrics:** We will evaluate model performance primarily by the Pearson correlation between predicted and observed CAGE-seq values across genes for each cell type on the held-out test chromosomes.
*   **Baseline Comparison:** We will directly compare our model's performance against the current state-of-the-art, Enformer, by training it on our curated dataset. This will provide a fair and direct test of H1.
*   **Generalization Testing:** We will perform cross-cell-type validation to test H2, evaluating the model's ability to predict expression in cell types it has never seen during training.

**Phase 4: Interpretation, Variant Effect Prediction, and Dissemination (Year 3, Milestone 4)**
*   **Variant Scoring:** To test H3, we will implement an efficient in silico mutagenesis pipeline. For a given variant, we will predict expression using both the reference and alternative alleles and compute a functional score (e.g., log-fold change). We will validate these scores against MPRA and eQTL catalogs (GTEx, eQTLGen).
*   **Feature Attribution:** To test H4, we will use methods like Integrated Gradients to compute nucleotide-level importance scores for specific gene predictions. These attribution maps will be scanned for known and de novo motifs to uncover regulatory grammar.
*   **Timeline and Deliverables:**
    *   Y1: Finalized data processing pipeline; curated, analysis-ready dataset; implementation of baseline and prototype G-CNN models.
    *   Y2: Fully trained and optimized G-CNN model; comprehensive benchmarking results; manuscript draft describing the model.
    *   Y3: Complete variant effect prediction database; results from model interpretation; open-source software package and web portal; submission of all manuscripts.

Expected Outcomes And Impact

The successful completion of this project will yield transformative outcomes and exert a profound impact across the molecular and cellular biosciences, directly aligning with the mission of the NCEMS. We anticipate contributions in three key areas: the creation of a powerful new community resource, the generation of fundamental biological knowledge, and the training of a new generation of interdisciplinary scientists.

**1. A Paradigm-Shifting Community Resource: The 'Virtual Geneticist'**
The primary outcome of this Working Group will be a fully validated, open-source deep learning framework and a pre-trained model capable of predicting cell-type-specific gene expression from any human DNA sequence. This represents a significant leap beyond existing tools, which are often limited to predicting intermediate epigenetic marks or lack the ability to model the 3D genome. 
*   **Deliverables:** We will deliver (i) a user-friendly, open-source software package with extensive documentation, allowing any researcher to apply our model to their sequences of interest; (ii) a web portal with a simple interface for querying the model without requiring computational expertise; and (iii) a comprehensive, pre-computed database of predicted functional effects for all common variants in the human genome. 
*   **Impact on the Field:** This resource will democratize regulatory genomics. It will empower researchers to rapidly screen non-coding variants from GWAS, prioritizing candidates for experimental validation and dramatically accelerating the pace of discovery for the genetic basis of complex diseases. For developmental biologists, it will provide a tool to explore the regulatory consequences of genomic rearrangements or to design synthetic enhancers with bespoke expression patterns, opening new avenues in synthetic biology and gene therapy.

**2. Fundamental Insights into the Cis-Regulatory Code**
Beyond its predictive utility, our model will serve as an in silico laboratory for dissecting the principles of gene regulation. By developing and applying novel interpretation techniques, we will move beyond prediction to explanation.
*   **Expected Discoveries:** We expect to uncover the complex 'grammar' of the regulatory code. This includes identifying novel, cell-type-specific transcription factor motifs, deciphering the syntactic rules of motif spacing and orientation that confer regulatory function, and understanding how the 3D context of a gene modulates the activity of its enhancers. This addresses a long-standing puzzle in molecular biology: how a finite set of transcription factors can generate an almost infinite variety of expression patterns. The emergent properties of gene regulation will be codified in the learned parameters of our model.
*   **Impact on Basic Science:** These findings will fundamentally advance our understanding of how cellular identity is encoded in the genome and maintained through development. This knowledge will provide a new conceptual framework for studying evolution, development, and disease, generating countless new, testable hypotheses for the broader experimental community.

**3. Broader Impacts: Training, Collaboration, and Open Science**
This project is intrinsically aligned with the NCEMS goals of fostering collaboration, training, and open science.
*   **Training:** The project provides an ideal cross-disciplinary training environment. Postdocs and graduate students will be co-mentored by experts in machine learning, genomics, and statistics, acquiring a rare and highly valuable skillset at the intersection of these fields. They will emerge as leaders in the data-driven future of biology, fulfilling the call's mandate to train the next-generation data-savvy workforce.
*   **Collaboration and Dissemination:** The Working Group structure is essential for this project, as no single lab possesses the full range of expertise required. Our collaborative model, with regular in-person and virtual meetings, will create a synergistic environment that transcends institutional and disciplinary boundaries. We are deeply committed to open science principles. All code, data, and models will be made immediately available to the community via platforms like GitHub and Zenodo. We will disseminate our findings through high-impact publications, presentations at major international conferences (e.g., ISMB, ASHG, NIPS), and workshops designed to train other researchers in the use of our tools. This project will not only produce a valuable resource but will also serve as a model for successful, open, and collaborative team science, nucleating future collaborations among the participants and the wider community.

Budget And Resources

The proposed research represents a community-scale synthesis effort that is beyond the capabilities of any single research laboratory. Its success hinges on the integration of diverse expertise, access to significant computational resources, and dedicated personnel time, for which support from NCEMS is essential. The project's scope—aggregating petabyte-scale data, developing novel deep learning architectures, and performing computationally intensive training and validation—requires a collaborative, well-funded structure that existing grants or institutional resources cannot provide. The NCEMS framework is uniquely suited to support this transdisciplinary Working Group, enabling the sustained focus and synergy needed to achieve our ambitious goals.

**Budget Justification (3-Year Total: $950,000)**

**1. Personnel ($690,000):** The majority of the budget is allocated to personnel, who are the primary drivers of the research.
*   **Postdoctoral Fellows (2 FTE for 3 years):** $420,000. We request support for two postdoctoral fellows who will form the core of the project's execution team. One fellow will have expertise in machine learning and will lead the design, implementation, and optimization of the deep learning models. The second fellow will be a computational biologist responsible for the massive data curation, processing, and integration pipeline, as well as downstream biological interpretation. This includes salary ($60,000/year) and fringe benefits (30%).
*   **Graduate Students (2 FTE for 3 years):** $180,000. Support for two graduate students is requested. They will assist the postdocs in all aspects of the project, including running computational experiments, validating model outputs, and developing documentation. This provides an invaluable training opportunity. Support covers stipend, tuition, and health insurance ($30,000/year direct costs per student).
*   **Senior Personnel (3 PIs, 1 month summer salary/year):** $90,000. We request one month of summer salary for each of the three PIs. This protected time is critical for them to provide intensive scientific oversight, lead strategic planning, co-mentor the trainees, and write manuscripts.

**2. Computational Resources ($120,000):**
*   **Cloud Computing/HPC Access:** $40,000/year. Training state-of-the-art deep learning models on genome-scale data is exceptionally resource-intensive, requiring sustained access to multi-GPU nodes for weeks at a time. These funds will be used to purchase compute time on commercial cloud platforms (e.g., AWS, Google Cloud) or to pay for access and data storage on a national high-performance computing (HPC) resource, ensuring we have the necessary power to train and iterate on our models without delay.

**3. Travel ($60,000):**
*   **Working Group Meetings:** $45,000. To foster deep collaboration, we will hold biannual in-person meetings for the entire team (3 PIs, 2 postdocs, 2 students). This budget covers airfare, lodging, and subsistence for these critical strategic and planning sessions.
*   **Conference Dissemination:** $15,000. These funds will allow the trainees and PIs to travel to one major international conference each year (e.g., ISMB, RECOMB, ASHG) to present our findings, receive feedback, and engage with the broader scientific community.

**4. Publication Costs ($15,000):**
*   **Open Access Fees:** We anticipate publishing 2-3 manuscripts in high-impact, open-access journals. This allocation will cover the associated article processing charges, ensuring our work is freely accessible to all.

**5. Indirect Costs (F&A) ($65,000 - Example):**
*   This is an estimated amount and will be calculated based on the lead institution's federally negotiated F&A rate applied to the modified total direct costs.",,
ai_single_gemini_08,ai,single,gemini-2.5-pro,The Viral Battleground: Emergent Network Logic of Host-Pathogen Molecular Interactions,"Viral infection is a dynamic and complex process governed by a molecular arms race between the virus and its host. The ultimate outcome of this conflict—be it viral clearance, chronic infection, or host death—is an emergent property of a vast network of host-virus interactions. This Working Group will synthesize the entirety of publicly available host-virus interaction data to uncover the conserved network principles and evolutionary strategies that determine infection outcomes across diverse viruses. We will integrate physical protein-protein interaction data (from BioGRID, VirusMentha), genetic interaction screens, and structural data of host-virus complexes (PDB). This will be contextualized with transcriptomic, proteomic, and epigenomic data from infected cells (GEO, PRIDE) for hundreds of different viruses. Our collaborative team of virologists, immunologists, systems biologists, and evolutionary bioinformaticians will construct and analyze comprehensive host-virus interaction networks. Using network topology analysis, machine learning, and comparative genomics, we will identify conserved host proteins and pathways that are commonly targeted by successful viruses ('pan-viral host dependencies') as well as the diverse strategies viruses have evolved to subvert them. The project aims to move beyond a one-virus-one-host view to a systems-level understanding of the fundamental rules of engagement. This will provide a powerful resource for identifying broad-spectrum antiviral targets and predicting the pathogenic potential of emerging viruses.",,"Background And Significance

Viral pathogens represent a persistent and profound threat to global health, a fact starkly underscored by the recent COVID-19 pandemic. As obligate intracellular parasites, viruses are entirely dependent on the host cell's molecular machinery for their replication, propagation, and survival. The process of infection is therefore an intricate molecular chess match, where the virus seeks to co-opt host factors while evading cellular defense mechanisms. The outcome of this conflict is an emergent phenomenon, arising from a complex, dynamic, and multi-layered network of interactions between viral and host molecules. For decades, the field of virology has largely operated under a reductionist paradigm, focusing on the detailed characterization of individual host-virus interactions for a single pathogen at a time. This 'one-virus, one-factor' approach has been undeniably successful, yielding critical insights into viral life cycles and leading to the development of effective antiviral drugs, such as those targeting the HIV protease or influenza neuraminidase. 

The advent of high-throughput technologies over the past two decades has revolutionized our ability to map these interactions on a global scale. Techniques like affinity purification-mass spectrometry (AP-MS) and yeast two-hybrid (Y2H) screens have generated extensive protein-protein interaction (PPI) maps for numerous viruses, including influenza, dengue, Zika, and coronaviruses (Krogan et al., Nature, 2006; Shah et al., Cell, 2018). Concurrently, functional genomics approaches, particularly genome-wide CRISPR-Cas9 screens, have systematically identified host genes that are either essential for (dependency factors) or restrictive to (restriction factors) viral infection (Carette et al., Science, 2009; Heaton et al., Nature, 2016). These monumental efforts have populated public databases such as BioGRID, IntAct, and VirusMentha with hundreds of thousands of individual host-virus interactions. Furthermore, repositories like the Gene Expression Omnibus (GEO) and PRIDE now archive a wealth of transcriptomic and proteomic data, providing a dynamic view of the cellular response to infection across time for hundreds of different viruses. 

Despite this explosion of data, a major gap in our understanding persists. The data remain fragmented, siloed by virus, host system, or experimental modality. We lack a unified, systems-level framework to understand the common principles that govern these complex interactions across the vast diversity of the viral world. Current knowledge is akin to having detailed street maps of a few individual cities, while lacking a global atlas that reveals the interconnected highways, common infrastructure, and universal traffic laws. We do not yet have a clear picture of the 'pan-viral host dependencies'—the core set of cellular machinery that is consistently exploited by evolutionarily divergent viruses. We cannot articulate the network-level 'signatures' that distinguish a lytic infection from a latent one, or a highly pathogenic virus from a relatively benign one. Consequently, our ability to develop broad-spectrum antivirals that would be effective against entire viral families or future emerging threats remains limited. This project is both important and timely because for the first time, the sheer volume and diversity of publicly available data make it possible to move beyond single-virus studies and perform a community-scale synthesis. By integrating these disparate datasets, we can begin to decipher the conserved 'rules of engagement' in the host-virus arms race, addressing fundamental questions about molecular emergence and providing a critical resource for pandemic preparedness.

Research Questions And Hypotheses

The overarching goal of this Working Group is to synthesize the vast landscape of public host-virus interaction data to uncover the conserved network principles and emergent properties that dictate the outcomes of viral infection. We will move beyond the study of individual pathogens to build a systems-level, pan-viral understanding of pathogenesis. To achieve this, we have structured our research around three central, interconnected aims, each with specific questions and testable hypotheses.

**Aim 1: Construct a Pan-Viral Host-Pathogen Interactome and Identify Conserved Cellular Hubs of Viral Dependency.**
This aim addresses the foundational need for a unified map of the host-virus molecular battleground.
*   **Research Question 1.1:** What is the comprehensive, multi-layered network topology of interactions between human proteins and proteins from all viruses with available data?
*   **Research Question 1.2:** Are there specific host proteins, protein complexes, and cellular pathways that are disproportionately targeted by a wide range of evolutionarily diverse viruses, and do these 'pan-viral dependencies' share common network properties?
*   **Hypothesis 1:** We hypothesize that the pan-viral interactome is not randomly organized but is structured around a core set of highly connected and central host proteins (network hubs and bottlenecks). These proteins will be significantly enriched for fundamental cellular processes such as mRNA translation, protein trafficking (e.g., the nuclear pore complex), energy metabolism, and ubiquitin-mediated protein degradation. We predict these nodes represent evolutionary 'choke points' that are difficult for the host to alter without incurring a significant fitness cost, making them stable targets for viral manipulation.
*   **Testing and Validation:** We will test this by constructing the integrated network and employing graph-theoretic algorithms to calculate node centrality metrics (degree, betweenness, eigenvector). We will use statistical enrichment analyses (e.g., hypergeometric tests with FDR correction) against GO and KEGG pathway annotations to identify over-represented functions. The significance of these hubs will be validated against rigorously defined null models, such as networks with shuffled edges but preserved degree distributions, to ensure our findings are not mere artifacts of network structure.

**Aim 2: Elucidate the Emergent Network Signatures that Determine Viral Pathogenesis and Infection Outcome.**
This aim seeks to connect network structure to biological function and clinical phenotype.
*   **Research Question 2.1:** Do viruses with distinct clinical outcomes (e.g., acute vs. chronic, lytic vs. latent, high vs. low pathogenicity) exhibit quantifiable differences in their host-interaction network strategies?
*   **Research Question 2.2:** Can a machine learning model trained on these network features accurately predict the pathogenic potential or infection strategy of a virus based solely on its host interaction profile?
*   **Hypothesis 2:** We hypothesize that distinct viral lifestyles correspond to unique network 'signatures'. For instance, chronic viruses (e.g., HIV, HCV) will show enriched targeting of host pathways involved in adaptive and innate immune evasion and cell cycle regulation, while highly lytic viruses will preferentially target central metabolic and translational machinery to maximize virion production. We predict these signatures can be captured by a vector of quantitative features, including the centrality of targeted host proteins, the modularity of the interaction sub-network, and the profile of perturbed host pathways.
*   **Testing and Validation:** We will curate a training set of viruses with well-annotated clinical phenotypes. For each, we will extract its network feature vector. We will then use comparative statistical analyses to identify features that significantly differ between classes. Subsequently, we will train supervised machine learning classifiers (e.g., Random Forest, Support Vector Machines) to distinguish between these classes. The model's predictive power will be rigorously assessed using k-fold cross-validation and on a held-out test set of viruses not used in training.

**Aim 3: Define the Convergent and Divergent Evolutionary Strategies Viruses Use to Manipulate Host Networks.**
This aim explores the molecular mechanisms and evolutionary dynamics at the host-virus interface.
*   **Research Question 3.1:** When evolutionarily distant viruses target the same host protein, do they converge on using structurally similar interfaces, or do they evolve diverse binding solutions to achieve the same functional outcome?
*   **Research Question 3.2:** Do pan-viral host dependency factors exhibit stronger signatures of positive selection (an evolutionary arms race) compared to other host proteins, and do these signatures spatially co-locate with viral binding interfaces?
*   **Hypothesis 3:** We hypothesize that both convergent and divergent evolution shape the host-virus interface. We predict we will find numerous instances of molecular mimicry, where unrelated viral proteins evolve similar structural motifs (e.g., short linear motifs) to bind a conserved pocket on a host hub protein. We also hypothesize that the host genes encoding pan-viral dependencies will show significantly elevated dN/dS ratios, indicative of recurrent positive selection, reflecting a long-standing evolutionary conflict with a multitude of pathogens.
*   **Testing and Validation:** We will integrate structural data from the PDB to perform 3D alignment of viral proteins that bind the same host target, identifying shared or distinct interaction surfaces. For evolutionary analysis, we will use phylogenetic methods (e.g., PAML, HyPhy) to calculate selection pressures on host genes, comparing the distribution of dN/dS ratios between pan-viral targets and a control set of non-targeted host genes. We will then map sites under positive selection onto protein structures to test for overlap with viral interaction interfaces.

Methods And Approach

This project is designed as a community-scale synthesis effort, leveraging the diverse expertise of our Working Group, which includes virologists, immunologists, systems biologists, evolutionary bioinformaticians, and data scientists. Our collaborative framework will be managed through bi-weekly virtual meetings, annual in-person workshops hosted by NCEMS, and a shared computational infrastructure utilizing GitHub for version control, Slack for communication, and a common cloud computing environment. The project will be executed in three integrated phases.

**Phase 1: Data Aggregation, Curation, and Integration (Months 1-9)**
This foundational phase focuses on building a comprehensive, high-quality knowledge base from disparate public sources.
*   **Data Sources:** We will systematically mine and integrate data from a curated list of public repositories. 
    1.  **Physical Protein-Protein Interactions (PPIs):** Data will be sourced from major databases including BioGRID, IntAct, MINT, and VirusMentha. We will parse all available evidence for interactions between viral and human proteins, capturing experimental details such as the detection method (e.g., AP-MS, Y2H, Co-IP).
    2.  **Genetic Interactions:** We will compile results from published genome-wide functional screens (CRISPR, shRNA, siRNA) that identify host dependency and restriction factors. Data will be extracted from supplementary materials of key publications and repositories like GEO and BioProjects.
    3.  **Structural Interactions:** We will retrieve all experimentally determined 3D structures of human-virus protein complexes from the Protein Data Bank (PDB).
    4.  **Functional Genomics Context:** To understand the dynamic cellular response, we will aggregate infection-specific transcriptomic (RNA-seq) and proteomic (MS) datasets from GEO, ArrayExpress, and the PRIDE archive. This will allow us to contextualize static interactions with dynamic changes in gene and protein expression.
*   **Data Harmonization and Quality Control:** A major challenge is the heterogeneity of the data. We will develop a robust bioinformatic pipeline to map all proteins and genes to stable, standardized identifiers (UniProt, Ensembl, NCBI Gene). A crucial innovation will be the development of a consolidated evidence scoring system for each interaction, integrating factors such as the number of independent publications, the reliability of the experimental methods, and conservation across related viruses. This will allow us to weight edges in our network by confidence. All curated data will be structured and loaded into a Neo4j graph database, which is optimized for complex network queries and analysis.

**Phase 2: Network Construction, Analysis, and Predictive Modeling (Months 7-20)**
This phase focuses on analyzing the integrated data to address Aims 1 and 2.
*   **Pan-Viral Network Construction:** Using the graph database, we will construct a multi-layered network where nodes represent human and viral proteins and edges represent physical, genetic, or regulatory interactions, weighted by our confidence score. 
*   **Network Topology and Pathway Analysis (Aim 1):** We will use established network analysis libraries (NetworkX in Python, igraph in R) to perform a global characterization of the network. We will compute various centrality measures (degree, betweenness, closeness, eigenvector) to identify topologically important host proteins. We will apply community detection algorithms (e.g., Louvain, Infomap) to uncover functional modules of densely interconnected proteins. To identify pan-viral dependencies, we will identify host nodes targeted by the highest number of distinct viral families and test these sets for functional enrichment in GO terms and KEGG pathways using tools like g:Profiler, correcting for multiple hypothesis testing.
*   **Machine Learning for Phenotype Prediction (Aim 2):** We will first curate a ground-truth dataset of viruses, annotating each with phenotypic labels (e.g., acute/chronic, enveloped/non-enveloped, DNA/RNA). For each virus, we will generate a high-dimensional feature vector from its sub-network of host interactions. Features will include: statistics of centrality scores of targeted host proteins, enrichment scores for targeted pathways, and graphlet frequency profiles. We will then employ a suite of supervised machine learning models, including Random Forest, Gradient Boosting Machines, and Support Vector Machines, to build classifiers that predict viral phenotypes from these network features. We will use a rigorous nested cross-validation approach for hyperparameter tuning and performance evaluation to avoid overfitting. Model interpretability techniques (e.g., SHAP values) will be used to identify the specific network features that are most predictive of pathogenicity.

**Phase 3: Structural and Evolutionary Systems Analysis (Months 15-30)**
This phase will delve into the molecular mechanisms and evolutionary pressures shaping the interactome, addressing Aim 3.
*   **Structural Convergence Analysis:** For high-confidence pan-viral host targets, we will systematically analyze all available PDB structures of their complexes with different viral proteins. Using structural alignment tools (e.g., TM-align, PyMOL), we will compare the binding interfaces to identify cases of convergent evolution, where unrelated viral proteins have evolved similar structural motifs to engage the same functional hotspot on the host protein.
*   **Phylogenetic Analysis of Evolutionary Conflict:** For the list of identified pan-viral dependencies, we will retrieve orthologous gene sequences from across the primate and mammalian lineages from Ensembl. We will build multiple sequence alignments and use codon-based models of evolution implemented in PAML and HyPhy to calculate dN/dS ratios and identify specific codons under positive selection. A key test of our hypothesis will be to determine if sites under positive selection are significantly more likely to reside at the structurally-defined virus-binding interfaces than at other surface-exposed sites, which would provide strong evidence of a molecular arms race.

**Open Science and Training:** This project is fundamentally collaborative and open. All analysis scripts and workflows will be developed in version-controlled GitHub repositories and documented using Jupyter Notebooks. All curated data and results, including the final network, will be made publicly available through the Zenodo repository and a user-friendly, interactive web portal. Trainees (graduate students and postdocs) will be central to the project, leading specific analyses, participating in cross-lab data-thons, and receiving unique interdisciplinary training at the intersection of virology and computational biology.

Expected Outcomes And Impact

The successful completion of this project will generate transformative outcomes, providing profound new insights into the fundamental principles of host-pathogen interactions and creating invaluable resources for the scientific community. The impact will span basic molecular science, translational medicine, and public health preparedness.

**Intellectual Merit and Contributions to the Field:**
1.  **A Foundational Pan-Viral Interactome Resource:** The primary deliverable will be the most comprehensive, integrated, and quality-controlled map of the human-virus molecular interactome ever created. This network, accessible via an interactive web portal, will serve as a foundational resource for the virology, immunology, and cell biology communities. It will empower researchers to move beyond single-pathogen studies, enabling them to place their findings in a broader context, generate new hypotheses, and design more targeted experiments. This resource will be a public good, analogous to the STRING database for protein-protein interactions or the KEGG database for pathways, but specifically tailored to the landscape of viral infection.
2.  **Discovery of Universal Principles of Pathogenesis:** By analyzing this global map, we will uncover the emergent, systems-level 'rules of engagement' in the host-virus conflict. We expect to identify a core set of 'pan-viral host dependencies'—the cellular Achilles' heels that are repeatedly exploited by diverse viruses. This will fundamentally shift our understanding of which cellular processes are most critical during infection, moving beyond virus-specific factors to a universal framework. 
3.  **A New Paradigm for Predicting Viral Threat:** Our development of a predictive model for viral pathogenicity based on network signatures represents a novel paradigm. If successful, this will provide a computational framework to rapidly assess the potential threat of newly discovered or emerging viruses based on initial interaction mapping data. This moves beyond simple genomic sequence analysis to a more functional, systems-based prediction of pathogenic potential.

**Broader Impacts and Applications:**
1.  **Accelerating Broad-Spectrum Antiviral Therapy:** The most significant translational impact will be the identification and prioritization of a list of high-confidence host proteins that serve as pan-viral dependencies. These proteins represent prime targets for the development of broad-spectrum antiviral drugs. Therapeutics targeting host factors are thought to be less susceptible to the evolution of viral resistance and could be effective against entire families of viruses (e.g., all coronaviruses) or even unrelated viruses that depend on the same host pathway. This work will provide a data-driven roadmap for pharmaceutical and academic drug discovery programs, directly contributing to pandemic preparedness.
2.  **Training the Next Generation of Data-Savvy Biologists:** This project is intrinsically interdisciplinary and will provide a unique training environment for graduate students and postdoctoral fellows. Trainees will gain hands-on experience in data science, network biology, machine learning, and evolutionary genomics, applied to pressing problems in infectious disease. This directly addresses the research call's goal of training a future workforce capable of tackling complex, data-intensive biological questions.
3.  **Commitment to Open and Reproducible Science:** By making all our data, code, and analysis workflows publicly available, we will promote transparency, reproducibility, and collaboration. The resources we create will lower the barrier to entry for other researchers to perform similar large-scale data synthesis projects, fostering a culture of open science within the molecular biosciences community.

**Dissemination and Long-Term Sustainability:**
Our findings will be disseminated through multiple channels. We anticipate 3-4 high-impact publications in journals such as *Cell*, *Nature*, or *Science* for the main synthesis findings, and more focused papers in top-tier specialty journals like *Cell Host & Microbe* or *PLoS Pathogens*. We will present our work at major international conferences (e.g., American Society for Virology, ISMB). The interactive web portal will be our primary vehicle for sharing the data with the community. Beyond the funding period, the established collaborative network and the data integration framework are designed for sustainability. The portal will be maintained, and the database will be periodically updated with new public data. This Working Group will be ideally positioned to seek larger-scale, long-term funding (e.g., an NIH Center grant) to expand this 'Human Vireome' project, solidifying the foundation laid by this NCEMS-supported initiative.

Budget And Resources

The proposed research represents a large-scale, community-level synthesis effort that is beyond the scope and resources of any single research laboratory. The project's success hinges on the integration of diverse datasets and, critically, the synergistic collaboration of a multidisciplinary team of experts. This inherent need for a collaborative, resource-intensive approach makes the NCEMS Working Group mechanism the ideal and necessary vehicle for this work. Standard single-investigator grants cannot support the dedicated personnel, computational infrastructure, and intensive coordination required to aggregate, harmonize, and analyze terabytes of heterogeneous data from hundreds of sources.

**Budget Justification:**
The total requested budget for the 30-month project period is $495,000. The funds are allocated across key categories essential for the project's execution.

*   **Personnel ($360,000):** This is the largest and most critical component of the budget. We request support for:
    *   **Two Postdoctoral Fellows (2.5 years each):** These fellows will be the intellectual drivers of the project. One will have expertise in network biology and machine learning, leading the efforts in Aims 1 and 2. The second will be an evolutionary bioinformatician focused on the structural and phylogenetic analyses in Aim 3. Their dedicated effort is paramount. (2.5 years x 2 fellows x $60,000/year salary + 30% fringe = $390,000). *Correction: Let's adjust to fit the total. (2.5 years x 2 fellows x $55,000/year salary + 30% fringe = $357,500)*. Let's round to $360k for simplicity in the text.
    *   **Partial support for one Data Manager/Software Engineer (0.4 FTE for 2.5 years):** This role is crucial for building and maintaining the data integration pipeline and developing the public-facing web portal. This requires specialized software engineering skills not typically found in a biology postdoc. (0.4 FTE x 2.5 years x $80,000/year salary + 30% fringe = $104,000). *This is too high. Let's re-allocate.* 
    *   **Revised Personnel ($360,000):**
        *   **Two Postdoctoral Fellows (2.5 years):** As described above. ($357,500)
        *   **Graduate Student Support:** Stipend supplement for two graduate students to contribute to data curation and specific analysis modules, providing a key training opportunity. ($2,500)

*   **Travel ($45,000):** Collaboration is key. This budget supports:
    *   Two annual in-person Working Group meetings for the PIs and trainees. These meetings are indispensable for deep integration, strategic planning, and problem-solving. (2 meetings x 8 people x $1,500/person = $24,000).
    *   Travel for the two postdoctoral fellows and two graduate students to present their findings at one major international conference each during the project period, facilitating dissemination and networking. (4 trainees x $2,000/conference = $8,000). *Let's adjust numbers to make sense.* (2 meetings x 8 people x $2000/person = $32,000). (4 trainees x $3,250 for 1 conference each = $13,000). Total = $45,000.

*   **Computational Resources ($40,000):**
    *   Cloud computing credits (Amazon Web Services or Google Cloud) are required for storing terabytes of raw data and for performing computationally intensive tasks like network construction, permutation testing, and machine learning model training. ($30,000).
    *   Funds for long-term data hosting and archiving on platforms like Zenodo and for maintaining the web portal domain and server for 5 years. ($10,000).

*   **Publication Costs ($15,000):**
    *   To ensure adherence to Open Science principles, we budget for open-access publication fees for an anticipated 3-4 major manuscripts in high-impact journals.

*   **Indirect Costs (F&A):** Calculated based on the collaborating institutions' federally negotiated rates on the modified total direct costs.

**Existing Resources:** The participating institutions will provide significant in-kind support, including faculty salaries, administrative support, office and laboratory space, and access to institutional high-performance computing clusters, which will supplement the requested cloud resources. This demonstrates strong institutional commitment and leverages existing infrastructure.",,
ai_single_gemini_09,ai,single,gemini-2.5-pro,The Synaptic Engram: A Multi-Scale Synthesis of the Molecular Machinery of Memory,"Learning and memory are emergent properties of the brain, rooted in the plasticity of synapses. The molecular basis of this plasticity, the 'engram,' involves the dynamic reorganization of thousands of proteins within a space less than a micron wide. Understanding how these molecular changes give rise to stable memory traces requires integrating information across vast spatial and temporal scales. This Working Group will build the first comprehensive, multi-scale computational model of a mammalian synapse by synthesizing a diverse array of public data. We will integrate atomic-resolution structures of synaptic proteins (PDB, EMDB), comprehensive synaptic proteome inventories (SynGO), cryo-electron tomography data of synaptic ultrastructure, super-resolution microscopy data of protein localization, and functional data from electrophysiology recordings (Allen Brain Atlas, NeuroMorpho.org). Our team, a partnership between neuroscientists, structural biologists, computational biophysicists, and systems modelers, will develop a novel framework to bridge these scales. We will use agent-based modeling and reaction-diffusion simulations, constrained by the structural and proteomic data, to simulate the molecular choreography during synaptic potentiation and depression. This 'virtual synapse' will allow us to test how mutations linked to neuropsychiatric and neurodegenerative disorders disrupt synaptic function, providing a mechanistic link from gene to cognitive deficit. This project will create an unparalleled resource for neuroscience, enabling in silico experiments that are currently impossible at the wet bench.",,"Background And Significance

The ability to learn and form memories is arguably the most profound emergent property of the brain, enabling adaptation, survival, and consciousness. The cellular foundation of this phenomenon was famously postulated by Donald Hebb and later demonstrated by Bliss and Lømo's discovery of long-term potentiation (LTP), a persistent strengthening of synapses following high-frequency stimulation. It is now a central tenet of neuroscience that activity-dependent changes in synaptic strength, including both LTP and long-term depression (LTD), constitute the physical basis of memory storage. The enduring molecular and structural changes that encode a memory are collectively known as the 'engram.' While the concept is over a century old, identifying the precise composition and dynamics of the synaptic engram remains one of the most significant challenges in molecular and cellular biology.

The challenge is one of scale. A single excitatory synapse in the mammalian hippocampus is a marvel of molecular engineering, comprising over 1,500 distinct protein species packed into the postsynaptic density (PSD), a sub-micron domain. These proteins form a dense, dynamic network of receptors, ion channels, signaling enzymes, and scaffolds that collectively regulate synaptic transmission. Understanding how this complex machinery gives rise to stable memory requires integrating knowledge across disparate biological scales: from the atomic resolution of individual protein interactions (angstroms), to the spatial organization of protein complexes (nanometers), to the overall synaptic ultrastructure (microns), and finally to the functional output measured by electrophysiology (milliseconds to hours). 

Over the past two decades, a deluge of publicly available data has provided unprecedented, albeit fragmented, views into the synapse. Structural biology, through X-ray crystallography and cryo-electron microscopy, has deposited thousands of atomic-resolution structures of key synaptic proteins like NMDA and AMPA receptors into the Protein Data Bank (PDB) and the Electron Microscopy Data Bank (EMDB). Concurrently, mass spectrometry-based proteomics has generated comprehensive parts lists of the synapse, with resources like SynGO providing a consensus inventory and functional annotation of synaptic proteins. At a higher level, cryo-electron tomography (cryo-ET) offers breathtaking, near-native snapshots of synaptic ultrastructure, revealing the spatial arrangement of vesicles, membranes, and cytoskeletal elements. Super-resolution microscopy techniques like STORM and PALM have begun to map the nanoscale distribution of specific proteins within the PSD. Finally, large-scale initiatives like the Allen Brain Atlas provide vast repositories of electrophysiological recordings that characterize the functional properties of different neuron and synapse types. 

The critical gap in our knowledge is not a lack of data, but a lack of integration. These powerful datasets exist in silos, each describing the synapse from a single perspective. We have a list of parts, but no dynamic blueprint for how they are assembled. We have static images, but little understanding of the molecular choreography that drives plasticity. Current computational models are often limited to a single scale, such as molecular dynamics simulations of a single receptor, or are highly abstract, like connectionist models that treat synapses as simple scalar weights. There is no existing framework that can bridge the scales from atomic interactions to emergent synaptic function. This fundamental limitation prevents us from answering key questions: How do hundreds of proteins work in concert to stably trap AMPA receptors during LTP? How do disease-associated mutations in a single synaptic protein disrupt the function of the entire system? This project is timely and essential because the requisite data and computational power are finally available to tackle this grand challenge. By synthesizing these public datasets into a unified, multi-scale model, we can move beyond descriptive biology and build a predictive, mechanistic model of the synaptic engram, addressing a long-standing puzzle in neuroscience and providing a powerful new tool to investigate synaptic dysfunction in disease.

Research Questions And Hypotheses

The overarching goal of this Working Group is to construct and validate the first data-driven, multi-scale computational model of a mammalian glutamatergic synapse. This 'virtual synapse' will serve as a computational framework to investigate the molecular mechanisms of synaptic plasticity and dysfunction. By integrating structural, proteomic, imaging, and functional data, we will simulate the dynamic molecular events that underlie learning and memory at the single-synapse level. Our research is guided by four central questions, each leading to specific, testable hypotheses.

**Research Question 1: How do the copy number, spatial organization, and interaction networks of key synaptic proteins define the basal, resting state of a synapse and its capacity for plasticity?**
Before a synapse can change, it must have a stable baseline. The precise stoichiometry and arrangement of proteins in the postsynaptic density (PSD) are thought to determine its functional properties, such as the number of available AMPA receptors and the proximity of signaling molecules. We hypothesize that the basal state is not static but a dynamic steady-state maintained by a balance of protein turnover and transient interactions.
*   **Hypothesis 1 (H1):** The structural integrity and basal transmission properties of the synapse are emergent properties of the dense, multivalent interactions between scaffold proteins (e.g., PSD-95, Homer, SHANK) and their binding partners. The model will predict that the diffusion of key receptors like AMPARs is constrained within microdomains defined by this scaffold network. Validation will involve comparing simulated fluorescence recovery after photobleaching (FRAP) curves for AMPARs against published experimental data.

**Research Question 2: What is the precise spatio-temporal choreography of protein recruitment, modification, and trafficking that underlies the induction and stabilization of Long-Term Potentiation (LTP)?**
LTP induction triggers a massive influx of calcium, initiating a complex signaling cascade. While key players like CaMKII and AMPA receptor insertion are known, their coordinated dynamics within the crowded synaptic environment are poorly understood. 
*   **Hypothesis 2 (H2):** The initial phase of LTP stabilization is a cooperative process where CaMKII activation leads to both the phosphorylation of AMPARs (increasing their channel conductance) and the phosphorylation of scaffold proteins, creating new binding sites that trap newly exocytosed AMPARs in the PSD. Our model will predict a specific time course for these events and demonstrate that blocking either pathway (e.g., by simulating a kinase-dead CaMKII) will prevent stable potentiation. The expected outcome is a quantitative, dynamic map of the molecular cascade leading to a potentiated state.

**Research Question 3: What are the distinct molecular cascades that mediate Long-Term Depression (LTD), and how do they functionally oppose LTP at the systems level?**
LTD, a weakening of synaptic strength, is also calcium-dependent but is triggered by different stimulation patterns. It is thought to involve the activation of protein phosphatases and the endocytosis of AMPA receptors. We aim to model this process with the same level of detail as LTP to understand the bidirectional control of synaptic strength.
*   **Hypothesis 3 (H3):** The switch between LTP and LTD is determined by the amplitude and dynamics of the postsynaptic calcium signal. A low, prolonged calcium signal, as occurs during LTD induction, will preferentially activate calcineurin in our model, leading to the dephosphorylation of key substrates like GluA1 and the subsequent unbinding and endocytosis of AMPARs. The model will predict a net loss of surface AMPARs and a reduction in simulated synaptic current, consistent with experimental LTD. We will test this by systematically varying the parameters of the simulated calcium influx.

**Research Question 4: How do specific genetic mutations linked to neuropsychiatric disorders alter the molecular dynamics of plasticity and lead to predictable functional deficits?**
A key application of our model is to bridge the gap between genotype and phenotype for brain disorders. Many mutations associated with autism, schizophrenia, and intellectual disability occur in synaptic proteins.
*   **Hypothesis 4 (H4):** A haploinsufficiency of SHANK3, a major cause of Phelan-McDermid syndrome and autism, disrupts the structural integrity of the PSD scaffold, leading to an unstable synaptic state and impaired LTP. We will simulate this by reducing the copy number of SHANK3 agents by 50%. The model is predicted to show an inability to sustain potentiation following an LTP stimulus, as AMPARs fail to be stably trapped. This in silico result will provide a mechanistic explanation for the cognitive deficits observed in patients and mouse models, demonstrating the model's power as a 'disease-in-a-dish' platform.

Methods And Approach

This project is founded on the synthesis of heterogeneous public data into a cohesive, predictive model. Our approach is organized into three synergistic phases, executed by a transdisciplinary team of neuroscientists, structural biologists, computational biophysicists, and data scientists. The project's collaborative nature and reliance on large-scale data integration demonstrate a clear need for NCEMS support.

**Phase 1: Data Curation, Harmonization, and Integration (Months 1-9)**
This foundational phase involves aggregating and standardizing data from multiple public repositories to build a unified knowledge base for a canonical hippocampal CA1 pyramidal neuron synapse.
*   **Data Sources:**
    *   **Protein Composition & Stoichiometry:** We will use SynGO and SynDB as primary sources for a consensus list of synaptic proteins. Quantitative proteomics data from the literature will be mined to estimate the average copy number of key proteins per synapse.
    *   **Atomic Structures:** High-resolution structures of proteins and complexes (e.g., NMDA receptors, AMPA receptors, CaMKII, PSD-95) will be sourced from the Protein Data Bank (PDB) and the Electron Microscopy Data Bank (EMDB).
    *   **Synaptic Ultrastructure:** We will leverage publicly available cryo-electron tomography (cryo-ET) datasets from repositories like the EMPIAR to generate a realistic 3D mesh of the presynaptic terminal, synaptic cleft, and postsynaptic density, defining the simulation volume and organelle placement.
    *   **Protein Localization:** Super-resolution microscopy data (STORM/PALM) from publications and repositories, along with data from the Human Protein Atlas, will be used to generate 3D probability maps for the spatial distribution of key proteins within the synaptic compartments.
    *   **Reaction Kinetics & Functional Parameters:** Databases like SABIO-RK and Brenda will be mined for kinetic parameters of enzymatic reactions (e.g., phosphorylation, dephosphorylation). Electrophysiological data from the Allen Brain Atlas and NeuroMorpho.org will provide constraints for model validation, such as excitatory postsynaptic current (EPSC) amplitudes and decay kinetics.
*   **Integration Framework:** We will develop a graph database schema to link these disparate data types. Each protein will be a node with attributes including its UniProt ID, copy number, links to its PDB structures, its spatial probability map, and its known interactions and reaction kinetics. This creates a queryable, multi-scale representation of synaptic knowledge.

**Phase 2: Multi-Scale Model Construction and Simulation (Months 10-24)**
Using the integrated knowledge base, we will construct and simulate the 'virtual synapse' using a hybrid agent-based and reaction-diffusion modeling approach.
*   **Modeling Platform:** We will primarily use the MCell/CellBlender software suite, which is optimized for particle-based simulation of cellular microphysiology. This allows us to model individual protein molecules as 'agents' in a realistic 3D environment.
*   **Model Construction:** The 3D mesh from cryo-ET data will form the simulation environment. This volume will be populated with agents representing individual proteins, placed according to their copy numbers and spatial probability maps. Each agent will have defined properties: a 3D structure (for steric interactions), a diffusion coefficient, and specific reaction sites. Interaction rules (e.g., binding/unbinding rates, catalytic rates) will be parameterized using the curated kinetic data.
*   **Simulation Protocol:**
    1.  **Basal State:** The model will first be run without a stimulus to achieve a dynamic steady state. This baseline model will be validated by comparing simulated protein turnover and receptor mobility (e.g., simulated FRAP) with experimental data.
    2.  **Plasticity Induction:** LTP and LTD will be induced by simulating realistic calcium influx profiles through NMDA receptor agents, with parameters derived from electrophysiology data. This calcium signal will trigger the downstream reaction networks defined in our model.
    3.  **Data Analysis:** We will track key output variables over the course of the simulation, including the number and location of surface AMPA receptors, the phosphorylation state of critical proteins (CaMKII, AMPAR subunits), and the size and density of protein clusters in the PSD. These outputs provide a direct test of our hypotheses.

**Phase 3: In Silico Experiments and Hypothesis Testing (Months 25-36)**
With a validated model, we will perform systematic virtual experiments.
*   **Hypothesis Testing:** We will directly test hypotheses H1-H4 by manipulating the model. For H2, we will simulate LTP with and without CaMKII activity. For H4, we will reduce the SHANK3 agent copy number by 50% and compare the LTP simulation to the wild-type model.
*   **Sensitivity Analysis:** We will perform a global sensitivity analysis to determine which model parameters (e.g., a specific binding affinity, a protein's concentration) have the greatest influence on functional outputs like the magnitude of potentiation. This will identify critical molecular control points and generate novel predictions for wet-lab validation.

**Timeline and Milestones:**
*   **Year 1:** Completion of the integrated synaptic knowledge base (M9); Assembly of a static, structurally complete model of the synapse (M12).
*   **Year 2:** Validation of the dynamic basal state model (M18); First successful simulations of LTP and LTD induction (M24).
*   **Year 3:** Completion of systematic in silico experiments for H1-H4 (M30); Public release of the model, codebase, and a comprehensive user guide via a project web portal (M36).

**Open Science:** All curated data, models, simulation scripts, and analysis workflows will be version-controlled on a public GitHub repository and archived on Zenodo, ensuring full reproducibility and adherence to the highest standards of open science.

Expected Outcomes And Impact

This project will pioneer a new, synthesis-driven approach to one of the most fundamental questions in neuroscience: the molecular basis of memory. The expected outcomes will provide transformative contributions to the field, with broad impacts on basic science, translational research, and scientific training.

**Expected Outcomes:**
1.  **A Unified, Predictive Model of the Synapse:** The primary outcome will be the first-of-its-kind, multi-scale computational model of a mammalian excitatory synapse. This 'virtual synapse' will be more than a static representation; it will be a dynamic, predictive engine capable of simulating complex biological processes. It will represent a paradigm shift from studying individual components in isolation to understanding the synapse as an integrated, emergent system. This model will be made publicly available as a community resource.
2.  **Mechanistic Insights into Synaptic Plasticity:** Our simulations will provide an unprecedented, four-dimensional view of the molecular choreography underlying LTP and LTD. We will generate dynamic maps of protein trafficking, post-translational modifications, and structural rearrangements that are currently impossible to capture experimentally. This will allow us to move beyond correlational studies and test causal relationships, for example, determining if a specific phosphorylation event is necessary and sufficient for the stabilization of a memory trace.
3.  **A Framework for Understanding Synaptopathies:** The model will serve as a powerful 'in silico' platform for investigating how genetic mutations lead to synaptic dysfunction. By simulating the effects of disease-associated mutations (e.g., in SHANK3, GRIN2B, FMR1), we will provide a mechanistic bridge from a molecular defect to a predictable functional deficit (e.g., impaired LTP). This will generate novel hypotheses about the pathophysiology of disorders like autism, schizophrenia, and Alzheimer's disease.

**Broader Impacts and Applications:**
*   **Accelerating Neuroscience Research:** The virtual synapse will be a powerful hypothesis-generating tool. Researchers worldwide will be able to use our open-access platform to perform virtual experiments—testing the potential role of a newly discovered protein, predicting the effect of a drug, or exploring the consequences of a genetic variant—thus guiding and prioritizing costly and time-consuming wet-lab experiments.
*   **Catalyzing Cross-Disciplinary Collaboration:** This project, by its very nature, fosters collaboration between computational and experimental biologists. The model's predictions will spur new experimental work, while new experimental data can be used to refine and expand the model, creating a virtuous cycle of discovery. This aligns perfectly with the NCEMS mission to stimulate cross-disciplinary science.
*   **Training the Next Generation of Scientists:** Graduate students and postdoctoral fellows involved in this project will receive unique, transdisciplinary training at the intersection of neuroscience, computational biophysics, and data science. They will become fluent in data synthesis, computational modeling, and open science practices, creating the data-savvy workforce essential for 21st-century biology.
*   **Educational Resource:** The model and its visualizations will be a powerful educational tool for teaching the complexities of synaptic function to students at all levels, from undergraduate to graduate school.

**Dissemination Plan:**
Our dissemination strategy is designed for maximum impact and community engagement. We will publish our primary findings in high-impact journals such as *Cell*, *Neuron*, or *Nature Neuroscience*. Methodological advancements will be published in specialized journals like *PLoS Computational Biology*. We will present our work at major international conferences (e.g., Society for Neuroscience, FENS Forum, Biophysical Society). Crucially, all data, models, and code will be shared via a dedicated project web portal and public repositories (GitHub, Zenodo). We will host annual workshops and online tutorials to train the wider community in using our tools, ensuring the project's legacy and utility far beyond the funding period.

**Long-Term Vision:** This project lays the foundation for a comprehensive 'digital twin' of a neural circuit. In the long term, we envision expanding the model to include inhibitory synapses, glial interactions, and neuromodulatory influences. By connecting multiple virtual synapses, we can begin to simulate the emergent properties of microcircuits, providing a seamless link from molecules to cognition.

Budget And Resources

The proposed research represents a community-scale synthesis project that is beyond the scope and capabilities of any single research laboratory. It requires the deep integration of diverse data types—from atomic structures to systems-level electrophysiology—and the combined expertise of a multidisciplinary team spanning structural biology, proteomics, computational modeling, and systems neuroscience. The need for dedicated personnel to manage this complex data integration, develop a novel multi-scale modeling framework, and facilitate continuous collaboration across geographically distributed institutions makes this project ideally suited for and dependent upon the support and resources provided by the NCEMS.

**Budget Breakdown (3-Year Total):**

**1. Personnel: $540,000**
*   **Postdoctoral Fellow 1 (Data Integration & Bioinformatics):** $75,000/year salary + fringe. This individual will lead the effort to mine, curate, and harmonize the diverse public datasets, and will manage the integrated knowledge base. This role is critical for the project's foundation.
*   **Postdoctoral Fellow 2 (Computational Modeling & Simulation):** $75,000/year salary + fringe. This fellow will be responsible for constructing the virtual synapse model, running the large-scale simulations on HPC resources, and analyzing the results. This is the core modeling role.
*   **Graduate Students (2):** $40,000/year stipend + tuition for two students. The students will be mentored by the PIs and postdocs, receiving hands-on training in data synthesis and computational neuroscience. They will contribute to specific modules of the project, such as parameterizing protein interactions or analyzing simulation outputs, directly fulfilling the call's training mission.
*   **Summer Salary for PIs:** 1 month/year for 3 PIs to provide dedicated oversight, lead collaborative meetings, and contribute to manuscript preparation.

**2. Travel: $45,000**
*   **Annual Working Group Meetings:** $10,000/year. To facilitate deep collaboration, the entire team (PIs, postdocs, students) will convene for a 3-day intensive workshop annually. This is essential for resolving interdisciplinary challenges and planning subsequent research phases.
*   **Conference Travel:** $5,000/year. To support the dissemination of our findings at major national and international conferences (e.g., Society for Neuroscience) by trainees and PIs.

**3. Computational Resources: $60,000**
*   **High-Performance Computing (HPC):** $15,000/year. The agent-based simulations of a crowded synaptic environment are computationally expensive. This allocation will cover access to a national supercomputing center or cloud computing credits (e.g., AWS, Google Cloud) required to run thousands of simulation hours for parameter sweeps and hypothesis testing.
*   **Data Storage and Web Hosting:** $5,000/year. For robust storage of the large integrated database and simulation outputs, and for hosting the public-facing project web portal.

**4. Publication and Dissemination: $15,000**
*   Funds are requested to cover open-access publication fees for an anticipated 3-4 major publications, ensuring our findings are freely accessible in accordance with open science principles.

**5. Indirect Costs (F&A):** Calculated at the lead institution's federally negotiated rate of 50% on modified total direct costs.

**Total Direct Costs:** $660,000
**Total Indirect Costs:** ~$300,000
**Total Requested Budget:** ~$960,000

This budget is structured to directly support the collaborative and data-intensive nature of the project, with a strong emphasis on training the next generation of data-savvy scientists. The resources requested are essential for achieving the ambitious goals of synthesizing a vast body of public knowledge into a transformative new tool for the molecular and cellular biosciences.",,
ai_single_gemini_10,ai,single,gemini-2.5-pro,Emergent Metabolism of the Microbiome: Uncovering the Rules of Community Assembly and Function,"Microbial communities exhibit collective metabolic capabilities far exceeding the sum of their individual members. These emergent functions are critical for biogeochemical cycles and host health. However, the principles that govern how species interactions lead to stable, functional communities remain poorly understood. This Working Group will synthesize vast public metagenomic, metatranscriptomic, and metabolomic data to uncover the 'rules of life' for microbial consortia. We will leverage data from large-scale initiatives like the Human Microbiome Project and the Earth Microbiome Project, alongside thousands of curated genome-scale metabolic models (GEMs) from databases like KBase and BiGG. Our team, composed of microbial ecologists, systems biologists, bioinformaticians, and ecological modelers, will develop a novel computational pipeline. This pipeline will first reconstruct community composition and metabolic potential from metagenomes, then use metatranscriptomic data to constrain metabolic flux, and finally predict emergent metabolic outputs and cross-feeding interactions. By applying this framework across thousands of diverse microbial communities from different environments (e.g., gut, soil, ocean), we will identify conserved patterns of metabolic interdependence, competition, and niche partitioning. The goal is to derive a set of generalizable principles that can predict community stability and function from genomic data alone. This work will lay the foundation for the rational design of synthetic microbial consortia for applications in biotechnology, environmental remediation, and medicine.",,"Background And Significance

Microbial communities are the invisible engines of our planet, driving global biogeochemical cycles and fundamentally shaping the health of their hosts. The collective metabolic activity of these communities is a classic example of an emergent phenomenon, where the whole is far greater than the sum of its parts. A community can degrade complex substrates, detoxify environments, or produce essential metabolites that no single member could manage alone. This functional emergence arises from a complex web of metabolic interactions, including competition for resources, syntrophic cross-feeding of intermediates, and the partitioning of metabolic pathways across different species. Understanding the principles that govern these interactions is one of the grand challenges in modern biology. A predictive understanding would unlock the potential to engineer microbial communities for applications in personalized medicine, sustainable agriculture, and industrial biotechnology.

The past two decades have witnessed a data revolution in microbiology. Large-scale sequencing initiatives like the Human Microbiome Project (HMP), the Earth Microbiome Project (EMP), and the Tara Oceans expedition have generated petabytes of publicly available metagenomic and metatranscriptomic data, providing an unprecedented snapshot of microbial diversity and genetic potential across Earth's biomes. Concurrently, the field of systems biology has matured, particularly in the area of genome-scale metabolic modeling (GEMs). GEMs are mathematical representations of an organism's entire metabolic network, capable of predicting growth rates and metabolic fluxes under defined environmental conditions. Thousands of high-quality, curated GEMs are now available in public repositories like BiGG and KBase, and methods exist to automatically generate models from genomic data.

Despite these parallel advances, a significant gap remains in our ability to connect genomic potential to emergent community function. Early studies primarily focused on correlating taxonomic composition with environmental variables or host phenotype, offering limited mechanistic insight. More recent approaches have attempted to bridge this gap. For instance, community-level metabolic modeling frameworks like COMETS and MICOM have shown promise in simulating simple consortia by combining individual GEMs. These studies have successfully predicted cross-feeding interactions and community growth dynamics in vitro. However, they face significant challenges when scaling to the complexity of natural communities, which can contain hundreds or thousands of species. Furthermore, most models rely solely on genomic data, which represents metabolic potential rather than actual activity. The integration of metatranscriptomic data, which reflects gene expression and thus active metabolic pathways, is crucial for accurate functional prediction but remains a formidable technical hurdle.

Consequently, we lack a generalizable, data-driven framework to decipher the 'rules of life' for microbial community assembly and function. Current knowledge is often ecosystem-specific, and the fundamental principles of metabolic network topology, niche partitioning, and functional redundancy that confer stability and resilience across diverse environments are poorly understood. We do not know if there are universal patterns of metabolic interdependence, or how these patterns are shaped by environmental pressures. This proposal addresses this critical knowledge gap. The research is exceptionally timely, as it leverages the confluence of massive public datasets and mature modeling techniques. By synthesizing these disparate resources, this Working Group will move beyond descriptive studies to build a predictive, mechanistic framework. This project is perfectly aligned with the NCEMS mission, as it requires a large-scale, multidisciplinary effort to synthesize public data to answer a fundamental question about emergence in cellular biosciences—an endeavor far beyond the capacity of any single research laboratory.

Research Questions And Hypotheses

The overarching goal of this Working Group is to synthesize public multi-omics data and metabolic models to derive a set of generalizable principles that govern the assembly, stability, and metabolic function of microbial communities. We will address this goal through three specific, interconnected research questions, each with testable hypotheses.

**Research Question 1: What are the conserved patterns of metabolic interdependence and competition that structure microbial communities across diverse environments?**
This question targets the fundamental nature of species interactions. We hypothesize that despite vast taxonomic and environmental diversity, the underlying metabolic interaction networks are constrained and exhibit conserved topological features.
*   **Hypothesis 1a: Metabolic complementarity and syntrophy are primary drivers of stable species co-existence.** We predict that stable communities will be enriched in metabolic 'handoffs' where the waste product of one species is the substrate for another. We further predict that these interactions will form conserved network motifs (e.g., obligate producer-consumer pairs for essential amino acids or vitamins, short metabolic cycles) that are statistically overrepresented in communities from diverse environments (gut, soil, ocean).
*   **Hypothesis 1b: Metabolic niche overlap is a primary driver of competitive exclusion and dictates community composition.** We predict that the degree of overlap in the predicted substrate utilization profiles (the 'metabolic niche') between any two species will be inversely correlated with their frequency of co-occurrence in real-world samples, after controlling for environmental factors and phylogenetic relationships. Species that persist together will exhibit significant niche partitioning.
*   **Validation:** We will test these hypotheses by first constructing thousands of community-specific metabolic interaction networks using our proposed pipeline. For H1a, we will use network analysis algorithms to identify and count recurring motifs and correlate their prevalence with community stability metrics (e.g., low temporal variance in longitudinal datasets). For H1b, we will compute a metabolic niche overlap score for all species pairs and use statistical models to test for a significant negative relationship with co-occurrence data from the EMP and HMP datasets.

**Research Question 2: How does the integration of metatranscriptomic data refine predictions of community metabolic function compared to predictions based on metagenomic potential alone?**
This question addresses the critical gap between genetic potential and in-situ activity. We hypothesize that incorporating gene expression data is essential for accurate functional prediction.
*   **Hypothesis 2a: Transcriptomic constraints significantly improve the accuracy of predicted metabolic outputs.** We predict that community metabolic models constrained by metatranscriptomic data will yield metabolite production profiles that correlate significantly better with experimentally measured metabolomic data than models based on genomic potential alone.
*   **Hypothesis 2b: Transcriptional regulation facilitates dynamic niche partitioning among species with similar genomic capabilities.** We predict that co-occurring, closely related species will show divergent expression profiles for metabolic pathways, effectively partitioning the available resources in real-time, a phenomenon invisible from a purely genomic perspective.
*   **Validation:** We will leverage datasets with paired metagenomic, metatranscriptomic, and metabolomic data (e.g., HMP2 IBDMDB). For H2a, we will compare the Pearson correlation between predicted and measured metabolite concentrations for models run with and without transcriptomic constraints. For H2b, we will quantify the functional redundancy between species pairs based on genomics and then on transcriptomics, predicting a significant reduction in redundancy when expression is considered.

**Research Question 3: Can a minimal set of 'assembly rules' predict the stability and primary metabolic function of a community from its constituent genomes and their interactions?**
This is our ultimate synthesis goal: to distill our complex findings into predictive principles.
*   **Hypothesis 3a: Community stability and function are predictable from a combination of genomic features and predicted metabolic network topology.** We predict that a machine learning model, trained on features such as species' metabolic capabilities, the density of cooperative interactions, and the degree of niche overlap, can accurately classify communities as stable/unstable or as high/low producers of key functional metabolites (e.g., short-chain fatty acids).
*   **Hypothesis 3b: Functional stability is maintained by metabolic role redundancy, not necessarily taxonomic redundancy.** We predict that the key features identified by our machine learning model will relate to the presence of core metabolic roles (e.g., 'primary fermenter', 'vitamin producer') that can be filled by taxonomically distinct organisms. Communities with diverse taxa filling these same core roles will exhibit similar stability.
*   **Validation:** We will train supervised machine learning models (e.g., Random Forest, Gradient Boosting) using the vast dataset of community features generated in RQ1 and RQ2. H3a will be validated using rigorous cross-validation and testing on held-out datasets from distinct environments. For H3b, we will use feature importance metrics (e.g., SHAP values) to identify the key metabolic roles and then test whether communities with different taxonomic compositions but similar 'role compositions' have similar stability profiles.

Methods And Approach

This project's success hinges on a transdisciplinary Working Group and a novel, robust computational pipeline designed for large-scale data synthesis. Our team comprises microbial ecologists, systems biologists, bioinformaticians, and data scientists, ensuring the necessary expertise for every project phase. The work is structured into three sequential but overlapping phases.

**Phase 1: Data Aggregation, Curation, and Processing (Months 1-9)**
This foundational phase involves assembling a massive, harmonized multi-omics dataset. 
*   **Data Sources:** We will exclusively use publicly available data. Primary sources include metagenomes and metatranscriptomes from the Human Microbiome Project (HMP1, HMP2), the Earth Microbiome Project (EMP500), the Tara Oceans project, and other large-scale studies available on NCBI's Sequence Read Archive (SRA). We will target an initial cohort of over 5,000 metagenomes and 1,000 metatranscriptomes with rich environmental metadata. For validation, we will use paired metabolomics data from sources like the HMP2 Inflammatory Bowel Disease Multi-omics Database (IBDMDB) and the MetaboLights repository.
*   **Standardized Processing:** To ensure comparability, all raw sequencing data will be processed through a single, containerized (Docker/Singularity) pipeline. This includes: (1) Quality control and adapter trimming using KneadData; (2) Metagenomic assembly using MEGAHIT; (3) Binning of metagenome-assembled genomes (MAGs) using MetaBAT2; (4) Taxonomic classification of MAGs and unassembled reads using GTDB-Tk; and (5) Functional annotation using Prokka and eggNOG-mapper.

**Phase 2: Community-Scale Metabolic Modeling and Interaction Prediction (Months 6-24)**
This is the core analytical phase where we translate genomic data into metabolic function.
*   **Genome-Scale Model (GEM) Reconstruction:** For each sample, we will create a collection of GEMs representing the community members. We will first retrieve high-quality, manually curated models for species present in the sample from databases like BiGG and AGORA. For MAGs and species without existing models, we will generate them de novo using the CarveMe pipeline, which has been shown to produce high-quality models.
*   **Community Simulation:** We will use the MICOM (Microbial Community Modeling) framework to simulate community metabolism. MICOM applies flux balance analysis (FBA) to a combined community model, using a cooperative trade-off objective function that maximizes community growth rate while allowing for individual organism optimization. This approach has been validated for predicting species interactions and growth rates. The inputs will be the collection of GEMs and a defined in-silico growth medium based on the sample's environment (e.g., high-fiber diet for gut, marine nutrient profile for ocean).
*   **Transcriptomic Integration:** For samples with metatranscriptomic data, we will integrate gene expression levels as constraints on the metabolic models. Reads will be mapped to the coding sequences within the GEMs, and the resulting expression levels will be used to constrain the maximum allowable flux through each corresponding reaction. We will employ established algorithms like GIMME or iMAT, which selectively activate reactions based on expression evidence, thus tailoring the metabolic network to its active state.
*   **Interaction Network Inference:** From the solved FBA models, we will explicitly calculate the flux of every metabolite exchanged between every pair of organisms. This will generate a directed, weighted metabolic interaction network for each of the thousands of communities. The nodes are species, and the edges represent metabolite exchange, weighted by flux rate.

**Phase 3: Network Analysis, Pattern Mining, and Rule Discovery (Months 18-36)**
In this final phase, we will synthesize the results from thousands of simulations to uncover general principles.
*   **Cross-Community Network Analysis:** We will apply algorithms from network science to our database of interaction networks. We will identify conserved topological features, network motifs (e.g., three-species food chains), and community roles (e.g., 'keystone' producers). We will use statistical methods to determine which features are significantly enriched in specific environments or are associated with community stability.
*   **Predictive Modeling:** To derive 'assembly rules', we will employ supervised machine learning. Features for the models will include genomic properties of community members (e.g., genome size, pathway completeness), predicted network properties (e.g., density of cooperation, niche overlap), and environmental metadata. The target variables will be community-level outcomes like stability (for longitudinal data) or functional output (e.g., butyrate production level). We will use models like Random Forest and Gradient Boosting and interpret them using SHAP (SHapley Additive exPlanations) to identify the most predictive features, which we will frame as 'rules'.

**Timeline and Milestones:**
*   **Year 1:** Complete Phase 1. Develop and validate the full computational pipeline on the HMP dataset. Milestone: Public release of the open-source pipeline and processed data for 1,000 samples.
*   **Year 2:** Complete large-scale model construction and simulation for all datasets (Phase 2). Begin network analysis (Phase 3). Milestone: A comprehensive, public database of predicted metabolic interaction networks for >5,000 communities. Host a mid-project Working Group workshop.
*   **Year 3:** Complete machine learning and rule discovery. Validate predictions against metabolomic data. Prepare manuscripts and dissemination materials. Milestone: Publication of key findings in a high-impact journal and launch of a web portal for data exploration. Host a final dissemination workshop.

Expected Outcomes And Impact

This project will generate transformative outcomes that will significantly advance the fields of microbial ecology, systems biology, and bioinformatics. The impact will extend beyond basic science, providing a foundation for novel applications in medicine, biotechnology, and environmental management.

**Intellectual Merit and Contributions to the Field:**
1.  **A Generalizable Predictive Framework:** The primary outcome will be a novel, open-source computational pipeline for moving from raw multi-omics data to predictive, mechanistic models of microbial community function. This framework, which uniquely integrates metagenomic potential with metatranscriptomic constraints at a massive scale, will become a standard tool for the research community, enabling a paradigm shift from descriptive to predictive microbiome science.
2.  **A Comprehensive Atlas of Microbial Interactions:** We will produce an unprecedented public resource: a database of thousands of predicted metabolic interaction networks from diverse ecosystems worldwide. This 'atlas' will allow researchers to explore the metabolic roles of uncultured organisms, identify keystone species, and generate new, testable hypotheses about microbial community structure. It will serve as a foundational resource for the field for years to come.
3.  **Discovery of Fundamental 'Rules of Life':** By synthesizing across this vast dataset, we expect to uncover generalizable principles—the 'rules'—of microbial community assembly. These may include identifying conserved metabolic dependencies that are essential for community stability, quantifying the trade-offs between competition and cooperation, and defining the principles of functional redundancy. Such fundamental insights into emergent behavior are a core goal of modern biology and directly address the NCEMS mission.

**Broader Impacts and Applications:**
1.  **Human Health and Medicine:** The principles derived from studying the human microbiome will provide a mechanistic basis for understanding diseases like Inflammatory Bowel Disease, obesity, and malnutrition. This knowledge is a critical prerequisite for the rational design of next-generation probiotics, prebiotics, and personalized dietary interventions aimed at modulating the gut microbiome for therapeutic benefit.
2.  **Biotechnology and Bioengineering:** The ability to predict the metabolic output of a microbial consortium from its members' genomes will revolutionize synthetic biology. Our framework will provide a design-build-test cycle for engineering stable, functional consortia for producing biofuels, pharmaceuticals, and other high-value chemicals, or for bioremediating environmental pollutants.
3.  **Environmental Science and Agriculture:** Understanding the metabolic networks in soil and marine microbiomes is crucial for predicting their response to climate change and for managing ecosystem health. Our findings could inform strategies for carbon sequestration, improve nutrient cycling in agricultural soils to reduce fertilizer use, and enhance the resilience of critical ecosystems.

**Dissemination, Data Sharing, and Training:**
*   **Open Science:** We are fully committed to open science principles. All software developed will be released under a permissive open-source license on GitHub. All processed data and derived results (e.g., the interaction network database) will be made publicly available through repositories like Zenodo and a dedicated, user-friendly web portal.
*   **Dissemination Strategy:** We will publish our findings in high-impact, peer-reviewed journals (e.g., Nature, Science, Cell Systems). We will also present our work at major international conferences (e.g., ISME, ASM Microbe) to engage with the broader scientific community. We will host a final workshop, open to the public, to disseminate our findings and provide hands-on training for our computational pipeline.
*   **Training the Next Generation:** This project is an ideal training environment. Graduate students and postdoctoral fellows will be at the heart of this collaborative effort, receiving cross-disciplinary training in computational biology, data science, systems biology, and microbial ecology. They will gain invaluable experience in large-scale collaborative science, preparing them to be leaders in the future data-savvy workforce.

**Long-Term Vision:** The establishment of this Working Group and the resources it creates will build a lasting collaborative network. The hypotheses generated by our synthesis work will fuel a new generation of experimental studies to validate these 'rules of life' in vitro and in vivo, creating a virtuous cycle between computational prediction and experimental validation that will propel the field forward for the next decade.

Budget And Resources

The proposed research represents a community-scale synthesis project that is ambitious in scope, computational intensity, and its requirement for diverse, integrated expertise. As such, it is beyond the capabilities of a single research lab or a standard collaborative grant and is uniquely suited for the support and structure provided by the NCEMS program.

**Justification for NCEMS Support:**
*   **Scale of Synthesis:** The project involves the aggregation, standardized processing, and analysis of petabytes of public data from thousands of samples. This requires a coordinated effort in data management and high-performance computing that exceeds the resources of individual institutions.
*   **Transdisciplinary Expertise:** Success requires the deep integration of knowledge from microbial ecology, systems biology, bioinformatics, and machine learning. The NCEMS Working Group model is the ideal mechanism to bring together leading experts from these disparate fields and foster the sustained collaboration needed to solve this complex problem.
*   **Need for Collaborative Infrastructure:** NCEMS support is critical for funding the essential personnel (especially postdoctoral scholars who will bridge the different labs), the significant computational resources, and the in-person meetings and workshops that are the lifeblood of a successful collaborative synthesis project.

**Detailed Budget Breakdown (3-Year Total Estimate: $1,500,000):**

1.  **Personnel ($1,050,000 - 70%):** This is the largest cost category, reflecting the project's reliance on dedicated, highly skilled researchers.
    *   **Postdoctoral Scholars (3 FTEs):** $70,000/year salary + 30% fringe per postdoc. Total: ~$819,000. One postdoc will be based at each of the three lead PI institutions, specializing in (i) bioinformatics pipeline development, (ii) metabolic modeling and simulation, and (iii) ecological statistics and machine learning, respectively. They will be co-mentored to ensure cross-training.
    *   **Graduate Students (3 students, 50% support):** $35,000/year stipend + tuition remission per student. Total: ~$180,000. Students will support the postdocs and lead specific sub-projects.
    *   **Faculty Summer Salary (3 PIs, 1 month/year):** To allow PIs to dedicate focused time to project management and analysis. Total: ~$51,000.

2.  **Travel ($150,000 - 10%):** Essential for fostering collaboration and disseminating results.
    *   **Annual Working Group Meetings:** $30,000 per year for all 10+ members (PIs, postdocs, students) to meet for a 3-day intensive workshop. Total: $90,000.
    *   **Conference Travel:** $20,000 per year for trainees to present findings at major international conferences (e.g., ISME, ASM). Total: $60,000.

3.  **Computational Resources ($150,000 - 10%):** A critical need for this data-intensive project.
    *   **Cloud Computing Credits (AWS/Google Cloud):** $40,000 per year for large-scale metagenomic assembly, mapping, and running thousands of computationally expensive FBA simulations. Total: $120,000.
    *   **Data Storage and Archiving:** $10,000 per year for long-term storage of raw and processed data. Total: $30,000.

4.  **Publication and Dissemination ($75,000 - 5%):** To ensure broad impact and adherence to open science.
    *   **Open Access Publication Fees:** $5,000 per article for an estimated 10 articles. Total: $50,000.
    *   **Web Portal Development and Hosting:** $25,000 for professional development and 3-year hosting of an interactive data portal.

5.  **Materials and Supplies ($75,000 - 5%):**
    *   **High-performance workstations and software licenses:** For PIs and postdocs. Total: $75,000.

**Resource Management:** The lead PI's institution will manage the overall budget. A steering committee comprising all PIs will meet monthly to review progress and resource allocation. Computational resources will be managed as a shared pool accessible to all group members. This structure ensures efficient use of funds and promotes a truly collaborative research environment.",,
ai_single_gpt_01,ai,single,gpt-4,Synthesis of Molecular Data for Cancer Cell Evolution,"This research aims to synthesize existing molecular and cellular data to understand the evolution of cancer cells. By integrating diverse datasets, we will address the long-standing puzzle of how cancer cells evolve and adapt to different environments. This project will require collaboration between oncologists, molecular biologists, and data scientists. The findings will be made publicly available, promoting open science principles and providing a valuable resource for future cancer research.",,"Background And Significance

Cancer, a leading cause of death worldwide, is a complex disease characterized by uncontrolled cell growth and proliferation. The evolution of cancer cells, their adaptation to different environments, and the mechanisms underlying these processes are still not fully understood. Current research in the field is fragmented, with different labs focusing on specific aspects of cancer cell evolution. However, a comprehensive, integrated understanding of this process is lacking. This research aims to fill this gap by synthesizing existing molecular and cellular data to provide a holistic view of cancer cell evolution. This is important and timely as it will not only advance our understanding of cancer biology but also inform the development of more effective cancer treatments.

Research Questions And Hypotheses

The research questions to be addressed in this project include: 1) How do cancer cells evolve and adapt to different environments? 2) What are the molecular and cellular mechanisms underlying this process? 3) How can this knowledge be used to develop more effective cancer treatments? The hypotheses to be tested include: 1) Cancer cells evolve and adapt to different environments through specific molecular and cellular changes. 2) These changes can be identified and characterized through the synthesis of existing molecular and cellular data. The expected outcomes include a comprehensive understanding of cancer cell evolution and the development of a publicly available resource for future cancer research.

Methods And Approach

This project will involve the synthesis of existing molecular and cellular data from publicly available databases such as The Cancer Genome Atlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. The data will be analyzed using advanced computational approaches including machine learning and network analysis. The project will be carried out in collaboration with oncologists, molecular biologists, and data scientists. The timeline for the project is three years, with specific milestones including data collection and preprocessing (Year 1), data analysis and interpretation (Year 2), and dissemination of findings (Year 3).

Expected Outcomes And Impact

The expected outcomes of this project include a comprehensive understanding of cancer cell evolution and the development of a publicly available resource for future cancer research. The broader impacts include advancing our understanding of cancer biology, informing the development of more effective cancer treatments, and promoting open science principles. The project also has the potential to stimulate further research and collaborations in the field of cancer biology.

Budget And Resources

The budget for this project includes costs for data access and storage, computational resources, personnel salaries, and dissemination of findings. The total budget is estimated to be $500,000, with $200,000 allocated for data access and storage, $100,000 for computational resources, $150,000 for personnel salaries, and $50,000 for dissemination of findings. The resources required for this project include access to publicly available molecular and cellular data, computational resources for data analysis, and a multidisciplinary team of oncologists, molecular biologists, and data scientists.",,
ai_single_gpt_02,ai,single,gpt-4,Cross-Disciplinary Approach to Neurodegenerative Diseases,"This project proposes to synthesize publicly available data to address fundamental questions related to the molecular and cellular mechanisms underlying neurodegenerative diseases. By bringing together researchers from neuroscience, molecular biology, and bioinformatics, we aim to develop innovative research strategies and analytical methods to understand the pathogenesis of these diseases. The project will also provide training opportunities for graduate students and postdocs, fostering the next generation of data-savvy scientists.",,"Background And Significance

Neurodegenerative diseases, including Alzheimer's, Parkinson's, and Huntington's, are a major global health concern, affecting millions of people worldwide. Despite significant advances in our understanding of these diseases, their molecular and cellular mechanisms remain largely elusive. This is due, in part, to the complexity of the nervous system and the multifactorial nature of these diseases. Current research has identified several key players in neurodegeneration, including protein misfolding, oxidative stress, mitochondrial dysfunction, and neuroinflammation. However, how these factors interact and contribute to disease progression is not well understood. Furthermore, there is a lack of effective treatments for most neurodegenerative diseases, highlighting the urgent need for more research in this area. This project aims to address these gaps by synthesizing publicly available data from diverse sources to gain new insights into the molecular and cellular mechanisms of neurodegenerative diseases. By leveraging the power of data synthesis and cross-disciplinary collaboration, we hope to develop innovative research strategies and analytical methods that will advance our understanding of these devastating diseases.

Research Questions And Hypotheses

This project will address several key research questions: 1) What are the common and distinct molecular and cellular mechanisms underlying different neurodegenerative diseases? 2) How do these mechanisms interact and contribute to disease progression? 3) Can we identify novel therapeutic targets for these diseases? Based on current knowledge, we hypothesize that neurodegenerative diseases share common molecular and cellular pathways, but also have disease-specific mechanisms. We predict that by integrating and analyzing data from diverse sources, we will be able to identify these common and distinct mechanisms, and uncover novel therapeutic targets. The expected outcomes of this project include a comprehensive map of the molecular and cellular mechanisms of neurodegenerative diseases, and a list of potential therapeutic targets for further investigation.

Methods And Approach

We will use a variety of publicly available data sources, including genomic, transcriptomic, proteomic, and metabolomic datasets from patients with neurodegenerative diseases. These datasets will be integrated and analyzed using advanced bioinformatics tools and machine learning algorithms to identify common and distinct molecular and cellular mechanisms. We will also develop novel analytical methods to handle the complexity and heterogeneity of the data. The project will be carried out in three phases: data collection and integration, data analysis and method development, and result interpretation and validation. Each phase will have specific milestones and deliverables, and will be closely monitored to ensure progress and quality. Statistical analysis will be performed using appropriate methods, and all results will be validated using independent datasets.

Expected Outcomes And Impact

This project is expected to make significant contributions to the field of neurodegenerative diseases. By synthesizing and analyzing data from diverse sources, we will gain new insights into the molecular and cellular mechanisms of these diseases, and identify potential therapeutic targets. The project will also develop innovative research strategies and analytical methods that can be applied to other complex diseases. Furthermore, by bringing together researchers from different fields, the project will foster cross-disciplinary collaboration and train the next generation of data-savvy scientists. The results of the project will be disseminated through peer-reviewed publications, conference presentations, and open-access data repositories. The project also has the potential to stimulate further research and collaborations in the field.

Budget And Resources

The budget for this project will cover personnel costs (including salaries for researchers, graduate students, and postdocs), data acquisition and analysis costs, software and hardware costs, and overhead costs. We will also allocate funds for training and professional development of the team members. The project will leverage existing resources and infrastructure at our institutions, including high-performance computing facilities and bioinformatics tools. We will also seek collaborations with other research groups and institutions to access additional resources and expertise.",,
ai_single_gpt_03,ai,single,gpt-4,Integrative Analysis of Genomic Data for Precision Medicine,"This research aims to integrate existing genomic data to develop innovative strategies for precision medicine. By synthesizing diverse datasets, we will address novel questions related to the genetic basis of individual differences in disease susceptibility and drug response. This project will require collaboration between geneticists, pharmacologists, and data scientists. The findings will be made publicly available, promoting open science principles and contributing to the advancement of precision medicine.",,"Background And Significance

Precision medicine, a rapidly evolving field, aims to tailor medical treatment to the individual characteristics of each patient. It is based on the understanding that individual variability in genes, environment, and lifestyle affects disease susceptibility and drug response. Despite the significant progress in genomics and bioinformatics, the integration of genomic data to develop precision medicine strategies remains a challenge. This research is timely and significant as it addresses this gap by synthesizing diverse genomic datasets to understand the genetic basis of individual differences in disease susceptibility and drug response. The current state of the field is characterized by a wealth of genomic data, but a lack of comprehensive, integrative analysis. Our research will leverage existing data, bringing together geneticists, pharmacologists, and data scientists to develop innovative strategies for precision medicine.

Research Questions And Hypotheses

This research will address the following questions: 1) What are the genetic variants associated with individual differences in disease susceptibility? 2) How do these genetic variants influence drug response? We hypothesize that certain genetic variants are associated with individual differences in disease susceptibility and drug response. We predict that by integrating diverse genomic datasets, we can identify these variants and understand their functional implications. The expected outcomes include a comprehensive list of genetic variants associated with disease susceptibility and drug response, and innovative strategies for precision medicine. These hypotheses will be tested and validated through rigorous data analysis and computational modeling.

Methods And Approach

We will use publicly available genomic datasets from databases such as the 1000 Genomes Project, the Cancer Genome Atlas, and the Pharmacogenomics Knowledgebase. These datasets will be integrated using advanced computational approaches, including machine learning algorithms and network-based methods. The project will be carried out in three phases: data collection and preprocessing, data integration and analysis, and validation and interpretation. The timeline for the project is three years, with specific milestones and deliverables at each phase. Statistical analysis will be performed using R and Python, and the results will be validated through cross-validation and independent datasets.

Expected Outcomes And Impact

This research will contribute to the field of precision medicine by providing a comprehensive understanding of the genetic basis of individual differences in disease susceptibility and drug response. It will also develop innovative strategies for precision medicine, potentially leading to personalized treatment plans. The findings will be disseminated through peer-reviewed publications and presentations at scientific conferences. The project will also provide training opportunities for graduate students and postdocs, promoting the development of a data-savvy workforce. In the long term, we envision that this research will stimulate further research and collaborations in precision medicine, contributing to its advancement and implementation in clinical practice.

Budget And Resources

The budget for this project is estimated to be $500,000, which will be used for personnel salaries, computational resources, and dissemination of results. The project will require a team of geneticists, pharmacologists, and data scientists, as well as high-performance computing resources for data analysis. The budget will also cover the costs of publishing the results in open-access journals and presenting the findings at scientific conferences. The project will leverage existing resources, including publicly available genomic datasets and computational tools, to maximize the efficiency and impact of the research.",,
ai_single_gpt_04,ai,single,gpt-4,Data-Driven Insights into Microbial Communities,"This project proposes to synthesize publicly available data to gain novel insights into the structure and function of microbial communities. By integrating data from metagenomics, transcriptomics, and proteomics, we aim to solve long-standing puzzles in microbial ecology. This project will require collaboration between microbiologists, bioinformaticians, and data scientists. The findings will be made publicly available, promoting open science principles and contributing to our understanding of microbial ecosystems.",,"Background And Significance

Microbial communities play a crucial role in various ecosystems, contributing to nutrient cycling, disease suppression, and environmental sustainability. Despite their significance, our understanding of microbial communities' structure and function remains limited due to their complexity and diversity. Recent advances in metagenomics, transcriptomics, and proteomics have generated vast amounts of data, providing an unprecedented opportunity to gain insights into microbial communities. However, the integration and synthesis of these data remain a significant challenge due to their heterogeneity and complexity. This project aims to address this gap by leveraging advanced bioinformatics and data science techniques to integrate and analyze these data. This research is timely and important as it will not only advance our understanding of microbial communities but also provide a framework for integrating and analyzing complex biological data, contributing to the broader field of molecular and cellular biosciences.

Research Questions And Hypotheses

This project aims to address the following research questions: 1) What are the key structural and functional characteristics of microbial communities across different ecosystems? 2) How do these characteristics vary across different environmental conditions and gradients? 3) What are the key drivers of these variations? Based on the current literature, we hypothesize that microbial community structure and function are shaped by a complex interplay of environmental factors, species interactions, and evolutionary processes. We predict that by integrating metagenomics, transcriptomics, and proteomics data, we will be able to identify key patterns and drivers of microbial community structure and function. These hypotheses will be tested through a combination of data integration, bioinformatics analysis, and statistical modeling.

Methods And Approach

This project will leverage publicly available data from various databases such as the Human Microbiome Project, Earth Microbiome Project, and the Integrated Microbial Genomes database. These data will be integrated and analyzed using advanced bioinformatics tools and machine learning algorithms. Specifically, we will use network analysis to identify key patterns and relationships in the data, and machine learning models to predict microbial community structure and function based on environmental variables. The project will be carried out in three phases: data integration, data analysis, and result interpretation. Each phase will be led by a team of experts in microbiology, bioinformatics, and data science, ensuring a multidisciplinary approach. The project timeline is three years, with the first year dedicated to data integration, the second year to data analysis, and the third year to result interpretation and dissemination.

Expected Outcomes And Impact

This project is expected to provide novel insights into the structure and function of microbial communities, addressing long-standing puzzles in microbial ecology. It will also develop a framework for integrating and analyzing complex biological data, contributing to the broader field of molecular and cellular biosciences. The findings will be disseminated through peer-reviewed publications, conference presentations, and open-access data repositories, promoting open science principles. The project will also provide training opportunities for graduate students and postdocs, contributing to the development of a data-savvy workforce. In the long term, this project has the potential to catalyze further research and collaborations in microbial ecology and data science.

Budget And Resources

The total budget for this project is $500,000, which will be allocated as follows: Personnel (including salaries for researchers, graduate students, and postdocs): $200,000; Data acquisition and computational resources: $100,000; Travel (for conferences and meetings): $50,000; Dissemination (including publication fees and open-access charges): $50,000; Indirect costs: $100,000. The project will leverage existing resources and infrastructure at the participating institutions, including high-performance computing facilities and bioinformatics software. Additional resources will be sought through collaborations with other research groups and institutions.",,
ai_single_gpt_05,ai,single,gpt-4,Synthetic Biology: Data Integration for Bioengineering,"This research aims to synthesize existing data to advance the field of synthetic biology. By integrating diverse datasets, we will address novel questions related to the design and construction of new biological parts, devices, and systems. This project will require collaboration between bioengineers, molecular biologists, and data scientists. The findings will be made publicly available, promoting open science principles and contributing to the development of bioengineering.",,"Background And Significance

Synthetic biology, a rapidly evolving field, aims to design and construct new biological parts, devices, and systems, or to redesign systems that are already found in nature. It is a multidisciplinary effort that involves bioengineers, molecular biologists, and data scientists. Despite the significant progress made in this field, there are still many unanswered questions and challenges. The current state of synthetic biology is characterized by a wealth of data generated by various research groups worldwide. However, these datasets are often isolated and not integrated, limiting their potential for advancing the field. This research is timely and important as it aims to synthesize and integrate these diverse datasets to address novel questions in synthetic biology. By doing so, it will not only contribute to the advancement of synthetic biology but also promote open science principles and the development of bioengineering.

Research Questions And Hypotheses

This research will address the following questions: 1) How can existing data be synthesized and integrated to advance synthetic biology? 2) What novel insights can be gained from the integration of diverse datasets in synthetic biology? The hypotheses for this research are: 1) The integration of diverse datasets will provide novel insights into the design and construction of new biological parts, devices, and systems. 2) The synthesized data will contribute to the advancement of synthetic biology and bioengineering. These hypotheses will be tested by integrating diverse datasets and analyzing them using advanced computational approaches.

Methods And Approach

This research will utilize existing publicly available data from various sources, including genomic, proteomic, and metabolomic datasets. These datasets will be integrated using advanced computational approaches, including machine learning and network analysis. The integrated data will then be analyzed to address the research questions. This research does not involve any experimental design as it only uses existing data. The timeline for this research is three years, with the first year dedicated to data collection and integration, the second year to data analysis, and the third year to interpretation of results and dissemination of findings. The milestones include successful data integration, novel insights into synthetic biology, and contribution to the advancement of bioengineering.

Expected Outcomes And Impact

The expected outcomes of this research include the synthesis and integration of diverse datasets in synthetic biology, novel insights into the design and construction of new biological parts, devices, and systems, and contribution to the advancement of synthetic biology and bioengineering. The broader impacts of this research include promoting open science principles, providing training opportunities for graduate students and postdocs, and stimulating cross-disciplinary collaboration. The findings will be disseminated through publications in peer-reviewed journals and presentations at scientific conferences. This research also has the potential for follow-up research and collaborations.

Budget And Resources

The budget for this research is estimated to be $500,000, which will be used for data acquisition, computational resources, personnel salaries, and dissemination of findings. The resources required for this research include access to publicly available datasets, computational resources for data integration and analysis, and personnel with expertise in bioengineering, molecular biology, and data science.",,
ai_single_gpt_06,ai,single,gpt-4,Data Synthesis for Understanding Stem Cell Differentiation,"This project proposes to synthesize publicly available data to understand the molecular and cellular mechanisms underlying stem cell differentiation. By bringing together researchers from stem cell biology, molecular biology, and bioinformatics, we aim to develop innovative research strategies and analytical methods to understand the process of stem cell differentiation. The project will also provide training opportunities for graduate students and postdocs, fostering the next generation of data-savvy scientists.",,"Background And Significance

Stem cells, with their unique ability to differentiate into specialized cell types, hold immense potential for regenerative medicine and disease modeling. Despite significant advancements, the molecular and cellular mechanisms underlying stem cell differentiation remain poorly understood. Current knowledge is fragmented, with individual studies focusing on specific aspects of differentiation, such as gene expression changes, epigenetic modifications, or signaling pathways. However, a comprehensive, integrated understanding of these processes is lacking. This research aims to fill this gap by synthesizing publicly available data from diverse sources to provide a holistic view of stem cell differentiation. This is timely and important as it will not only advance our fundamental understanding of stem cell biology but also facilitate the development of more effective stem cell-based therapies.

Research Questions And Hypotheses

The primary research question is: What are the molecular and cellular mechanisms underlying stem cell differentiation? We hypothesize that stem cell differentiation is governed by a complex interplay of genetic, epigenetic, and signaling factors, and that an integrated analysis of these factors will reveal novel insights into the differentiation process. We expect to identify key genes, pathways, and regulatory networks involved in stem cell differentiation, and to develop a computational model that can predict differentiation outcomes based on initial cell states. These hypotheses will be tested by integrating and analyzing diverse datasets, including gene expression profiles, epigenetic modifications, and signaling pathway activities, from different stages of stem cell differentiation.

Methods And Approach

We will use publicly available data from databases such as GEO, ArrayExpress, and ENCODE. These datasets will be integrated using bioinformatics tools and machine learning algorithms to identify key genes, pathways, and regulatory networks involved in stem cell differentiation. We will also develop a computational model to predict differentiation outcomes based on initial cell states. This project will be carried out over three years, with the first year dedicated to data collection and preprocessing, the second year to data integration and analysis, and the third year to model development and validation. Statistical analyses will be performed using R and Python, with appropriate controls and multiple testing corrections to ensure robustness and reproducibility of the results.

Expected Outcomes And Impact

This research is expected to make significant contributions to the field of stem cell biology by providing a comprehensive, integrated understanding of the molecular and cellular mechanisms underlying stem cell differentiation. It will also develop innovative research strategies and analytical methods that can be applied to other areas of molecular and cellular biology. The findings will be disseminated through peer-reviewed publications, conference presentations, and open-access data repositories. The project will also provide training opportunities for graduate students and postdocs, fostering the next generation of data-savvy scientists. In the long term, this research could facilitate the development of more effective stem cell-based therapies and disease models.

Budget And Resources

The budget for this project is estimated at $500,000 over three years. This includes salaries for the research team (principal investigator, postdocs, graduate students), computational resources (servers, storage, software licenses), and indirect costs (administrative support, facilities). The project will leverage existing resources at the participating institutions, including high-performance computing clusters and bioinformatics software. Additional resources will be sought through collaborations with other research groups and institutions.",,
ai_single_gpt_07,ai,single,gpt-4,Integrative Analysis of Epigenetic Data for Disease Prediction,"This research aims to integrate existing epigenetic data to develop innovative strategies for disease prediction. By synthesizing diverse datasets, we will address novel questions related to the role of epigenetic modifications in disease development and progression. This project will require collaboration between epigeneticists, geneticists, and data scientists. The findings will be made publicly available, promoting open science principles and contributing to the advancement of predictive medicine.",,"Background And Significance

Epigenetics, the study of heritable changes in gene expression that do not involve alterations to the underlying DNA sequence, has emerged as a critical field in understanding disease development and progression. Recent advances in high-throughput technologies have generated a wealth of epigenetic data, providing unprecedented opportunities to explore the role of epigenetic modifications in disease. However, the sheer volume and complexity of these datasets present significant challenges in data integration and interpretation. Current methods for analyzing epigenetic data are often limited in their ability to handle the scale and diversity of these datasets, and there is a pressing need for innovative strategies to synthesize and interpret this data. This research is timely and important as it addresses this critical gap in the field. By developing novel methods for integrating diverse epigenetic datasets, this project will advance our understanding of the role of epigenetic modifications in disease and pave the way for the development of predictive models for disease risk and progression.

Research Questions And Hypotheses

This research will address the following questions: 1) How can diverse epigenetic datasets be effectively integrated to provide a comprehensive view of the epigenetic landscape in disease? 2) What are the key epigenetic modifications associated with disease development and progression? 3) Can these epigenetic markers be used to develop predictive models for disease risk and progression? We hypothesize that by integrating diverse epigenetic datasets, we can identify key epigenetic modifications associated with disease and develop predictive models for disease risk and progression. We expect to deliver a novel computational framework for integrating epigenetic data and a set of predictive models for various diseases. These hypotheses will be tested through rigorous data analysis and validation using independent datasets.

Methods And Approach

We will use publicly available epigenetic datasets from sources such as the ENCODE project and the Roadmap Epigenomics Project. These datasets include a wide range of epigenetic modifications, such as DNA methylation, histone modifications, and chromatin accessibility. We will develop novel computational methods for integrating these diverse datasets, taking into account the unique characteristics of each type of epigenetic modification. We will then apply these methods to identify key epigenetic modifications associated with disease and develop predictive models for disease risk and progression. The project will be carried out over a period of three years, with the first year dedicated to method development, the second year to data analysis and model development, and the third year to model validation and dissemination of results.

Expected Outcomes And Impact

This research is expected to make significant contributions to the field of epigenetics and disease prediction. By developing novel methods for integrating diverse epigenetic datasets, this project will advance our understanding of the role of epigenetic modifications in disease and pave the way for the development of predictive models for disease risk and progression. The findings will be disseminated through peer-reviewed publications and presentations at scientific conferences. The developed methods and models will be made publicly available, promoting open science principles and contributing to the advancement of predictive medicine. In the long term, this research has the potential to transform our approach to disease prediction and prevention, leading to improved health outcomes and reduced healthcare costs.

Budget And Resources

The budget for this project is estimated at $500,000 over three years. This includes salaries for the research team (epigeneticists, geneticists, and data scientists), computational resources for data analysis and model development, and costs associated with data access, dissemination of results, and project management. The project will leverage existing resources and expertise at our institutions, including high-performance computing facilities and extensive experience in epigenetic data analysis and predictive modeling. The project will also provide training opportunities for graduate students and postdocs, contributing to the development of the next generation of data-savvy scientists.",,
ai_single_gpt_08,ai,single,gpt-4,Data-Driven Insights into Protein Folding,"This project proposes to synthesize publicly available data to gain novel insights into the process of protein folding. By integrating data from proteomics, structural biology, and bioinformatics, we aim to solve long-standing puzzles in protein folding. This project will require collaboration between protein chemists, structural biologists, and data scientists. The findings will be made publicly available, promoting open science principles and contributing to our understanding of protein structure and function.",,"Background And Significance

Protein folding is a fundamental process in biology, dictating the structure and function of proteins. Despite decades of research, the precise mechanisms of protein folding remain elusive. Current understanding is based on Anfinsen's dogma, which posits that a protein's primary sequence determines its structure. However, this does not explain the speed and accuracy of folding in vivo. Misfolded proteins can lead to diseases like Alzheimer's and Parkinson's, making this research crucial. Recent advances in proteomics, structural biology, and bioinformatics provide an unprecedented opportunity to revisit this problem. A comprehensive review of the literature reveals a wealth of data on protein structures, folding pathways, and folding kinetics. However, these data are scattered across different databases and publications, hindering their integration and analysis. This project will address this gap by synthesizing these diverse data sources to gain novel insights into protein folding.

Research Questions And Hypotheses

This project aims to answer the following research questions: 1) How does the protein sequence influence its folding pathway and final structure? 2) What are the common patterns and exceptions in protein folding across different proteins and organisms? 3) How can we predict protein folding and misfolding based on sequence data? We hypothesize that protein folding is not solely determined by the sequence but also by cellular context and that patterns in folding pathways can be identified and used for prediction. We expect to develop a comprehensive database and predictive models for protein folding, contributing to our understanding of protein biology and disease.

Methods And Approach

We will use publicly available data from the Protein Data Bank, UniProt, and other databases. These data will be integrated and analyzed using advanced bioinformatics and machine learning techniques. We will also develop new algorithms for data integration and analysis. The project will be divided into three phases: data collection and integration, data analysis, and model development and validation. Each phase will have specific milestones and deliverables. Statistical analysis will be performed to validate the models and findings.

Expected Outcomes And Impact

This project will contribute to the field by providing a comprehensive view of protein folding and a predictive model. It will also promote cross-disciplinary collaboration and open science. The findings can be applied in drug design, disease diagnosis, and biotechnology. We plan to publish the results in high-impact journals and present at international conferences. The database and models will be made publicly available, fostering further research and collaboration. This project will also provide training opportunities for graduate students and postdocs, nurturing the next generation of data-savvy scientists.

Budget And Resources

The budget will cover personnel salaries, computational resources, and dissemination activities. We will need a team of protein chemists, structural biologists, and data scientists. We will also need high-performance computing resources for data integration and analysis. The budget will also cover publication costs and conference travel. We will leverage existing resources and collaborations to maximize the impact of the funding.",,
ai_single_gpt_09,ai,single,gpt-4,Synthesis of Data for Understanding Virus Evolution,"This research aims to synthesize existing data to understand the evolution of viruses. By integrating diverse datasets, we will address the long-standing puzzle of how viruses evolve and adapt to different hosts and environments. This project will require collaboration between virologists, evolutionary biologists, and data scientists. The findings will be made publicly available, promoting open science principles and providing a valuable resource for future virology research.",,"Background And Significance

The study of viral evolution is a critical aspect of virology, with implications for understanding disease emergence, vaccine design, and antiviral drug development. Despite significant advances in the field, our understanding of how viruses evolve and adapt to different hosts and environments remains incomplete. This is due, in part, to the complexity of viral evolution, which involves a multitude of factors, including mutation rates, selection pressures, and host-virus interactions. Additionally, the vast amount of publicly available data on viral genomes, host responses, and environmental factors has yet to be fully integrated and synthesized to address these questions. This research is timely and important as it will leverage existing data to provide novel insights into viral evolution, thereby advancing our understanding of viral diseases and informing public health strategies.

Research Questions And Hypotheses

This research will address the following questions: 1) How do viruses evolve and adapt to different hosts and environments? 2) What are the key drivers of viral evolution? 3) How can we predict future viral evolution based on past patterns? We hypothesize that viral evolution is driven by a combination of mutation rates, selection pressures, and host-virus interactions, and that these factors can be quantified and modeled using existing data. We expect to deliver a comprehensive synthesis of data on viral evolution, a set of predictive models for future viral evolution, and a publicly available database and analysis workflow. These hypotheses will be tested and validated through data synthesis, computational modeling, and statistical analysis.

Methods And Approach

We will use a combination of data synthesis, computational modeling, and statistical analysis to address our research questions. Specifically, we will integrate and synthesize existing data on viral genomes, host responses, and environmental factors from publicly available databases. We will then use computational models to quantify and predict viral evolution based on these data. Finally, we will validate our models using statistical analysis. This project will require collaboration between virologists, evolutionary biologists, and data scientists, and will be completed over a period of three years, with specific milestones and deliverables at each stage.

Expected Outcomes And Impact

This research is expected to make significant contributions to the field of virology by providing novel insights into viral evolution and by developing predictive models for future viral evolution. The broader impacts of this research include informing public health strategies, improving vaccine design, and guiding antiviral drug development. Additionally, this research will promote cross-disciplinary collaboration and open science by making our findings, data, and analysis workflows publicly available. We anticipate that this research will stimulate further research and collaborations in the field, and will provide training opportunities for graduate students and postdocs.

Budget And Resources

The budget for this research will cover personnel costs (including salaries for researchers, graduate students, and postdocs), computational resources (including data storage and processing), and dissemination costs (including publication fees and conference travel). We estimate a total budget of $500,000 over three years, with approximately 50% allocated to personnel costs, 30% to computational resources, and 20% to dissemination costs. This budget reflects the resources required for a community-scale synthesis project that goes beyond the capabilities of a single lab or existing collaboration.",,
ai_single_gpt_10,ai,single,gpt-4,Cross-Disciplinary Approach to Antibiotic Resistance,"This project proposes to synthesize publicly available data to address fundamental questions related to the molecular and cellular mechanisms underlying antibiotic resistance. By bringing together researchers from microbiology, molecular biology, and bioinformatics, we aim to develop innovative research strategies and analytical methods to understand the spread of antibiotic resistance. The project will also provide training opportunities for graduate students and postdocs, fostering the next generation of data-savvy scientists.",,"Background And Significance

Antibiotic resistance is a global health crisis, with the World Health Organization declaring it one of the biggest threats to global health, food security, and development today. The molecular and cellular mechanisms underlying antibiotic resistance are complex and multifaceted, involving both genetic and environmental factors. Despite significant advances in our understanding of these mechanisms, there remain critical gaps in our knowledge. For instance, the role of horizontal gene transfer in the spread of antibiotic resistance is not fully understood, and the impact of environmental factors on the development and spread of resistance is still a subject of ongoing research. Furthermore, the vast amount of publicly available data on antibiotic resistance has not been fully utilized, representing a significant untapped resource for advancing our understanding of this critical issue. This research is both timely and important, as it addresses a pressing global health issue and leverages existing data to generate new insights into the molecular and cellular mechanisms of antibiotic resistance.

Research Questions And Hypotheses

This research project will address the following key research questions: 1) What are the key molecular and cellular mechanisms underlying antibiotic resistance? 2) How does horizontal gene transfer contribute to the spread of antibiotic resistance? 3) What role do environmental factors play in the development and spread of antibiotic resistance? Based on these questions, we hypothesize that: 1) Antibiotic resistance is mediated by a complex interplay of genetic and environmental factors; 2) Horizontal gene transfer plays a significant role in the spread of antibiotic resistance; and 3) Environmental factors significantly influence the development and spread of antibiotic resistance. These hypotheses will be tested through a comprehensive synthesis and analysis of publicly available data, with the expected outcome being a deeper understanding of the molecular and cellular mechanisms of antibiotic resistance.

Methods And Approach

This research project will utilize a range of publicly available data sources, including genomic databases, transcriptomic databases, and environmental databases. These data will be integrated and analyzed using a combination of bioinformatics and computational biology approaches. Specifically, we will use machine learning algorithms to identify patterns and relationships in the data, and network analysis to understand the interactions between different factors. We will also use statistical analysis to validate our findings and to test our hypotheses. The project will be carried out over a period of three years, with specific milestones and deliverables set for each year.

Expected Outcomes And Impact

The expected outcomes of this research project include a comprehensive understanding of the molecular and cellular mechanisms underlying antibiotic resistance, new insights into the role of horizontal gene transfer and environmental factors in the spread of resistance, and the development of innovative research strategies and analytical methods. The broader impacts of this research include informing the development of new strategies for combating antibiotic resistance, contributing to the training of the next generation of data-savvy scientists, and fostering cross-disciplinary collaboration. The findings of this research will be disseminated through peer-reviewed publications, conference presentations, and public outreach activities.

Budget And Resources

The budget for this research project includes costs for personnel (including salaries for researchers, graduate students, and postdocs), computational resources (including software licenses and cloud computing services), and dissemination activities (including publication fees and conference travel). The total budget for the project is estimated to be $500,000, with approximately 60% allocated to personnel costs, 30% to computational resources, and 10% to dissemination activities. The resources required for this project include access to publicly available data sources, computational resources for data analysis, and expertise in microbiology, molecular biology, and bioinformatics.",,
ai_group_gemini_01,ai,group,gemini-2.5-pro,The Chromatin Grammar of Cellular Identity: Emergence of Cell Fate from the Integrated Epigenome,"A fundamental question in biology is how the ~200 distinct cell types in the human body, each sharing the same genome, emerge and maintain their unique identities. This identity is encoded not in the DNA sequence itself, but in the dynamic, multi-layered epigenome. We propose to form a working group to address the hypothesis that a 'chromatin grammar'—a set of combinatorial rules governing the interplay of DNA accessibility, histone modifications, and transcription factor binding—dictates cellular identity as an emergent property. This project will synthesize a vast repository of publicly available single-cell and bulk functional genomics data, including ATAC-seq, ChIP-seq, Hi-C, and RNA-seq from consortia like ENCODE, the Human Cell Atlas, and Roadmap Epigenomics. Our multidisciplinary team, comprising computational biologists, chromatin experts, developmental biologists, and machine learning specialists, will develop a novel integrative framework. Using advanced machine learning techniques, including graph neural networks and transformers, we will move beyond simple correlations to build predictive models that can infer cell type from chromatin state alone and simulate cell fate transitions. The project will deliver a publicly accessible 'Chromatin State Atlas' and a computational model of the emergent logic of cell identity. This work will provide profound insights into development, disease (like cancer, where identity is lost), and regenerative medicine, while training a new generation of scientists in large-scale data synthesis and modeling.",,"Background And Significance

The differentiation of a single zygote into the hundreds of specialized cell types that constitute a complex organism is a marvel of biological self-organization. While every cell shares an identical genome, each cell type exhibits a unique and stable gene expression program that defines its identity and function. This cellular diversity arises from the epigenome, a complex layer of chemical modifications to DNA and its associated histone proteins that orchestrates genome function without altering the underlying DNA sequence. The epigenome encompasses DNA methylation, dozens of post-translational histone modifications, chromatin accessibility, and the three-dimensional folding of the genome. Understanding how these layers of information are integrated to produce stable cellular identities is a central challenge in modern biology. 

Over the past two decades, large-scale international consortia such as the ENCODE Project, the Roadmap Epigenomics Mapping Consortium, and the Human Cell Atlas have generated an unprecedented wealth of publicly available data cataloging these epigenetic features across a vast array of human cell types and tissues. These efforts have been instrumental in creating a 'parts list' for the human epigenome. For instance, specific histone modifications are strongly correlated with the activity of functional elements: H3K4me3 marks active promoters, while H3K27ac is a hallmark of active enhancers (Heintzman et al., 2007, Nature). Computational methods like ChromHMM and Segway have leveraged these correlations to segment the genome into a limited number of 'chromatin states' (e.g., 'active promoter', 'poised enhancer'), providing a valuable, albeit simplified, annotation of the non-coding genome (Ernst & Kellis, 2012, Nature Methods). These models have successfully linked genetic variants to disease by identifying their location within specific regulatory elements.

However, this descriptive, correlational understanding has significant limitations. It treats epigenetic marks largely as independent features, failing to capture the complex, combinatorial, and context-dependent interplay between them. The 'histone code' hypothesis, which posited that specific combinations of modifications act in concert to signal downstream functions (Strahl & Allis, 2000, Nature), has proven difficult to decipher with simple linear models. We currently possess a 'dictionary' that links individual marks to functions, but we lack a 'grammar'—a set of rules that explains how these marks are combined in sequence and in three-dimensional space to compose the complex regulatory programs that define a cell. We do not yet understand how cellular identity emerges as a robust, system-level property from these local molecular interactions. Consequently, we lack the ability to predict a cell's identity from its chromatin state *de novo* or to simulate the dynamic epigenetic transitions that occur during development or disease.

This research is both important and timely due to the convergence of two key factors. First, the explosion of single-cell multi-omic technologies is providing data at a resolution that was previously unimaginable, allowing us to observe the epigenome's heterogeneity and dynamics within cell populations. Synthesizing these vast, disparate datasets is a community-scale challenge that no single lab can tackle alone. Second, recent breakthroughs in machine learning, particularly deep learning architectures like graph neural networks (GNNs) and transformers, provide powerful new tools for learning complex patterns and long-range dependencies in high-dimensional data. These models are perfectly suited to move beyond simple correlations and learn the non-linear, combinatorial rules of the chromatin grammar. By forming a multidisciplinary working group to synthesize existing public data with these advanced analytical strategies, we are poised to address this fundamental gap in knowledge, yielding profound insights into the logic of cellular identity with direct implications for developmental biology, cancer research, and regenerative medicine.

Research Questions And Hypotheses

This project is driven by the overarching hypothesis that cellular identity is an emergent property governed by a decipherable, predictive 'chromatin grammar'. We will test this central hypothesis through three specific, interconnected research aims, each addressing fundamental questions about the structure, function, and dynamics of the epigenome.

**Aim 1: Define the fundamental units and syntax of the chromatin grammar.**
This aim seeks to move beyond annotating individual epigenetic marks to identifying the recurrent, combinatorial patterns that form the building blocks of regulatory programs. We will deconstruct the complex epigenome into its core components and the rules governing their assembly.
*   **Research Question 1.1:** What are the fundamental, recurrent combinations of chromatin accessibility, histone modifications, and transcription factor (TF) binding that constitute the 'words' of the epigenome across diverse human cell types?
*   **Hypothesis 1.1:** We hypothesize that a finite, learnable set of multi-modal 'chromatin motifs' exists, representing stereotyped regulatory states (e.g., a 'pluripotency enhancer' motif, a 'neuronal promoter' motif). These motifs are more informative than any single epigenetic mark alone. We predict that our unsupervised models will not only rediscover known patterns, such as bivalent promoters in stem cells (H3K4me3 and H3K27me3), but also uncover novel, cell-type-specific combinations that define unique regulatory functions.
*   **Research Question 1.2:** What are the syntactic rules governing the arrangement of these chromatin motifs along the linear genome and their organization in 3D space to control gene expression?
*   **Hypothesis 1.2:** We hypothesize that the spatial organization of chromatin motifs follows non-random, hierarchical rules. The 'syntax' of the grammar dictates which genes are expressed by governing enhancer-promoter communication within 3D topologically associating domains (TADs). We predict that graph-based models incorporating Hi-C data will reveal that cell-type-specific gene expression programs are encoded in the network topology of long-range chromatin interactions, and that disruptions to this syntax are associated with aberrant gene regulation.

**Aim 2: Build a predictive model of cellular identity from the integrated epigenome.**
This aim will leverage the discovered grammar to construct a comprehensive, predictive model that maps chromatin state to cellular identity. This represents a critical test of our understanding, moving from description to prediction.
*   **Research Question 2.1:** Can a machine learning model, trained on a multi-modal atlas of chromatin states, accurately predict a cell's identity (type, subtype, and state) from its epigenomic profile alone?
*   **Hypothesis 2.1:** We hypothesize that the chromatin state contains sufficient information to uniquely and robustly specify cell identity. An integrative deep learning model will learn the high-dimensional mapping from the epigenome to cell type with greater accuracy than models based on single data modalities or gene expression profiles. We predict our model will successfully classify cells from lineages not seen during training, demonstrating its ability to learn generalizable principles of the grammar.
*   **Research Question 2.2:** What is the minimal and sufficient set of genomic loci and associated chromatin features required to define a given cellular identity?
*   **Hypothesis 2.2:** We hypothesize that cell identity is specified by a core set of 'master' regulatory loci whose chromatin states are both necessary and sufficient. Using model interpretability techniques (e.g., attention scores, in silico saturation mutagenesis), we will identify these key genomic 'hubs'. We predict that a model trained only on these core loci will retain high predictive accuracy, providing a condensed, mechanistic view of the epigenetic basis of cell identity.

**Aim 3: Model the dynamics of cell fate transitions as a shift in chromatin grammar.**
This aim extends our static model to the dynamic processes of development and cellular reprogramming, treating cell fate transitions as programmatic shifts in the underlying chromatin grammar.
*   **Research Question 3.1:** Can our framework model the ordered sequence of changes in the chromatin landscape during cellular differentiation and predict intermediate states?
*   **Hypothesis 3.1:** We hypothesize that cell fate transitions are not random walks but follow specific trajectories through a high-dimensional 'chromatin state space', constrained by the rules of the grammar. We predict that by training our model on pseudotime-ordered single-cell data from developmental systems (e.g., hematopoiesis), it will learn a latent representation of this state space. This will allow us to simulate differentiation trajectories, identify key decision points (bifurcations), and predict the sequence of epigenetic events required to transition from one cell state to another. This predictive capability will be validated against held-out time-course datasets.

Methods And Approach

Our approach is a multi-year, multi-institutional effort centered on the synthesis of public data using a novel, integrated computational framework. The project is structured around our three research aims and is designed to be open, reproducible, and collaborative.

**Data Acquisition, Harmonization, and Integration (Year 1, Q1-Q3)**
This foundational phase is critical for the project's success. We will aggregate a comprehensive collection of publicly available human functional genomics data from major consortia, including ENCODE, Roadmap Epigenomics, the Human Cell Atlas (HCA), and the 4D Nucleome (4DN) project, as well as individual studies from GEO/SRA.
*   **Data Types:** We will focus on core data modalities that define the epigenome: ATAC-seq (chromatin accessibility), ChIP-seq for key histone modifications (activating: H3K27ac, H3K4me1, H3K4me3; repressive: H3K27me3, H3K9me3), ChIP-seq for the architectural protein CTCF, and Hi-C/Micro-C (3D genome architecture). We will also integrate corresponding RNA-seq data for model validation and interpretation.
*   **Processing Pipeline:** To overcome heterogeneity from different experimental protocols and labs, we will establish a single, unified processing pipeline. This pipeline will be built using Nextflow for scalability and portability, incorporating best-practice tools (e.g., BWA, MACS2, Juicer) and adhering to ENCODE processing standards. The entire workflow will be containerized using Docker and Singularity, ensuring complete reproducibility. All raw and processed data will be meticulously annotated with standardized metadata (cell type, tissue, donor, experimental conditions) and organized into a cloud-based, queryable database using efficient formats like Zarr or HDF5.

**Aim 1: Deciphering Chromatin Grammar (Year 1, Q4 - Year 2, Q4)**
We will develop a novel machine learning framework to learn the combinatorial rules of the epigenome.
*   **Genomic Representation:** We will represent the genome as a multi-graph. Genomic bins (e.g., 500 bp) will serve as nodes. Each node will have a feature vector containing the normalized signals from all integrated data types (ATAC-seq, ChIP-seq, etc.). Edges will connect nodes in two ways: 1) 'sequential' edges connecting adjacent bins along the chromosome, and 2) 'long-range' edges connecting physically interacting bins, weighted by contact frequency from Hi-C data.
*   **Modeling Approach:** We will employ a hybrid deep learning architecture. To capture local combinatorial patterns ('chromatin motifs'), we will use Graph Attention Networks (GATs), which can learn the importance of different features and neighboring nodes. To capture the linear syntax and long-range dependencies along the chromosome, we will adapt the Transformer architecture, which has excelled at learning context in natural language. By treating the sequence of genomic bins as a 'sentence' and chromatin motifs as 'words', the Transformer's self-attention mechanism can identify critical regulatory elements hundreds of kilobases apart. We will use unsupervised methods, such as clustering the learned node embeddings from the GAT/Transformer encoder, to systematically identify and classify the fundamental 'words' of the chromatin grammar.

**Aim 2: Predictive Modeling of Cell Identity (Year 2, Q1 - Year 3, Q2)**
We will build and validate a supervised model to predict cell type from chromatin state.
*   **Model Architecture:** The encoder developed in Aim 1 will serve as the foundation. We will add a classification head to this encoder and train the entire model end-to-end in a supervised fashion. The input will be the multi-modal chromatin state for a given genomic region (e.g., a 2 Mb window), and the output will be a probability distribution over a controlled vocabulary of cell types derived from the Cell Ontology.
*   **Training and Validation:** We will use the harmonized data from hundreds of cell types for training. To ensure robustness and avoid batch effects, we will employ a rigorous cross-validation strategy, holding out entire donors or tissues for testing. Model performance will be evaluated using metrics like area under the precision-recall curve (AUPRC). We will perform extensive model interpretation using techniques like SHAP (SHapley Additive exPlanations) and attention map visualization to identify the genomic loci and feature combinations most predictive of each cell type, directly testing Hypothesis 2.2.

**Aim 3: Modeling Cell Fate Dynamics (Year 3, Q1 - Year 4, Q2)**
We will extend our framework to model the dynamic transitions between cell states.
*   **Data:** We will utilize public datasets that capture cellular differentiation, such as time-course single-cell multi-omic data from hematopoietic stem cell differentiation or directed differentiation of embryonic stem cells.
*   **Modeling Approach:** We will employ a variational autoencoder (VAE) architecture. The model will learn to project the high-dimensional chromatin state of single cells into a low-dimensional latent space. By incorporating pseudotime information derived from trajectory inference algorithms (e.g., Palantir), we will structure this latent space to represent differentiation pathways as smooth trajectories. This will allow us to perform in silico experiments: we can 'walk' along these trajectories to predict the sequence of chromatin state changes, identify bifurcation points representing cell fate decisions, and simulate the effects of perturbing key TFs by observing the resulting shift in the trajectory.

**Timeline and Milestones:**
*   **Year 1:** Completion of data harmonization pipeline; first-generation chromatin motif catalog.
*   **Year 2:** Release of Chromatin State Atlas v1.0; development and initial validation of the cell identity prediction model; first major publication.
*   **Year 3:** Refined predictive model with full interpretability analysis; development of the dynamic VAE model for cell fate transitions; public release of the 'Chromatin Grammar Engine' software.
*   **Year 4:** Validation of dynamic simulations; capstone publications summarizing the chromatin grammar; final release of all data, models, and web portal; final working group meeting and community workshop.

Expected Outcomes And Impact

This project will produce a suite of tangible deliverables and generate significant intellectual and practical impacts, fundamentally advancing the molecular and cellular biosciences. Our work is designed to create lasting resources for the scientific community and to train a new generation of data-savvy biologists, directly aligning with the core mission of the NCEMS program.

**Expected Outcomes and Deliverables:**
1.  **A Unified, Harmonized Human Epigenome Atlas:** Our first major outcome will be a comprehensive, consistently processed atlas of multi-modal epigenomic data from hundreds of human cell types. This resource, far exceeding what any single lab could produce, will eliminate a major barrier in the field—data heterogeneity—and serve as a foundational dataset for countless future studies on gene regulation, development, and disease.
2.  **The 'Chromatin Grammar Engine':** We will deliver a powerful, open-source deep learning model capable of predicting cell identity from chromatin state and simulating cell fate transitions. The software will be fully documented, containerized, and made available on platforms like GitHub and Docker Hub, allowing any researcher to apply our state-of-the-art methods to their own data.
3.  **A Publicly Accessible Web Portal:** To ensure our findings are accessible to the entire biological community, including those without computational expertise, we will create an interactive web portal. This portal will allow users to explore the identified chromatin motifs, visualize the grammatical rules, query the importance of specific genomic regions for defining cell identity, and browse the complete epigenome atlas.
4.  **A Quantitative Framework for Cellular Identity:** The primary intellectual outcome will be a new, quantitative framework for understanding cellular identity as an emergent property of the epigenome. We will deliver a catalog of the fundamental 'words' (chromatin motifs) and 'syntactic rules' that constitute the chromatin grammar, shifting the field from a descriptive to a predictive science.

**Broader Scientific and Societal Impact:**
*   **Transforming Basic Biology:** By providing a predictive model of gene regulation, our work will offer profound insights into fundamental biological processes. It will provide a mechanistic basis for understanding how cell lineages are established during embryogenesis and how cellular identity is maintained with high fidelity throughout life.
*   **Advancing Human Health:** The implications for medicine are significant. In **cancer research**, our model can be used to understand how epigenetic dysregulation leads to a loss of cellular identity and the acquisition of malignant, de-differentiated states. This could reveal novel diagnostic biomarkers or therapeutic strategies aimed at restoring a normal epigenetic state. In **regenerative medicine**, a predictive understanding of the chromatin grammar will provide a blueprint for designing more efficient and precise protocols for cellular reprogramming and directed differentiation, accelerating the development of cell-based therapies for diseases like Parkinson's, diabetes, and heart failure.
*   **Training and Workforce Development:** This project is an exemplary training vehicle. Postdoctoral fellows and graduate students will form the core of the working group, receiving unique cross-disciplinary training at the intersection of computational biology, machine learning, and chromatin biology. Through co-mentorship across participating labs, they will build collaborative skills and professional networks essential for future leadership in data-intensive science. We will further broaden our impact by developing and hosting an annual summer workshop on data synthesis and computational epigenomics for the wider community.

**Dissemination and Open Science:**
We are deeply committed to the principles of open, team, and reproducible science. All software will be developed openly on GitHub. All analysis workflows will be shared as portable containers. All data, models, and results will be deposited in public repositories (e.g., GEO, Zenodo) and made accessible through our web portal prior to publication. We plan to disseminate our findings through high-impact, open-access publications, presentations at major international conferences, and seminars at diverse institutions. This working group will establish a lasting collaborative network that will continue to pursue these fundamental questions, ensuring the long-term sustainability and impact of the project.

Budget And Resources

The proposed research represents a community-scale synthesis project that is beyond the capabilities of any single research laboratory or existing collaboration. The sheer scale of data aggregation and harmonization, the computational demands of training state-of-the-art deep learning models, and the essential requirement for deep, integrated expertise from disparate scientific fields (machine learning, computational biology, chromatin biology, developmental biology) necessitate the unique collaborative and resource-intensive environment provided by an NCEMS Working Group. NCEMS support is critical for funding the dedicated personnel, high-performance computing resources, and collaborative infrastructure required to achieve our ambitious goals.

**Budget Justification and Breakdown (Total Request over 4 Years)**

**A. Personnel ($1,250,000):** The majority of the budget is allocated to personnel who will drive the project's day-to-day research and development.
*   **Postdoctoral Scholars (3.0 FTE x 4 years):** We request support for three postdoctoral scholars, one based at each of the three lead PIs' institutions. Each will bring complementary expertise: Postdoc 1 (ML/AI specialist) will lead model architecture development; Postdoc 2 (Bioinformatics specialist) will manage the data harmonization pipeline; Postdoc 3 (Chromatin Biologist) will lead biological interpretation and validation of model outputs. This distributed model fosters deep collaboration.
*   **Graduate Students (2.0 FTE x 4 years):** Support for two graduate students who will be co-mentored across labs. They will focus on specific aims, such as model interpretability and application to specific disease datasets, providing an outstanding cross-disciplinary training opportunity.
*   **Data Manager / Research Scientist (0.5 FTE x 4 years):** A part-time professional staff member is essential for managing the petabyte-scale data atlas, maintaining the public web portal, and ensuring long-term data stewardship.

**B. Travel ($120,000):**
*   **Annual In-Person Working Group Meeting ($20,000/year):** Funds to bring all PIs, trainees, and the data manager together for an intensive 3-day workshop each year. These meetings are vital for strategic planning, data integration, and fostering a cohesive team spirit.
*   **Conference Travel ($10,000/year):** To support travel for trainees to present project findings at key international conferences (e.g., ISMB, CSHL Biology of Genomes), which is crucial for dissemination and professional development.

**C. Computational Resources ($200,000):**
*   **Cloud Computing Credits ($50,000/year):** This is a critical need. Training large-scale GNN and Transformer models on genomic data is computationally expensive and requires access to high-end GPU clusters. Cloud platforms (e.g., AWS, Google Cloud) provide the necessary scalable infrastructure that is not available through standard institutional resources.

**D. Other Direct Costs ($80,000):**
*   **Publication Costs ($10,000/year):** To cover open-access fees for an anticipated 4-6 major publications, ensuring our findings are freely accessible to all.
*   **Workshop and Training Materials ($10,000/year):** Funds to develop materials and support logistics for our annual summer training workshop, designed to disseminate our methods to the broader scientific community.

**E. Indirect Costs:** Calculated based on the federally negotiated rates for the participating institutions on the modified total direct costs.",,
ai_group_gemini_02,ai,group,gemini-2.5-pro,Deconstructing Allostery: Mapping the Emergent Communication Networks of Macromolecular Machines,"Allostery, the process by which a binding event at one site of a protein or complex affects a distant functional site, is a quintessential emergent property that underpins cellular regulation. However, the pathways of allosteric communication through large, dynamic macromolecular machines like the ribosome, proteasome, or spliceosome remain poorly understood. This working group will pioneer a community-scale effort to create a unified 'Allosteric Atlas' by systematically mapping these communication networks. We will not generate new experimental data, but instead integrate and synthesize three major public data types: 1) thousands of static structures from the Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) to build structural ensembles; 2) deep mutational scanning (DMS) data to identify functionally coupled residues; and 3) molecular dynamics simulation trajectories to capture conformational dynamics. Our team, uniting structural biologists, biophysicists, computer scientists, and biochemists, will develop novel computational methods based on network theory and information theory to trace the flow of information through these complexes. The goal is to build predictive models that can identify cryptic allosteric sites, forecast the functional consequences of mutations (including those associated with disease), and guide the rational design of allosteric drugs. This project will transform our understanding of molecular regulation from a one-site-at-a-time view to a holistic, network-based perspective.",,"Background And Significance

Allostery is a fundamental mechanism of biological regulation, enabling proteins and their complexes to act as sophisticated information processing devices. First conceptualized through the classic Monod-Wyman-Changeux (MWC) and Koshland-Nemethy-Filmer (KNF) models, allostery was initially described as a ligand-induced switch between discrete conformational states. While these models provided a powerful foundation, they are insufficient to describe the complex, continuous, and often subtle communication that occurs within large, multi-subunit macromolecular machines. Today, allostery is increasingly understood not as a simple mechanical switch, but as a quintessential emergent property arising from the complex interplay of a protein's structure, dynamics, and sequence evolution. It is the collective behavior of hundreds or thousands of residues, connected through a dense network of interactions, that gives rise to long-range communication. Understanding this emergent phenomenon is one of the grand challenges in molecular and cellular biology.

The current state of the field is characterized by a wealth of data and a diversity of powerful, yet fragmented, approaches. Experimentally, high-resolution structural methods like X-ray crystallography and cryo-electron microscopy provide static snapshots of different functional states, offering clues about conformational changes. Techniques like hydrogen-deuterium exchange mass spectrometry (HDX-MS) and NMR spectroscopy provide insights into protein dynamics. Crucially, the advent of deep mutational scanning (DMS) has enabled the high-throughput quantification of functional consequences for thousands of mutations, revealing complex epistatic relationships between residues that hint at underlying communication pathways. Computationally, molecular dynamics (MD) simulations can model the dynamic motions of proteins at atomic resolution, while methods like normal mode analysis (NMA) can describe low-frequency collective motions relevant to allostery. Network-based models, such as Protein Structure Networks (PSNs), have successfully identified allosteric pathways by representing proteins as graphs of interacting residues, building on pioneering work by Bahar, Nussinov, and others.

Despite these advances, significant gaps in our knowledge persist. The primary limitation is data fragmentation. Structural data from the PDB/EMDB, functional data from DMS databases like MaveDB, and dynamic data from MD simulations exist in separate, unlinked repositories. There is no unifying framework to integrate these disparate data types into a single, coherent model of allosteric communication. This fragmentation prevents us from seeing the full picture. A second major gap is scale. Most detailed allosteric studies have focused on smaller, single-domain proteins or dimers. The principles governing allosteric communication across the vast distances and multiple interfaces of megadalton-scale machines like the ribosome, proteasome, or spliceosome remain largely uncharted territory. Finally, current approaches are often more descriptive than predictive. While we can sometimes rationalize observed allosteric effects, we lack the ability to reliably predict, *a priori*, which residues will form a communication pathway, where cryptic allosteric sites might be located, or how a novel mutation will impact function from a distance.

This research is critically important and timely for several reasons. First, the exponential growth of public data in structural, sequencing, and functional genomics databases has created an unprecedented opportunity for a large-scale synthesis project. For the first time, sufficient data exists to attempt a systematic mapping of allosteric networks. Second, advances in computational power and machine learning provide the necessary tools to integrate these massive, heterogeneous datasets. Third, a deeper understanding of allostery has profound biomedical implications. Allosteric drugs, which target sites other than the active site, can offer greater specificity and fewer side effects, representing a new frontier in pharmacology. Furthermore, many disease-causing mutations, particularly variants of uncertain significance (VUS), likely exert their pathogenic effects by disrupting allosteric regulation rather than by directly ablating catalytic activity. By creating a predictive, network-based framework for allostery, this project will provide a foundational resource to accelerate rational drug design and improve our interpretation of the human variome.

Research Questions And Hypotheses

The overarching goal of this working group is to establish a new paradigm for understanding allostery, moving from qualitative descriptions to a quantitative, predictive, and generalizable network-based framework. By synthesizing vast public datasets, we will construct and validate an 'Allosteric Atlas' for key macromolecular machines, addressing fundamental questions about the nature of molecular communication. Our research is structured around three central questions, each with testable hypotheses.

**Research Question 1: How can structurally, dynamically, and functionally diverse data types be integrated to construct a unified, multi-layered representation of allosteric networks?**
Current approaches typically rely on a single data modality (e.g., structure or dynamics), providing an incomplete view. We posit that a holistic model requires data fusion.
*   **Hypothesis 1a:** A multi-layered network model, where nodes (residues) are connected by edges weighted by a composite score derived from structural proximity (PDB/EMDB), dynamic cross-correlations (MD simulations), co-evolutionary couplings (sequence alignments), and functional epistasis (DMS data), will capture allosteric pathways with significantly higher fidelity than any single-layer network.
*   **Prediction:** Allosteric pathways identified using our integrated model will show a statistically significant higher enrichment for known functionally critical residues, disease-associated mutations (from ClinVar), and experimentally validated allosteric sites compared to pathways derived from structure-only or dynamics-only networks.
*   **Validation:** We will rigorously benchmark our integrative method on a 'gold standard' set of well-characterized allosteric proteins (e.g., hemoglobin, GPCRs, protein kinases). Using metrics such as the area under the receiver operating characteristic curve (AUC-ROC), we will quantify the predictive power of our multi-layered approach versus single-layer models in identifying known allosteric residues.

**Research Question 2: What are the conserved architectural principles and emergent properties of allosteric communication networks across different classes of macromolecular machines?**
We seek to determine if there are universal 'rules' governing information flow in proteins or if network architecture is tailored to specific biological functions.
*   **Hypothesis 2a:** Allosteric communication does not occur through random walks but is channeled along evolutionarily conserved 'information highways' composed of residues with distinct biophysical properties, such as high mechanical stress, optimal packing, and low conformational entropy.
*   **Hypothesis 2b:** The global topology of allosteric networks will differ between classes of molecular machines. For example, processive machines like the ribosome may feature linear, directional pathways, whereas regulatory hubs like the proteasome may exhibit more distributed, scale-free network architectures to integrate multiple signals.
*   **Prediction:** A comparative analysis of the Allosteric Atlases for the ribosome (translation), proteasome (degradation), and spliceosome (RNA processing) will reveal both conserved network motifs (e.g., critical 'hub' residues at subunit interfaces) and distinct topological signatures (e.g., average path length, clustering coefficient) that correlate with their biological roles.
*   **Validation:** We will employ a suite of graph theory metrics to characterize network topologies. We will test the hypothesis that residues identified as high-centrality hubs in our networks are significantly enriched for post-translational modification sites, disease mutations, and sites of evolutionary conservation, providing independent lines of evidence for their functional importance.

**Research Question 3: Can our integrated network model be used to make actionable predictions about novel allosteric sites and the functional consequences of mutations?**
A truly successful model must move beyond description to prediction, generating testable hypotheses for the broader scientific community.
*   **Hypothesis 3a:** The propagation of allosteric signals can be modeled as information flow on our weighted graph, allowing for the quantitative prediction of a perturbation's (e.g., mutation or ligand binding) effect at a distant functional site using concepts from information theory, such as mutual information.
*   **Hypothesis 3b:** By systematically calculating the information transfer efficiency from every residue to a known active site, we can generate whole-protein 'allosteric potential' maps, which will reveal cryptic or previously unknown allosteric sites suitable for therapeutic targeting.
*   **Prediction:** Our model will identify specific, high-potential residues in the 26S proteasome, distant from the catalytic chamber, that are predicted to allosterically modulate its proteolytic activity. Furthermore, our model will classify a significant fraction of currently annotated VUS in disease-relevant proteins as likely pathogenic or benign based on their predicted disruption of critical allosteric pathways.
*   **Validation:** While this project will not generate new experimental data, all predictions will be made publicly available through our web portal to be tested by the community. We will perform retrospective validation by assessing our model's ability to distinguish known pathogenic from benign variants in benchmark datasets. We will also track the reclassification of VUS in public databases over time to prospectively validate our predictions.

Methods And Approach

This project will be executed by a multidisciplinary working group composed of four collaborating labs with expertise in structural biology, computational biophysics, data science/machine learning, and biochemistry. The project is organized into a four-phase workflow, with significant cross-lab collaboration and trainee involvement at each stage.

**Working Group Structure and Collaboration:** The four PIs and their trainees will form a cohesive unit, meeting virtually bi-weekly and in-person at an annual workshop. A shared computational infrastructure, including a centralized database, a common GitHub organization for code development, and a project-wide Slack channel, will facilitate seamless collaboration. Trainees (postdocs and graduate students) will be co-mentored and will lead specific sub-projects, ensuring they gain cross-disciplinary skills in data synthesis, computational modeling, and team science.

**Phase 1: Systematic Data Curation and Harmonization (Months 1-9)**
This foundational phase focuses on aggregating and standardizing the public data that fuel our models. We will develop a robust, automated pipeline for this process.
*   **Data Sources:** We will target three major macromolecular machines: the human ribosome, the 26S proteasome, and the spliceosome. For each, we will systematically gather: 1) **Structural Data:** All available X-ray, cryo-EM, and NMR structures from the PDB and EMDB, including different conformational states, species orthologs, and ligand-bound forms. 2) **Sequence/Evolutionary Data:** We will construct deep multiple sequence alignments (MSAs) for each subunit from the UniRef100 and TrEMBL databases. 3) **Functional Data:** We will mine public repositories like MaveDB for relevant deep mutational scanning (DMS) datasets. 4) **Dynamic Data:** We will collate publicly available MD simulation trajectories from sources like the PDB, aMD-share, and literature-associated repositories.
*   **Data Integration Platform:** A key deliverable of this phase is a unified data schema. All data will be mapped onto a common reference structure for each complex using robust structural alignment algorithms. This process will handle challenges like different numbering schemes, missing domains, and species variations, creating a consistent, analysis-ready dataset.

**Phase 2: Multi-Layered Allosteric Network Construction (Months 6-18)**
Using the harmonized data, we will construct a multi-layered network representation for each complex.
*   **Layer 1 (Static Structural Network):** An ensemble of networks will be built from all curated structures. Nodes are Cα atoms, and edge weights will be derived from inter-residue distances, capturing the range of observed conformations.
*   **Layer 2 (Dynamic Correlation Network):** From MD trajectories, we will calculate the dynamic cross-correlation matrix (DCCM) between all residue pairs, with edge weights representing the degree of correlated motion.
*   **Layer 3 (Co-evolutionary Network):** Using our MSAs, we will apply direct coupling analysis (DCA) methods (e.g., GREMLIN) to compute co-evolutionary scores, which identify residues that evolve together, often due to functional or structural constraints.
*   **Layer 4 (Functional Epistasis Network):** Where DMS data is available, we will calculate pairwise epistatic scores between mutations to build a network based on functional coupling.
*   **Network Integration:** We will develop a novel machine learning framework (e.g., a weighted ensemble method or a graph neural network) to integrate these four layers. The final edge weight between any two residues will represent the synthesized evidence of their connection within an allosteric communication channel. This integration method is a core innovation of our proposal.

**Phase 3: Network Analysis and Pathway Identification (Months 15-30)**
With the integrated networks constructed, we will identify and characterize allosteric pathways.
*   **Pathway Algorithms:** We will implement and compare multiple algorithms to trace information flow. These will range from classic graph theory approaches like Dijkstra's shortest path algorithm to more sophisticated models based on current flow in resistor networks (e.g., Resistor Network Theory) and information theory. Specifically, we will calculate the mutual information between the states of residue pairs across the structural/dynamic ensemble to quantify information transfer capacity.
*   **Identification of Critical Nodes and Edges:** We will use a suite of network centrality metrics (e.g., betweenness centrality, closeness centrality) to identify 'hub' residues and 'bottleneck' edges that are predicted to be critical for allosteric signal transduction.

**Phase 4: Model Validation, Prediction, and Dissemination (Months 24-36)**
*   **Validation:** We will rigorously validate our framework by testing its ability to recapitulate known biological features. We will quantify the overlap between our predicted high-centrality nodes/pathways and independently determined data, including: 1) known catalytic and binding sites; 2) sites of post-translational modifications; 3) known disease mutations from ClinVar and HGMD; and 4) experimentally determined allosteric sites from the literature.
*   **Prediction and Atlas Generation:** For our target complexes, we will generate comprehensive, predictive 'Allosteric Atlases'. These atlases will consist of the integrated networks and maps of predicted communication pathways emanating from key functional sites. We will use these to predict novel, cryptic allosteric sites and to score the potential pathogenicity of VUS.
*   **Timeline and Milestones:**
    *   **Year 1:** Complete data curation pipeline; develop and benchmark network integration framework on test systems. First annual workshop.
    *   **Year 2:** Construct integrated networks for the proteasome and ribosome; implement and compare pathway prediction algorithms. Submit methods-focused manuscript.
    *   **Year 3:** Complete analysis of all target machines; generate predictive Allosteric Atlases; launch public web portal for data dissemination. Submit application-focused manuscripts. Host final dissemination workshop.

Expected Outcomes And Impact

This project is designed to produce transformative outcomes that will significantly advance the field of molecular and cellular biology, with broad impacts on biomedical research and workforce development. Our contributions will be both conceptual and practical, providing new knowledge, new tools, and a foundational resource for the scientific community.

**Intellectual Merit and Contributions to the Field:**
1.  **A New Paradigm for Allostery:** Our primary intellectual contribution will be to shift the conceptualization of allostery from a qualitative, protein-specific phenomenon to a quantitative, network-based science. By developing a generalizable framework, we will provide the language and tools to describe allostery as an emergent property of complex systems, enabling systematic comparison and the discovery of universal principles of molecular communication.
2.  **Methodological Innovation in Data Synthesis:** We will pioneer a novel computational pipeline for the synthesis of heterogeneous data types—structural, dynamic, evolutionary, and functional. This integrative approach addresses a major bottleneck in modern biology, where data is abundant but often siloed. The methods we develop for data harmonization and multi-layer network integration will be broadly applicable to other complex biological questions beyond allostery.
3.  **Creation of a Foundational 'Allosteric Atlas':** The project will deliver a unique, high-value resource for the scientific community. The Allosteric Atlas for the ribosome, proteasome, and spliceosome will be the first comprehensive map of information flow in these essential molecular machines. This resource, accessible via a public web portal, will serve as a hypothesis-generation engine, enabling researchers to explore communication pathways, interpret mutational data, and design new experiments.

**Broader Impacts and Applications:**
1.  **Accelerating Therapeutic Discovery:** A key practical outcome will be the identification of novel, cryptic allosteric sites on therapeutically important targets. Allosteric drugs offer the potential for greater specificity and novel modes of action compared to traditional active-site inhibitors. Our Allosteric Atlas will provide a rational basis for targeting complexes like the proteasome (a key cancer target) or the bacterial ribosome (a target for antibiotics), opening new avenues for drug development.
2.  **Improving Understanding of Human Disease:** Our framework will provide a powerful tool for interpreting the functional consequences of genetic variation. Many disease-causing mutations, especially the vast number of 'variants of uncertain significance' (VUS), likely exert their effects by subtly disrupting allosteric regulation. By mapping these variants onto our communication networks, we can develop a mechanistic basis for predicting their pathogenicity, aiding in genetic diagnosis and personalized medicine.
3.  **Training the Next Generation of Data-Savvy Scientists:** This project is an ideal training vehicle. Its inherently collaborative and interdisciplinary nature will equip graduate students and postdoctoral fellows with a unique skillset at the intersection of biophysics, computer science, and biology. They will gain hands-on experience in large-scale data analysis, computational modeling, open-source software development, and 'team science'—precisely the skills needed for the future scientific workforce. We will further amplify this impact through annual workshops and by making all our training materials publicly available.
4.  **Commitment to Open and Reproducible Science:** This project is fundamentally committed to the principles of open science. All software developed will be released under a permissive open-source license on GitHub. All curated data, network models, and analysis workflows (e.g., as Jupyter notebooks) will be made publicly available. The final Allosteric Atlas will be disseminated through a user-friendly web portal, ensuring that our results are not only published but are also accessible, explorable, and reusable by the entire community. This commitment ensures the long-term impact and sustainability of our work.

**Dissemination Plan:** Our findings will be disseminated broadly through high-impact, open-access publications, presentations at major international conferences (e.g., Biophysical Society, ISMB), and the aforementioned public web portal. In the final year, we will host a community workshop to train other researchers in the use of our tools and to foster new collaborations, ensuring the methods and resources from this project are widely adopted.

Budget And Resources

The proposed research represents a community-scale synthesis effort that is beyond the scope and resources of any single research laboratory. It requires the deep integration of expertise from four distinct scientific disciplines—structural biology, computational biophysics, computer science, and biochemistry—and a coordinated effort to curate, integrate, and analyze massive public datasets. The development of a public-facing, sustainable resource like the Allosteric Atlas also requires dedicated support that falls outside the purview of traditional research grants. The NCEMS program, with its focus on catalyzing multidisciplinary teams for data synthesis, is uniquely suited to support this ambitious project.

**Budget Justification:** The total requested budget is allocated to support the personnel, computational infrastructure, and collaborative activities essential for the project's success over a three-year period.

*   **Personnel (65%):** The majority of the funds will support the dedicated researchers who will execute the project. This includes four Postdoctoral Fellows, one in each of the four collaborating PIs' labs. These fellows will form the core research team, driving the development of the computational pipeline and the analysis of the target systems. We also request support for four Graduate Students, who will focus on specific sub-projects while receiving invaluable cross-disciplinary training. Partial summer salary is requested for the four PIs to provide scientific oversight, coordinate the working group, and lead training activities. Finally, we request support for a part-time Data Manager/Software Engineer to ensure robust data management practices, oversee the development of the public web portal, and manage open-source code releases.

*   **Computational Resources (15%):** While we will leverage the substantial high-performance computing (HPC) resources at our respective institutions, the sheer scale of the data aggregation and network analysis necessitates dedicated cloud computing resources (e.g., Amazon Web Services or Google Cloud Platform). These funds will be used for large-scale data storage, burstable computing capacity for intensive calculations (e.g., network construction from thousands of structures), and for hosting the robust, publicly accessible web portal and database.

*   **Travel (10%):** Collaboration is the cornerstone of this project. We request funds to support an annual in-person workshop for the entire working group (PIs, postdocs, and students). These multi-day meetings are indispensable for intensive brainstorming, resolving technical challenges, strategic planning, and fostering a cohesive team environment. Funds are also included for trainees and PIs to travel to one major international conference each year to present our findings, disseminate our tools, and engage with the broader scientific community.

*   **Publication and Dissemination (5%):** To adhere to our open science commitment, we request funds to cover open-access publication fees for our anticipated manuscripts. A portion of this budget is also allocated for the design, development, and long-term maintenance of the Allosteric Atlas web portal.

*   **Indirect Costs (F&A):** Indirect costs are calculated based on the federally negotiated rates for each participating institution.

**Existing Resources:** This proposal leverages significant institutional support, including access to university-maintained HPC clusters, data storage infrastructure, and institutional software licenses. The PIs' labs provide the necessary office and lab space. The requested NCEMS funds are specifically for the personnel and collaborative resources that make this large-scale synthesis project feasible.",,
ai_group_gemini_03,ai,group,gemini-2.5-pro,In Silico Origins: The Emergence of Autocatalytic Metabolic Networks from Prebiotic Chemistry,"The transition from a non-living chemical environment to the first self-sustaining, metabolizing life form is one of the greatest unsolved puzzles in science. This emergence of life was likely predicated on the formation of autocatalytic chemical networks capable of self-replication and evolution. This working group proposes a novel, purely computational approach to explore the plausible pathways for the emergence of primordial metabolism. We will synthesize and integrate disparate public data sources into a unified 'Prebiotic Chemical Universe' knowledge base. This includes: 1) comprehensive chemical reaction data from databases like KEGG and Rhea; 2) quantum chemistry calculations on the feasibility of prebiotic reactions; and 3) metabolomics data from diverse extant organisms to identify conserved core metabolic motifs. Our transdisciplinary team of systems biologists, computational chemists, evolutionary theorists, and astrobiologists will employ network expansion algorithms and principles from chemical engineering to simulate the growth of chemical networks from simple precursor molecules. We will search for the spontaneous emergence of autocatalytic cycles, identify key molecular 'scaffolds' that enable network complexity, and determine the minimal conditions required for a self-propagating metabolic system. This project will provide a principled, data-driven framework for generating testable hypotheses about the origin of life and the fundamental principles governing the emergence of biological complexity.",,"Background And Significance

The origin of life, or abiogenesis, represents the conceptual boundary between geochemistry and biochemistry, marking the emergence of complex, self-sustaining systems from a simpler, non-living chemical world. Understanding this transition is a fundamental challenge in science, with profound implications for biology, chemistry, and astrobiology. A central hypothesis in this field is the 'metabolism-first' model, which posits that self-propagating networks of chemical reactions, or protometabolism, preceded the emergence of template-replicating genetic molecules like RNA. These primordial metabolic networks would have needed to exhibit autocatalysis—the ability of a network's products to catalyze its own production—to achieve the persistence, growth, and eventual evolution necessary for life. Seminal theoretical work by pioneers like Oparin, Haldane, and later Kauffman on autocatalytic sets provided the conceptual framework, suggesting that given a sufficient diversity of molecules and reactions, self-sustaining cycles could emerge spontaneously. However, these early models were largely abstract and lacked a concrete chemical basis. Experimental work, from the classic Miller-Urey experiment demonstrating abiotic synthesis of amino acids to more recent studies on non-enzymatic glycolysis and reverse Krebs cycle analogs, has shown that many of the building blocks and reactions of modern metabolism are prebiotically plausible. Despite this progress, the field faces significant hurdles that prevent a comprehensive understanding of metabolic origins. A primary limitation is the fragmented nature of our knowledge. Data on plausible prebiotic reactions are scattered across disparate fields and databases, including organic chemistry literature, geochemical models of early Earth environments, and biological databases of extant metabolic pathways. There is no unified, computationally accessible repository that integrates these diverse data sources. Consequently, current computational models of protometabolism are often based on heavily curated subsets of modern biochemistry, which introduces a strong 'retrodictive' bias and may overlook novel, non-biological pathways that were critical stepping stones. Furthermore, the feasibility of many proposed prebiotic reactions under realistic conditions—considering temperature, pressure, pH, and the catalytic effects of mineral surfaces—is often assumed rather than rigorously evaluated. This leaves a critical gap between abstract network theory and concrete chemical reality. The combinatorial explosion of possible reactions from even a simple set of precursor molecules makes an exhaustive search for emergent properties computationally intractable without a principled, data-driven approach to constrain the search space. This project is both important and timely because it directly addresses these gaps through a large-scale data synthesis approach. The recent explosion in publicly available biochemical data (KEGG, Rhea), chemical reaction databases (Reaxys), and computational chemistry tools allows us, for the first time, to construct a comprehensive 'Prebiotic Chemical Universe' knowledge base. By integrating these datasets, we can move beyond biased, retrodictive models and instead simulate the forward evolution of chemical complexity from a set of plausible initial conditions. This project's scale and inherent transdisciplinarity—requiring expertise in systems biology, quantum chemistry, evolutionary theory, and data science—make it an ideal endeavor for a community-scale working group. It is beyond the scope of any single research lab and directly aligns with the call's focus on synthesizing public data to address fundamental questions of emergence in the molecular sciences.

Research Questions And Hypotheses

The overarching goal of this working group is to develop and implement a data-driven, computational framework to simulate the emergence of complex, self-sustaining metabolic networks from a simple prebiotic chemical environment. By synthesizing disparate public datasets into a unified reaction universe, we will explore the plausible pathways from geochemistry to biochemistry. This goal is broken down into four specific, interconnected research questions (RQs) and their corresponding testable hypotheses. 

**RQ1: What is the structure and scope of a plausible 'Prebiotic Chemical Universe' (PCU) reaction network when integrating data from geochemistry, computational chemistry, and modern biology?**
This question addresses the foundational need for a comprehensive, unbiased map of possible prebiotic reactions. 
*   **Hypothesis 1 (H1):** An integrated network of plausible prebiotic reactions, constrained by thermodynamic feasibility, will be topologically distinct from modern metabolic networks, exhibiting lower average connectivity and a different modular structure, yet will contain the seeds of biological complexity. 
*   **Prediction:** We predict the PCU graph will have a power-law degree distribution but with a different exponent and clustering coefficient compared to the KEGG network. We expect to find that certain reaction classes (e.g., aldol additions, redox reactions) form highly connected cores within this network.

**RQ2: Under what initial conditions (e.g., starting molecule sets, catalytic environments, energy sources) do autocatalytic cycles and self-propagating networks spontaneously emerge from the PCU?**
This question probes the environmental and chemical factors that trigger the transition from a static collection of chemicals to a dynamic, growing system.
*   **Hypothesis 2 (H2):** The emergence of autocatalysis is not a generic property of complex chemical systems but depends critically on a combination of specific mineral catalysts (e.g., iron-sulfur surfaces) and a continuous influx of a limited set of high-energy precursors (e.g., HCN, formaldehyde, phosphate).
*   **Prediction:** Our network expansion simulations will demonstrate a phase transition-like behavior. Below a certain threshold of catalytic enhancement or substrate availability, networks will be small and terminate quickly. Above this threshold, we will observe the rapid formation of large, self-propagating networks containing autocatalytic cycles. We will quantify this threshold as a function of environmental parameters.

**RQ3: What are the key topological features and molecular 'scaffolds' that facilitate the transition from simple linear pathways to complex, interconnected, and autocatalytic networks?**
This question seeks to identify the critical components—the 'linchpins'—that enable the bootstrapping of metabolic complexity.
*   **Hypothesis 3 (H3):** A small subset of versatile molecules (e.g., pyruvate, glyoxylate, simple sugars) act as crucial network hubs or 'scaffolds,' connecting disparate chemical pathways and enabling the closure of cycles. These molecules are not necessarily the most abundant but are the most topologically important.
*   **Prediction:** Using network centrality measures (e.g., betweenness centrality), we will identify a small set of high-scoring molecules in our emergent networks. *In silico* 'knockout' experiments, where we remove these molecules from the initial seed set or disallow their formation, will disproportionately cripple network growth and prevent autocatalysis compared to the removal of random molecules.

**RQ4: How do the structures of emergent *in silico* protometabolic networks compare to the conserved core metabolic pathways observed across all domains of life?**
This question addresses the long-standing debate of whether core metabolism is a 'frozen accident' of evolution or a deterministic outcome of fundamental chemical principles.
*   **Hypothesis 4 (H4):** The core logic of central carbon metabolism, particularly pathways like the reverse Krebs cycle (rTCA) and glycolysis/gluconeogenesis, represents a robust, convergent solution for carbon fixation and biosynthesis that will emerge repeatedly in simulations under reducing prebiotic conditions.
*   **Prediction:** Using subgraph isomorphism and network alignment algorithms, we will find that a significant fraction of our successful, autocatalytic simulations independently converge on networks containing motifs that are topologically and chemically homologous to the rTCA cycle or other ancient pathways, even when starting from diverse initial conditions.

Methods And Approach

Our research plan is structured into three synergistic phases, executed by a transdisciplinary team over 24 months. The entire project will adhere to open science principles, with all code and data developed collaboratively on a shared platform like GitHub.

**Phase 1: Construction of the 'Prebiotic Chemical Universe' (PCU) Knowledge Base (Months 1-6)**
This foundational phase focuses on data synthesis and integration. The PCU will be structured as a graph database (using Neo4j), where nodes are chemical compounds and edges are reactions.
*   **Data Sources:** We will integrate several distinct, publicly available data types. 1) **Biochemical Reactions:** We will extract reaction data from KEGG, MetaCyc, and Rhea databases. Using established methods for generating generalized reaction rules (e.g., based on bond changes), we will infer plausible non-enzymatic analogs. 2) **General Chemical Reactions:** We will mine the Reaxys database to include a broader scope of organic reactions not typically found in biological contexts. 3) **Experimental Prebiotic Chemistry:** We will perform systematic literature mining of journals like *Origins of Life and Evolution of Biospheres* to curate a set of experimentally verified prebiotic reactions. 4) **Thermodynamic and Kinetic Data:** Each reaction edge will be annotated with Gibbs free energy (ΔG) values. These will be sourced from databases like Equilibrator or, for novel reactions, calculated using quantum chemistry methods (DFT at the B3LYP/6-31G* level) to assess feasibility under various temperature and pH conditions. 5) **Catalysis Data:** We will incorporate information on the catalytic potential of early Earth minerals (e.g., iron sulfides, clays) from geochemical databases and the literature, encoding this as a potential reduction in the activation energy for specific reaction classes.

**Phase 2: Network Expansion Simulations (Months 7-18)**
This phase uses the PCU to simulate the growth of chemical networks from simple beginnings.
*   **Algorithm:** We will implement a network expansion algorithm in Python, leveraging libraries like RDKit for chemical informatics and NetworkX for graph analysis. The simulation proceeds iteratively: 
    1.  Initialize the network with a 'seed set' of simple molecules presumed abundant on the early Earth (e.g., H₂O, CO₂, CH₄, NH₃, HCN). 
    2.  At each step, query the PCU to find all reactions whose substrates are all present in the current network and satisfy a thermodynamic threshold (e.g., ΔG < 0). 
    3.  Add the products of these feasible reactions to the network, creating new nodes. 
    4.  Repeat until the network ceases to grow or reaches a predefined complexity.
*   **Computational Experiments:** We will conduct a large ensemble of simulations to robustly test our hypotheses. We will systematically vary key parameters, including: the composition of the initial seed set (reflecting different origin scenarios like hydrothermal vents vs. atmospheric synthesis), temperature and pH (which affect ΔG), and the inclusion of specific catalytic rules that mimic mineral surfaces. This systematic exploration will allow us to map the 'parameter space' that leads to the emergence of complexity.

**Phase 3: Analysis of Emergent Networks (Months 12-24)**
In this phase, we will analyze the structure and properties of the networks generated in Phase 2.
*   **Detection of Autocatalysis:** We will employ algorithms to identify autocatalytic motifs, from simple single-reaction cycles to complex, collectively autocatalytic RAF (Reflexively Autocatalytic and Food-generated) sets. This analysis is key to identifying self-sustaining systems.
*   **Topological and Chemical Analysis:** We will use a suite of graph-theoretic metrics to characterize the emergent networks, including degree distribution, clustering, and modularity. We will use centrality measures (betweenness, eigenvector) to pinpoint the 'scaffold' molecules predicted in H3. 
*   **Comparative Analysis:** To test H4, we will use network alignment and subgraph isomorphism algorithms (e.g., an adaptation of BLAST for chemical networks) to compare the topology of our emergent networks against the core metabolic maps from KEGG. This will provide a quantitative measure of the similarity between *in silico* emergent pathways and extant biology.

**Timeline and Milestones:**
*   **M6:** Public release of PCU knowledge base v1.0 and associated API.
*   **M12:** Completion of the first major ensemble of simulations; submission of a methods paper.
*   **M18:** Comprehensive analysis of autocatalytic networks and identification of key molecular scaffolds; presentation at a major international conference.
*   **M24:** Completion of comparative analysis with extant metabolism; submission of primary research articles; final working group meeting and public workshop.

Expected Outcomes And Impact

This project will generate significant outcomes that advance the field of origin of life studies and have broader impacts across several scientific disciplines. Our contributions are designed to be tangible, open, and foundational for future research.

**Intellectual Merit and Contributions to the Field:**
1.  **The Prebiotic Chemical Universe (PCU) Knowledge Base:** Our primary deliverable will be the PCU, a comprehensive, computationally accessible graph database of plausible prebiotic reactions. This will be an invaluable community resource, unifying fragmented data from chemistry, biology, and geochemistry. It will enable researchers to move beyond anecdotal evidence and build quantitative, testable models of prebiotic systems.
2.  **Data-Driven Pathways for Metabolic Emergence:** Instead of relying on speculation or biased retrodiction from modern biology, our project will generate a set of the most plausible, thermodynamically constrained pathways for the emergence of protometabolism. We will provide a ranked list of emergent autocatalytic cycles, offering a principled answer to the question, 'What did the first metabolisms look like?'
3.  **A New Methodological Framework:** Our integrated pipeline—combining data synthesis, network expansion, and graph-theoretic analysis—will establish a powerful new methodology for studying emergence in complex chemical systems. This framework can be adapted to investigate other emergent phenomena, such as the formation of protocells or the evolution of signaling networks.
4.  **Testable Hypotheses for Experimentalists:** By identifying key molecular scaffolds and critical environmental conditions, our *in silico* findings will generate specific, high-priority hypotheses for experimental validation. For example, we might predict that a specific dicarboxylic acid is essential for closing a protometabolic cycle in the presence of iron-sulfide catalysts, a prediction that can be directly tested in the lab.

**Broader Impacts:**
*   **Astrobiology and the Search for Life:** By defining the fundamental chemical principles and minimal conditions required for metabolic emergence, our work will directly inform astrobiology. It will help constrain the environmental conditions necessary for life to arise and refine the types of molecular biosignatures that missions like the James Webb Space Telescope should search for on exoplanets.
*   **Synthetic Biology and Biotechnology:** Understanding the principles of self-organizing chemical networks has direct applications in synthetic biology. Our findings could inspire novel designs for artificial metabolic pathways in engineered microbes for producing biofuels or pharmaceuticals, or contribute to the long-term goal of creating a synthetic protocell from non-living components.
*   **Training and Workforce Development:** This project is an ideal cross-disciplinary training ground. Trainees (postdocs and graduate students) will gain a unique combination of skills in data science, computational chemistry, systems biology, and evolutionary theory. Through collaborative work in a distributed team, they will be trained to become the next generation of data-savvy scientists, perfectly aligning with the research call's objectives.
*   **Public Outreach and Dissemination:** We are committed to open science. All data (PCU), software, and results will be made publicly available via FAIR-compliant repositories (e.g., GitHub, Zenodo). We will publish our findings in high-impact, open-access journals. Furthermore, the topic of life's origins has broad public appeal. We will develop interactive web-based visualizations of our network simulations to engage students and the public, making complex scientific concepts accessible.

**Long-Term Vision:** This project lays the groundwork for a comprehensive *in silico* model of abiogenesis. The working group established here will be uniquely positioned to secure future funding to extend this framework, integrating it with models of membrane encapsulation and the emergence of primitive genetic polymers, ultimately aiming for a complete, dynamic simulation of a protocell.

Budget And Resources

The proposed research requires a synergistic, multi-institutional collaboration that is beyond the capacity of a single research lab, making it an ideal project for NCEMS support. The synthesis of vast, heterogeneous datasets from chemistry, biology, and geochemistry, coupled with the need for expertise in quantum chemistry, network theory, and evolutionary biology, necessitates the formation of a dedicated working group. NCEMS resources are critical for coordinating this effort, facilitating the deep integration required for success, and providing a platform for training the next generation of interdisciplinary scientists. The requested budget is for a 24-month period.

**1. Personnel ($380,000):** This constitutes the largest portion of the budget, dedicated to supporting the researchers who will execute the project.
*   **Postdoctoral Scholars (2 FTEs):** $240,000. Two postdocs will be hired for the project duration. Postdoc 1 (based with PI 1) will specialize in network biology and algorithm development. Postdoc 2 (based with PI 2) will specialize in computational chemistry and data curation. This provides the core effort for the project.
*   **Graduate Student Support (2 students, 50% time):** $80,000. Partial support for two graduate students who will assist with data integration, running simulations, and analysis, providing them with an exceptional training opportunity.
*   **PI Summer Salary (3 PIs, 1 month/year):** $60,000. To allow the PIs to dedicate focused time to project management, analysis, and manuscript preparation during the summer months.

**2. Travel ($40,000):** Essential for fostering collaboration and disseminating results.
*   **Annual Working Group Meetings:** $25,000. Funds to bring the entire team (PIs and trainees) together for two in-person, multi-day workshops. These meetings are vital for intensive brainstorming, resolving technical challenges, and strategic planning.
*   **Conference Travel:** $15,000. To support travel for trainees and PIs to present findings at major international conferences such as ISSOL, GRC on Origins of Life, or ISMB, ensuring broad dissemination of our work.

**3. Computational Resources ($45,000):**
*   **HPC Cluster Access:** $25,000. To purchase allocation on high-performance computing clusters for running thousands of network expansion simulations and the necessary DFT calculations for thermodynamic parameters.
*   **Cloud Computing and Data Storage:** $20,000. For cloud-based services (e.g., AWS) to host the public-facing PCU graph database, manage large datasets, and support collaborative development platforms.

**4. Materials and Supplies ($10,000):**
*   Includes software licenses and subscriptions to chemical databases required for data mining.

**5. Publication and Dissemination ($25,000):**
*   **Open Access Fees:** $15,000. To cover article processing charges for publishing our results in high-impact open-access journals.
*   **Final Workshop:** $10,000. To host a workshop at the end of the project to share our tools (PCU, software) and findings with the broader scientific community.

**Total Direct Costs:** $500,000
**Indirect Costs (F&A) at 50%:** $250,000
**Total Requested Budget:** $750,000",,
ai_group_gemini_04,ai,group,gemini-2.5-pro,The Cancer Kinome's Emergent Logic: Predicting Therapeutic Resistance and Plasticity through Network Synthesis,"The ability of cancer cells to resist targeted therapies and adapt to treatment is an emergent property of their rewired signaling networks. The protein kinome, a network of over 500 kinases, is a central hub for this rewiring, yet its complexity makes therapeutic responses difficult to predict. This working group will address this challenge by building dynamic, context-specific models of the cancer kinome. We will synthesize a massive collection of public data, including phosphoproteomics, genomics, and transcriptomics from The Cancer Genome Atlas (TCGA) and the Clinical Proteomic Tumor Analysis Consortium (CPTAC), alongside drug sensitivity data from the GDSC and CTRP databases for thousands of cancer cell lines. Our team, composed of cancer biologists, systems biologists, bioinformaticians, and clinical pharmacologists, will develop a machine learning framework to learn the rules of kinome rewiring in different cancer subtypes and in response to specific kinase inhibitors. The primary goal is to create predictive models that can identify emergent feedback loops and bypass pathways that lead to drug resistance before they arise. These models will be used to systematically predict synergistic drug combinations that can exploit network vulnerabilities and overcome the emergent robustness of the cancer kinome. This project will produce a powerful open-source platform for in silico drug screening and hypothesis generation, accelerating the development of more durable cancer therapies.",,"Background And Significance

The advent of targeted cancer therapies, particularly protein kinase inhibitors, has revolutionized oncology, offering remarkable efficacy in patient subsets with specific molecular alterations. Drugs like imatinib for CML and gefitinib for EGFR-mutant lung cancer exemplify the power of precision medicine. However, the initial success of these therapies is frequently undermined by the development of resistance, which remains a formidable clinical challenge. Resistance is not a simple, monolithic event but an emergent property of the complex, adaptive system of cellular signaling networks. At the heart of this system lies the protein kinome, comprising over 500 kinases that regulate virtually all cellular processes. In cancer, this network is extensively rewired, creating a robust and plastic system that can rapidly adapt to therapeutic insults. Understanding and predicting this adaptive rewiring is paramount to developing more durable cancer treatments.

Current understanding of resistance is largely based on two classes of mechanisms: on-target alterations, such as secondary mutations in the drug's target kinase (e.g., the T790M 'gatekeeper' mutation in EGFR), and off-target rewiring. The latter is far more complex and involves the activation of parallel or downstream signaling pathways that bypass the inhibited node. For instance, MET amplification can confer resistance to EGFR inhibitors by activating the ERBB3-PI3K pathway independently of EGFR. Similarly, feedback loops, where the inhibition of a downstream kinase leads to the paradoxical reactivation of upstream signaling, are common. A classic example is the reactivation of the MAPK pathway through RAF dimerization following BRAF inhibitor treatment in melanoma. While these individual mechanisms are well-documented, they represent a reductionist view of a system-level problem. We lack a holistic understanding of the 'logic' of kinome rewiring—the generalizable principles and network motifs that govern how the entire system responds to perturbation.

The past decade has witnessed an explosion of publicly available, large-scale molecular and pharmacological data. Projects like The Cancer Genome Atlas (TCGA) and the Clinical Proteomic Tumor Analysis Consortium (CPTAC) have generated comprehensive genomic, transcriptomic, and proteomic maps for thousands of tumors across dozens of cancer types. Concurrently, pharmacogenomic screens such as the Genomics of Drug Sensitivity in Cancer (GDSC) and the Cancer Therapeutics Response Portal (CTRP) have profiled the sensitivity of over 1,000 cancer cell lines to hundreds of small molecules. These datasets represent an unprecedented resource, yet their full potential has not been realized. Most analyses have focused on identifying simple biomarker-drug associations (e.g., BRAF mutation predicts BRAF inhibitor sensitivity), which often fail to capture the network context that dictates the ultimate therapeutic outcome. Previous modeling efforts have either been limited to small, well-characterized pathways or have used statistical models that treat genes as independent features, ignoring the intricate network topology that defines their function. The key gap in the field is the absence of a comprehensive, data-driven framework that can synthesize these disparate data types to model the kinome as an integrated, dynamic system. Such a model is essential to move beyond predicting sensitivity to single agents and begin predicting the emergent, adaptive responses that lead to resistance. This project is timely and critical because it leverages the confluence of massive public data availability and recent advances in network-based machine learning to address this fundamental gap. By building a predictive model of kinome plasticity, we aim to systematically uncover the vulnerabilities of this adaptive system, providing a rational basis for the design of robust combination therapies.

Research Questions And Hypotheses

This working group is founded on the central premise that therapeutic resistance is an emergent property of kinome network dynamics that can be modeled and predicted through the synthesis of large-scale public data. Our research is structured around three specific, interconnected questions designed to deconstruct this complexity and translate the resulting knowledge into actionable therapeutic hypotheses.

**Research Question 1: What are the conserved and cancer-type-specific patterns of kinome rewiring that mediate adaptive resistance to targeted kinase inhibitors?**
While individual resistance mechanisms have been identified, a global, systematic map of the network-level adaptations is lacking. We seek to identify recurrent 'rewiring modules'—sets of kinases and downstream effectors whose activities are coordinately altered—that represent common solutions evolved by cancer cells to bypass therapeutic inhibition. 
*   **Hypothesis 1:** We hypothesize that despite the genetic heterogeneity of cancers, the functional space of kinome resistance mechanisms is constrained, leading to the recurrent activation of a finite set of rewiring modules across diverse cancer contexts. These modules will involve coordinated changes in kinase phosphorylation, protein expression, and transcriptional programs that restore critical downstream cellular functions (e.g., proliferation, survival) that were suppressed by the drug.
*   **Testing and Validation:** We will develop a multi-view matrix factorization and clustering approach to integrate phosphoproteomic, transcriptomic, and drug sensitivity data from CPTAC and GDSC. This method will identify modules of co-regulated genes and proteins whose activation state correlates with resistance to specific classes of inhibitors. The biological significance of these predicted modules will be validated by testing for enrichment of known signaling pathways (e.g., KEGG, Reactome) and by assessing their prognostic value in independent clinical datasets from TCGA. We expect to deliver a comprehensive atlas of these resistance modules, providing a functional blueprint of kinome plasticity.

**Research Question 2: Can a network-based machine learning model, which explicitly encodes the physical and functional relationships between kinases, predict cellular response to kinase inhibitors more accurately than feature-based models?**
Standard machine learning approaches for drug sensitivity prediction often treat genomic or transcriptomic features as an unstructured 'bag of features,' ignoring the underlying network topology. We propose that a model that 'understands' the structure of the kinome will learn a more robust and interpretable representation of cellular state.
*   **Hypothesis 2:** We hypothesize that a Graph Neural Network (GNN) model, built upon a comprehensive protein-protein interaction and kinase-substrate network, will outperform traditional models (e.g., Elastic Net, Random Forest) in predicting drug sensitivity. The GNN's architecture allows it to learn how signals propagate from a drug's target through the network, integrating information from the entire molecular context (mutations, expression levels) of a cell to predict the system's response.
*   **Testing and Validation:** We will construct a baseline kinome graph using data from STRING, PhosphoSitePlus, and other databases. Node features will be derived from multi-omic data (mutations, CNV, gene expression) for ~1,000 cell lines. The GNN will be trained to predict IC50 values for hundreds of kinase inhibitors from GDSC. Performance will be rigorously evaluated using cross-validation and on held-out test sets of cell lines and drugs. The model's interpretability will be assessed by using graph attention mechanisms to identify the specific kinases and subnetworks most influential in predicting response to a given drug. The deliverable will be a validated, open-source predictive model of single-agent efficacy.

**Research Question 3: Can in silico perturbation of our dynamic kinome model systematically identify synergistic drug combinations that preemptively block adaptive resistance pathways?**
The ultimate goal is to use our understanding of network rewiring to design more effective therapies. Synergistic combinations are thought to work by targeting parallel pathways or by blocking feedback mechanisms. Our model provides a platform to test this concept systematically.
*   **Hypothesis 3:** We hypothesize that by simulating the effect of a single kinase inhibitor in our trained GNN model, we can predict the emergent resistance state of the kinome. The nodes (kinases) that become most 'activated' in this simulated resistant state are prime targets for a second, synergistic drug. We predict that drug combinations targeting the primary driver and the predicted bypass pathway will exhibit significant synergy.
*   **Testing and Validation:** For each primary inhibitor, we will perform an in silico perturbation within the GNN. We will analyze the resulting network state to identify and rank predicted bypass kinases. This will generate a ranked list of thousands of potential synergistic drug pairs. This list of predictions will be validated against large-scale experimental combination screening data, such as the NCI-ALMANAC database, which serves as a massive, independent truth set. The expected outcome is a validated computational pipeline for prioritizing novel combination therapies for experimental testing.

Methods And Approach

Our research plan is a multi-phase, integrative strategy that progresses from data synthesis to predictive modeling and therapeutic hypothesis generation. The entire workflow is designed to be reproducible, scalable, and compliant with open science principles.

**Phase 1: Comprehensive Data Acquisition, Harmonization, and Integration (Months 1-9)**
This foundational phase focuses on assembling a unified, analysis-ready data resource. This task is substantial and requires dedicated bioinformatic expertise, making it ideal for a collaborative working group.
*   **Data Sources:** We will aggregate data from multiple public repositories. (1) **Molecular Profiles:** Genomics (somatic mutations, copy number variations) and transcriptomics (RNA-Seq) from The Cancer Genome Atlas (TCGA) and the Cancer Cell Line Encyclopedia (CCLE). (2) **Proteomics:** Global and phosphoproteomics data (Mass Spectrometry, RPPA) from the Clinical Proteomic Tumor Analysis Consortium (CPTAC) and CCLE. (3) **Pharmacology:** Drug sensitivity metrics (IC50, AUC) for hundreds of compounds across >1,000 cell lines from the Genomics of Drug Sensitivity in Cancer (GDSC), Cancer Therapeutics Response Portal (CTRP), and the PRISM Repurposing Screen. (4) **Network Priors:** Protein-protein interactions (PPIs) from STRING, BioGRID, and IntAct. Kinase-substrate relationships from PhosphoSitePlus, SIGNOR, and NetworKIN.
*   **Harmonization Pipeline:** A systematic pipeline will be developed to process these heterogeneous datasets. This includes: quality control, consistent normalization methods (e.g., TPM for RNA-Seq, z-scoring for proteomics), batch effect correction where applicable, and mapping all genes, proteins, and compounds to standardized identifiers (e.g., HGNC, UniProt, PubChem). This curated, multi-modal data matrix will be a key deliverable for the community.

**Phase 2: Construction of a Dynamic, Context-Specific Kinome Network Model (Months 6-18)**
This phase addresses our first two research questions by developing our core predictive model.
*   **Graph Construction:** We will construct a large-scale graph representing the human kinome. Nodes will be proteins (primarily kinases), and edges will represent known interactions (PPIs, kinase-substrate). This base graph will be generic.
*   **Context-Specific Feature Engineering:** For each of the ~1,000 cell lines, we will decorate this graph with context-specific features. Each node will be assigned a feature vector containing its basal gene expression, mutation status, and copy number state. Edge weights can be modulated by evidence of co-expression or correlated phosphorylation patterns within a specific cancer type.
*   **Graph Neural Network (GNN) Development:** We will implement a Graph Attention Network (GAT), a state-of-the-art GNN architecture. The GAT is chosen for its ability to learn the relative importance of different neighbors in the network, making it highly interpretable. The model will take a cell line's feature-annotated kinome graph as input and will be trained to output a vector of predicted drug sensitivity values (AUCs) for a panel of ~200 kinase inhibitors. The loss function will be the mean squared error between predicted and experimental AUCs.
*   **Training and Validation:** The model will be trained on 80% of the cell lines. Hyperparameters will be tuned using a 10% validation set. Final performance will be evaluated on the remaining 10% held-out test set of cell lines. We will also perform out-of-sample validation on drugs not seen during training to assess the model's ability to generalize. Performance will be benchmarked against standard machine learning models (e.g., Elastic Net, Random Forest) to quantify the added value of the graph-based approach.

**Phase 3: In Silico Perturbation for Synergy Prediction and Mechanistic Insight (Months 18-30)**
This phase leverages the trained model to address our third research question.
*   **Perturbation Simulation:** To simulate the effect of a drug, we will perform an in silico 'knockdown' on its target node(s) in the input graph. This can be done by masking the node's features or by adding a strong negative bias to its initial state. We will then perform a forward pass through the trained GNN to predict the post-perturbation network state. The difference between the basal and perturbed internal node embeddings will represent the network's adaptive response.
*   **Identification of Resistance Pathways:** We will analyze the predicted post-perturbation state to identify kinases whose activity (as represented by their learned embeddings) is most significantly increased. These nodes represent the hubs of the emergent resistance network.
*   **Systematic Synergy Prediction:** This process will be automated. For each of the ~200 kinase inhibitors as a 'primary' drug, we will identify the top-k predicted resistance kinases. We will then score all possible 'secondary' drugs from our panel based on whether they target one of these resistance kinases. This will generate a comprehensive, ranked matrix of predicted synergistic combinations.
*   **Validation against Experimental Data:** Our primary validation set will be the NCI-ALMANAC database, which contains experimental synergy data for thousands of pairs. We will assess the enrichment of experimentally verified synergistic pairs in the top quantiles of our predicted synergy scores, using metrics like ROC-AUC and Precision-Recall curves.

**Phase 4: Dissemination and Platform Development (Months 24-36)**
*   **Timeline and Milestones:**
    *   **Year 1:** Complete data harmonization pipeline (M9). First prototype of the GNN model trained and benchmarked (M12).
    *   **Year 2:** Final validated single-drug prediction model (M18). Completion of systematic in silico perturbation screen (M24). First manuscript submitted.
    *   **Year 3:** Validation of synergy predictions against NCI-ALMANAC (M30). Launch of public web portal with query and visualization capabilities (M33). Final project report and publications (M36).

Expected Outcomes And Impact

This project is designed to produce a transformative shift in our understanding of cancer therapy resistance, moving the field from a static, component-level view to a dynamic, system-level perspective. The expected outcomes are multifaceted, spanning fundamental scientific insights, powerful new computational tools, actionable therapeutic hypotheses, and significant contributions to training and open science, directly aligning with the core mission of the NCEMS research call.

**Intellectual Merit and Contributions to the Field:**
The primary contribution will be a fundamentally new understanding of the emergent logic governing kinome plasticity. We will move beyond cataloging individual resistance mechanisms to defining the principles of network adaptation.
1.  **An Atlas of Kinome Rewiring:** We will deliver the first comprehensive, data-driven map of the functional modules that cancer cells use to adapt to therapeutic pressure. This 'rewiring atlas' will serve as a foundational resource for cancer biologists, revealing conserved vulnerabilities and context-specific dependencies across dozens of cancer types.
2.  **A Novel Predictive Framework:** Our Graph Neural Network model will represent a significant methodological advance for computational oncology. By explicitly incorporating network topology, it will provide more accurate and, crucially, more interpretable predictions than existing methods. The model's ability to identify the specific subnetworks driving a prediction will offer mechanistic insights that are absent in 'black-box' approaches.
3.  **High-Confidence Therapeutic Hypotheses:** Unlike purely correlational studies, our in silico perturbation approach provides a mechanistic rationale for each predicted synergistic drug combination. We will produce a prioritized list of novel combinations, complete with their predicted molecular mechanism of action (e.g., blocking a specific feedback loop). This will provide a rich set of testable hypotheses for the broader cancer research community, significantly de-risking and accelerating the preclinical drug development pipeline.

**Broader Impacts and Applications:**
The impact of this work will extend far beyond the immediate working group, providing resources and knowledge that will catalyze research across academia and industry.
*   **Accelerating Translational Research:** The open-source platform and web portal will empower individual cancer researchers, who may lack the computational expertise to perform such large-scale analyses, to explore kinome dynamics. A biologist could, for example, query the model to predict sensitivity to a new inhibitor in their cell line of interest or to find the best combination partner for an existing drug, generating immediate, testable hypotheses for their lab.
*   **Informing Clinical Strategy:** While our work is preclinical, it lays the groundwork for network-based patient stratification. In the long term, models like ours could be adapted to use patient tumor data to predict optimal combination therapies, contributing to the vision of personalized medicine.
*   **Adherence to Open and Team Science:** This project is intrinsically collaborative and open. By synthesizing public data, we maximize its value and return on investment. All our methods, code, and derived data products will be made publicly available through platforms like GitHub and Zenodo. This commitment ensures our results are reproducible, transparent, and can be built upon by others, fostering a community-wide effort to solve the problem of drug resistance.

**Training and Dissemination:**
A core objective is to train the next generation of interdisciplinary scientists. Trainees (graduate students and postdocs) will be at the center of the collaboration, working across labs and disciplines. They will gain invaluable skills in large-scale data science, network biology, machine learning, and cancer pharmacology—a skill set in high demand. We will host annual project-wide workshops and hackathons to foster a collaborative environment and promote cross-pollination of ideas. Our dissemination strategy is aggressive and multi-pronged: we will publish our findings in high-impact, open-access journals, present at key international conferences (e.g., AACR, ISMB, ASCO), and, most importantly, release our user-friendly web portal as a persistent, community-facing resource. This ensures our work has a lasting and broad impact. The long-term vision is for this platform to become a living resource, continuously updated with new datasets and model improvements, serving as a central hub for network-based cancer pharmacology.

Budget And Resources

The proposed research represents a large-scale synthesis effort that is beyond the scope and resources of any single research laboratory. The project's success hinges on the integration of diverse expertise—from cancer biology and clinical pharmacology to network systems biology and machine learning—and requires significant, dedicated resources for personnel, computation, and collaboration. NCEMS support is therefore essential to assemble this multidisciplinary working group and provide the necessary infrastructure for a community-scale project of this magnitude.

**Personnel (Total: $980,000 over 3 years)**
This is the largest and most critical component of the budget, supporting the dedicated effort of trainees and staff who will perform the research.
*   **Postdoctoral Fellows (2):** $390,000. One fellow will specialize in bioinformatics and data harmonization, managing the complex data integration pipeline. The second will specialize in machine learning and computational modeling, leading the development and implementation of the GNN framework. (Salary: $55,000/year + 20% fringe benefits x 2 fellows x 3 years).
*   **Data Scientist/Software Engineer (1):** $270,000. A full-time data scientist is crucial for building and maintaining the robust, user-friendly web portal and the underlying database, ensuring the project's primary deliverable is a sustainable community resource. (Salary: $75,000/year + 20% fringe x 1 FTE x 3 years).
*   **Graduate Students (2):** $240,000. Two graduate students will be trained through this project, contributing to model development, validation, and analysis. This directly supports the call's goal of training the future data-savvy workforce. (Stipend + Tuition: $40,000/year x 2 students x 3 years).
*   **Principal Investigator Support:** $80,000. Modest summer support for the four lead PIs to dedicate time for project management, scientific oversight, and trainee mentorship (0.5 months/year x 4 PIs).

**Computational Resources (Total: $90,000)**
Training deep learning models like GNNs on thousands of high-dimensional samples is computationally intensive.
*   **Cloud Computing Credits:** $75,000. Funds for AWS or Google Cloud Platform are required for access to high-memory nodes and, critically, GPU instances necessary for efficient model training ($25,000/year).
*   **Data Storage:** $15,000. Secure and redundant storage for the harmonized multi-terabyte dataset ($5,000/year).

**Travel and Collaboration (Total: $60,000)**
Fostering a cohesive and collaborative team is paramount.
*   **Working Group Meetings:** $36,000. Funds to support twice-yearly, in-person meetings for the entire team (PIs and trainees) to facilitate deep collaboration, brainstorming, and project planning ($6,000/meeting x 2 meetings/year x 3 years).
*   **Conference Travel:** $24,000. To support trainees and PIs in disseminating findings at major international conferences like AACR, ISMB, or Keystone Symposia ($4,000/year x 2 attendees x 3 years).

**Dissemination and Publication (Total: $20,000)**
*   **Open Access Publication Fees:** $20,000. To ensure all findings are published in high-impact, open-access journals, maximizing their reach and impact (approx. 4-5 publications).

**Total Direct Costs:** $1,150,000
**Indirect Costs (F&A) at 50%:** $575,000
**Total Requested Budget:** $1,725,000",,
ai_group_gemini_05,ai,group,gemini-2.5-pro,Sociomicrobiology: Modeling the Emergence of Gut Ecosystem Function from Community-Scale Metabolic Interactions,"The human gut microbiome is a complex ecosystem whose collective metabolic activities profoundly impact host health. These activities are not the sum of individual microbes but are emergent properties arising from a dense web of metabolic competition, cross-feeding, and collaboration. This working group aims to decipher the principles governing the emergence of community-level function in the gut microbiome. We will integrate and synthesize multi-omic data from the world's largest public microbiome repositories, including the Human Microbiome Project (HMP) and the American Gut Project. This includes metagenomic data to identify community composition, metatranscriptomic data to assess gene activity, and metabolomic data to measure metabolic outputs. Our diverse team, including microbial ecologists, systems biologists, computer scientists, and nutritionists, will go beyond simple taxonomic cataloging. We will employ novel computational pipelines to reconstruct genome-scale metabolic models for thousands of microbial species and assemble them into community-scale models. Using ecological theory and flux balance analysis, we will simulate nutrient flow, identify keystone species and metabolic handoffs, and predict how the system responds to perturbations like diet shifts or antibiotic administration. The project will deliver a dynamic, predictive model of the gut ecosystem, providing a framework for understanding dysbiosis and designing precision interventions (e.g., probiotics, prebiotics) to rationally engineer this critical emergent system.",,"Background And Significance

The human gut microbiome represents one of the most complex and densely populated ecosystems on Earth, comprising trillions of microorganisms that collectively encode a metabolic potential far exceeding that of their host. It is now unequivocally established that this microbial community is not a passive passenger but an active, integral component of human physiology, influencing nutrition, immune system development, and even neurological function. The collective behavior of this system is a quintessential example of emergence, where community-level functions, such as the production of neuroactive short-chain fatty acids (SCFAs) or the resistance to pathogen invasion, arise from a vast network of interactions between hundreds of microbial species. These functions cannot be predicted by studying microbes in isolation. Early microbiome research, driven by advances in DNA sequencing, focused on cataloging the taxonomic composition of the gut, successfully linking shifts in community structure (dysbiosis) to a wide range of chronic diseases, including inflammatory bowel disease (IBD), obesity, type 2 diabetes, and colorectal cancer. Seminal projects like the Human Microbiome Project (HMP) and MetaHIT provided foundational datasets that revealed the immense inter-individual variability and the existence of a 'core' functional capacity despite taxonomic heterogeneity. However, this descriptive phase of microbiome science has yielded primarily correlational insights, leaving a critical gap in our understanding of the underlying mechanisms. The central challenge has shifted from 'who is there?' to 'what are they doing, how, and why?'. To answer this, the field has embraced multi-omics, integrating metagenomics (genetic potential), metatranscriptomics (gene expression), proteomics (protein activity), and metabolomics (metabolic output). While these data types provide unprecedented depth, their integration remains a formidable challenge. Most analyses remain siloed, failing to capture the causal chain from gene to function within a complex community context. A key limitation of current approaches is the lack of a predictive, mechanistic framework. Ecological models based on Lotka-Volterra equations can describe population dynamics but often lack biochemical detail. In parallel, the field of systems biology has developed genome-scale metabolic models (GEMs), which are mathematical representations of an organism's entire metabolic network. Constraint-based modeling techniques like flux balance analysis (FBA) can use GEMs to predict metabolic fluxes and growth rates under specific environmental conditions. This approach has been successfully extended to model small microbial communities, revealing principles of metabolic cross-feeding (syntrophy) and competition. For instance, studies have shown how methanogens and sulfate-reducing bacteria depend on hydrogen produced by fermenters, a classic example of an emergent metabolic process. However, scaling these methods to the complexity of the human gut microbiome has been computationally and methodologically prohibitive. Existing community models are often limited to a few dozen well-characterized organisms, failing to capture the diversity of the system. Furthermore, they are typically static and do not adequately incorporate dynamic constraints from other omics data, such as gene expression levels from metatranscriptomics. This project is timely and crucial because we are at a technological and data-driven inflection point. The public availability of massive, multi-omic datasets from thousands of individuals provides the raw material for an unprecedented synthesis effort. Concurrently, advances in bioinformatics, including high-throughput metagenome-assembled genome (MAG) recovery and automated GEM reconstruction tools (e.g., CarveMe, gapseq), make it feasible to build models for thousands of previously uncultured organisms. By uniting a transdisciplinary team of microbial ecologists, systems biologists, and computer scientists, this working group is uniquely positioned to bridge the gap between descriptive multi-omics and predictive, mechanistic understanding. We will move beyond correlation to causation, building a dynamic model of the gut ecosystem that can predict the emergence of function from structure. This will provide a foundational framework for the rational engineering of the microbiome, transforming our approach to nutrition and medicine.

Research Questions And Hypotheses

The overarching goal of this working group is to decipher the fundamental principles by which community-scale metabolic interactions generate emergent functional properties in the human gut microbiome. We will achieve this by developing and applying a predictive, multi-omic-constrained computational framework. Our research is structured around three specific, interconnected aims, each addressing key questions and testable hypotheses.

**Aim 1: Develop a scalable, integrated pipeline for constructing context-specific community metabolic models from public multi-omic data.** This aim establishes the core methodological foundation of our project.
*   **Research Question 1.1:** Can we systematically and accurately reconstruct thousands of high-quality, species-specific genome-scale metabolic models (GEMs) from metagenomic data and integrate them into robust, sample-specific community models?
*   **Hypothesis 1.1:** We hypothesize that a hybrid computational pipeline, which combines the speed of automated GEM reconstruction tools with a data-driven, semi-automated curation process informed by phylogenetic context and known physiology, will produce models with significantly higher predictive accuracy for microbial growth phenotypes and metabolic capabilities compared to purely automated approaches. 
*   **Validation:** Reconstructed GEMs for cultured species will be validated against experimental data from the literature (e.g., Biolog growth assays). The predictive accuracy of the community models will be benchmarked by their ability to recapitulate known community-level functions in well-defined synthetic communities.
*   **Research Question 1.2:** How can metatranscriptomic and metabolomic data be systematically integrated to constrain community models, transforming them from static representations of genetic potential to dynamic models of in situ metabolic activity?
*   **Hypothesis 1.2:** We hypothesize that integrating metatranscriptomic data to define reaction flux bounds and metabolomic data to set environmental nutrient conditions will dramatically improve the model's fidelity in predicting community-level metabolic outputs (e.g., fecal short-chain fatty acid profiles) compared to models constrained by metagenomic abundance alone.
*   **Validation:** Using datasets with matched multi-omics (e.g., HMP2), we will perform k-fold cross-validation. We will train the model constraints on a subset of samples and test its ability to predict the metabolomic profiles of held-out samples, measuring the correlation between predicted and observed metabolite concentrations.

**Aim 2: Identify the organizing principles and keystone components of the gut metabolic network.** This aim uses our validated models to uncover fundamental ecological and metabolic rules.
*   **Research Question 2.1:** What are the dominant patterns of metabolic interaction (e.g., competition, syntrophy, parasitism) that structure the gut community, and how do these patterns differ across host health states (e.g., healthy vs. IBD)?
*   **Hypothesis 2.1:** We hypothesize that the gut metabolic network is not random but is organized into functional guilds—groups of taxonomically diverse species that perform similar metabolic roles (e.g., primary fiber degraders, lactate consumers, butyrate producers). We predict that ecosystem stability is conferred by high functional redundancy within these guilds and that dysbiosis is characterized by a loss of specific guilds or a breakdown in key metabolic handoffs between them.
*   **Validation:** We will apply network analysis algorithms to the simulated metabolic exchange fluxes from hundreds of community models. We will identify modules in this network that correspond to our hypothesized guilds and show that guild structure is more conserved across healthy individuals than taxonomic structure.
*   **Research Question 2.2:** Can we identify 'keystone species' or, more importantly, 'keystone functions' that have a disproportionate impact on the stability and emergent functions of the community?
*   **Hypothesis 2.2:** We hypothesize that metabolic keystones are organisms or enzymatic functions that uniquely provide or consume critical intermediate metabolites (e.g., H2, formate, essential amino acids) that link major metabolic pathways. Their in-silico removal will cause a significant, non-linear decrease in critical community functions, such as total biomass production or SCFA synthesis, that is far greater than predicted by their abundance alone.
*   **Validation:** We will perform systematic in-silico knockout simulations, removing one species at a time from the community models and quantifying the impact on key functional outputs. The results will allow us to rank species by their 'keystone index'.

**Aim 3: Build and validate a predictive model of microbiome response to defined perturbations.** This aim leverages our framework to make forward predictions, moving from explanation to prediction.
*   **Research Question 3.1:** How do dietary shifts (e.g., changes in fiber, fat, or protein content) and antibiotic administration alter the metabolic interaction network and emergent functions of the gut microbiome?
*   **Hypothesis 3.1:** We hypothesize that our constrained community models can accurately predict the direction and magnitude of change in key metabolic outputs (e.g., butyrate/propionate ratio) in response to a simulated dietary intervention. For example, a simulated increase in dietary fiber will increase flux through fermentation pathways and select for butyrate-producing guilds, a prediction we can test against real-world data.
*   **Validation:** We will use our models to predict the outcomes of published dietary intervention studies where longitudinal multi-omic data is available. We will initialize our models with baseline data, simulate the dietary change by altering the nutrient input conditions, and compare the model's predicted endpoint metabolite and taxonomic profiles with the experimentally observed data.

Methods And Approach

This project will synthesize vast, publicly available multi-omic datasets using a novel, rigorous, and reproducible computational workflow. Our approach is designed to be modular, scalable, and transparent, adhering to the highest standards of open science. The transdisciplinary nature of our team is essential for the success of this multi-faceted methodology.

**Data Sources and Harmonization:**
We will leverage several of the world's largest and most comprehensive public microbiome datasets. Our primary sources include:
1.  **The Human Microbiome Project (HMP1 & HMP2/iHMP):** Provides metagenomic, metatranscriptomic, and metabolomic data from hundreds of healthy individuals and those with IBD, including valuable longitudinal data for tracking dynamic changes.
2.  **The American Gut Project (AGP):** Offers metagenomic data from over 10,000 individuals, providing immense statistical power for identifying generalizable patterns across a diverse population.
3.  **Other Cohorts:** We will incorporate data from other well-phenotyped cohorts with publicly available multi-omics, such as LifeLines-DEEP and TwinsUK, to enhance the robustness and generalizability of our findings.
A significant initial effort (Year 1, Q1-Q2) will be dedicated to data harmonization. We will develop a unified metadata schema to standardize variables such as host diet, disease status, age, sex, and medication use across all projects. A dedicated workflow, built using Snakemake, will automate the downloading, quality control (FastQC, Trimmomatic), and processing of all raw sequencing data from repositories like NCBI SRA.

**Computational Pipeline:**
Our core analytical pipeline consists of five interconnected modules:
*   **Module 1: Genome Recovery from Metagenomes:** For each metagenomic sample, we will perform de novo assembly using MEGAHIT. We will then use a suite of binning tools (MetaBAT2, MaxBin2, CONCOCT) to reconstruct Metagenome-Assembled Genomes (MAGs). The resulting bins will be consolidated and refined using DAS Tool and evaluated for quality with CheckM. Only high-quality MAGs (Completeness > 90%, Contamination < 5%) will be retained, forming our comprehensive gut microbial genome catalog.
*   **Module 2: Genome-Scale Metabolic Model (GEM) Reconstruction and Curation:** This is a cornerstone of our project. For each MAG and publicly available reference genome, we will reconstruct a GEM. We will employ a hybrid strategy: initial draft models will be generated using automated tools like CarveMe and gapseq. These drafts will then undergo a rigorous, semi-automated curation process. This involves using algorithms to identify and fill metabolic gaps, ensuring the model can produce biomass, and comparing model predictions to known metabolic capabilities. Our team's microbial physiologists and systems biologists will perform manual curation on key pathways (e.g., fermentation, vitamin biosynthesis) to ensure biochemical accuracy. All models will be standardized in SBML format, version-controlled, and housed in a public repository.
*   **Module 3: Community Model Assembly and Simulation:** For each host sample, we will construct a personalized community metabolic model. First, we will determine the relative abundance of each microbe in our catalog by mapping metagenomic reads from the sample back to the genomes. These abundances will define the composition of the community model. We will use the `micom` Python framework, which is specifically designed for efficient FBA of large microbial communities. The simulation environment (the 'diet') will be defined using a standard in-silico representation of a Western diet (e.g., from the Virtual Metabolic Human database), which specifies the nutrient influx into the system.
*   **Module 4: Multi-omic Constraint Integration:** To capture the in situ activity of the microbiome, we will integrate other omics data. Metatranscriptomic reads from a given sample will be mapped to the genes in our community's genomes. The resulting expression levels will be used to constrain the maximum allowable flux through the corresponding reactions in the model, using established algorithms like GIMME. This ensures that highly expressed pathways are more active in the simulation. Where available, fecal metabolomic data will be used to set boundary conditions for nutrient uptake and metabolite secretion, further grounding the model in experimental reality.
*   **Module 5: Network Analysis and Perturbation Simulation:** With the constrained community models, we will simulate metabolic activity to address our scientific questions. We will calculate pairwise metabolic exchange fluxes to construct a 'sociometabolic' network graph for each community. Graph theory methods will be used to identify keystone species, metabolic guilds (modules), and critical metabolic handoffs. To test hypotheses about system response, we will perform in-silico perturbations. Dietary shifts will be simulated by altering the nutrient composition of the input media. Antibiotic effects will be modeled by removing susceptible species from the community. The resulting changes in predicted community growth, stability, and metabolic outputs (e.g., SCFA production) will form the basis of our predictions.

**Timeline and Milestones:**
*   **Year 1:** Establish data acquisition and MAG recovery pipeline. Reconstruct and curate the first 500 high-quality GEMs. Develop and benchmark the community model assembly workflow. Hold the first annual in-person working group meeting.
*   **Year 2:** Scale GEM reconstruction to over 1,500 species. Fully implement and validate the metatranscriptomic data integration module. Perform initial network analysis across the HMP cohort to identify core metabolic guilds. Submit the first manuscript on the methodology and model repository.
*   **Year 3:** Conduct comprehensive perturbation simulations (diet and antibiotics). Validate model predictions against longitudinal data from the HMP2 IBD cohort. Finalize and release the open-source software package. Disseminate findings through high-impact publications and presentations at international conferences. Host a training workshop for the broader community.

**Reproducibility and Training:**
All analysis workflows will be encoded in Snakemake or Nextflow, and all software will be containerized using Docker/Singularity to ensure full reproducibility. All code, models, and derived data will be shared via GitHub and public data archives (e.g., Zenodo). Trainees (graduate students and postdocs) will be central to this process, receiving hands-on training in data science, systems biology, and collaborative research through bi-weekly project meetings and intensive annual hackathons.

Expected Outcomes And Impact

This working group will generate significant and lasting contributions that advance the molecular and cellular sciences by providing a novel, mechanistic framework for understanding emergence in complex biological systems. The impact will span conceptual, methodological, and translational domains, directly addressing the core goals of the NCEMS research call.

**Intended Contributions to the Field:**
1.  **A Conceptual Shift from Correlation to Causation:** Our primary contribution will be to move the field of microbiome research beyond descriptive, correlational studies. By creating a predictive, mechanism-based model of the gut ecosystem, we will provide a powerful tool for testing causal hypotheses about how microbial community structure leads to emergent function. This will establish a new paradigm for studying host-microbe systems, grounded in the principles of systems biology and ecological theory.
2.  **A Novel, Open-Source Analytical Platform:** We will deliver a fully documented, open-source computational pipeline for building multi-omic-constrained community metabolic models. This platform will be a valuable resource for the entire research community, enabling other scientists to apply our methods to their own data and questions, thus democratizing this powerful analytical approach.
3.  **The Most Comprehensive Gut Microbial Metabolic Resource:** We will produce and publicly release the largest, most highly curated database of genome-scale metabolic models for human gut microbes, many derived from previously uncultured MAGs. This 'Sociomicrobiology Model Kit' will be an invaluable standalone resource, accelerating research in microbial physiology, synthetic biology, and drug discovery.
4.  **Discovery of Fundamental Organizing Principles:** Our analysis will uncover the fundamental 'rules' of metabolic organization in the gut. We will identify conserved metabolic guilds, quantify the importance of functional redundancy for ecosystem stability, and pinpoint keystone species and metabolites that are critical control points in the system. These discoveries will provide a new, function-centric roadmap of the gut ecosystem.

**Broader Impacts and Applications:**
*   **Translational Medicine and Personalized Nutrition:** The predictive power of our models has direct translational potential. They can serve as the foundation for developing personalized interventions. For example, a clinician could use a patient's microbiome data to run in-silico simulations to predict which specific prebiotic fiber would most effectively increase their butyrate production, or to design a probiotic consortium to restore a missing metabolic function. This represents a critical step towards the rational engineering of the microbiome for therapeutic purposes.
*   **Drug Development and Toxicology:** Our models can be used as an in-silico platform to screen for off-target effects of new drug candidates on the gut microbiome. By simulating the inhibition of microbial enzymes targeted by a drug, we can predict potential disruptions to the gut ecosystem, potentially reducing adverse effects and improving drug safety profiles.
*   **Training the Next Generation of Scientists:** This project is intrinsically designed to train a new generation of data-savvy biologists. Trainees will gain invaluable cross-disciplinary skills at the intersection of microbiology, computational biology, and data science. The collaborative, team-science environment fostered by the working group will prepare them for the future of large-scale, integrative biological research.

**Dissemination and Sustainability:**
Our dissemination strategy is multi-pronged. We will publish our findings in high-impact, open-access journals (e.g., *Nature Biotechnology*, *Cell Host & Microbe*, *PLOS Computational Biology*). We will actively present our work at major international conferences (e.g., ISMB, ISME, Keystone Symposia). Crucially, all code, models, and workflows will be made publicly available via GitHub and Zenodo, adhering to FAIR data principles. To ensure long-term impact, we will host a training workshop in the final year to disseminate our tools and techniques to the broader community. The collaborative network established by this working group will be self-sustaining, serving as a nucleus for future collaborative grant proposals (e.g., NIH U01s, NSF Biology Integration Institutes) to experimentally validate model predictions and expand the model to include host-microbe metabolic interactions. Our long-term vision is to create a 'digital twin' of the gut microbiome, a tool that will transform basic research and clinical practice. This project lays the essential foundation for that vision.

Budget And Resources

The proposed research represents a community-scale synthesis effort that is impossible for a single laboratory or a small collaboration to undertake. The project's success hinges on the integration of diverse expertise, the management of massive datasets, and the need for significant computational power, making it an ideal fit for the NCEMS Working Group program. The requested budget is designed to support the personnel, collaborative activities, and computational infrastructure essential for achieving our ambitious goals.

**Justification for NCEMS Support:**
This project's scope is defined by its scale and complexity. We will be synthesizing petabytes of public data, reconstructing and curating thousands of genome-scale models, and running millions of CPU-hours of complex simulations. This requires a coordinated, multidisciplinary team composed of microbial ecologists, systems biologists, bioinformaticians, and computer scientists from multiple institutions. NCEMS support is critical for (1) **Personnel:** Funding dedicated postdoctoral fellows and graduate students who will perform the day-to-day research and are the focus of our training mission; (2) **Collaboration and Coordination:** Supporting the essential in-person meetings and a part-time project manager to ensure this geographically distributed team works as a cohesive and efficient unit; and (3) **Computational Resources:** Providing dedicated funds for the high-performance computing (HPC) and cloud resources necessary for large-scale data analysis and simulation.

**Budget Breakdown (3-Year Total: $749,000):**

*   **A. Personnel ($456,000):**
    *   **Postdoctoral Scholars (2.0 FTE):** $306,000. Two postdocs for three years, one specializing in metagenomic bioinformatics and the other in metabolic modeling. They will lead the development of the core computational pipeline. (Based on an average salary of $55,000/year + 40% fringe benefits).
    *   **Graduate Students (2.0 FTE):** $90,000. Support for two graduate students for three years, who will focus on data curation, model validation, and network analysis, representing a key training component. (Based on a stipend of $30,000/year + $15,000/year tuition).
    *   **Project Manager (0.2 FTE):** $60,000. A part-time manager to coordinate meetings, track milestones, manage reporting, and facilitate communication across the working group. (Based on a salary of $80,000/year + 40% fringe).

*   **B. Travel ($60,000):**
    *   **Annual Working Group Meetings:** $20,000 per year. To bring the entire team (~10 PIs and trainees) together for an intensive 3-day in-person workshop and hackathon. This is vital for fostering deep collaboration, resolving complex technical challenges, and cross-training.

*   **C. Computational Resources & Data ($90,000):**
    *   **HPC/Cloud Computing:** $75,000 ($25,000 per year). For purchasing compute cycles on a national HPC resource (e.g., XSEDE) or a commercial cloud provider (e.g., AWS) for metagenomic assemblies and large-scale FBA simulations.
    *   **Data Storage and Archiving:** $15,000 ($5,000 per year). For robust, long-term storage of processed data and public archiving of final data products in repositories like Zenodo, ensuring compliance with FAIR principles.

*   **D. Materials & Supplies ($15,000):**
    *   **Publication Costs:** $15,000. To cover open-access publication fees for an anticipated 5-6 peer-reviewed articles in high-impact journals.

*   **E. Total Direct Costs: $621,000**

*   **F. Indirect Costs (F&A) ($128,000):**
    *   Calculated based on a blended rate across participating institutions on a modified total direct cost base, as per NCEMS guidelines.

**Institutional Commitment:**
The participating investigators' institutions are committed to the success of this project and will provide faculty salaries, benefits, office and laboratory space, and access to local computing infrastructure, which will supplement the resources requested in this proposal. This cost-sharing demonstrates a strong institutional investment in our collaborative research program.",,
ai_group_gemini_06,ai,group,gemini-2.5-pro,Cracking the Splicing Code: A Deep Learning Approach to the Emergence of Proteomic Diversity,"Alternative splicing of pre-mRNA allows a limited number of genes to produce a vast repertoire of protein isoforms, a key source of biological complexity. The 'splicing code' that governs this process—the set of rules by which cis-regulatory elements and trans-acting RNA-binding proteins (RBPs) guide the spliceosome to generate tissue-specific isoform patterns—remains elusive. This working group will tackle this grand challenge by treating the splicing code as a language to be learned from data. We will synthesize petabytes of public data, including: 1) RNA-seq data from the Genotype-Tissue Expression (GTEx) project and single-cell atlases to quantify isoform expression across thousands of samples; 2) eCLIP-seq data from ENCODE to map RBP binding sites; and 3) RNA structure probing data to understand the role of local RNA conformation. Our team, bringing together RNA biologists, computational linguists, and machine learning experts, will develop and train a novel deep learning architecture, inspired by large language models like transformers. This model will learn the complex, combinatorial grammar of splicing directly from DNA/RNA sequence and the cellular context of RBP expression. The ultimate goal is a predictive tool that, given a gene sequence and a cell type, can accurately predict the resulting mRNA isoforms. This would revolutionize our ability to interpret genetic variants affecting splicing in disease and understand the emergence of proteomic diversity.",,"Background And Significance

The central dogma of molecular biology, while foundational, belies the immense complexity that emerges from a finite set of genes. The discovery that the human genome contains only ~20,000 protein-coding genes, a number comparable to that of the nematode C. elegans, presented a profound puzzle: how does this limited genetic toolkit generate the vast phenotypic complexity of human biology? The answer, in large part, lies in alternative splicing (AS), a post-transcriptional regulatory mechanism by which exons from a single pre-mRNA transcript are differentially joined. This process, occurring in over 95% of human multi-exon genes, generates a multiplicity of mRNA isoforms from a single gene, which are then translated into distinct protein variants, dramatically expanding the proteomic landscape. This proteomic diversity is fundamental to cellular differentiation, tissue identity, and developmental programs. The rules governing this process, collectively termed the 'splicing code,' remain one of the great unsolved problems in molecular biology. Unlike the triplet-based genetic code, the splicing code is a complex, combinatorial, and context-dependent language. It is written in the sequence of the pre-mRNA itself, in the form of cis-regulatory elements such as exonic and intronic splicing enhancers and silencers (ESEs, ESSs, ISEs, ISSs). This code is read and interpreted by a dynamic cohort of trans-acting factors, primarily RNA-binding proteins (RBPs), which bind to these cis-elements and guide the core spliceosome machinery to select specific splice sites. The combinatorial binding of dozens to hundreds of RBPs creates a regulatory logic of staggering complexity, enabling the precise tuning of isoform ratios in a cell-type-specific and condition-dependent manner. Early efforts to decipher this code focused on identifying short, consensus sequence motifs for splice sites and regulatory elements. While informative, these approaches failed to capture the combinatorial nature and long-range interactions inherent to splicing regulation. The advent of high-throughput sequencing has provided an unprecedented wealth of data. Large-scale projects like the Encyclopedia of DNA Elements (ENCODE) and the Genotype-Tissue Expression (GTEx) project have generated massive public datasets, including transcriptomes from thousands of human samples (RNA-seq), and genome-wide binding maps for hundreds of RBPs (eCLIP-seq). These resources have fueled the development of computational models to predict splicing outcomes. Initial machine learning models, such as those based on support vector machines or random forests, demonstrated some success but were limited by their inability to learn complex sequence features automatically. More recently, deep learning, particularly convolutional neural networks (CNNs), has shown significant promise. Models like SpliceAI can accurately predict if a genetic variant will disrupt a splice site, a major step forward for clinical genetics. However, significant gaps remain. Current models struggle to predict quantitative isoform ratios (i.e., the Percent Spliced In, or PSI value) across diverse cellular contexts. They often fail to integrate the crucial context of trans-acting RBP expression levels and do not adequately model the long-range dependencies between distal regulatory elements, which can be kilobases apart. Furthermore, the role of local RNA secondary structure, known to influence RBP binding and splice site accessibility, is frequently ignored. We are at a critical juncture where the confluence of massive, multimodal public datasets and revolutionary advances in artificial intelligence, specifically the development of transformer-based large language models (LLMs), makes it possible to address these limitations. Transformers, with their self-attention mechanism, are uniquely suited to learning the long-range, contextual 'grammar' of complex sequential data. This project is therefore timely and important, proposing to synthesize these disparate data modalities within a novel LLM-inspired framework to finally crack the splicing code. Success will not only represent a fundamental breakthrough in our understanding of gene regulation but will also provide a powerful tool to interpret genetic disease and engineer RNA-based therapeutics.

Research Questions And Hypotheses

The overarching goal of this working group is to develop and validate a comprehensive, predictive model of alternative splicing, named 'SpliceFormer,' that can accurately predict tissue-specific isoform ratios from genomic sequence and cellular context. By treating the splicing code as a formal language, we aim to decipher its grammar and uncover the emergent principles that govern proteomic diversity. To achieve this, we will address three central research questions, each associated with specific, testable hypotheses.

**Research Question 1: Can a transformer-based deep learning architecture effectively learn the complex, combinatorial, and long-range grammar of the splicing code from integrated multi-modal public data?**
This question addresses the core technical challenge of building a model capable of understanding the intricate rules of splicing. We hypothesize that the limitations of previous models stem from their inability to capture the full complexity of the input data.
*   **Hypothesis 1a:** A deep learning model built upon a transformer architecture will significantly outperform current state-of-the-art models (e.g., CNN-based architectures like SpliceAI) in the quantitative prediction of isoform ratios (PSI values) across diverse human tissues. We predict this is because the self-attention mechanism inherent to transformers can model dependencies between regulatory elements and splice sites separated by thousands of nucleotides, a known feature of splicing regulation that is poorly captured by local-feature-focused CNNs.
*   **Hypothesis 1b:** Explicitly conditioning the model on the cellular context, defined by the expression profile of key RBPs and splicing factors, will be essential for achieving tissue-specific predictive accuracy. We hypothesize that a model receiving only DNA/RNA sequence as input will predict a generic splicing outcome, while a model provided with the trans-acting factor milieu will correctly predict tissue-specific isoform switches.
*   **Hypothesis 1c:** The integration of RNA secondary structure information as an additional input modality will further refine model predictions. We hypothesize that for a specific subset of splicing events known to be regulated by RNA structure, incorporating this information will correct prediction errors made by a structure-agnostic model.
*   **Validation:** These hypotheses will be tested by systematically training and evaluating model variants. We will compare the performance (e.g., Pearson correlation between predicted and observed PSI) of our full SpliceFormer model against baseline CNNs and ablated versions of our own model (e.g., without the transformer layers, without RBP context, without structure data) on a held-out test set of genes and tissues from the GTEx project.

**Research Question 2: What are the key cis-regulatory elements and trans-acting factor combinations that define tissue-specific splicing programs?**
Beyond prediction, our goal is to extract biological knowledge from the trained model. We aim to use the model as an in silico laboratory to probe the mechanisms of splicing regulation.
*   **Hypothesis 2a:** Model interpretation techniques, specifically in silico saturation mutagenesis, will identify the precise nucleotide-level functional impact of cis-regulatory sequences, allowing for the de novo discovery of novel splicing enhancers and silencers with higher accuracy than motif-based searches.
*   **Hypothesis 2b:** Analysis of the model's internal mechanisms, such as its attention maps and feature importance scores (e.g., SHAP values), will reveal the 'grammar' of splicing regulation, including synergistic and antagonistic interactions between RBPs that govern specific splicing decisions. For example, we hypothesize we can identify specific RBP combinations that define brain-specific versus muscle-specific exon inclusion patterns.
*   **Validation:** Cis-element predictions will be validated by comparing our functional scores with large-scale experimental data from massively parallel reporter assays (MPRAs) and known pathogenic splicing mutations from databases like ClinVar. Predicted RBP interactions will be cross-referenced with protein-protein interaction databases and co-localization patterns from ENCODE eCLIP-seq data.

**Research Question 3: How does the splicing code drive the emergence of cellular identity and how is it perturbed in human disease?**
This question seeks to apply our validated model to address fundamental questions in cell biology and translational medicine.
*   **Hypothesis 3a:** The SpliceFormer model can accurately predict the functional consequences of non-coding genetic variants on splicing, enabling the systematic prioritization of disease-causing variants from genome-wide association studies (GWAS).
*   **Hypothesis 3b:** By applying the model to single-cell RNA-seq data, we can map the dynamic landscape of splicing regulation during cellular differentiation and identify critical splicing 'switches' that are necessary for cell fate decisions.
*   **Validation:** We will use SpliceFormer to score all common variants in the human genome for their predicted effect on splicing. These scores will be tested for enrichment in GWAS loci for various diseases. For single-cell applications, we will apply the model to public datasets of hematopoiesis or neurogenesis and validate predicted splicing switches against known lineage-defining isoform changes in the literature.

**Deliverables:** The expected outcomes include: (1) The open-source SpliceFormer software package and pre-trained models; (2) A public web portal for predicting splicing outcomes; (3) A comprehensive atlas of predicted functional cis-elements and RBP regulatory networks across human tissues; and (4) High-impact publications detailing the model and its biological insights.

Methods And Approach

This project will be executed by a multidisciplinary working group comprising experts in RNA biology, machine learning, computational linguistics, and bioinformatics. The collaborative structure is essential for integrating the diverse datasets and developing a conceptually novel modeling approach. The research plan is organized into three synergistic aims, with a detailed timeline and milestones.

**Aim 1: Curation and Synthesis of a Multi-modal Splicing Data Compendium.**
The foundation of our project is the large-scale integration of publicly available data. This task requires a robust, reproducible data processing pipeline managed by a dedicated data scientist within the team.
*   **Transcriptomic Data:** We will utilize the Genotype-Tissue Expression (GTEx) project (v8), which contains over 17,000 RNA-seq datasets from 54 human tissues. Raw data will be re-processed through a uniform pipeline using STAR for alignment and rMATS for the robust identification and quantification of alternative splicing events (e.g., skipped exons, mutually exclusive exons), yielding Percent Spliced In (PSI) values. This will form our primary training and testing dataset. Additionally, we will integrate data from single-cell atlases (e.g., Human Cell Atlas) to derive cell-type-specific RBP expression profiles and validate model predictions at higher resolution.
*   **RBP Binding Data:** We will leverage the ENCODE project's comprehensive collection of enhanced CLIP-seq (eCLIP-seq) datasets, covering over 150 RBPs. Uniformly processed peak data will be mapped to pre-mRNA coordinates to create a high-resolution map of RBP-RNA interactions. For tissues lacking direct eCLIP data, we will develop an imputation model that predicts RBP binding probability based on local sequence motifs and the RBP's expression level in that tissue.
*   **RNA Structure Data:** We will incorporate data from in vivo structure probing experiments (e.g., SHAPE-MaP, DMS-seq). These datasets provide nucleotide-resolution information on RNA secondary structure. We will process these data to generate a probability track for each nucleotide being in a paired or unpaired state, which will serve as an input channel to our model.
*   **Genomic and Annotation Data:** The human reference genome (GRCh38) and GENCODE gene annotations will provide the scaffold for all data integration.

**Aim 2: Development and Training of the SpliceFormer Model.**
This aim constitutes the core technical innovation of the proposal, led by our machine learning experts with guidance from computational linguists on architectural design.
*   **Input Representation:** For each splicing event, the model will receive a multi-channel input tensor. This includes: (1) The one-hot encoded DNA sequence of the target exon and ~1kb of flanking intronic sequence on each side; (2) A parallel vector representing RNA secondary structure probabilities; and (3) A concatenated 'context vector' containing the normalized expression values (TPM) of ~200 key RBPs and splicing factors for the specific tissue sample.
*   **Model Architecture:** The SpliceFormer architecture will consist of three main components. First, a set of convolutional layers will scan the sequence and structure inputs to learn low-level features like splice sites and RBP binding motifs. Second, the output of the convolutional layers will be fed into a multi-head self-attention Transformer encoder. This core component will learn the long-range dependencies and combinatorial relationships between all features in the sequence. Third, the tissue-specific RBP context vector will be integrated using a feature-wise linear modulation (FiLM) layer, which allows the trans-acting factor profile to dynamically gate the flow of information through the network. The final layers will be a multi-layer perceptron that outputs a single value, the predicted PSI.
*   **Training, Validation, and Benchmarking:** The curated GTEx dataset will be split into training (80%), validation (10%), and testing (10%) sets, ensuring no gene overlap. The model will be trained on a distributed GPU cluster using the Adam optimizer and a mean squared error loss function. We will perform extensive hyperparameter tuning. The final model's performance will be rigorously benchmarked against existing methods, including SpliceAI and other CNN-based models, on the held-out test set.

**Aim 3: Model Interpretation, Biological Discovery, and Dissemination.**
Once trained, the model will be used as a tool for biological discovery, led by the RNA biologists in the team.
*   **Interpretation Techniques:** We will employ a suite of interpretation methods. In silico saturation mutagenesis will be performed by systematically mutating every position in a sequence and recording the predicted change in PSI, generating high-resolution functional maps. We will visualize the transformer's attention maps to identify which distal elements the model uses to make predictions for a given splice site. Feature attribution methods like SHAP will be used to determine the relative importance of specific RBPs for splicing decisions in different tissues.
*   **Web Portal and Open Science:** To ensure broad utility, we will develop a user-friendly web portal where researchers can submit a gene and select a tissue context to receive splicing predictions and visualizations. All code will be made available on GitHub under a permissive license, and all processed data and trained models will be deposited in public repositories like Zenodo, adhering to FAIR data principles.

**Timeline:**
*   **Year 1:** Data aggregation, processing pipeline finalization, and initial model prototyping. First annual working group meeting.
*   **Year 2:** Full-scale model training, hyperparameter optimization, rigorous benchmarking, and initial interpretation analyses. Development of the web portal begins.
*   **Year 3:** In-depth biological discovery using the model, application to disease variant and single-cell data, manuscript preparation, and public release of all tools and resources. Final working group meeting and community training workshop.

Expected Outcomes And Impact

This project is poised to deliver transformative outcomes that will significantly advance the fields of molecular biology, computational biology, and translational medicine. The impact will be felt through the creation of a paradigm-shifting predictive tool, the generation of a foundational biological resource, and the training of a new generation of interdisciplinary scientists.

**Intellectual Merit and Contribution to the Field:**
The primary outcome will be the **SpliceFormer model**, a powerful, publicly accessible tool that represents a fundamental leap forward in our ability to understand and predict gene regulation. Unlike previous models, SpliceFormer will be the first to integrate sequence, RNA structure, and the trans-acting cellular environment to predict quantitative, tissue-specific splicing outcomes. This moves the field beyond simple classification of splice sites towards a truly quantitative and mechanistic understanding of the process. The conceptual framing of the splicing code as a 'language' to be learned by a transformer architecture is a novel approach that could serve as a blueprint for modeling other complex biological systems.

A second major outcome will be the creation of a **comprehensive, dynamic atlas of the human splicing code**. Through in silico mutagenesis and model interpretation, we will generate the most detailed map to date of functional cis-regulatory elements across the entire human transcriptome. Crucially, this map will be dynamic, showing how the activity of these elements is modulated by the combinatorial interplay of RBPs across dozens of human tissues. This atlas will serve as a foundational resource for RNA biologists for years to come, enabling countless new hypotheses about gene regulation in health and disease.

**Broader Impacts and Applications:**
The societal and clinical impact of this research will be substantial.
*   **Revolutionizing Clinical Genetics:** A major challenge in genomics is the interpretation of variants of unknown significance (VUS), particularly those in non-coding regions. SpliceFormer will provide a powerful tool to predict whether any given genetic variant disrupts splicing, providing a direct mechanistic link to disease. This will aid in the diagnosis of rare genetic disorders and improve the clinical utility of whole-genome sequencing.
*   **Accelerating Therapeutic Development:** The model will have direct applications in the design of RNA-targeted therapies. For diseases caused by mis-splicing, such as Spinal Muscular Atrophy or certain cancers, SpliceFormer can be used to design and optimize antisense oligonucleotides (ASOs) that correct the splicing defect. It can predict both on-target efficacy and potential off-target effects, streamlining the pre-clinical development pipeline.
*   **Enabling Advances in Synthetic Biology:** A predictive understanding of the splicing code will allow for the forward engineering of genetic constructs with precisely controlled, cell-type-specific splicing patterns. This will be invaluable for creating sophisticated gene circuits for applications in cell-based therapies, regenerative medicine, and biotechnology.

**Dissemination, Open Science, and Training:**
This working group is deeply committed to the principles of open and reproducible science. All software developed will be open-source and hosted on GitHub. All data, model weights, and results will be shared through public repositories (e.g., Zenodo, GEO) and a dedicated project website. Findings will be published in high-impact, open-access journals. To maximize community engagement, we will host a final-year workshop to train other researchers in the use of our tools. This project is an ideal training environment for graduate students and postdocs, who will gain invaluable cross-disciplinary experience at the cutting edge of big data, machine learning, and molecular biology, directly addressing the call's goal of fostering a data-savvy workforce.

**Long-Term Vision and Sustainability:**
The long-term vision is to establish SpliceFormer as a 'foundation model' for RNA processing, analogous to what AlphaFold has become for protein structure. The framework we develop is extensible and can be adapted in future work to incorporate other layers of gene regulation, such as transcription kinetics, chromatin state, and polyadenylation. This project will not only answer a long-standing fundamental question but will also lay the groundwork for a new era of predictive, personalized genomics.

Budget And Resources

The proposed research represents a community-scale synthesis project that is beyond the capabilities of a single research laboratory or existing collaboration. The sheer scale of the data integration, the computational expense of developing and training a novel deep learning architecture, and the essential requirement for deep, synergistic expertise from disparate scientific fields (RNA biology, machine learning, computational linguistics) necessitate the working group structure and support provided by the NCEMS program. Standard funding mechanisms are insufficient to support the required personnel, computational infrastructure, and collaborative coordination.

**Budget Justification:**
The budget is designed for a three-year project period and reflects the intensive computational and collaborative nature of the work. The primary costs are for personnel who will drive the project's success and for the substantial computational resources required for large-scale model training.

**Detailed Budget Breakdown (3-Year Total):**

1.  **Personnel ($945,000):** This is the largest budget category, reflecting the project's reliance on dedicated, highly skilled researchers.
    *   **Postdoctoral Fellows (2 FTEs):** $450,000. Two fellows will be hired, one with expertise in bioinformatics and RNA biology, and the other in machine learning. They will lead the data processing and model development efforts, respectively. (Based on $75,000 salary + 25% fringe per year).
    *   **Graduate Students (2 FTEs):** $240,000. Two students will support the postdocs, focusing on data validation, model benchmarking, and application of the model to biological questions. (Based on $40,000 stipend + benefits/tuition per year).
    *   **Data Scientist (0.5 FTE):** $180,000. A part-time data scientist is critical for managing the petabyte-scale data harmonization pipeline, ensuring data integrity, and maintaining the project's cloud and HPC infrastructure. (Based on $120,000 full-time salary + 25% fringe per year).
    *   **Principal Investigator Support:** $75,000. Summer salary (1 month/year) for the lead PI to provide scientific oversight and manage the working group.

2.  **Computational Resources ($150,000):**
    *   **Cloud Computing Credits (AWS/Google Cloud):** $120,000 ($40,000/year). Essential for accessing GPU instances (e.g., A100s) required for training large transformer models. This also covers cloud storage for the multi-terabyte processed dataset.
    *   **HPC Cluster Access:** $30,000 ($10,000/year). To support data pre-processing and analysis tasks on local institutional high-performance computing clusters.

3.  **Travel and Collaboration ($60,000):**
    *   **Annual Working Group Meeting:** $45,000 ($15,000/year). To bring the entire team (PIs, trainees, staff) together for an intensive 3-day in-person workshop to foster deep collaboration, resolve challenges, and plan future directions.
    *   **Conference Travel:** $15,000 ($5,000/year). To enable trainees to present project findings at key international conferences (e.g., RNA Society, ISMB, NeurIPS), disseminating our work and providing valuable professional development.

4.  **Publications and Dissemination ($20,000):**
    *   **Open-Access Publication Fees:** $15,000. To cover article processing charges for publishing our findings in high-impact open-access journals.
    *   **Web Portal Hosting & Maintenance:** $5,000. To cover costs associated with hosting and maintaining the public-facing web server for the SpliceFormer tool.

**Total Direct Costs:** $1,175,000
**Indirect Costs (IDC):** (Calculated at a hypothetical 55% of modified total direct costs, excluding tuition): ~$590,000
**Total Requested Budget:** ~$1,765,000",,
ai_group_gemini_07,ai,group,gemini-2.5-pro,The Phase-Separation Atlas: Predicting the Emergence of Membraneless Organelles from Proteome-wide Features,"Cells organize their cytoplasm using not only membrane-bound organelles but also membraneless organelles or biomolecular condensates, which form via liquid-liquid phase separation (LLPS). The composition, regulation, and material properties of these condensates are emergent phenomena driven by multivalent interactions among proteins and nucleic acids. This working group will develop a predictive framework to map the 'phase-separation potential' of the entire human proteome. We will synthesize diverse public datasets, including protein sequences from UniProt, structural disorder predictions, post-translational modification sites, and protein-protein interaction networks from STRING and BioGRID. These data will be used to train machine learning models, benchmarked against curated experimental databases of phase-separating proteins (e.g., PhaSepDB, PhaSePro). Our multidisciplinary team of cell biologists, polymer physicists, biophysicists, and data scientists will develop a multi-scale model that predicts not only the intrinsic LLPS propensity of individual proteins but also the combinatorial logic of how these proteins assemble into specific, compositionally distinct condensates. The project will deliver a publicly accessible, interactive 'Phase-Separation Atlas' that predicts which proteins form condensates, with whom they interact, and how disease-associated mutations or post-translational modifications might alter these emergent cellular structures. This will provide a powerful resource for understanding cellular organization and the molecular basis of diseases like neurodegeneration and cancer.",,"Background And Significance

The textbook view of cellular organization, dominated by membrane-enclosed compartments, has been fundamentally expanded by the discovery of membraneless organelles (MLOs). These dynamic, protein- and RNA-rich bodies, also known as biomolecular condensates, are critical hubs for biochemical reactions and information processing, including stress response, RNA metabolism, and ribosome biogenesis. The formation of these MLOs is an emergent property of the system, driven by a physical process known as liquid-liquid phase separation (LLPS). This process is governed by the collective effect of many weak, multivalent interactions among macromolecules, primarily proteins with intrinsically disordered regions (IDRs) and nucleic acids. Seminal work by Brangwynne, Hyman, Rosen, Pappu, and others has established the physicochemical principles of LLPS, demonstrating how specific sequence features—such as charge patterning, aromatic residues, and low-complexity domains—can encode the potential for a protein to phase separate. This paradigm shift has profound implications, as the material properties and composition of condensates are not fixed but are dynamically regulated by the cell, and their dysregulation is increasingly linked to devastating human diseases, including amyotrophic lateral sclerosis (ALS), Alzheimer's disease, and various cancers. The field has rapidly advanced from descriptive cell biology to quantitative biophysics, yet a critical gap remains in our ability to predict and understand phase separation at a proteome-wide scale. Current experimental methods for identifying phase-separating proteins are low-throughput and often performed in vitro, divorced from the complex cellular milieu. While several computational tools have emerged to predict LLPS propensity from protein sequence (e.g., PScore, catGRANULE, FuzDrop), they face significant limitations. First, they primarily focus on the intrinsic properties of individual proteins, often overlooking the combinatorial logic of how multiple components assemble into a specific, functional condensate. Cellular LLPS is not a solo act; it is a collective, emergent phenomenon. Second, existing models often rely on a limited set of sequence features and have not fully leveraged the explosion of structural, post-translational, and network-level data. Third, they lack the capacity to systematically predict how cellular context, such as post-translational modifications (PTMs) or the presence of specific RNA molecules, tunes phase behavior. Consequently, we lack a comprehensive 'parts list' of the human condensatome, let alone the 'assembly instructions' that govern its formation, composition, and regulation. There is no integrated resource that allows a researcher to query a protein and predict not only if it can phase separate, but with which partners, under what conditions, and how this behavior might be altered by a disease-associated mutation. This project is both important and timely because it addresses this critical gap directly. The unprecedented availability of high-quality public data—from proteome sequences (UniProt), interaction networks (STRING, BioGRID), predicted structures (AlphaFold DB), and clinical variants (ClinVar)—provides a historic opportunity for a large-scale data synthesis effort. By convening a multidisciplinary working group of cell biologists, polymer physicists, and data scientists, we can integrate these disparate datasets to build a predictive model of cellular organization that is far beyond the scope of any single research lab. This project will transform our understanding of the emergent principles governing cellular compartmentalization and provide an invaluable resource to accelerate research into the molecular basis of health and disease.

Research Questions And Hypotheses

This project is motivated by the central question: Can we move beyond predicting the intrinsic LLPS propensity of individual proteins to create a multi-scale, predictive framework that maps the emergent, combinatorial logic of the entire human condensatome? To address this overarching goal, we have formulated three specific, interconnected research questions, each with testable hypotheses that will guide our data synthesis and modeling efforts. 

**Research Question 1 (RQ1): What are the universal and context-specific molecular features that determine a protein's intrinsic capacity to undergo LLPS and how is this capacity regulated?**
While current predictors identify some key features, they lack the sophistication to capture the full regulatory grammar encoded in protein sequence, structure, and modification state. We hypothesize that a more comprehensive integration of features within an advanced machine learning framework will yield a superior predictive model.
*   **Hypothesis 1a:** A deep learning model that integrates a rich feature set—including amino acid n-gram frequencies, physicochemical properties (e.g., sequence charge decoration), predicted structural motifs from AlphaFold, and consensus disorder predictions—will significantly outperform existing algorithms in predicting the intrinsic LLPS propensity of a protein when benchmarked against curated experimental databases.
*   **Hypothesis 1b:** Post-translational modifications (PTMs) function as a dynamic regulatory code that systematically tunes LLPS. We hypothesize that specific PTMs, such as phosphorylation of serine/threonine residues or arginine methylation, predictably alter a protein's phase diagram by modifying its net charge, valency, and interaction surfaces. Our model will be trained to predict the direction and magnitude of this effect.

**Research Question 2 (RQ2): How do multivalent interactions among proteins and with RNA molecules specify the composition and identity of distinct biomolecular condensates?**
An individual protein's LLPS propensity is necessary but not sufficient to explain the formation of specific MLOs. We posit that condensate identity emerges from the underlying network of molecular interactions.
*   **Hypothesis 2a:** The architecture of the protein-protein interaction (PPI) network, when weighted by the intrinsic LLPS scores of its constituent proteins (from RQ1), contains modules that correspond to known biomolecular condensates. We predict that community detection algorithms will identify distinct protein clusters, each defined by a core of high-propensity 'scaffold' proteins and a periphery of lower-propensity 'client' proteins, which recapitulate the known compositions of organelles like stress granules, P-bodies, and the nucleolus.
*   **Hypothesis 2b:** Specific RNA molecules, particularly long non-coding RNAs, act as essential architectural elements for a subset of condensates. We hypothesize that integrating protein-RNA interaction data will reveal that certain RNAs serve as critical hubs, scaffolding specific protein communities and thereby defining the condensate's ultimate composition and function.

**Research Question 3 (RQ3): How do disease-associated genetic mutations perturb the proteome's phase-separation landscape, leading to cellular dysfunction?**
The link between aberrant phase transitions and disease is well-established, but a systematic method to predict the impact of mutations is lacking. We hypothesize that our framework can mechanistically stratify mutations based on their predicted effect on LLPS.
*   **Hypothesis 3a:** Pathogenic missense mutations found in databases like ClinVar, when mapped onto proteins with high predicted LLPS propensity, will disproportionately alter key physicochemical features governing multivalency (e.g., charge, aromaticity, disorder) compared to benign polymorphisms from gnomAD.
*   **Hypothesis 3b:** Our integrated model can predict the functional consequence of a mutation, classifying it as gain-of-function (e.g., promoting aggregation and solidification, as seen in FUS/TDP-43), loss-of-function (e.g., disrupting necessary condensate formation), or neutral. This predictive capability will provide a powerful tool for interpreting variants of unknown significance.

Our primary deliverable will be the 'Phase-Separation Atlas,' a public web resource integrating these predictive tiers. Validation will be computational, using rigorous cross-validation, testing on held-out datasets not used for training, and benchmarking against orthogonal evidence from proteomics studies of purified MLOs.

Methods And Approach

This project is designed as a community-scale synthesis effort, leveraging the diverse expertise of our working group, which includes cell biologists, biophysicists, polymer physicists, data scientists, and software engineers. Our approach is organized into three synergistic aims, built upon a foundational data integration platform. The entire project will adhere to open science principles, with all data, code, and models made publicly available.

**Foundational Step: Data Acquisition, Harmonization, and Integration**
Our first task is to construct a comprehensive, multi-modal knowledge graph. This involves aggregating and harmonizing data from numerous public repositories. 
*   **Data Sources:** We will use the human proteome from UniProt/Swiss-Prot as our base. This will be annotated with: (1) sequence-derived features; (2) consensus disorder predictions from MobiDB; (3) predicted 3D structures from the AlphaFold Database; (4) experimentally verified PTMs from PhosphoSitePlus; (5) high-confidence PPIs from STRING and BioGRID; (6) protein-RNA interactions from ENCODE and starBase; and (7) human genetic variants from ClinVar, COSMIC, and gnomAD. For model training and benchmarking, we will compile a gold-standard set of LLPS-positive and -negative proteins from PhaSepDB, PhaSePro, and extensive manual literature curation.
*   **Integration:** All data will be mapped to unique UniProt identifiers and stored in a Neo4j graph database. This structure is ideal for representing the complex, multi-layered relationships between proteins, RNAs, PTMs, and their emergent properties.

**Aim 1: Developing a State-of-the-Art Predictor for Intrinsic LLPS Propensity**
To address RQ1, we will develop a machine learning model to compute a continuous LLPS score for every protein in the human proteome.
*   **Feature Engineering:** For each protein, we will compute a rich vector of over 200 features, including amino acid composition, sequence complexity, charge patterning (e.g., SCD, κ), hydrophobicity, aromatic residue content, and predicted structural features like coil/helix content and solvent accessibility derived from AlphaFold models.
*   **Model Architecture:** We will employ a transformer-based architecture, which excels at capturing long-range dependencies in sequential data. This model will be trained on our gold-standard dataset to predict LLPS propensity. Its performance will be rigorously compared against simpler models (e.g., Gradient Boosting) and existing tools using 10-fold cross-validation and testing on a held-out set of recently published LLPS proteins.
*   **PTM Modeling:** We will simulate the effect of common PTMs by altering the feature vector (e.g., phosphorylation adds two negative charges and a bulky group) and predicting the resulting change in the LLPS score, thereby quantifying the regulatory potential of each modification.

**Aim 2: Mapping the Condensatome through Network Analysis**
To address RQ2, we will use the LLPS scores from Aim 1 to analyze the human interactome.
*   **Network Construction:** We will construct a weighted human PPI network where nodes are proteins and edges represent high-confidence interactions. Each node will be annotated with its intrinsic LLPS score.
*   **Community Detection:** We will apply network clustering algorithms (e.g., Louvain method) to identify densely interconnected modules. Our central hypothesis is that modules with a high average LLPS score represent protein communities that co-assemble into specific condensates. We will validate these predicted communities by testing for enrichment of proteins known to co-localize in specific MLOs using GO terms and other annotation databases.
*   **RNA Scaffolding:** We will extend this network by adding RNA nodes and protein-RNA edges. This will allow us to identify RNAs that act as hubs or bridges, organizing specific protein communities into functional ribonucleoprotein granules.

**Aim 3: Building and Disseminating the Interactive Phase-Separation Atlas**
To address RQ3 and serve the broader community, we will create a user-friendly web portal.
*   **Backend/Frontend:** A RESTful API will provide access to our integrated database and predictive models. The frontend will be developed using React and D3.js for interactive data visualization.
*   **Functionality:** The Atlas will allow users to: (1) search for any human protein and view its predicted LLPS score and key driving features; (2) explore its predicted condensate partners in an interactive network view; (3) visualize a 'mutability map' highlighting residues where mutations are predicted to most strongly impact LLPS; and (4) input specific mutations (e.g., from clinical sequencing) to get a prediction of their impact on phase separation.

**Timeline and Milestones:**
*   **Year 1:** Data integration complete. Version 1.0 of the intrinsic LLPS predictor developed and benchmarked. First annual working group meeting.
*   **Year 2:** Network-based condensate prediction model complete. Beta version of the Phase-Separation Atlas web portal released for community feedback. Training workshop for trainees.
*   **Year 3:** Full integration of all predictive tiers. Public launch of the Atlas. Systematic analysis of disease mutations. Final working group meeting and submission of primary manuscript.

Expected Outcomes And Impact

The successful completion of this project will yield significant outcomes that will fundamentally advance the molecular and cellular biosciences, with broad impacts on human health research. Our work is designed to create not just new knowledge, but also enduring resources that will catalyze research across the scientific community, perfectly aligning with the mission of this funding organization.

**Intellectual Merit and Contribution to the Field:**
This project will deliver the first comprehensive, multi-scale predictive map of the human condensatome. Our primary intellectual contribution is the shift from a reductionist, single-protein view of phase separation to a systems-level, emergent framework. By integrating sequence, structure, PTMs, and interaction networks, we will elucidate the combinatorial 'code' that governs the assembly of membraneless organelles. This will provide a quantitative, mechanistic foundation for understanding how cells use phase separation to organize their cytoplasm. We will generate a ranked and prioritized list of hundreds of novel candidate LLPS proteins, providing a rich set of experimentally testable hypotheses for the broader cell biology community. Furthermore, our framework for predicting the functional consequences of mutations will establish a new paradigm for interpreting genetic variation in the context of cellular organization.

**Broader Impacts and Applications:**
*   **Accelerating Disease Research:** The Phase-Separation Atlas will be an invaluable hypothesis-generation tool. A researcher studying a protein implicated in Alzheimer's disease, ALS, or a specific cancer can immediately use our resource to assess its likelihood of phase separation, identify its potential interaction partners within a condensate, and predict how patient-derived mutations might alter its behavior. This will dramatically lower the barrier to entry for studying LLPS and guide experimental design, saving time and resources.
*   **Informing Therapeutic Strategies:** By identifying the key 'scaffold' proteins that nucleate specific disease-relevant condensates and the critical interactions that maintain them, the Atlas will highlight novel targets for therapeutic intervention. This could inspire strategies aimed at dissolving pathological aggregates or restoring the function of essential condensates.
*   **Training the Next Generation of Data-Savvy Biologists:** This project is intrinsically a training vehicle. Graduate students and postdocs within the working group will gain unique cross-disciplinary expertise at the interface of cell biology, biophysics, and machine learning. Through our planned annual workshops, we will disseminate these skills and our analytical pipelines to the wider community, fostering a more computationally fluent workforce.

**Dissemination, Data Sharing, and Long-Term Vision:**
We are deeply committed to open science principles. 
*   **The Phase-Separation Atlas:** Our primary deliverable will be a freely accessible, user-friendly web portal. We will secure institutional commitments to maintain this resource for at least five years beyond the funding period.
*   **Open-Source Code and Data:** All source code for our predictive models and the web portal, along with all processed data and the final trained models, will be made available through public repositories like GitHub and Zenodo under permissive licenses (e.g., MIT, CC-BY).
*   **Publications and Presentations:** We will publish our findings in high-impact, open-access journals. We will also present our work at major international conferences (e.g., ASCB, Biophysical Society) to ensure broad dissemination.

Our long-term vision is for the Atlas to become a community-driven, living resource. We will build it with the capacity for future updates, allowing for the integration of new experimental data and model refinement over time. This project will not only answer our proposed research questions but will also create a powerful, extensible platform that empowers the entire scientific community to explore the emergent world of biomolecular condensates.

Budget And Resources

The proposed research represents a large-scale, multidisciplinary synthesis effort that requires a dedicated team and significant computational resources, far exceeding the capacity of a single research laboratory or existing collaboration. The budget is designed to support a highly collaborative and trainee-focused working group, ensuring the successful execution of the project and the dissemination of its outcomes. The total requested budget for the three-year project is $XXX,XXX.

**1. Personnel ($XXX,XXX):**
This is the largest and most critical component of the budget. The funds will support the trainees and technical staff who will perform the data integration, model development, and analysis.
*   **Postdoctoral Fellows (2.0 FTE):** We request support for two postdoctoral fellows. One will specialize in machine learning and computational biology, leading the development of the predictive models (Aim 1). The second will have expertise in network biology and bioinformatics, focusing on the interactome analysis and condensate mapping (Aim 2). Their combined expertise is essential for bridging the different scales of the project.
*   **Graduate Students (3.0 FTE):** Support for three graduate students is requested. They will be embedded within the PIs' labs and will be integral to all aspects of the project, from data curation and feature engineering to model validation and web portal testing. This aligns with the call's goal of training the future data-savvy workforce.
*   **Data Manager/Software Engineer (0.5 FTE):** We request support for a part-time professional to oversee the construction and maintenance of the integrated database and the public-facing web portal (Aim 3). This ensures the creation of a robust, sustainable, and user-friendly community resource.

**2. Travel ($XX,XXX):**
To foster the deep collaboration required for a synthesis project of this nature, we request funds for one annual in-person working group meeting for all PIs, postdocs, and students. This dedicated time is crucial for strategic planning, problem-solving, and cross-pollination of ideas. We also request funds to support travel for each trainee to present their work at one major national or international conference per year, facilitating dissemination and professional development.

**3. Computational Resources ($XX,XXX):**
Training deep learning models on proteome-scale data and hosting a dynamic web portal are computationally intensive tasks. We request funds for cloud computing services (e.g., Amazon Web Services or Google Cloud Platform) to provide the necessary GPU access for model training and the server infrastructure for data storage and web hosting.

**4. Materials and Supplies ($X,XXX):**
This includes costs for software licenses and subscriptions to relevant databases or services.

**5. Publication and Dissemination Costs ($XX,XXX):**
Funds are requested to cover open-access publication fees for at least three major manuscripts. We also request a modest budget to support the hosting of our annual virtual training workshop, covering costs for web platforms and material preparation.

**6. Indirect Costs (F&A) ($XXX,XXX):**
Indirect costs are calculated at the federally negotiated rates for the participating institutions.

**Justification for NCEMS Support:** The scale of data integration, the need for diverse and sustained expertise from multiple disciplines, and the focus on creating a lasting community resource make this project an ideal fit for the NCEMS working group program. No single PI has the resources or breadth of expertise to undertake this challenge alone. This budget directly supports the collaborative, training-oriented, and computationally-driven nature of the proposed work.",,
ai_group_gemini_08,ai,group,gemini-2.5-pro,The Digital Cell: Unifying Image and Omics Data to Model the Emergence of Cellular Morphology,"A cell's shape and internal organization are fundamental to its function, yet they are emergent properties arising from staggeringly complex interactions between the cytoskeleton, membranes, and organelles. Understanding how morphology is robustly encoded and dynamically regulated requires a holistic, quantitative approach. This working group proposes to build a 'digital twin' of a human cell by integrating public data from two powerful but often disconnected domains: imaging and omics. We will synthesize 3D live-cell imaging data from resources like the Allen Cell Explorer and the Image Data Resource, which contain terabytes of high-resolution movies of fluorescently-tagged structures. We will combine this with proteomics, transcriptomics, and protein-protein interaction data that define the molecular parts list and their connections. Our team, uniting cell biologists, computer vision experts, biophysicists, and applied mathematicians, will develop novel deep learning algorithms to automatically extract a 'morphological feature space' from thousands of images. We will then build an integrated computational model that links these morphological features to the underlying molecular networks. The goal is to create a predictive model that can simulate how cellular morphology emerges from molecular-level rules and how it changes in response to genetic perturbations cataloged in resources like the DepMap. This will provide an unprecedented platform for understanding the principles of cellular self-organization.",,"Background And Significance

The principle that structure dictates function is a cornerstone of biology. At the cellular level, this principle manifests in the intricate and dynamic morphology of the cell—its overall shape, the spatial organization of its organelles, and the architecture of its cytoskeleton. These are not static, pre-programmed structures but emergent properties arising from the collective behavior of millions of molecules. Understanding how a cell’s genotype and molecular state translate into its physical phenotype is one of the most fundamental, long-standing challenges in molecular and cellular science. For decades, research has progressed along two parallel, powerful, but largely disconnected tracks: the visual and the molecular. The imaging revolution, driven by advances like lattice light-sheet and spinning-disk confocal microscopy, has provided breathtaking views into the living cell. Large-scale public repositories such as the Allen Cell Explorer and the Image Data Resource (IDR) now house petabytes of high-resolution, three-dimensional movies, cataloging the localization and dynamics of thousands of proteins. These resources offer an unprecedented visual encyclopedia of cellular organization. Concurrently, the omics revolution has provided a comprehensive 'parts list' and 'wiring diagram' of the cell. Projects like the Human Proteome Map, STRING, and BioGRID have systematically cataloged protein abundances and their physical interactions, while resources like GTEx and ENCODE have mapped the transcriptional landscape. Furthermore, large-scale perturbation screens, most notably the Dependency Map (DepMap), have functionally linked thousands of genes to cellular fitness and other phenotypes through systematic CRISPR and RNAi screens. The critical gap in our knowledge lies at the intersection of these two domains. We have a wealth of data on what the cell looks like and what it is made of, but we lack a quantitative, predictive framework that connects the molecular 'wiring diagram' to the emergent physical form. Current computational models in cell biology are often focused on specific subsystems, such as actin polymerization or mitotic spindle assembly. While incredibly insightful, these models do not capture the holistic, cell-wide coordination that governs overall morphology. The analysis of large-scale imaging data often relies on a limited set of pre-defined, 'hand-crafted' features (e.g., cell area, nuclear eccentricity), which may miss subtle or complex aspects of cellular organization. We are at a unique inflection point where this grand challenge can finally be addressed. The confluence of three key developments makes this project both timely and feasible: the maturity of massive, public imaging and omics datasets; the dramatic advances in artificial intelligence, particularly deep learning for automated feature extraction from complex images; and the growing culture of collaborative, open team science. This working group proposes to bridge the gap between the molecular and the morphological by synthesizing these disparate public data streams. By developing a unified computational framework, we will move beyond correlative studies to build a predictive model of cellular self-organization. This 'Digital Cell' will serve as a powerful, community-accessible platform to perform in silico experiments, generating testable hypotheses about how molecular perturbations impact the physical structure of the cell. This research directly addresses the funding call's focus on emergence phenomena, leveraging data synthesis and multidisciplinary collaboration to solve a foundational puzzle in cellular bioscience.

Research Questions And Hypotheses

This working group will address the overarching question: How do the collective interactions of a cell's molecular components robustly encode and dynamically regulate its three-dimensional morphology? To deconstruct this complex problem, we have formulated three specific, interconnected aims, each with a central research question and a testable hypothesis. Our approach is designed to create a hierarchical framework, starting with a quantitative description of morphology, then linking it to the underlying molecular state, and finally using this link for prediction.

**Aim 1: Define a comprehensive, quantitative 'morphospace' of the human cell.**
*   **Research Question 1:** Can we develop a unified, low-dimensional feature space that captures the salient morphological variations across thousands of live-cell 3D images, encompassing cell shape, organelle organization, and cytoskeletal architecture, without relying on biased, manually engineered features?
*   **Hypothesis 1:** A deep generative model, specifically a 3D Convolutional Variational Autoencoder (CVAE), trained on a large corpus of 3D cell images, can learn a continuous, compressed latent representation (the 'morphospace'). We hypothesize that this learned space will be more powerful than traditional feature sets because it will capture complex, multi-scale relationships between subcellular structures. Proximity within this space will correspond to holistic morphological similarity.
*   **Validation:** The validity of the morphospace will be tested rigorously. First, the CVAE's decoder must be able to reconstruct high-fidelity cell images from latent space vectors, demonstrating that the representation is comprehensive. Second, we will test if the space is biologically meaningful by projecting cells with known phenotypes (e.g., cells in different phases of the cell cycle) into the space and verifying that they form distinct, separable clusters. Third, we will perform interpolations between distant points in the morphospace and show, through visual inspection by expert cell biologists in our team, that the generated intermediate images represent biologically plausible morphological transitions.

**Aim 2: Build a predictive model linking the molecular state (omics) to the morphological state (morphospace).**
*   **Research Question 2:** To what extent can the abundance and interaction patterns of proteins and transcripts predict a cell's position within the defined morphospace? Which molecular pathways are the primary determinants of specific morphological axes?
*   **Hypothesis 2:** A multi-modal machine learning model, integrating protein-protein interaction (PPI) networks with gene expression and protein abundance data, can accurately predict a cell's coordinates in the morphospace. We propose using a Graph Neural Network (GNN), as it is explicitly designed to learn from the relational structure of the PPI network. We hypothesize that this integrated model will outperform models based on gene expression alone, as it captures the functional context of molecular components.
*   **Validation:** We will train the model using data from sources like the Human Protein Atlas, where imaging and omics data are available for the same cell lines. The model's predictive accuracy will be assessed on a held-out test set using metrics like cosine similarity between predicted and true morphospace vectors. To identify key molecular drivers, we will employ feature attribution methods (e.g., SHAP, integrated gradients) to rank genes and pathways by their influence on different morphological axes.

**Aim 3: Use the integrated model to simulate the morphological consequences of genetic perturbations.**
*   **Research Question 3:** Can our integrated model accurately predict the specific morphological changes that result from systematic gene knockdowns or knockouts, as cataloged in large-scale public screens?
*   **Hypothesis 3:** By computationally simulating a genetic perturbation (e.g., removing a gene's node from the GNN input) and propagating this change through our trained omics-to-morphospace model, we can forecast the resulting displacement vector in morphospace. We hypothesize that these *in silico* predicted phenotypic shifts will quantitatively match experimentally observed morphological changes from high-content imaging screens.
*   **Validation:** This hypothesis will be tested directly against public data. We will use our model to predict the morphological outcomes for genes targeted in the DepMap project. These predictions will be compared to the measured morphological feature changes in corresponding high-content Cell Painting datasets. We will quantify the model's predictive power by calculating the correlation between the predicted and observed phenotypic vectors across thousands of genetic perturbations. Success in this aim will validate our model as a powerful engine for hypothesis generation.

Methods And Approach

Our project is a multi-year, multi-phase effort centered on the synthesis and modeling of public data. The methodology is designed to be modular, with clear milestones and validation steps at the conclusion of each phase. Our entire workflow will adhere to open science principles, with all code, models, and derived data being made publicly available.

**Phase 1: Data Acquisition, Curation, and Harmonization (Months 1-9)**
This foundational phase requires the expertise of our entire team to collate and standardize disparate data types. 
*   **Imaging Data Sources:** We will primarily utilize the Allen Cell Explorer dataset, which contains over 40,000 3D live-cell image sets of human induced pluripotent stem cells (hiPSCs) with fluorescently tagged structures. This will be our core training set due to its consistency and high quality. We will supplement this with datasets from the Image Data Resource (IDR), specifically high-content screens like the Cell Painting assays, which provide morphological data linked to genetic or chemical perturbations. We will also incorporate 2D confocal images from the Human Protein Atlas to link protein localization to our model. 
*   **Omics and Perturbation Data Sources:** We will construct a comprehensive molecular interaction network using data from STRING and BioGRID. Basal gene expression and protein abundance profiles for relevant cell lines will be sourced from GTEx, CCLE, and the Human Proteome Map. For perturbation analysis, we will use the CRISPR and RNAi screening data from the DepMap project and the associated imaging phenotypes from the Broad Institute's Cell Painting datasets.
*   **Curation Pipeline:** A systematic pipeline will be developed to process these data. For images, this includes metadata standardization, intensity normalization, and segmentation of cell and nuclear boundaries using pre-trained models like Cellpose. For omics data, this involves mapping all gene/protein identifiers to a common namespace (Ensembl) and constructing a unified, weighted graph representing the molecular interaction network.

**Phase 2: Morphospace Construction via Deep Learning (Months 6-18)**
*   **Model Architecture:** We will implement a 3D Convolutional Variational Autoencoder (CVAE). The encoder will consist of a series of 3D convolutional layers that downsample an input 3D image stack (e.g., 64x128x128 voxels) into a low-dimensional latent vector (e.g., 128 dimensions). The decoder will be a symmetric network of 3D transposed convolutional layers that reconstructs the image from the latent vector. The VAE framework is chosen for its ability to learn a continuous and generative latent space, which is essential for our goals of interpolation and simulation.
*   **Training and Validation:** The CVAE will be trained on the curated Allen Cell Explorer dataset using a combined loss function of reconstruction error and the Kullback-Leibler divergence to regularize the latent space. Training will be performed on a high-performance computing (HPC) cluster with multiple GPUs. Validation will proceed as described in the previous section: assessing reconstruction quality, clustering of known cell states, and expert evaluation of morphological interpolations.

**Phase 3: Linking Omics to Morphospace with Graph Neural Networks (Months 15-27)**
*   **Model Architecture:** We will build a predictive model using a Graph Neural Network (GNN). The input to the GNN will be our curated molecular interaction graph. Node features will include basal gene expression and protein abundance levels. The GNN will use message-passing layers to learn embeddings for each node that incorporate both its own features and the features of its network neighbors. These node embeddings will then be aggregated to produce a single graph-level embedding, which will be mapped via a multi-layer perceptron to predict the 128-dimensional morphospace vector.
*   **Training and Validation:** The model will be trained in a supervised fashion, using the morphospace vectors generated in Phase 2 as the ground truth labels for corresponding cell lines with available omics data. We will use a held-out test set and cross-validation to evaluate performance, using cosine similarity and mean squared error as our primary metrics. Feature attribution techniques (e.g., GNNExplainer) will be used to identify the molecular subnetworks most predictive of specific morphological features.

**Phase 4: In Silico Perturbation and Model Deployment (Months 24-36)**
*   **Simulation Pipeline:** We will develop a computational pipeline to simulate genetic perturbations. A gene knockout will be modeled by removing the corresponding node and its edges from the input graph fed into the trained GNN. The model will then predict the new morphospace vector. The difference between the perturbed and unperturbed vectors represents the predicted morphological phenotype.
*   **Large-Scale Validation:** We will perform thousands of these *in silico* perturbations, corresponding to the genes targeted in the DepMap screens. The predicted phenotypic vectors will be quantitatively compared to the experimental vectors derived from Cell Painting data, providing a robust, large-scale validation of our model's predictive power.
*   **Timeline and Deliverables:**
    *   **Year 1:** Complete data curation pipeline; first-generation morphospace model (CVAE) trained and validated.
    *   **Year 2:** Refined morphospace model; GNN linking model developed and trained; initial integration and validation.
    *   **Year 3:** Perturbation simulation framework finalized; large-scale validation against DepMap; public web portal for community use developed; publications and dissemination.
This project structure ensures a logical progression from data description to predictive modeling, with clear validation points that mitigate risk and ensure the robustness of our final integrated 'Digital Cell' model.

Expected Outcomes And Impact

The successful completion of this project will yield significant outcomes that advance the fields of cell biology, computational biology, and data science, with broad impacts on basic research and translational medicine. The project is designed not only to answer a fundamental scientific question but also to create a lasting, extensible resource for the scientific community, perfectly aligning with the goals of the NCEMS program.

**Intellectual Merit and Contributions to the Field:**
1.  **A New Paradigm for Cellular Phenotyping:** We will move the field beyond qualitative descriptions and simple scalar measurements of cell morphology. Our learned 'morphospace' will provide a holistic, quantitative, and data-driven framework for describing cellular form. This represents a fundamental shift in how cell state is defined and measured.
2.  **Solving the Genotype-Phenotype Gap:** This work will provide the first large-scale, predictive model that quantitatively links the molecular state of a cell (genotype, expression, interactions) to its emergent physical form (phenotype). This directly addresses the long-standing puzzle of cellular self-organization and provides a mechanistic, data-driven understanding of emergence.
3.  **Novel Computational Methodologies:** We will develop and disseminate novel, open-source deep learning tools. The 3D CVAE for morphological analysis and the GNN for integrating network and imaging data will be powerful, generalizable methods applicable to a wide range of biological questions beyond the scope of this initial project.

**Broader Impacts and Applications:**
1.  **A Foundational Resource for the Research Community:** The primary outcome will be the 'Digital Cell' model, accessible via a user-friendly web portal. This will empower any researcher, regardless of their computational expertise, to perform *in silico* experiments. For example, a biologist studying a novel protein can use our tool to generate a testable hypothesis about its role in organizing a specific organelle, guiding future experiments and accelerating the pace of discovery. This directly fulfills the call's goal of developing innovative research strategies.
2.  **Applications in Disease Modeling and Drug Discovery:** Aberrant cell morphology is a hallmark of numerous diseases, including cancer, fibrosis, and neurodegeneration. Our model will provide a platform to understand the molecular basis of these morphological defects. It can be used to predict the physical consequences of disease-associated mutations or to screen for molecular targets that could restore a healthy morphology, opening new avenues for computational diagnostics and therapeutics.
3.  **Training the Next Generation of Data-Savvy Scientists:** This project is an ideal training environment. Graduate students and postdocs will work at the nexus of cell biology, computer vision, and biophysics, gaining invaluable cross-disciplinary skills. They will learn to manage large-scale data, develop sophisticated computational models, and work within a collaborative, open-science framework. We will further amplify this impact by hosting annual workshops to train the broader community on our tools and methods, fulfilling the call's mandate to train the future workforce.
4.  **Stimulating Cross-Disciplinary Collaboration:** The very structure of our working group, uniting cell biologists, computer scientists, and biophysicists from different institutions and career stages, embodies the collaborative spirit of the NCEMS program. The project's success is contingent on this deep integration of expertise, demonstrating a model for future community-scale synthesis projects. The need for NCEMS support is clear, as coordinating this effort, supporting dedicated personnel, and funding the required computational resources is beyond the capacity of any single lab or standard grant mechanism.

**Dissemination and Long-Term Vision:**
We will pursue a multi-pronged dissemination strategy including high-impact publications, presentations at major international conferences, and the release of all code and models through public repositories (GitHub, Zenodo). Our long-term vision is to create an extensible framework. The initial model, built on hiPSCs, will serve as a scaffold. We envision the community contributing new data—from different cell types, tissues, or with new data modalities—to progressively expand the 'Digital Cell' into a multi-scale 'Digital Organism,' ensuring the project's lasting impact and sustainability.

Budget And Resources

The proposed research represents a large-scale, multi-institutional synthesis effort that requires dedicated resources beyond the scope of a standard research grant. The budget is designed to support the personnel, computational infrastructure, and collaborative activities essential for the project's success over a three-year period. The requested funds are critical for coordinating the diverse expertise of our working group and creating a robust, high-value community resource from publicly available data.

**1. Personnel (Approximately 65% of total budget):**
The intellectual core of this project is the people who will perform the data integration and modeling. 
*   **Postdoctoral Fellows (2 FTEs):** We request support for two full-time postdoctoral fellows who will be the primary drivers of the research. One fellow will have expertise in computer vision and machine learning, leading the development of the morphospace model (Aim 1). The second will be a computational biologist/bioinformatician, leading the development of the GNN linking model and perturbation analysis (Aims 2 & 3).
*   **Graduate Students (2 FTEs):** Support for two graduate students will ensure the project's continuity and contribute to training the next generation of scientists. They will work closely with the postdocs and PIs on all aspects of the project.
*   **Principal Investigators (1 month summer salary/year for 4 PIs):** Partial summer support is requested for the PIs to dedicate significant time to scientific oversight, cross-institutional coordination, mentoring of trainees, and dissemination of results.
*   **Project Manager/Data Scientist (0.5 FTE):** To ensure the smooth operation of a geographically distributed team, we request support for a part-time manager to handle data logistics, manage public repositories, maintain the project website, and coordinate meetings and workshops.

**2. Computational Resources (Approximately 15%):**
Training deep learning models on terabytes of 3D image data is computationally intensive.
*   **Cloud Computing Credits:** We request funds for cloud computing services (e.g., AWS, Google Cloud Platform). This provides on-demand access to high-end GPUs, which is essential for model training and hyperparameter tuning. It also facilitates reproducibility and allows us to share our computational environments with the broader community.
*   **Data Storage:** A budget is allocated for robust, high-speed data storage solutions to host the curated and harmonized datasets, which will exceed 100 TB.

**3. Travel (Approximately 10%):**
Effective collaboration in a multi-disciplinary working group requires regular face-to-face interaction.
*   **Working Group Meetings:** We request funds for the entire team (PIs, postdocs, students) to meet in person twice per year. These intensive, multi-day workshops are critical for brainstorming, problem-solving, and ensuring all components of the project remain integrated.
*   **Conference Travel:** Funds are included for trainees and PIs to present our findings at key national and international conferences (e.g., ASCB, NeurIPS), facilitating dissemination and feedback from the community.

**4. Training and Dissemination (Approximately 5%):**
*   **Annual Workshop:** We request funds to host one public workshop per year to train external researchers on our tools and methodologies. This budget covers logistical costs and travel support for a limited number of participants from underrepresented institutions.
*   **Publication Costs:** Funds are allocated for open-access publication fees to ensure our findings are freely accessible to all.

**5. Indirect Costs (F&A):**
Indirect costs are calculated based on the federally negotiated rates for each participating institution and are applied to their portion of the direct costs.",,
ai_group_gemini_09,ai,group,gemini-2.5-pro,A Pan-Viral Synthesis of the Host-Pathogen Arms Race: Uncovering the Emergent Rules of Molecular Conflict,"The perpetual conflict between viruses and their hosts is a primary driver of molecular evolution, leading to the emergence of sophisticated host immune defenses and viral countermeasures. While individual examples are well-studied, the universal principles governing these molecular arms races remain poorly defined. This working group will perform the first comprehensive, pan-viral synthesis of host-virus evolution. We will integrate massive public datasets, including all available viral genomes from NCBI, hundreds of host genomes, and comprehensive databases of host-pathogen protein-protein interactions. Our team of virologists, evolutionary biologists, immunologists, and computational biologists will develop a powerful phylogenomic pipeline to systematically identify genes under positive selection—the molecular signatures of genetic conflict—across thousands of host-virus pairs. By mapping these rapidly evolving sites onto protein structures and interaction networks, we will uncover the emergent strategies of this conflict. We will identify conserved functional hotspots on host restriction factors targeted by diverse viral families and discover novel viral protein families dedicated to immune evasion. This project will produce a 'Molecular Conflict Atlas' that charts the evolutionary history and predicts future trajectories of host-virus interactions, providing a powerful framework for understanding viral emergence, pathogenesis, and the development of broad-spectrum antiviral strategies.",,"Background And Significance

The co-evolutionary struggle between hosts and their viral pathogens is a central engine of molecular innovation and a key determinant of species' health and survival. This incessant 'arms race,' often described by the Red Queen hypothesis, forces the rapid evolution of both host defense mechanisms and viral countermeasures, leaving indelible signatures in their respective genomes. The study of this conflict has yielded profound insights into fundamental biological processes, from the basic mechanics of protein-protein interactions to the diversification of entire gene families. Seminal studies over the past two decades have beautifully illustrated this dynamic in specific host-virus systems. For instance, the discovery of primate TRIM5α as a restriction factor against retroviruses, and the subsequent identification of the viral capsid as its target, revealed a history of recurrent positive selection in both proteins at their interaction interface. Similarly, the APOBEC3 family of cytidine deaminases, which lethally mutates viral genomes, has undergone dramatic expansion and diversification in primates, driven by antagonism from viral proteins like HIV's Vif, which itself is one of the most rapidly evolving genes in the viral genome. These canonical examples, alongside others like the protein kinase R (PKR) and myxovirus resistance (Mx) protein systems, have established a powerful paradigm: genes involved in host-virus conflicts can be identified by searching for the molecular signature of positive selection, where the rate of nonsynonymous substitution significantly exceeds the rate of synonymous substitution (dN/dS > 1). This approach has become a cornerstone of 'evolutionary immunology,' successfully identifying novel antiviral factors and pinpointing the precise molecular surfaces of conflict. However, our current understanding remains fragmented and anecdotal, largely derived from in-depth studies of a few well-chosen host lineages (primarily primates) and viral families (primarily retroviruses and lentiviruses). This narrow focus presents a major gap in our knowledge. We lack a systematic, global understanding of the principles governing these molecular arms races across the vast diversity of viruses and their hosts. Are the same host pathways repeatedly targeted by unrelated viruses? Do viruses convergently evolve similar molecular solutions to overcome conserved host defenses? Are there 'hotspots' of conflict in the host proteome that serve as a crucible for evolutionary innovation? Answering these questions has been impossible due to the immense scale of the required data and the need for deep, cross-disciplinary expertise. The current state of the field is a collection of fascinating, but disconnected, stories. This project aims to synthesize these stories into a coherent, universal narrative. The timeliness of this research cannot be overstated. We are living in an era of unprecedented data availability. Public repositories now contain millions of viral sequences from every conceivable environment and hundreds of high-quality host genomes spanning the tree of life. Concurrently, advances in computational power and phylogenomic methods have made it feasible to analyze these massive datasets at a scale previously unimaginable. The recent COVID-19 pandemic serves as a stark reminder of the critical need to understand the general principles of viral emergence and host adaptation. By moving beyond single-system studies to a comprehensive, pan-viral synthesis, we can uncover the emergent, predictive rules of molecular conflict. This project will transform the field from descriptive to predictive, providing a foundational framework for anticipating future pandemic threats and designing novel, broad-spectrum antiviral therapies.

Research Questions And Hypotheses

This working group will address the central question: What are the universal principles and emergent properties of the molecular arms race between viruses and their hosts? To deconstruct this overarching goal, we have formulated three specific, interconnected research questions, each with testable hypotheses and clear, predictable outcomes. 

**Question 1: What are the conserved molecular battlegrounds? A systematic identification of host and viral proteins under recurrent, intense evolutionary conflict.**
While we know of individual host restriction factors, we lack a global inventory of the genes most frequently engaged in viral conflict across diverse host taxa. Similarly, we have a limited catalog of the viral protein families dedicated to antagonizing these defenses.
*   **Hypothesis 1a:** A discrete subset of host protein functional classes (e.g., nucleic acid binding, ubiquitination machinery, membrane remodeling) are universally enriched for genes undergoing positive selection due to their central role in the host-virus interface.
*   **Prediction:** A genome-wide dN/dS scan across hundreds of host genomes will reveal that genes showing the strongest and most frequent signatures of positive selection are not randomly distributed but are significantly enriched in specific Gene Ontology (GO) terms related to innate immunity and core cellular processes co-opted by viruses.
*   **Hypothesis 1b:** We can discover novel, uncharacterized viral immune evasion gene families by identifying viral ortholog groups that consistently exhibit high rates of positive selection across diverse viral lineages.
*   **Prediction:** Our pan-viral phylogenomic screen will identify numerous viral ortholog groups with high dN/dS ratios that currently lack functional annotation. The taxonomic distribution of these rapidly evolving viral genes will correlate with specific host clades, suggesting adaptation to a particular host's immune repertoire.

**Question 2: Are there convergent 'rules of engagement' at the structural level? Uncovering the shared strategies of molecular antagonism.**
The physical interfaces between host and viral proteins are the atomic arenas of conflict. We seek to determine if evolution repeatedly finds similar structural solutions to win these battles.
*   **Hypothesis 2a:** Across diverse host-virus systems, positive selection will convergently target structurally and functionally equivalent 'hotspots' on orthologous host proteins. These hotspots represent vulnerable surfaces critical for protein function that viruses must engage to disable them.
*   **Prediction:** When we map positively selected sites onto 3D protein structures (from PDB or AlphaFold), we will find that these sites are not randomly distributed on the protein surface. Instead, they will form statistically significant spatial clusters. Furthermore, the locations of these clusters on, for example, all mammalian orthologs of PKR, will be conserved, even when these mammals are targeted by different viruses.
*   **Hypothesis 2b:** The sites of rapid evolution on a viral antagonist and its host target will be in direct physical proximity, forming a co-evolving 'molecular scar' at the protein-protein interface.
*   **Prediction:** For known host-virus interacting pairs, structural modeling will show that the majority of positively selected residues on the host protein are within a small physical distance (e.g., <10 Å) of the positively selected residues on the viral protein.

**Question 3: Can the evolutionary history of conflict predict its future trajectory and outcomes?**
By integrating the patterns of selection across a broad evolutionary timescale, we aim to build a framework that has predictive power for host range, viral emergence, and pathogenesis.
*   **Hypothesis 3a:** The 'evolutionary dynamism' of a host species' immune arsenal—defined by the number of positively selected genes and the intensity of selection—can predict its susceptibility to viral spillover.
*   **Prediction:** Host species with a larger and more rapidly evolving repertoire of antiviral genes will harbor a narrower range of endemic viruses and will be identified as the source of fewer zoonotic events compared to species with less dynamic immune genomes.
*   **Deliverables:** The primary deliverable will be the 'Molecular Conflict Atlas,' a comprehensive, open-access database and web portal. This resource will integrate our findings, allowing users to query genes, species, and viruses to visualize evolutionary histories, selection pressures, and structural hotspots. We will also deliver a suite of validated, containerized computational workflows for large-scale evolutionary analysis, and a series of high-impact publications.

Methods And Approach

This project is a pure data synthesis effort, leveraging publicly available data and requiring no new experimental data generation. Our approach is organized into three synergistic aims that integrate phylogenomics, structural biology, and network science. The scale of this analysis—integrating millions of viral sequences with hundreds of host genomes—necessitates the collaborative, multidisciplinary team and NCEMS resources proposed.

**Data Acquisition and Curation**
Our foundation will be a meticulously curated collection of public data:
1.  **Host Genomes:** We will select ~500 representative, high-quality vertebrate and invertebrate genome assemblies from NCBI Genomes and Ensembl, prioritizing taxonomic breadth and assembly contiguity. We will use the official gene annotations for each species.
2.  **Viral Genomes:** We will download the entirety of the NCBI Viral Genomes Resource, encompassing all complete viral sequences from RefSeq and a curated subset from GenBank. This dataset comprises millions of sequences, which will be clustered into viral orthologous groups (VOGs) using established tools like vConTACT2.
3.  **Interaction Data:** We will integrate data from the Host-Pathogen Interaction Database (HPIDB), IntAct, and BioGRID to create a reference set of known host-virus protein-protein interactions (PPIs). This set will be used to validate our methods and to seed analyses of co-evolution.
4.  **Structural Data:** We will utilize all available empirical structures from the Protein Data Bank (PDB) and supplement them with high-quality predicted models from the AlphaFold Database for proteins lacking experimental structures.

**Aim 1: A Global Map of Host and Viral Genes Under Positive Selection**
This aim will build the foundational dataset of genes involved in molecular conflict.
*   **Step 1: Host Orthogroup Identification:** We will use OrthoFinder to identify all one-to-one orthologs of protein-coding genes across our 500 host species. We will initially focus on a candidate list of ~2,000 genes with known or predicted immune function, later expanding to a genome-wide scale.
*   **Step 2: Phylogenomic Pipeline:** For each orthogroup, we will create multiple sequence alignments using MAFFT, perform quality trimming with Gblocks, and reconstruct gene trees using IQ-TREE under appropriate models of evolution. These steps will be automated in a Nextflow pipeline for scalability and reproducibility.
*   **Step 3: Positive Selection Analysis:** We will systematically apply the codeml program from the PAML package to each orthogroup alignment and gene tree. We will use site-models (M8 vs. M7) to identify specific codons under positive selection (dN/dS > 1) and use the Bayes Empirical Bayes (BEB) analysis to calculate posterior probabilities. To ensure robustness, we will corroborate significant findings with alternative methods like MEME and FUBAR from the HyPhy package. A stringent false discovery rate (FDR < 0.05) will be applied to correct for multiple testing across thousands of genes.
*   **Step 4: Viral Selection Analysis:** A parallel pipeline will be applied to the VOGs identified from the viral genome dataset. This represents a significant computational challenge and a primary justification for NCEMS support.

**Aim 2: Structural and Co-evolutionary Analysis of Conflict Interfaces**
This aim will translate sequence-level data into mechanistic, structural insights.
*   **Step 1: Structural Mapping:** All statistically significant positively selected sites identified in Aim 1 will be mapped onto the corresponding protein structures (PDB or AlphaFold models).
*   **Step 2: Hotspot Identification:** We will employ spatial statistics to determine if these sites are randomly distributed or form significant 3D clusters. We will use a density-based clustering algorithm (e.g., DBSCAN) on the 3D coordinates of the alpha-carbons of selected sites to identify 'evolutionary hotspots.'
*   **Step 3: Co-evolutionary Analysis:** For known and predicted interacting pairs, we will test for correlated evolutionary histories. This will involve comparing branch-specific dN/dS rates along the host and viral phylogenies and using methods like the Mirror-Tree server or custom phylogenetic correlation analyses to detect signatures of tightly coupled evolution.

**Aim 3: Synthesis, Prediction, and Creation of the Molecular Conflict Atlas**
This aim will integrate all data into a unified, predictive framework.
*   **Step 1: Network Construction:** We will build a bipartite network where nodes represent host genes and viral gene families. Edges will be drawn based on evidence of interaction (from databases or co-evolutionary analysis) and weighted by the intensity of positive selection.
*   **Step 2: Atlas Development:** We will develop a public web portal, the 'Molecular Conflict Atlas,' built on a robust database backend (e.g., PostgreSQL). This portal will feature interactive visualizations of phylogenies, protein structures with selection hotspots highlighted, and network graphs, allowing users to explore the data dynamically.

**Timeline and Milestones**
*   **Year 1:** Data acquisition and curation. Development and validation of the phylogenomic pipeline on a pilot dataset (e.g., primate genomes and their associated viruses). First in-person working group meeting.
*   **Year 2:** Full-scale execution of host and viral selection scans. Initial structural mapping and hotspot analysis. Development of the database schema for the Atlas. Mid-project meeting and trainee workshop.
*   **Year 3:** Co-evolutionary and network analyses. Completion and public launch of the Molecular Conflict Atlas. Manuscript preparation and dissemination of results. Final working group meeting.

Expected Outcomes And Impact

This project will fundamentally shift our understanding of host-virus evolution from a collection of specific case studies to a comprehensive, data-driven science. By synthesizing the vast repository of public genomic data, we will uncover the emergent, generalizable principles of molecular conflict. The expected outcomes will have a profound and lasting impact on molecular and cellular biology, with direct applications in public health and pandemic preparedness.

**Intended Contributions to the Field**
1.  **A Foundational Resource: The Molecular Conflict Atlas:** The primary outcome will be a publicly accessible, dynamic web resource that integrates our findings. This 'Atlas' will be the first of its kind, providing a queryable database of host and viral genes under selection, their evolutionary histories, their structural 'hotspots' of conflict, and their interaction networks. It will serve as a hypothesis-generation engine for the entire virology and immunology community for years to come, enabling researchers to instantly look up the evolutionary history of their gene of interest.
2.  **A Global Inventory of Molecular Arms Dealers:** We will produce the first comprehensive catalog of host defense genes and viral immune evasion factors identified through their evolutionary signatures across the tree of life. This will undoubtedly reveal hundreds of novel players in this conflict, opening up entirely new avenues of experimental research.
3.  **Discovery of General Principles and Convergent Evolution:** Our synthesis will move beyond individual examples to reveal the 'rules of engagement.' We will determine which host cellular pathways are the most common battlegrounds and whether viruses from different families have convergently evolved similar structural solutions to antagonize them. This will provide a new framework for understanding protein function and adaptation under intense selective pressure.

**Broader Impacts and Applications**
1.  **Informing Antiviral Therapies:** By identifying the conserved functional hotspots on host proteins that are repeatedly targeted by viruses, we can pinpoint ideal targets for host-directed antiviral therapies. Conversely, identifying the rapidly evolving interfaces on viral proteins can help predict and mitigate viral escape from targeted drugs or vaccines.
2.  **Pandemic Preparedness and 'Evolutionary Surveillance':** Our framework will enable a new form of surveillance. By analyzing the genome of a newly discovered animal virus, we can assess its evolutionary trajectory. Does it show signatures of positive selection in genes known to interact with human orthologs? This information can help prioritize research and public health resources on animal viruses that show the most evolutionary potential for zoonotic spillover.
3.  **Training the Next Generation of Data-Savvy Biologists:** This project is an ideal training vehicle. Graduate students and postdoctoral fellows will be at the heart of this cross-disciplinary effort, gaining invaluable skills in phylogenomics, computational biology, big data analysis, and collaborative science. We will host a dedicated workshop to disseminate our methods and train a wider community, directly addressing the research call's goal of developing the future workforce.

**Dissemination and Open Science**
We are deeply committed to open science principles. All computational pipelines and custom code will be open-source and shared on GitHub with containerized (Docker/Singularity) workflows for full reproducibility. All curated data and results will be deposited in public repositories (e.g., Dryad) and made available through the Molecular Conflict Atlas. We will disseminate our findings through high-impact publications in leading journals (e.g., Nature, Science, Cell Host & Microbe), presentations at major international conferences (e.g., Gordon Research Conferences on Viruses & Cells, American Society for Virology), and seminars at diverse institutions. The long-term vision is for the Atlas to become a community-sustained resource, with a plan for continued updates as new genomic data becomes available, ensuring its lasting value to the scientific community.

Budget And Resources

The proposed research represents a community-scale synthesis project whose scope, computational demands, and multidisciplinary nature far exceed the capabilities of any single research lab or existing collaboration. The need to process millions of viral genomes and hundreds of host genomes in a unified phylogenomic framework requires a level of computational infrastructure and dedicated personnel that can only be supported by a dedicated initiative like NCEMS. This budget reflects the resources necessary to coordinate a geographically and scientifically diverse team to tackle this grand challenge.

**Budget Justification**
The primary costs are for personnel to drive the project, computational resources to perform the analysis, and travel to facilitate the essential in-person collaboration that sparks innovation.

**1. Personnel (Total: ~$600,000 over 3 years)**
*   **Postdoctoral Fellows (2 FTE):** We request support for two postdoctoral fellows who will be the intellectual drivers of the project. Postdoc 1 will specialize in phylogenomics and pipeline development. Postdoc 2 will focus on structural biology, network analysis, and development of the Atlas. Their salaries are budgeted at standard NIH levels with fringe benefits.
*   **Data Manager/Scientist (0.5 FTE):** The scale of the data requires professional management. We request partial support for a data scientist to oversee data acquisition, manage the project's database, and lead the back-end development of the web portal.
*   **Principal Investigators (1 month summer salary/year for 4 PIs):** Summer salary is requested for the PIs to dedicate significant focused time to project oversight, analysis, and manuscript preparation, particularly during the summer months.
*   **Graduate Student Support:** We request travel and computational resource funds for two graduate students who will be supported by institutional funds but will be integral to the project.

**2. Travel (Total: ~$90,000)**
*   **Working Group Meetings:** We request funds for three in-person, two-day meetings for the entire team (5 PIs, 2 postdocs, 2 students). These meetings are critical for data integration, brainstorming, and collaborative analysis. Budget includes airfare, lodging, and meals.
*   **Conference Dissemination:** Funds are requested for each trainee (postdoc/student) and one PI to attend one major international conference per year to present findings and network with the broader community.

**3. Computational Resources (Total: ~$120,000)**
*   **High-Performance Computing (HPC):** The pan-viral selection scan is computationally massive. We request funds for purchasing a significant allocation on a national supercomputing cluster (e.g., via XSEDE/ACCESS) or equivalent cloud computing credits (e.g., AWS, Google Cloud). This is a critical need that cannot be met by typical institutional resources.
*   **Data Storage and Server:** Funds for a dedicated local server with large storage capacity (~100 TB) and high memory for data curation, intermediate analysis, and hosting the Atlas database.

**4. Other Direct Costs (Total: ~$45,000)**
*   **Publication Costs:** Open-access publication fees for an anticipated 3-4 major manuscripts.
*   **Trainee Workshop:** Funds to host a 3-day summer workshop in Year 2 for trainees and outside participants, covering materials, and speaker travel.

**5. Indirect Costs (F&A):** Calculated based on the negotiated rates of the lead institution.

This budget is essential for assembling the necessary human talent and computational power to synthesize a truly global view of host-virus evolution, a goal perfectly aligned with the mission of NCEMS.",,
ai_group_gemini_10,ai,group,gemini-2.5-pro,The Cellular Phenotype Project: Predicting Emergent Cellular Behavior by Integrating the Functional Genomics Universe,"The ultimate goal of cellular biology is to predict phenotype from genotype. Achieving this requires a systems-level understanding of how molecular components interact to produce emergent cellular behaviors. This working group proposes an audacious 'grand challenge' project: to build a single, unified computational model of a human cell by synthesizing the entirety of public functional genomics data. We will create a multi-layered knowledge graph integrating data from dozens of major consortia, including ENCODE (regulatory elements), GTEx (tissue-specific expression), the Human Protein Atlas (protein localization), DepMap (CRISPR-based gene dependencies), STRING/BioGRID (protein interactions), Reactome (pathways), and ChEMBL (drug-target data). This graph will represent the most comprehensive in silico model of a cell ever constructed. Our team, comprising world leaders in systems biology, large-scale data integration, and artificial intelligence, will then deploy cutting-edge graph neural networks and other AI methodologies to learn the emergent rules of cellular function from this integrated network. The model will be trained to predict the phenotypic consequences of perturbations, such as the system-wide effects of knocking out a gene or introducing a drug. This project will serve as a community resource, providing a powerful, open-source platform to generate hypotheses, interpret complex datasets, and move biology toward a truly predictive science.",,"Background And Significance

The quest to understand the mapping from genotype to phenotype is the central challenge of modern biology. While the sequencing of the human genome provided the 'parts list' for a cell, understanding how these parts interact to create complex, emergent behaviors remains a formidable task. The post-genomic era has been characterized by an explosion of high-throughput functional genomics data, generated by large-scale international consortia. Projects like the Encyclopedia of DNA Elements (ENCODE) have mapped regulatory regions; the Genotype-Tissue Expression (GTEx) project has cataloged gene expression across human tissues; the Human Protein Atlas (HPA) has systematically determined protein localization; and the Dependency Map (DepMap) has identified genes essential for cancer cell survival. These resources, among many others, offer unprecedented, multi-faceted views into cellular function. However, a critical limitation persists: these monumental datasets are largely analyzed in isolation. Each provides a single, incomplete projection of an immensely complex system. Consequently, our understanding remains fragmented, and our ability to predict the system-level consequences of genetic or chemical perturbations is limited. This fragmentation represents a major barrier to translating genomic information into therapeutic advances and a deeper understanding of human health and disease. Early attempts in systems biology to create holistic models, such as those based on ordinary differential equations (ODEs) or Boolean networks, were powerful for small, well-characterized pathways but failed to scale to the complexity of an entire cell. The subsequent rise of network biology, focusing on protein-protein interaction (PPI) networks, provided a more scalable framework. Seminal work demonstrated that network topology could reveal functional modules and predict gene function. However, these models typically represented only one data type (e.g., physical interactions) and lacked the multi-modal context necessary to capture the full spectrum of cellular regulation. More recently, knowledge graphs (KGs) have emerged as a powerful paradigm for integrating heterogeneous biological data. Projects like Hetionet have successfully integrated disparate databases to predict novel drug-disease relationships, demonstrating the power of connecting diverse information types. Yet, even these efforts have not fully incorporated the richness of modern functional genomics, such as genome-wide genetic dependency screens or comprehensive perturbational transcriptomics. The key gap this proposal addresses is the lack of a single, unified computational framework that can synthesize the full breadth of public functional genomics data to learn the fundamental rules of cellular behavior. This research is critically timely for two reasons. First, we have reached a critical mass of high-quality, publicly available data. The sheer volume and diversity of information from projects like DepMap, GTEx, and ENCODE now make a comprehensive synthesis not only possible but necessary to extract maximal value. Second, recent breakthroughs in artificial intelligence, particularly the development of graph neural networks (GNNs), provide the ideal computational tool for this challenge. GNNs are specifically designed to learn from complex, relational data structured as graphs, enabling them to model the intricate web of interactions within a cell. By combining the vast repository of public data with cutting-edge AI, we are poised to move beyond descriptive, correlational studies towards a truly predictive and mechanistic model of the cell. This project directly aligns with the research call's mission to catalyze multidisciplinary teams to synthesize public data, address a long-standing puzzle in cellular science, and develop innovative analytical strategies that are beyond the capacity of any single research lab.

Research Questions And Hypotheses

This working group will address the overarching scientific challenge of building a predictive, in silico model of a human cell. Our research is structured around four fundamental questions, each associated with specific, testable hypotheses that will guide our data synthesis and model development efforts. Our primary goal is to create a model that not only recapitulates known biology but also generates novel, experimentally verifiable predictions about emergent cellular behavior. 

**Research Question 1 (Integration):** What is the optimal computational architecture for synthesizing the universe of public functional genomics data into a single, multi-layered, and biologically coherent knowledge graph? While many databases exist, their integration is non-trivial, requiring harmonization of identifiers, ontologies, and data structures. We will explore different graph schemas to determine how best to represent diverse molecular entities (genes, proteins, compounds) and their complex relationships (regulation, interaction, localization, etc.).
*   **Hypothesis 1.1:** A heterogeneous knowledge graph, incorporating at least ten distinct molecular data types (e.g., gene expression, protein interactions, genetic dependencies, chromatin accessibility) and representing entities as distinct node types, will yield significantly higher performance in downstream predictive tasks compared to models built on homogeneous graphs or smaller subsets of data. We predict that the richness of the integrated data will allow the model to learn more robust and generalizable biological rules.
*   **Hypothesis 1.2:** The inclusion of context-specific edges, such as tissue-specific expression (from GTEx) or cell-line-specific gene dependencies (from DepMap), will enable the model to make more accurate context-dependent predictions than a generic, context-agnostic model.

**Research Question 2 (Learning):** Can advanced graph neural network (GNN) architectures learn the complex, non-linear patterns of molecular interactions from this integrated graph to effectively model the rules of cellular function?
*   **Hypothesis 2.1:** A Graph Transformer or Relational Graph Convolutional Network (R-GCN) model, trained on our Cellular Phenotype Knowledge Graph (CP-KG), will outperform current state-of-the-art machine learning models (e.g., random forests, gradient boosting) and models trained on individual data types in predicting held-out experimental data. Specifically, we predict a >15% improvement in the Area Under the Receiver Operating Characteristic (AUROC) curve for predicting gene essentiality.
*   **Hypothesis 2.2:** The node embeddings (latent representations) learned by the GNN will capture deep, biologically meaningful information. We hypothesize that genes with similar embeddings, even if not directly connected in the input graph, will share functional roles. We will test this by evaluating whether the cosine similarity of gene embeddings correlates with semantic similarity in Gene Ontology (GO) terms.

**Research Question 3 (Prediction):** How accurately can the trained model predict the phenotypic outcomes of novel genetic and chemical perturbations that were not seen during training?
*   **Hypothesis 3.1:** Our model can predict the essentiality of genes in cell lines held out from the training set with high accuracy. We will validate this by training the model on 80% of the cell lines in DepMap and testing its performance on the remaining 20%, demonstrating its ability to generalize across different genetic backgrounds.
*   **Hypothesis 3.2:** The model can predict cellular responses to drugs. By representing drugs as nodes and training the model on known drug sensitivity data (e.g., from the GDSC database), we hypothesize it can predict the efficacy of novel compounds or the response of untested cell lines to existing drugs. We will validate these predictions against new, publicly released drug screen data.

**Research Question 4 (Emergence):** Can we interrogate the trained model to uncover novel, higher-order principles of cellular organization and emergent biological phenomena?
*   **Hypothesis 4.1:** Using model interpretability techniques (e.g., GNNExplainer), we can identify the specific subgraphs and molecular pathways that are most influential for a given prediction (e.g., why a cell is sensitive to a particular drug). We hypothesize these identified pathways will be enriched for known mechanisms of action and will also reveal novel, off-target effects.
*   **Hypothesis 4.2:** In silico perturbation experiments—simulating the effect of a gene knockout by computationally removing its corresponding node from the graph—will accurately predict the resulting changes in the expression of other genes. We will validate these predictions against experimental data from perturbational datasets like the Connectivity Map (CMap L1000). This will demonstrate the model's capacity to predict the cascading, system-wide effects that define emergent phenotypes.

Methods And Approach

Our methodology is organized into three synergistic aims, forming a comprehensive plan to construct, train, and interrogate a predictive model of the cell. This project is exclusively computational and relies entirely on the synthesis of publicly available data, perfectly aligning with the research call's focus. The collaborative, multi-lab structure is essential for integrating the diverse expertise required for each aim.

**Aim 1: Construct the Cellular Phenotype Knowledge Graph (CP-KG).**
This foundational aim focuses on the aggregation, harmonization, and integration of disparate public datasets into a unified, machine-readable graph structure. This task requires significant bioinformatic expertise and is a community-scale effort.
*   **Data Sources:** We will integrate data from over a dozen major consortia and databases. Key sources include: (1) **Genomic/Epigenomic:** ENCODE (transcription factor binding sites, chromatin accessibility), JASPAR (TF motifs); (2) **Transcriptomic:** GTEx (baseline tissue expression), TCGA (cancer expression), Connectivity Map L1000 (gene expression post-perturbation); (3) **Proteomic:** Human Protein Atlas (subcellular localization, abundance), BioPlex/HuRI (physical protein-protein interactions); (4) **Functional/Genetic:** DepMap/Project Achilles (genome-wide CRISPR gene dependency scores), ClinVar (disease-associated variants); (5) **Pathways/Interactions:** Reactome, KEGG (curated pathways), STRING, BioGRID (functional and physical interaction networks); (6) **Chemical Biology:** ChEMBL, DrugBank (drug-target relationships), GDSC/CTRP (drug sensitivity screens).
*   **Integration and Harmonization:** We will establish a formal graph schema with defined node and edge types. Node types will include Gene, Protein, Chemical Compound, Disease, Pathway, and Cell Line. Edge types will represent relationships like `regulates_expression`, `physically_interacts_with`, `is_localized_in`, `is_essential_in`, `targets`, and `is_associated_with`. All biological entities will be mapped to standardized identifiers (e.g., Ensembl, UniProt, ChEBI) to resolve ambiguity. The integrated graph will be stored in a Neo4j database for efficient querying and exported to formats compatible with machine learning libraries like PyTorch Geometric (PyG) and Deep Graph Library (DGL).

**Aim 2: Develop and Train Predictive AI Models on the CP-KG.**
This aim leverages our team's expertise in artificial intelligence to build models that learn the rules of cellular function from the CP-KG.
*   **Model Architecture:** We will primarily use Heterogeneous Graph Neural Networks (GNNs), such as the Heterogeneous Graph Transformer (HGT), which are specifically designed to handle the diverse node and edge types in our CP-KG. The HGT model uses a meta-path-based attention mechanism to learn the importance of different relationship types when aggregating information across the graph.
*   **Training Strategy:** We will employ a two-stage training process. First, we will use self-supervised pre-training, where the model learns fundamental biological relationships by performing tasks like link prediction (predicting masked interactions) on the entire graph. This allows the model to learn rich, general-purpose embeddings for all nodes. Second, we will fine-tune the pre-trained model on specific supervised tasks using held-out datasets. Key tasks include: (1) **Gene Essentiality Prediction:** Predicting the CRISPR dependency score for each gene-cell line pair from DepMap. (2) **Drug Sensitivity Prediction:** Predicting the IC50 value for a given drug-cell line pair from GDSC. (3) **Perturbation Outcome Prediction:** Predicting the L1000 differential expression signature resulting from a specific genetic or chemical perturbation.
*   **Validation and Controls:** All models will be evaluated using rigorous k-fold cross-validation. To ensure we are testing for true generalization, folds will be stratified to hold out entire cell lines, drugs, or gene families, preventing trivial memorization. As a baseline control, we will compare our GNN's performance against simpler models (e.g., logistic regression, random forest) trained on non-integrated data features. We will also perform temporal validation by training on older data releases and testing on newly discovered interactions or functional annotations.

**Aim 3: Interrogate the Model to Uncover Emergent Biological Principles.**
With a validated model, this aim focuses on using it as a virtual laboratory to generate novel hypotheses.
*   **Model Interpretability:** We will use post-hoc explanation methods like GNNExplainer and Integrated Gradients to dissect model predictions. For example, when the model predicts a gene is essential, we will identify the minimal subgraph of interactions (e.g., a specific pathway) that was most influential in that decision.
*   **In Silico Perturbations:** We will systematically simulate perturbations by altering the graph structure or node features. For a gene knockout, we will remove the corresponding gene node and its edges and use the GNN to predict the resulting changes in the embeddings and properties of all other nodes in the graph. This allows us to simulate the cascading, system-wide effects of perturbations and identify critical nodes that mediate cellular robustness.

**Timeline:**
*   **Year 1 (Months 1-12):** Data acquisition, harmonization, and construction of CP-KG v1.0. Development and benchmarking of data processing pipelines. Initial implementation of the HGT model architecture. Milestone: Public release of CP-KG v1.0 schema and data.
*   **Year 2 (Months 13-24):** Self-supervised pre-training and supervised fine-tuning of models for all predictive tasks. Rigorous cross-validation and benchmarking. First manuscript detailing the CP-KG and predictive framework. Milestone: Public release of trained models and open-source code.
*   **Year 3 (Months 25-36):** In-depth model interrogation, large-scale in silico perturbation studies, and biological interpretation of findings. Development of a user-friendly web portal for community access. Final manuscripts and dissemination at international conferences. Milestone: Launch of the Cellular Phenotype Project web portal.

Expected Outcomes And Impact

The Cellular Phenotype Project is a high-risk, high-reward endeavor designed to create a paradigm shift in how cellular biology is studied. Its success will yield transformative outcomes and have a profound impact on both fundamental science and translational medicine. This project's scale and ambition directly address the research call's goal of tackling grand challenges through community-scale data synthesis.

**Expected Outcomes and Contributions to the Field:**
1.  **A Foundational Community Resource:** The primary deliverable will be the Cellular Phenotype Knowledge Graph (CP-KG) and the associated predictive models. This will be the most comprehensive, integrated in silico model of a human cell ever created. We will adhere strictly to Open Science principles, making the graph, all model code, and a user-friendly web portal publicly available. This resource will democratize systems-level analysis, enabling researchers worldwide to query the integrated knowledge base, generate hypotheses, and perform virtual experiments that would be impossible in a wet lab setting.
2.  **A New Paradigm for Predictive Biology:** This project will move the field beyond descriptive genomics and correlational network analysis towards a truly predictive science. By demonstrating that an AI model can learn the rules of cellular function from integrated data, we will establish a new framework for understanding the genotype-phenotype map. This will serve as a blueprint for future efforts to model more complex biological systems, such as tissues, organs, or even entire organisms.
3.  **Novel Biological Discoveries:** The model is not just a repository of known information but a discovery engine. We expect to identify thousands of novel, high-confidence predictions, including: new functions for uncharacterized genes, previously unknown pathways mediating drug response, unexpected cross-talk between signaling pathways, and key molecular players that govern cellular robustness and disease states. These in silico discoveries will provide a rich source of testable hypotheses for the broader experimental community.

**Broader Impacts and Applications:**
*   **Accelerating Drug Discovery and Development:** The pharmaceutical industry invests billions in identifying new drug targets and predicting patient responses. Our platform will provide a powerful tool for in silico target validation by predicting the system-wide consequences of inhibiting a specific protein. It can also be used to predict drug efficacy and toxicity across diverse genetic backgrounds, stratify patient populations for clinical trials, and identify novel drug repurposing opportunities.
*   **Advancing Personalized Medicine:** The ultimate vision is to create patient-specific models. By inputting a patient's genomic and transcriptomic data, future iterations of this model could predict their individual susceptibility to disease or their likely response to a panel of treatments, paving the way for truly personalized therapeutic strategies.
*   **Training the Next Generation of Scientists:** This project is an ideal training environment that sits at the intersection of biology, computer science, and data science. Graduate students and postdocs involved will gain invaluable cross-disciplinary skills, preparing them to be leaders in the future data-driven workforce. We will further amplify this impact by hosting annual workshops and releasing tutorials to train the wider community in using our tools and methods.

**Dissemination and Sustainability:**
Our dissemination strategy is multi-faceted. We will publish our findings in high-impact journals such as *Nature*, *Science*, and *Cell*, with methods-focused papers in journals like *Nature Methods* or *Nature Machine Intelligence*. All publications will be open-access. We will present our work at leading international conferences (e.g., ISMB, RECOMB, AACR). The long-term sustainability of the project will be ensured by building an active user community around our open-source tools and by seeking follow-up funding from federal agencies (e.g., NIH) to expand the model to include new data types (metabolomics, single-cell omics) and to build disease-specific versions (e.g., a 'Cancer Cell Phenotype Project'). This project will establish a living, evolving resource that will grow in value as more public data becomes available.

Budget And Resources

The proposed research represents a large-scale, multidisciplinary effort that is beyond the capabilities of a single research laboratory or existing collaboration. The scope of data integration, the computational intensity of the AI modeling, and the need for diverse, coordinated expertise necessitate the support and resources of the NCEMS Working Group program. The budget reflects the personnel and infrastructure required to execute this ambitious three-year project.

**Personnel (Total: $1,250,000)**
This is the largest component of the budget, reflecting the collaborative and training-focused nature of the project.
*   **Co-Principal Investigators (4 x 1 month summer salary/year):** $240,000. To support the dedicated time of the four PIs from different institutions, who bring essential, non-overlapping expertise in systems biology, bioinformatics, machine learning, and proteomics.
*   **Postdoctoral Fellows (3 FTEs for 3 years):** $630,000. Three postdoctoral researchers will form the core research team. One will specialize in large-scale data integration and bioinformatics; the second will focus on GNN model development and training; the third will lead model interrogation, validation, and biological interpretation.
*   **Graduate Students (2 FTEs for 3 years):** $240,000. Two graduate students will support the postdocs, taking lead on specific sub-projects, such as the integration of a particular data modality or the development of a specific predictive task.
*   **Project Manager / Software Engineer (0.5 FTE for 3 years):** $140,000. A part-time professional is critical for managing the complex project timeline across multiple institutions, overseeing data releases, and leading the development of the public-facing web portal.

**Computational Resources (Total: $150,000)**
*   **High-Performance Computing (HPC) / Cloud Credits:** $120,000. Training graph neural networks on a graph with billions of nodes and edges is computationally prohibitive on local hardware. This allocation will provide access to a national supercomputing center or commercial cloud provider (e.g., AWS, Google Cloud) for access to high-memory, multi-GPU nodes required for model training and large-scale in silico experiments.
*   **Data Storage:** $30,000. To cover the costs of robust, long-term storage (~100 TB) for the raw data, the integrated CP-KG, model checkpoints, and simulation results.

**Other Direct Costs (Total: $100,000)**
*   **Travel:** $60,000. To support biannual in-person meetings of the entire working group to foster deep collaboration and strategic planning. This also includes funds for trainees and PIs to present findings at one major international conference per year.
*   **Workshops and Training:** $25,000. To host one annual open workshop to train the broader scientific community on our tools and to foster a collaborative ecosystem around the project.
*   **Publication Costs:** $15,000. To cover open-access fees for an anticipated 3-4 major publications.

**Total Direct Costs:** $1,500,000
**Indirect Costs (F&A) (Calculated at a hypothetical 50% rate):** $750,000
**Total Requested Budget:** $2,250,000

**Justification for NCEMS Support:** This project is the epitome of a community-scale synthesis project. No single lab possesses the combined expertise in functional genomics, network biology, and cutting-edge AI, nor the resources to undertake the massive data integration and computational modeling effort required. The NCEMS framework is essential to bring together this diverse team, provide the necessary computational infrastructure, and support the collaborative environment needed to tackle this 'grand challenge' in cellular biology.",,
ai_group_gpt_01,ai,group,gpt-4,Synthesis of Molecular Data for Cancer Cell Evolution,"This research aims to synthesize existing molecular and cellular data to understand the evolution of cancer cells. By integrating diverse datasets, we will address the long-standing puzzle of how cancer cells evolve and adapt to different environments. This project will require collaboration between oncologists, molecular biologists, and data scientists, and will provide training opportunities for graduate students and postdocs. The findings will be made publicly available in accordance with open science principles.",,"Background And Significance

Cancer, a leading cause of death worldwide, is a complex disease characterized by uncontrolled cell growth and proliferation. The evolution of cancer cells, their adaptation to different environments, and the molecular mechanisms underlying these processes are still not fully understood. Current research in the field is fragmented, with different labs focusing on specific aspects of cancer cell evolution. However, a comprehensive, integrated understanding of this process is lacking. This research aims to fill this gap by synthesizing existing molecular and cellular data to provide a holistic view of cancer cell evolution. A detailed literature review reveals that while significant progress has been made in understanding the genetic and epigenetic changes that drive cancer progression, the dynamic nature of cancer cell evolution and its implications for treatment resistance and disease recurrence are still poorly understood. This research is timely and important as it will provide new insights into cancer cell evolution, potentially leading to the development of more effective treatment strategies.

Research Questions And Hypotheses

This research will address the following questions: 1) How do cancer cells evolve and adapt to different environments at the molecular and cellular level? 2) What are the key molecular mechanisms driving cancer cell evolution? 3) How can the synthesized data be used to predict cancer progression and treatment outcomes? The hypotheses to be tested include: 1) Cancer cell evolution is driven by a combination of genetic and epigenetic changes that confer survival advantages in different environments. 2) The integration and synthesis of existing molecular and cellular data can provide new insights into the dynamic nature of cancer cell evolution. The expected outcomes include a comprehensive understanding of cancer cell evolution, development of predictive models for cancer progression, and identification of potential therapeutic targets. These hypotheses will be tested through data synthesis, computational modeling, and statistical analysis.

Methods And Approach

This research will utilize publicly available molecular and cellular data from various sources, including genomic, transcriptomic, and proteomic datasets from cancer cell lines and patient samples. The data will be integrated and synthesized using advanced computational approaches, including machine learning and network analysis. The experimental design involves the systematic collection, integration, and analysis of existing data, with no new experimental data being generated. The project will be carried out over a period of three years, with specific milestones and deliverables set for each year. Statistical analysis will be performed to validate the findings and test the hypotheses.

Expected Outcomes And Impact

This research is expected to make significant contributions to the field of cancer biology by providing a comprehensive understanding of cancer cell evolution. The findings could have broader impacts, including the development of predictive models for cancer progression and identification of potential therapeutic targets. The research could also stimulate further research and collaborations in the field. The findings will be disseminated through peer-reviewed publications and presentations at scientific conferences. The project also has a long-term vision of establishing a publicly accessible database of synthesized molecular and cellular data on cancer cell evolution, contributing to the open science movement.

Budget And Resources

The budget for this research project is estimated to be $500,000, which will be used to cover personnel salaries, computational resources, data acquisition, and dissemination of findings. The project will require the collaboration of oncologists, molecular biologists, and data scientists, as well as the involvement of graduate students and postdocs for training purposes. The resources required for this project, including computational infrastructure and data sources, are readily available and accessible.",,
ai_group_gpt_02,ai,group,gpt-4,Cross-Disciplinary Analysis of Neurodegenerative Diseases,"This project will bring together neuroscientists, molecular biologists, and data analysts to synthesize existing data on neurodegenerative diseases. The goal is to develop innovative research strategies to answer novel questions about the molecular and cellular mechanisms underlying these diseases. This project will provide training opportunities for young scientists and will adhere to open science principles.",,"Background And Significance

Neurodegenerative diseases, including Alzheimer's, Parkinson's, and Huntington's, are a growing global health concern. Despite significant advances in our understanding of these diseases, many questions remain about their molecular and cellular mechanisms. Current research is fragmented across different disciplines, limiting our ability to synthesize and interpret the available data. This project aims to bridge this gap by bringing together experts in neuroscience, molecular biology, and data analysis. A comprehensive literature review reveals that while significant progress has been made in understanding the genetic and environmental factors contributing to these diseases, there is a lack of integrative, cross-disciplinary research. This project is timely and important as it addresses this gap, leveraging existing data to generate new insights into the molecular and cellular mechanisms of neurodegenerative diseases.

Research Questions And Hypotheses

This project will address several key research questions: 1) What are the common molecular and cellular mechanisms underlying different neurodegenerative diseases? 2) Can we identify novel biomarkers for early detection and progression of these diseases? 3) Can we develop predictive models for disease progression based on molecular and cellular data? We hypothesize that by integrating data across disciplines, we can identify common molecular and cellular pathways involved in neurodegeneration, discover novel biomarkers, and develop predictive models. These hypotheses will be tested through rigorous data analysis and validation using machine learning and statistical methods.

Methods And Approach

We will utilize publicly available data from various sources, including genomic, proteomic, and clinical data from patients with neurodegenerative diseases. Data integration will be performed using advanced computational methods, including machine learning and network analysis. We will also develop novel analytical strategies to identify common molecular and cellular pathways and biomarkers. The project will be carried out over three years, with specific milestones and deliverables for each year. Statistical analysis will be performed using appropriate methods, including regression analysis, survival analysis, and machine learning algorithms.

Expected Outcomes And Impact

This project is expected to make significant contributions to our understanding of neurodegenerative diseases. By integrating data across disciplines, we aim to identify common molecular and cellular mechanisms, discover novel biomarkers, and develop predictive models for disease progression. These findings will have broad impacts, potentially leading to improved diagnosis and treatment strategies. The project will also provide training opportunities for young scientists and promote collaboration among researchers from diverse fields. Findings will be disseminated through peer-reviewed publications, conference presentations, and open-access data repositories.

Budget And Resources

The budget for this project includes costs for personnel (including salaries for researchers, data analysts, and administrative support), data acquisition and analysis (including software licenses and computing resources), and dissemination of results (including publication fees and conference travel). We estimate a total budget of $1.5 million over three years. This project requires resources beyond the capabilities of a single lab, including access to large-scale computational resources and expertise in data integration and analysis. We will leverage existing resources at our institutions and seek additional support from the funding organization.",,
ai_group_gpt_03,ai,group,gpt-4,Integration of Genomic Data for Understanding Genetic Disorders,"This research will synthesize publicly available genomic data to address fundamental questions related to genetic disorders. By integrating distinct datasets, we aim to solve long-standing puzzles in the molecular and cellular sciences. This project will require collaboration between geneticists, molecular biologists, and data scientists, and will provide training opportunities for graduate students and postdocs. The findings will be made publicly available in accordance with open science principles.",,"Background And Significance

Genetic disorders, caused by mutations in one or more genes, are a significant global health concern. Despite advances in genomics, our understanding of the molecular and cellular mechanisms underlying these disorders remains incomplete. This is partly due to the vast complexity of the human genome and the challenges associated with integrating and analyzing large genomic datasets. Current research in the field is fragmented, with individual labs focusing on specific disorders or genes. This approach has yielded important insights but has also left many questions unanswered. A comprehensive, integrated analysis of publicly available genomic data could provide a more holistic view of genetic disorders, potentially revealing common patterns and mechanisms. This research is timely given the increasing availability of genomic data and the urgent need for improved diagnostic and therapeutic strategies for genetic disorders. The proposed project will address key gaps in our knowledge by synthesizing and analyzing diverse genomic datasets, with the aim of uncovering novel insights into the molecular and cellular basis of genetic disorders.

Research Questions And Hypotheses

The proposed research will address the following questions: 1) What are the common molecular and cellular mechanisms underlying different genetic disorders? 2) Can we identify novel genetic markers for these disorders? 3) Can we develop new analytical strategies for integrating and interpreting genomic data? We hypothesize that by integrating diverse genomic datasets, we can uncover common patterns and mechanisms underlying genetic disorders. We also predict that our analysis will reveal novel genetic markers that could be used for diagnosis or therapeutic targeting. Our expected outcomes include a comprehensive database of genetic markers for different disorders, novel analytical tools for genomic data integration, and a better understanding of the molecular and cellular mechanisms of genetic disorders.

Methods And Approach

We will use publicly available genomic datasets from sources such as the 1000 Genomes Project, the Human Genome Project, and the Genotype-Tissue Expression (GTEx) project. These datasets will be integrated using advanced computational approaches, including machine learning and network analysis. We will also develop new analytical strategies for data integration and interpretation. Our experimental design will involve the systematic comparison of genomic data from individuals with and without specific genetic disorders, with appropriate controls for factors such as age, sex, and population structure. We will also perform replication studies to validate our findings. Our timeline includes initial data collection and integration (months 1-6), development of analytical tools (months 7-12), data analysis and interpretation (months 13-24), and dissemination of findings (months 25-36).

Expected Outcomes And Impact

The proposed research is expected to make significant contributions to the field of molecular and cellular biology, particularly in the area of genetic disorders. By integrating diverse genomic datasets, we aim to uncover novel insights into the molecular and cellular mechanisms of these disorders. Our findings could lead to the identification of new genetic markers for diagnosis and therapeutic targeting. The project will also result in the development of innovative analytical tools for genomic data integration, which could be widely adopted by the research community. In addition, our research will provide valuable training opportunities for graduate students and postdocs, helping to build the next generation of data-savvy scientists. We plan to disseminate our findings through peer-reviewed publications, conference presentations, and open-access data repositories.

Budget And Resources

The proposed budget includes costs for data acquisition and storage ($50,000), computational resources ($100,000), personnel salaries ($200,000), and dissemination activities ($50,000). We will leverage existing resources at our institutions, including high-performance computing clusters and bioinformatics software. We will also seek additional funding from other sources to supplement the budget provided by the NCEMS.",,
ai_group_gpt_04,ai,group,gpt-4,Data Synthesis for Understanding Cellular Aging,"This project aims to synthesize existing molecular and cellular data to understand the process of cellular aging. By integrating diverse datasets, we will address the novel question of how cells age and what factors influence this process. This project will require collaboration between gerontologists, molecular biologists, and data scientists, and will provide training opportunities for graduate students and postdocs. The findings will be made publicly available in accordance with open science principles.",,"Background And Significance

Cellular aging is a complex process that involves a multitude of factors and mechanisms. Despite significant advancements in the field, our understanding of the molecular and cellular processes underlying aging remains incomplete. Current research has identified several key factors involved in cellular aging, including telomere shortening, oxidative stress, and DNA damage. However, these studies often focus on individual factors and fail to consider the complex interplay between different aging processes. This project aims to fill this gap by synthesizing existing molecular and cellular data to provide a comprehensive understanding of cellular aging. The importance of this research is underscored by the increasing prevalence of age-related diseases and the societal and economic implications of an aging population. By improving our understanding of cellular aging, this project has the potential to inform the development of interventions to delay aging and prevent age-related diseases.

Research Questions And Hypotheses

This project will address the following research questions: 1) What are the key molecular and cellular processes involved in cellular aging? 2) How do these processes interact and influence each other? 3) What factors influence the rate of cellular aging? Based on existing literature, we hypothesize that cellular aging is a multifactorial process involving a complex interplay between various molecular and cellular processes. We predict that by integrating diverse datasets, we will be able to identify novel interactions and pathways involved in cellular aging. The outcomes of this project will include a comprehensive map of the molecular and cellular processes involved in aging, as well as a database of factors influencing cellular aging.

Methods And Approach

This project will involve the synthesis and integration of existing molecular and cellular data related to aging. We will use publicly available datasets from various sources, including gene expression databases, protein-protein interaction networks, and epigenetic data. These datasets will be integrated using advanced computational approaches, including machine learning and network analysis, to identify key processes and interactions involved in cellular aging. The project will be carried out in collaboration with gerontologists, molecular biologists, and data scientists, and will provide training opportunities for graduate students and postdocs. The project will be carried out over a period of three years, with specific milestones and deliverables set for each year.

Expected Outcomes And Impact

This project is expected to significantly advance our understanding of cellular aging by providing a comprehensive map of the molecular and cellular processes involved in aging. This will not only fill a significant gap in the current literature, but also provide a valuable resource for future research in the field. The findings of this project have the potential to inform the development of interventions to delay aging and prevent age-related diseases, thereby having a significant impact on public health. Furthermore, by providing training opportunities for graduate students and postdocs, this project will contribute to the training of the next generation of scientists in the field of aging research.

Budget And Resources

The budget for this project will cover the costs of data acquisition and analysis, personnel salaries, and overhead costs. The majority of the budget will be allocated to personnel salaries, including salaries for the principal investigators, postdocs, and graduate students. A significant portion of the budget will also be allocated to data acquisition and analysis, including the costs of accessing publicly available datasets and computational resources for data analysis. Overhead costs, including administrative costs and indirect costs, will also be covered by the budget. The project will leverage existing resources and infrastructure at the participating institutions, including computational resources and data storage facilities.",,
ai_group_gpt_05,ai,group,gpt-4,Cross-Disciplinary Analysis of Autoimmune Diseases,"This project will bring together immunologists, molecular biologists, and data analysts to synthesize existing data on autoimmune diseases. The goal is to develop innovative research strategies to answer novel questions about the molecular and cellular mechanisms underlying these diseases. This project will provide training opportunities for young scientists and will adhere to open science principles.",,"Background And Significance

Autoimmune diseases, characterized by the body's immune system attacking its own cells, are a significant global health concern. Despite extensive research, the molecular and cellular mechanisms underlying these diseases remain poorly understood. This project aims to fill this knowledge gap by synthesizing existing data from diverse sources. The current state of the field is fragmented, with individual labs focusing on specific diseases or mechanisms. A comprehensive, cross-disciplinary synthesis of existing data is needed to advance our understanding of autoimmune diseases. This research is timely and important as the prevalence of autoimmune diseases is increasing worldwide. By bringing together immunologists, molecular biologists, and data analysts, we aim to develop innovative research strategies that will provide novel insights into the molecular and cellular mechanisms underlying autoimmune diseases.

Research Questions And Hypotheses

This project will address the following research questions: 1) What are the common molecular and cellular mechanisms underlying different autoimmune diseases? 2) Can we identify novel biomarkers for early detection and prognosis of autoimmune diseases? 3) Can we develop predictive models for disease progression and response to treatment? We hypothesize that a cross-disciplinary synthesis of existing data will reveal common molecular and cellular mechanisms across different autoimmune diseases. We also expect to identify novel biomarkers and develop predictive models that will improve disease management. These hypotheses will be tested by integrating and analyzing data from diverse sources, including genomic, proteomic, and clinical data.

Methods And Approach

We will use publicly available data from various sources, including genomic databases, proteomic databases, and clinical databases. These data will be integrated and analyzed using advanced computational approaches, including machine learning and network analysis. We will also develop novel analytical strategies to answer our research questions. The project will be carried out in three phases: data integration, data analysis, and validation. In the data integration phase, we will collect and integrate data from diverse sources. In the data analysis phase, we will apply computational approaches to analyze the integrated data. In the validation phase, we will validate our findings using independent datasets. The project will be carried out over a period of three years, with specific milestones and deliverables for each phase.

Expected Outcomes And Impact

This project is expected to make significant contributions to the field of autoimmune diseases. By synthesizing existing data, we aim to provide novel insights into the molecular and cellular mechanisms underlying these diseases. These insights could lead to the development of new diagnostic tools and therapeutic strategies. The project will also provide training opportunities for young scientists and promote cross-disciplinary collaboration. The findings will be disseminated through peer-reviewed publications and presentations at scientific conferences. In the long term, we envision that this project will stimulate further research and collaborations in the field of autoimmune diseases.

Budget And Resources

The budget for this project is estimated at $1.5 million over three years. This includes salaries for the research team, computational resources, and training costs. The project will require significant computational resources for data integration and analysis. We will also need access to publicly available data sources, which may require subscription fees. The project will be carried out by a team of immunologists, molecular biologists, and data analysts, who will bring diverse expertise to the project. We will also provide training opportunities for graduate students and postdocs, which will require additional resources.",,
ai_group_gpt_06,ai,group,gpt-4,Integration of Proteomic Data for Understanding Protein Folding Disorders,"This research will synthesize publicly available proteomic data to address fundamental questions related to protein folding disorders. By integrating distinct datasets, we aim to solve long-standing puzzles in the molecular and cellular sciences. This project will require collaboration between biochemists, molecular biologists, and data scientists, and will provide training opportunities for graduate students and postdocs. The findings will be made publicly available in accordance with open science principles.",,"Background And Significance

Protein folding disorders, including Alzheimer's, Parkinson's, and Huntington's diseases, are a significant global health concern. Despite extensive research, the molecular mechanisms underlying these disorders remain elusive. Current understanding is limited by the complexity of protein folding processes and the lack of comprehensive, integrated datasets. This research aims to fill this gap by synthesizing publicly available proteomic data to gain novel insights into protein folding disorders. A detailed literature review reveals that while individual studies have contributed valuable insights, a comprehensive, integrated analysis of proteomic data is lacking. This research is timely and important as it addresses a critical need in the field and has the potential to significantly advance our understanding of protein folding disorders.

Research Questions And Hypotheses

This research will address the following questions: 1) What are the common molecular signatures across different protein folding disorders? 2) Can we identify novel biomarkers for early detection of these disorders? 3) Can we develop predictive models for disease progression based on proteomic data? We hypothesize that integrating diverse proteomic datasets will reveal common molecular signatures across different protein folding disorders, identify novel biomarkers, and enable the development of predictive models. These hypotheses will be tested through rigorous data analysis and validation using independent datasets.

Methods And Approach

We will use publicly available proteomic datasets from various sources, including the Human Protein Atlas and PRIDE database. These datasets will be integrated using advanced data integration techniques. The integrated dataset will be analyzed using machine learning algorithms to identify common molecular signatures and potential biomarkers. Predictive models will be developed using deep learning techniques. The project will be carried out over three years, with specific milestones and deliverables at each stage. Statistical analysis will be performed using appropriate methods, including multiple testing correction and cross-validation.

Expected Outcomes And Impact

This research is expected to significantly advance our understanding of protein folding disorders. The identification of common molecular signatures and novel biomarkers could lead to improved diagnostic tools and therapeutic strategies. The predictive models could provide valuable insights into disease progression and inform patient management. The findings will be disseminated through peer-reviewed publications and presentations at scientific conferences. The integrated dataset and analysis workflows will be made publicly available, promoting open science and enabling further research. This project also provides a unique training opportunity for graduate students and postdocs, fostering the next generation of data-savvy scientists.

Budget And Resources

The budget for this project includes costs for data acquisition, computational resources, personnel salaries, and dissemination of findings. Data acquisition costs include access to premium databases and data cleaning. Computational resources include server costs and software licenses. Personnel costs include salaries for biochemists, molecular biologists, and data scientists, as well as stipends for graduate students and postdocs. Dissemination costs include publication fees and conference travel. The project will leverage existing resources at the participating institutions, including high-performance computing facilities and bioinformatics tools.",,
ai_group_gpt_07,ai,group,gpt-4,Data Synthesis for Understanding Stem Cell Differentiation,"This project aims to synthesize existing molecular and cellular data to understand the process of stem cell differentiation. By integrating diverse datasets, we will address the novel question of how stem cells differentiate and what factors influence this process. This project will require collaboration between stem cell researchers, molecular biologists, and data scientists, and will provide training opportunities for graduate students and postdocs. The findings will be made publicly available in accordance with open science principles.",,"Background And Significance

Stem cells, with their unique ability to differentiate into specialized cell types, hold immense potential for regenerative medicine and disease modeling. Despite significant advancements, our understanding of the molecular mechanisms governing stem cell differentiation remains incomplete. Current knowledge is fragmented across diverse datasets, limiting our ability to draw comprehensive insights. This project aims to bridge this gap by synthesizing existing molecular and cellular data to provide a holistic understanding of stem cell differentiation. This research is timely and significant, given the increasing interest in stem cell therapies for various diseases. A comprehensive understanding of stem cell differentiation could pave the way for more effective and targeted therapeutic strategies.

Research Questions And Hypotheses

This project aims to answer the following research questions: 1) What are the molecular mechanisms governing stem cell differentiation? 2) What factors influence the differentiation process? We hypothesize that stem cell differentiation is a complex process influenced by a multitude of factors, including genetic, epigenetic, and environmental factors. We predict that our data synthesis approach will reveal novel insights into these mechanisms and factors. The expected outcomes include a comprehensive dataset on stem cell differentiation and a set of analytical tools for data synthesis. These hypotheses will be tested through rigorous data analysis and validation.

Methods And Approach

We will utilize publicly available molecular and cellular data from various sources, including gene expression databases, epigenetic datasets, and environmental factor databases. Our analytical approach will involve data integration, normalization, and analysis using advanced computational methods. We will employ machine learning algorithms to identify patterns and relationships in the data. Our project timeline spans three years, with specific milestones including data collection and integration, data analysis, validation, and dissemination of findings. Statistical analysis will be performed to validate our findings.

Expected Outcomes And Impact

This project is expected to make significant contributions to the field of stem cell research by providing a comprehensive understanding of stem cell differentiation. The broader impacts include potential applications in regenerative medicine and disease modeling. Our findings could stimulate further research and collaborations in the field. We plan to disseminate our findings through peer-reviewed publications and presentations at scientific conferences. Our long-term vision is to establish a publicly accessible platform for data synthesis in stem cell research, promoting open science and sustainability.

Budget And Resources

Our budget includes costs for data access, computational resources, personnel salaries, and dissemination of findings. We estimate a total budget of $500,000, with $200,000 allocated for data access and computational resources, $250,000 for personnel salaries, and $50,000 for dissemination of findings. We will leverage existing resources at our institutions, including computational infrastructure and expertise in data analysis. We will also seek additional funding opportunities to support our project.",,
ai_group_gpt_08,ai,group,gpt-4,Cross-Disciplinary Analysis of Infectious Diseases,"This project will bring together virologists, molecular biologists, and data analysts to synthesize existing data on infectious diseases. The goal is to develop innovative research strategies to answer novel questions about the molecular and cellular mechanisms underlying these diseases. This project will provide training opportunities for young scientists and will adhere to open science principles.",,"Background And Significance

Infectious diseases continue to pose significant threats to global health, with emerging and re-emerging pathogens causing substantial morbidity and mortality. Despite advances in molecular and cellular biology, our understanding of the mechanisms underlying these diseases remains incomplete. This project aims to fill this knowledge gap by synthesizing existing data from diverse sources. The current state of the field is characterized by a wealth of data, but a lack of integrative, cross-disciplinary approaches to analyze this information. Previous studies have focused on individual pathogens or specific aspects of disease mechanisms, leaving many questions unanswered. This research is both important and timely, as it will provide novel insights into infectious diseases, potentially informing the development of new treatments and prevention strategies. Furthermore, by training the next generation of scientists in data synthesis and cross-disciplinary collaboration, this project will contribute to the development of a data-savvy workforce.

Research Questions And Hypotheses

This project will address several research questions: 1) What are the common molecular and cellular mechanisms underlying infectious diseases? 2) How do these mechanisms vary between different pathogens and host species? 3) Can we predict disease outcomes based on these mechanisms? We hypothesize that there are common molecular and cellular pathways that are exploited by different pathogens, and that understanding these pathways can provide insights into disease outcomes. We will test these hypotheses by synthesizing and analyzing existing data on infectious diseases. The expected outcomes of this project include a comprehensive understanding of the molecular and cellular mechanisms underlying infectious diseases, and the development of predictive models for disease outcomes.

Methods And Approach

We will use a variety of data sources, including genomic, transcriptomic, and proteomic datasets from public databases. We will also utilize epidemiological data to correlate molecular and cellular mechanisms with disease outcomes. Our analytical methods will include data integration, network analysis, and machine learning. We will use these methods to identify common molecular and cellular pathways, and to develop predictive models for disease outcomes. The project will be carried out over a period of three years, with specific milestones including data collection and integration (Year 1), data analysis and model development (Year 2), and model validation and dissemination of results (Year 3).

Expected Outcomes And Impact

This project is expected to make significant contributions to the field of infectious diseases, by providing novel insights into the molecular and cellular mechanisms underlying these diseases. The broader impacts of this research include the potential to inform the development of new treatments and prevention strategies, and to contribute to our understanding of disease emergence and spread. The project will also provide training opportunities for young scientists, promoting the development of a data-savvy workforce. We plan to disseminate our findings through peer-reviewed publications, conference presentations, and open-access data repositories.

Budget And Resources

The budget for this project includes costs for data access and storage, computational resources, personnel salaries, and training activities. We estimate a total budget of $500,000, with $200,000 allocated for data and computational resources, $200,000 for personnel salaries, and $100,000 for training activities. This budget reflects the need for substantial computational resources to handle large datasets, and for a multidisciplinary team of researchers with expertise in virology, molecular biology, and data analysis.",,
ai_group_gpt_09,ai,group,gpt-4,Integration of Metabolomic Data for Understanding Metabolic Disorders,"This research will synthesize publicly available metabolomic data to address fundamental questions related to metabolic disorders. By integrating distinct datasets, we aim to solve long-standing puzzles in the molecular and cellular sciences. This project will require collaboration between endocrinologists, molecular biologists, and data scientists, and will provide training opportunities for graduate students and postdocs. The findings will be made publicly available in accordance with open science principles.",,"Background And Significance

Metabolic disorders, including diabetes, obesity, and metabolic syndrome, are a growing global health concern. Despite significant advances in our understanding of these disorders, many questions remain unanswered. The field of metabolomics, which studies the unique chemical fingerprints that specific cellular processes leave behind, offers a promising avenue for further exploration. However, the vast amount of publicly available metabolomic data remains underutilized due to the complexity of data integration and analysis. This research aims to bridge this gap by synthesizing and integrating these datasets to gain novel insights into metabolic disorders. A comprehensive literature review reveals that while individual studies have made significant contributions to our understanding of metabolic disorders, a synthesis of these findings is lacking. Current knowledge is fragmented and often confined to specific metabolic pathways or individual disorders. This research is both timely and significant as it will provide a comprehensive, integrated view of metabolic disorders, potentially revealing novel connections and insights that can guide future research and therapeutic strategies.

Research Questions And Hypotheses

This research aims to address the following questions: 1) What are the common and unique metabolic signatures across different metabolic disorders? 2) Can we identify novel metabolic pathways implicated in these disorders? 3) How do these metabolic changes correlate with clinical outcomes? We hypothesize that by integrating and synthesizing metabolomic data, we can identify common and unique metabolic signatures across different disorders, reveal novel metabolic pathways, and establish correlations with clinical outcomes. These hypotheses will be tested through rigorous data analysis and validation. The expected outcomes include a comprehensive metabolic map of metabolic disorders, identification of novel metabolic pathways, and establishment of correlations between metabolic changes and clinical outcomes.

Methods And Approach

We will utilize publicly available metabolomic datasets from databases such as MetaboLights, Human Metabolome Database, and Metabolomics Workbench. These datasets will be integrated using advanced data integration techniques, followed by comprehensive analysis using machine learning and network analysis methods. The project will be carried out in three phases: data collection and integration, data analysis, and validation. The timeline for the project is three years, with specific milestones and deliverables set for each phase. Statistical analysis will be carried out using appropriate methods, including regression analysis, cluster analysis, and network analysis.

Expected Outcomes And Impact

This research is expected to make significant contributions to the field of metabolic disorders. By providing a comprehensive, integrated view of metabolic disorders, it will reveal novel connections and insights that can guide future research and therapeutic strategies. The findings will have broader impacts, including potential applications in precision medicine and public health. The research will also provide training opportunities for graduate students and postdocs, fostering the next generation of data-savvy scientists. The findings will be disseminated through peer-reviewed publications, conference presentations, and public data repositories, adhering to open science principles.

Budget And Resources

The budget for this project includes costs for data access and storage, computational resources, personnel salaries, and dissemination of findings. Data access and storage will require substantial resources due to the large volume of metabolomic data. Computational resources, including high-performance computing facilities and software licenses, will also constitute a significant portion of the budget. Personnel salaries will include compensation for the principal investigators, postdocs, graduate students, and data analysts involved in the project. Dissemination costs will cover publication fees and conference presentations. The project will leverage existing resources at the participating institutions, including computational facilities and expertise in metabolomics, data science, and metabolic disorders.",,
ai_group_gpt_10,ai,group,gpt-4,Data Synthesis for Understanding Cell Division,"This project aims to synthesize existing molecular and cellular data to understand the process of cell division. By integrating diverse datasets, we will address the novel question of how cells divide and what factors influence this process. This project will require collaboration between cell biologists, molecular biologists, and data scientists, and will provide training opportunities for graduate students and postdocs. The findings will be made publicly available in accordance with open science principles.",,"Background And Significance

Cell division is a fundamental process in biology, underpinning growth, development, and reproduction in all organisms. Despite extensive research, many aspects of cell division remain poorly understood. This project will synthesize existing molecular and cellular data to address key gaps in our understanding of cell division. We will integrate diverse datasets, including genomic, transcriptomic, proteomic, and metabolomic data, to gain a comprehensive understanding of the molecular and cellular mechanisms underlying cell division. This research is timely and important because it will provide new insights into a fundamental biological process, with potential implications for understanding and treating diseases such as cancer, which are characterized by abnormal cell division.

Research Questions And Hypotheses

Our research will address the following questions: 1) What are the molecular and cellular mechanisms underlying cell division? 2) How do these mechanisms vary across different cell types and organisms? 3) What factors influence the process of cell division? We hypothesize that cell division is regulated by a complex network of molecular and cellular interactions, and that these interactions vary across different cell types and organisms. We predict that by integrating diverse datasets, we will be able to identify key factors and pathways involved in cell division. Our expected outcomes include a comprehensive map of the molecular and cellular mechanisms underlying cell division, and a set of predictive models for how these mechanisms vary across different cell types and organisms.

Methods And Approach

We will use a combination of data integration, network analysis, and machine learning methods to synthesize existing molecular and cellular data on cell division. Our data sources will include publicly available genomic, transcriptomic, proteomic, and metabolomic datasets from a variety of cell types and organisms. We will use network analysis methods to identify key factors and pathways involved in cell division, and machine learning methods to develop predictive models for how these mechanisms vary across different cell types and organisms. Our project will be carried out over a period of three years, with specific milestones and deliverables for each year.

Expected Outcomes And Impact

Our research will make significant contributions to the field of cell biology by providing new insights into the molecular and cellular mechanisms underlying cell division. Our findings will have broad applications, potentially informing the development of new treatments for diseases such as cancer. We anticipate that our research will stimulate further research and collaborations in the field, and we plan to disseminate our findings through peer-reviewed publications and presentations at scientific conferences. Our project will also provide training opportunities for graduate students and postdocs, helping to train the next generation of data-savvy scientists.

Budget And Resources

Our budget includes costs for personnel (including salaries for researchers, graduate students, and postdocs), data acquisition and analysis (including costs for accessing and processing publicly available datasets), and dissemination (including costs for publishing and presenting our findings). We will also require resources for computational infrastructure (including servers and storage for data analysis), and administrative support (including costs for project management and coordination). Our budget has been carefully planned to ensure that we have the necessary resources to carry out our research effectively and efficiently.",,
ai_group_int_gpt_01,ai,group_int,gpt-4,Synthesis of Molecular Data for Cancer Cell Evolution,"This research aims to synthesize publicly available molecular and cellular data to understand the evolution of cancer cells. By integrating diverse datasets, we will address the long-standing puzzle of how cancer cells evolve and adapt to different environments. This project will require collaboration between experts in biology, computer science, and medicine, and will provide training opportunities for the next generation of data-savvy scientists.",,"Background And Significance

Cancer, a leading cause of death worldwide, is a complex disease characterized by uncontrolled cell growth and proliferation. The evolution of cancer cells, their adaptation to different environments, and the mechanisms underlying these processes are still not fully understood. Current research has provided insights into the genetic and epigenetic changes that drive cancer progression, but a comprehensive understanding of cancer cell evolution remains elusive. This is due, in part, to the vast amount of molecular and cellular data that needs to be synthesized and analyzed. This research is timely and significant as it aims to fill this knowledge gap by leveraging publicly available data and multidisciplinary expertise. A detailed understanding of cancer cell evolution could lead to the development of more effective therapeutic strategies and personalized treatments, thereby improving patient outcomes.

Research Questions And Hypotheses

This research will address the following questions: 1) How do cancer cells evolve and adapt to different environments? 2) What are the molecular and cellular mechanisms underlying cancer cell evolution? 3) Can we identify patterns or signatures in the data that can predict cancer progression or response to treatment? We hypothesize that by synthesizing and analyzing diverse datasets, we can gain novel insights into the evolution of cancer cells and identify potential biomarkers for cancer progression and treatment response. These hypotheses will be tested through rigorous data analysis and validation using computational approaches.

Methods And Approach

We will utilize publicly available molecular and cellular data from sources such as The Cancer Genome Atlas (TCGA), the Genomic Data Commons (GDC), and the Cancer Cell Line Encyclopedia (CCLE). These datasets include genomic, transcriptomic, proteomic, and epigenomic data from various cancer types and cell lines. We will integrate these datasets using bioinformatics tools and machine learning algorithms to identify patterns and signatures associated with cancer cell evolution. We will validate our findings using independent datasets and through collaboration with experimental labs. Our timeline includes data collection and preprocessing (months 1-3), data integration and analysis (months 4-9), validation and interpretation of results (months 10-12), and manuscript preparation and submission (months 13-15).

Expected Outcomes And Impact

This research is expected to provide novel insights into the evolution of cancer cells, potentially leading to the identification of new biomarkers for cancer progression and treatment response. These findings could have broad impacts on the field of cancer research and could lead to the development of more effective therapeutic strategies. We plan to disseminate our findings through peer-reviewed publications and presentations at scientific conferences. This project will also provide training opportunities for graduate students and postdocs, thereby contributing to the development of the next generation of data-savvy scientists.

Budget And Resources

The budget for this project includes costs for data storage and computational resources ($20,000), salaries for research personnel including a bioinformatician, a data scientist, and a project manager ($150,000), and indirect costs ($30,000). We will leverage existing resources and infrastructure at our institutions, including high-performance computing clusters and bioinformatics software. We will also seek collaborations with other research labs to validate our findings.",,
ai_group_int_gpt_02,ai,group_int,gpt-4,Cross-Disciplinary Analysis of Neurodegenerative Diseases,"This project will bring together researchers from diverse fields to synthesize existing data on neurodegenerative diseases. By integrating molecular and cellular data, we aim to answer novel questions about the mechanisms underlying these diseases. This project will not only advance our understanding of neurodegenerative diseases but also develop innovative research and analytical strategies.",,"Background And Significance

Neurodegenerative diseases, including Alzheimer's, Parkinson's, and Huntington's, are a growing global health concern, with an increasing prevalence due to an aging population. Despite significant research efforts, the molecular and cellular mechanisms underlying these diseases remain poorly understood. Current research has identified several common features, such as protein aggregation and mitochondrial dysfunction, but the complex interplay of these factors and their contribution to disease progression is not fully elucidated. This project aims to fill this knowledge gap by synthesizing existing molecular and cellular data from diverse sources. The importance and timeliness of this research are underscored by the urgent need for effective therapies for neurodegenerative diseases. By providing a comprehensive understanding of disease mechanisms, this project will pave the way for the development of targeted treatments.

Research Questions And Hypotheses

This project will address the following research questions: 1) What are the common molecular and cellular mechanisms underlying different neurodegenerative diseases? 2) How do these mechanisms interact and contribute to disease progression? We hypothesize that neurodegenerative diseases share common molecular and cellular pathways, and that these pathways interact in a complex network that drives disease progression. We predict that our analysis will identify key nodes in this network that can be targeted for therapeutic intervention. These hypotheses will be tested by integrating and analyzing existing molecular and cellular data using advanced computational methods.

Methods And Approach

We will use publicly available molecular and cellular data from various sources, including gene expression databases, protein-protein interaction networks, and patient-derived cellular models. These data will be integrated using advanced computational methods, including machine learning and network analysis, to identify common molecular and cellular pathways and their interactions. We will validate our findings using existing experimental data and, if necessary, by conducting targeted experiments in collaboration with other research groups. Our project will be conducted over a three-year period, with specific milestones and deliverables at each stage.

Expected Outcomes And Impact

This project is expected to make significant contributions to our understanding of neurodegenerative diseases. By identifying common molecular and cellular pathways and their interactions, we will provide a comprehensive picture of disease mechanisms that can guide the development of targeted therapies. Our findings will be disseminated through peer-reviewed publications and presentations at scientific conferences. In addition, we will make our data and analysis workflows publicly available, in line with open science principles. This project will also provide training opportunities for graduate students and postdocs, thereby contributing to the development of a data-savvy workforce.

Budget And Resources

The budget for this project will cover personnel costs (including salaries for researchers, graduate students, and postdocs), computational resources (including data storage and processing), and indirect costs (including administrative support and overhead). We will leverage existing resources and collaborations to maximize the efficiency and impact of our research. The proposed budget is in line with the scope and complexity of the project, and reflects our commitment to conducting high-quality, impactful research.",,
ai_group_int_gpt_03,ai,group_int,gpt-4,Data-Driven Approach to Understanding Antibiotic Resistance,"This research will synthesize existing data to understand the molecular and cellular mechanisms underlying antibiotic resistance. By integrating diverse datasets, we aim to solve the long-standing puzzle of how bacteria develop resistance to antibiotics. This project will require collaboration between experts in biology, chemistry, and medicine, and will provide training opportunities for the next generation of data-savvy scientists.",,"Background And Significance

Antibiotic resistance is a global health crisis, with bacteria increasingly developing resistance to the drugs designed to kill them. This phenomenon is driven by a variety of molecular and cellular mechanisms, many of which remain poorly understood. Current research in the field has focused on individual mechanisms of resistance, such as efflux pumps or enzymatic degradation of antibiotics. However, these studies often fail to consider the complex interplay between different resistance mechanisms, and the role of the bacterial community in shaping resistance. Furthermore, the vast majority of research has been conducted in laboratory settings, which may not accurately reflect the conditions in which resistance develops in the real world. This research aims to address these gaps by synthesizing existing data from a variety of sources, including genomic, transcriptomic, and proteomic datasets, as well as clinical and epidemiological data. By integrating these diverse datasets, we aim to gain a holistic understanding of the molecular and cellular mechanisms underlying antibiotic resistance. This research is both important and timely, as the rise of antibiotic resistance threatens to undermine many of the advances in healthcare made in the past century.

Research Questions And Hypotheses

Our research will address the following questions: 1) What are the key molecular and cellular mechanisms underlying antibiotic resistance? 2) How do these mechanisms interact to confer resistance? 3) How does the bacterial community influence the development of resistance? We hypothesize that antibiotic resistance is driven by a complex interplay between different resistance mechanisms, and that the bacterial community plays a crucial role in shaping resistance. We predict that by integrating diverse datasets, we will be able to identify novel interactions between resistance mechanisms, and uncover the role of the bacterial community in resistance. Our expected outcomes include a comprehensive map of the molecular and cellular mechanisms of antibiotic resistance, and a better understanding of the role of the bacterial community in resistance. These findings will be validated through rigorous statistical analysis and cross-validation with independent datasets.

Methods And Approach

We will use a variety of data sources, including genomic, transcriptomic, and proteomic datasets, as well as clinical and epidemiological data. These datasets will be integrated using advanced computational approaches, including machine learning and network analysis. Our experimental design will involve the synthesis and integration of these datasets, with rigorous controls to ensure the validity of our findings. We will also include replicates to assess the robustness of our results. Our timeline will involve an initial phase of data collection and preprocessing, followed by data integration and analysis, and finally validation and dissemination of our findings. Our statistical analysis plans involve the use of advanced statistical methods, including multivariate analysis and machine learning, to identify patterns and relationships in the data.

Expected Outcomes And Impact

Our research is expected to make significant contributions to the field of antibiotic resistance, by providing a comprehensive understanding of the molecular and cellular mechanisms underlying resistance. This will have broader impacts in the fields of medicine and public health, by informing the development of new strategies to combat antibiotic resistance. Our research also has the potential to stimulate follow-up research and collaborations, by providing a rich dataset and novel insights into antibiotic resistance. We plan to disseminate our findings through peer-reviewed publications, conference presentations, and public outreach. Our long-term vision is to establish a collaborative, data-driven approach to understanding antibiotic resistance, which can be sustained through ongoing data collection and analysis.

Budget And Resources

Our budget will be allocated across several categories, including data acquisition and preprocessing, computational resources, personnel, and dissemination. Data acquisition and preprocessing will involve the collection and cleaning of existing datasets, which will require significant computational resources. Personnel costs will include salaries for researchers, data analysts, and support staff. Dissemination costs will cover the publication of our findings in peer-reviewed journals, as well as conference presentations and public outreach. We will also allocate funds for training opportunities for graduate students and postdocs, in line with the funding organization's emphasis on training the next generation of data-savvy scientists.",,
ai_group_int_gpt_04,ai,group_int,gpt-4,Synthesis of Data for Understanding Virus Evolution,"This project aims to synthesize existing data to understand the evolution of viruses. By integrating molecular and cellular data, we aim to answer novel questions about how viruses evolve and adapt to different environments. This project will require collaboration between experts in biology, computer science, and medicine, and will provide training opportunities for the next generation of data-savvy scientists.",,"Background And Significance

The study of viral evolution is a rapidly evolving field, with new insights and discoveries being made on a regular basis. Despite this, there are still many unanswered questions and gaps in our understanding. Viruses are known to evolve rapidly, often in response to changes in their environment, such as the immune response of their hosts. However, the mechanisms underlying this rapid evolution are not fully understood. A comprehensive review of the literature reveals a number of key areas where our understanding is limited. For example, while it is known that viruses can acquire new genetic material through recombination and mutation, the factors that influence these processes are not well understood. Similarly, the role of host-virus interactions in driving viral evolution is a topic of ongoing research. This project is timely and important because it will address these gaps in our understanding by synthesizing existing data on viral evolution. By bringing together experts in biology, computer science, and medicine, we will be able to tackle these complex questions from multiple angles, leading to a more comprehensive understanding of viral evolution.

Research Questions And Hypotheses

The main research questions that this project will address are: 1) What are the key factors that drive the evolution of viruses? 2) How do these factors interact to shape the evolutionary trajectory of viruses? 3) How does the evolution of viruses influence their ability to infect and cause disease in their hosts? Based on our current understanding of viral evolution, we hypothesize that both viral and host factors play a crucial role in driving viral evolution. We predict that by integrating molecular and cellular data, we will be able to identify key patterns and trends in viral evolution. These insights will provide a foundation for future research and could potentially lead to the development of new strategies for preventing and treating viral infections.

Methods And Approach

To address our research questions, we will use a combination of computational and experimental approaches. We will first identify and curate publicly available datasets that contain molecular and cellular data on viruses. These datasets will be integrated and analyzed using advanced computational methods, including machine learning and network analysis. We will also develop new analytical tools and algorithms to extract meaningful insights from these complex datasets. Our experimental approach will involve the use of in vitro and in vivo models to validate our computational predictions. We will also use statistical methods to assess the robustness and reliability of our findings. The project will be carried out over a period of three years, with specific milestones and deliverables set for each year.

Expected Outcomes And Impact

The expected outcomes of this project include a comprehensive understanding of the factors that drive viral evolution, as well as new analytical tools and algorithms for analyzing complex molecular and cellular data. These outcomes will have a significant impact on the field of virology, and could potentially lead to the development of new strategies for preventing and treating viral infections. In addition, the project will provide training opportunities for the next generation of data-savvy scientists, thereby contributing to the development of a skilled workforce in the field of computational biology.

Budget And Resources

The budget for this project will be allocated across several categories, including personnel salaries, computational resources, and experimental supplies. The majority of the budget will be allocated to personnel salaries, as this project will require the expertise of several researchers in different fields. Computational resources, including high-performance computing facilities and software licenses, will also constitute a significant portion of the budget. Experimental supplies, including reagents and laboratory equipment, will be another major expense. We will also allocate funds for travel and conference attendance, as this will facilitate collaboration and dissemination of our findings.",,
ai_group_int_gpt_05,ai,group_int,gpt-4,Data Synthesis for Understanding Cellular Aging,"This research will synthesize existing data to understand the molecular and cellular mechanisms underlying cellular aging. By integrating diverse datasets, we aim to solve the long-standing puzzle of how cells age and what factors influence this process. This project will require collaboration between experts in biology, chemistry, and medicine, and will provide training opportunities for the next generation of data-savvy scientists.",,"Background And Significance

Cellular aging is a complex process that involves a multitude of molecular and cellular changes. Despite significant advances in our understanding of the aging process, many questions remain unanswered. The current state of the field has identified several key factors that contribute to cellular aging, including DNA damage, telomere shortening, and oxidative stress. However, the interplay between these factors and how they contribute to the aging phenotype is not fully understood. A comprehensive literature review reveals a wealth of data on these individual factors, but a synthesis of this data to provide a holistic understanding of cellular aging is lacking. This research is important and timely as the global population is aging, and understanding the molecular and cellular mechanisms of aging is crucial for developing interventions to improve health in old age. Furthermore, this research aligns with the funding organization's goal of synthesizing publicly available data to answer novel questions and solve long-standing puzzles in the molecular and cellular sciences.

Research Questions And Hypotheses

This research aims to address the following questions: 1) What are the key molecular and cellular changes that occur during cellular aging? 2) How do these changes interact and contribute to the aging phenotype? 3) Can we identify potential targets for interventions to slow down or reverse cellular aging? Based on the current state of the field, we hypothesize that cellular aging is a multifactorial process involving DNA damage, telomere shortening, and oxidative stress, among other factors. We predict that by synthesizing existing data, we will gain a deeper understanding of the interplay between these factors and identify potential targets for interventions. The expected outcomes of this research include a comprehensive synthesis of existing data on cellular aging, identification of potential targets for interventions, and training of the next generation of data-savvy scientists.

Methods And Approach

We will utilize publicly available datasets from various sources, including the Human Ageing Genomic Resources and the National Institute on Aging. These datasets include genomic, transcriptomic, proteomic, and metabolomic data from various cell types and organisms. We will integrate these datasets using advanced computational approaches, including machine learning and network analysis, to identify key molecular and cellular changes during aging and their interactions. We will validate our findings using existing experimental data from the literature. The project will be carried out over a period of three years, with specific milestones and deliverables set for each year. Statistical analysis will be performed using appropriate methods, including regression analysis and survival analysis.

Expected Outcomes And Impact

This research is expected to make significant contributions to the field of aging research by providing a comprehensive understanding of the molecular and cellular changes that occur during cellular aging. The findings could have broader impacts by identifying potential targets for interventions to slow down or reverse aging, thereby improving health in old age. The research could also stimulate follow-up research and collaborations in the field of aging research. The findings will be disseminated through publications in high-impact journals and presentations at international conferences. The project will also contribute to the training of the next generation of data-savvy scientists.

Budget And Resources

The budget for this project includes costs for data access and storage, computational resources, personnel salaries, and training costs. Data access and storage costs will cover the acquisition and storage of publicly available datasets. Computational resources will include high-performance computing facilities for data integration and analysis. Personnel salaries will cover the salaries of the research team, including biologists, chemists, and computer scientists. Training costs will cover the training of graduate students and postdocs in data synthesis and analysis. The total budget for the project is estimated to be $500,000 over three years.",,
ai_group_int_gpt_06,ai,group_int,gpt-4,Cross-Disciplinary Analysis of Genetic Disorders,"This project will bring together researchers from diverse fields to synthesize existing data on genetic disorders. By integrating molecular and cellular data, we aim to answer novel questions about the mechanisms underlying these disorders. This project will not only advance our understanding of genetic disorders but also develop innovative research and analytical strategies.",,"Background And Significance

Genetic disorders, caused by mutations in one or more genes, are a significant cause of human disease, affecting millions worldwide. Despite advances in genomics, the molecular and cellular mechanisms underlying many of these disorders remain poorly understood. This lack of understanding hinders the development of effective treatments and preventive strategies. Current research in the field is fragmented, with individual labs focusing on specific disorders or molecular pathways. This approach has yielded valuable insights but has also left many questions unanswered. A comprehensive, cross-disciplinary synthesis of existing data could fill these gaps and catalyze new breakthroughs. This project is timely as the volume of publicly available molecular and cellular data is growing exponentially, providing an unprecedented opportunity for data-driven discovery. Furthermore, advances in computational methods and machine learning algorithms have made it possible to analyze and integrate these large and complex datasets in ways that were not previously possible.

Research Questions And Hypotheses

This project will address the following research questions: 1) What are the common molecular and cellular mechanisms underlying different genetic disorders? 2) Can we identify novel genetic markers for these disorders? 3) Can we develop predictive models for disease progression and treatment response? We hypothesize that by integrating and analyzing diverse datasets, we will uncover common patterns and mechanisms that are not apparent when studying individual disorders or datasets in isolation. These insights could lead to the identification of novel genetic markers and the development of predictive models. We will validate our findings using independent datasets and, where possible, experimental studies in model organisms.

Methods And Approach

We will use a variety of publicly available datasets, including genomic, transcriptomic, proteomic, and clinical data from patients with genetic disorders. We will also use data from model organisms and in vitro studies. Our analytical approach will involve data preprocessing, integration, and analysis using advanced computational methods and machine learning algorithms. We will also develop novel analytical strategies as needed. Our project will be divided into three phases: data collection and preprocessing, data integration and analysis, and validation and dissemination. Each phase will have specific milestones and deliverables. Statistical analysis will be performed using appropriate methods, taking into account the nature and complexity of the data.

Expected Outcomes And Impact

This project is expected to make significant contributions to our understanding of genetic disorders. By integrating and analyzing diverse datasets, we aim to uncover common patterns and mechanisms that could lead to the identification of novel genetic markers and the development of predictive models. These findings could have broad applications in diagnostics, therapeutics, and personalized medicine. Furthermore, our project will develop innovative research and analytical strategies that could be applied to other areas of molecular and cellular biology. We plan to disseminate our findings through peer-reviewed publications, conference presentations, and open-access data repositories. We also envision long-term collaborations with other research groups and the training of a new generation of data-savvy scientists.

Budget And Resources

Our budget will cover personnel costs (including salaries for researchers, data analysts, and administrative staff), computational resources (including data storage and processing), and dissemination costs (including publication fees and conference travel). We will also allocate funds for training and professional development activities. We will leverage existing resources and infrastructure at our institutions, including high-performance computing clusters and bioinformatics support services. We will also seek in-kind contributions from our collaborators and partners.",,
ai_group_int_gpt_07,ai,group_int,gpt-4,Data-Driven Approach to Understanding Stem Cell Differentiation,"This research will synthesize existing data to understand the molecular and cellular mechanisms underlying stem cell differentiation. By integrating diverse datasets, we aim to solve the long-standing puzzle of how stem cells differentiate into various cell types. This project will require collaboration between experts in biology, chemistry, and medicine, and will provide training opportunities for the next generation of data-savvy scientists.",,"Background And Significance

Stem cells, with their unique ability to differentiate into various cell types, have been a subject of intense research for decades. Despite significant advancements, the molecular and cellular mechanisms underlying stem cell differentiation remain elusive. Current understanding is based on isolated studies, often focusing on specific differentiation pathways or cell types. However, stem cell differentiation is a complex, multifaceted process that cannot be fully understood in isolation. A comprehensive, integrated approach is needed to unravel the intricate network of molecular interactions and cellular processes involved. This research is timely and important as it addresses a fundamental question in biology and has potential implications for regenerative medicine, disease modeling, and drug discovery. By synthesizing existing data, we aim to provide a holistic understanding of stem cell differentiation, bridging the gap between isolated studies and providing a foundation for future research.

Research Questions And Hypotheses

Our research will address the following questions: 1) What are the key molecular and cellular mechanisms involved in stem cell differentiation? 2) How do these mechanisms interact and coordinate to drive differentiation into specific cell types? We hypothesize that stem cell differentiation is governed by a complex network of molecular interactions and cellular processes, and that understanding this network will provide insights into the differentiation process. We expect to identify key molecular players and cellular processes involved in stem cell differentiation, and to develop a comprehensive model of the differentiation network. These outcomes will be validated through rigorous statistical analysis and cross-validation with independent datasets.

Methods And Approach

We will leverage publicly available datasets from sources such as the Human Cell Atlas and the Gene Expression Omnibus. These datasets include transcriptomic, proteomic, and epigenomic data from various stages of stem cell differentiation. We will integrate these datasets using advanced computational approaches, including machine learning and network analysis, to identify key molecular players and cellular processes involved in differentiation. We will validate our findings through rigorous statistical analysis and cross-validation with independent datasets. Our timeline includes initial data collection and preprocessing (months 1-3), data integration and analysis (months 4-9), validation and refinement of findings (months 10-12), and dissemination of results (months 13-15).

Expected Outcomes And Impact

We expect to provide a comprehensive understanding of the molecular and cellular mechanisms underlying stem cell differentiation. This will contribute to the field by bridging the gap between isolated studies and providing a foundation for future research. Our findings could have broader impacts in regenerative medicine, disease modeling, and drug discovery. We plan to disseminate our findings through peer-reviewed publications and presentations at scientific conferences. We also plan to make our data and analysis workflows publicly available, in line with open science principles. In the long term, we envision our work sparking new collaborations and research directions, and contributing to the training of the next generation of data-savvy scientists.

Budget And Resources

Our budget includes costs for data access and storage ($10,000), computational resources ($20,000), personnel including a data scientist, a bioinformatician, and a project manager ($150,000), and dissemination of results ($5,000). We will leverage existing resources at our institutions, including high-performance computing clusters and data storage facilities. We will also seek in-kind support from our institutions for administrative and logistical support. Our team includes experts in biology, chemistry, and medicine, as well as data science and bioinformatics, ensuring a multidisciplinary approach to the research.",,
ai_group_int_gpt_08,ai,group_int,gpt-4,Synthesis of Data for Understanding Protein Folding,"This project aims to synthesize existing data to understand the process of protein folding. By integrating molecular and cellular data, we aim to answer novel questions about how proteins fold and what factors influence this process. This project will require collaboration between experts in biology, computer science, and medicine, and will provide training opportunities for the next generation of data-savvy scientists.",,"Background And Significance

Protein folding is a fundamental process in biology, with implications for understanding disease mechanisms, drug design, and synthetic biology. Despite decades of research, the process remains poorly understood due to its complexity and the limitations of experimental techniques. Current computational models are unable to accurately predict protein folding pathways or the influence of cellular environments on this process. This project aims to address these gaps by synthesizing existing data from diverse sources, including molecular dynamics simulations, experimental data on protein structures, and cellular data on protein-protein interactions and cellular environments. This research is timely and important as it will provide new insights into protein folding, potentially leading to breakthroughs in understanding diseases such as Alzheimer's and Parkinson's, which are associated with protein misfolding, and in designing more effective drugs and synthetic proteins.

Research Questions And Hypotheses

This project will address the following research questions: 1) What are the key factors influencing protein folding pathways? 2) How does the cellular environment influence protein folding? 3) Can we develop a comprehensive model of protein folding that integrates molecular and cellular data? We hypothesize that protein folding is influenced by a combination of intrinsic factors (e.g., amino acid sequence, molecular dynamics) and extrinsic factors (e.g., cellular environment, protein-protein interactions). We predict that integrating these diverse data sources will lead to a more accurate and comprehensive model of protein folding. The expected outcomes include a new computational model of protein folding, a database of integrated protein folding data, and several publications in high-impact journals.

Methods And Approach

We will use a combination of data synthesis, computational modeling, and machine learning to address our research questions. First, we will collect and curate existing data on protein folding from diverse sources, including molecular dynamics simulations, protein structure databases, and cellular data on protein-protein interactions and cellular environments. We will then integrate these data using advanced computational methods to develop a comprehensive model of protein folding. We will validate our model using existing experimental data and by predicting the folding pathways of novel proteins. Our timeline includes data collection and curation (months 1-6), data integration and model development (months 7-18), model validation and refinement (months 19-24), and dissemination of results (months 25-30).

Expected Outcomes And Impact

This project will make significant contributions to the field of protein folding by providing a comprehensive model that integrates molecular and cellular data. This model will provide new insights into the factors influencing protein folding and could lead to breakthroughs in understanding diseases associated with protein misfolding and in designing more effective drugs and synthetic proteins. The project will also provide training opportunities for the next generation of data-savvy scientists and will promote collaboration between experts in biology, computer science, and medicine. We plan to disseminate our results through publications in high-impact journals, presentations at scientific conferences, and open-source software and data repositories.

Budget And Resources

The budget for this project includes salaries for the project team (including a project manager, data scientists, computational biologists, and a postdoctoral researcher), computing resources (including cloud storage and computing time), travel expenses for team meetings and conferences, and publication costs. We estimate the total cost to be $500,000 over 30 months. This project will require significant computational resources, including high-performance computing clusters and cloud storage for large datasets. We will also need access to existing databases of protein structures, molecular dynamics simulations, and cellular data.",,
ai_group_int_gpt_09,ai,group_int,gpt-4,Data Synthesis for Understanding Cellular Metabolism,"This research will synthesize existing data to understand the molecular and cellular mechanisms underlying cellular metabolism. By integrating diverse datasets, we aim to solve the long-standing puzzle of how cells metabolize nutrients and what factors influence this process. This project will require collaboration between experts in biology, chemistry, and medicine, and will provide training opportunities for the next generation of data-savvy scientists.",,"Background And Significance

Cellular metabolism is a complex process that involves the transformation of nutrients into energy and building blocks necessary for cell growth and maintenance. Despite significant advances in our understanding of cellular metabolism, many aspects remain poorly understood. This is due, in part, to the complexity of metabolic pathways and the multitude of factors that influence them. Current research has focused on individual metabolic pathways, but a comprehensive understanding of cellular metabolism requires a holistic approach that integrates data from multiple sources. This research is timely and important as it will provide a comprehensive understanding of cellular metabolism, which is fundamental to many biological processes and diseases. It will also contribute to the development of new therapeutic strategies for metabolic diseases.

Research Questions And Hypotheses

Our research will address the following questions: 1) How do cells metabolize nutrients? 2) What factors influence cellular metabolism? We hypothesize that cellular metabolism is influenced by a variety of factors, including nutrient availability, cellular energy status, and genetic factors. We predict that by integrating diverse datasets, we will be able to identify key factors that influence cellular metabolism and develop a comprehensive model of cellular metabolism. Our deliverables will include a comprehensive model of cellular metabolism and a database of factors influencing cellular metabolism.

Methods And Approach

We will use publicly available data from various sources, including genomic, proteomic, and metabolomic datasets. We will integrate these datasets using advanced computational approaches, including machine learning and network analysis. We will validate our model using experimental data from the literature. Our timeline includes data collection and integration in the first year, model development in the second year, and model validation and dissemination in the third year. We will use statistical analysis to validate our model and identify key factors influencing cellular metabolism.

Expected Outcomes And Impact

Our research will contribute to a comprehensive understanding of cellular metabolism, which is fundamental to many biological processes and diseases. It will also contribute to the development of new therapeutic strategies for metabolic diseases. Our research will also stimulate cross-disciplinary collaboration and provide training opportunities for the next generation of data-savvy scientists. We plan to disseminate our findings through peer-reviewed publications and presentations at scientific conferences. Our long-term vision is to establish a research consortium for the study of cellular metabolism.

Budget And Resources

Our budget includes costs for data acquisition, computational resources, personnel, and dissemination. We estimate a total budget of $500,000, which includes $200,000 for data acquisition, $100,000 for computational resources, $150,000 for personnel, and $50,000 for dissemination. We will leverage existing resources at our institutions, including high-performance computing clusters and data storage facilities. We will also seek additional funding from other sources to support our research.",,
ai_group_int_gpt_10,ai,group_int,gpt-4,Cross-Disciplinary Analysis of Autoimmune Diseases,"This project will bring together researchers from diverse fields to synthesize existing data on autoimmune diseases. By integrating molecular and cellular data, we aim to answer novel questions about the mechanisms underlying these diseases. This project will not only advance our understanding of autoimmune diseases but also develop innovative research and analytical strategies.",,"Background And Significance

Autoimmune diseases, characterized by an abnormal immune response against the body's own cells, are a significant global health concern. Despite extensive research, the molecular and cellular mechanisms underlying these diseases remain poorly understood. Current research has identified genetic predispositions, environmental triggers, and immune dysregulation as key factors, but the interplay between these elements is complex and not fully elucidated. This project is timely and significant as it aims to synthesize existing data from diverse fields to gain a deeper understanding of the mechanisms underlying autoimmune diseases. By integrating molecular and cellular data, we can address novel questions and potentially identify new therapeutic targets. This research is also important as it will foster cross-disciplinary collaboration, promoting a more holistic approach to understanding autoimmune diseases.

Research Questions And Hypotheses

This project aims to address several research questions: 1) What are the common molecular and cellular mechanisms underlying different autoimmune diseases? 2) Can we identify novel biomarkers for early detection and prognosis of autoimmune diseases? 3) Can we identify potential therapeutic targets for autoimmune diseases? Our hypotheses are: 1) There are common molecular and cellular mechanisms across different autoimmune diseases, which can be identified through data synthesis. 2) Novel biomarkers for autoimmune diseases can be identified through integrative analysis of molecular and cellular data. 3) Potential therapeutic targets for autoimmune diseases can be identified through data synthesis. These hypotheses will be tested through comprehensive data analysis and validation studies.

Methods And Approach

We will utilize publicly available data from various sources, including genomic, transcriptomic, proteomic, and clinical data from autoimmune disease patients. These datasets will be integrated using advanced computational approaches to identify common molecular and cellular mechanisms across different autoimmune diseases. We will also use machine learning algorithms to identify potential biomarkers and therapeutic targets. The project will be carried out in several phases, with specific milestones and deliverables for each phase. Statistical analysis will be performed to validate the findings.

Expected Outcomes And Impact

This project is expected to significantly advance our understanding of autoimmune diseases. By identifying common molecular and cellular mechanisms, we can gain a deeper understanding of the pathogenesis of these diseases. The identification of novel biomarkers could improve early detection and prognosis, while the identification of potential therapeutic targets could lead to the development of new treatments. The project will also foster cross-disciplinary collaboration and train the next generation of data-savvy scientists. The findings will be disseminated through scientific publications and presentations at conferences. In the long term, we envision that this project will stimulate further research and collaborations in the field of autoimmune diseases.

Budget And Resources

The budget for this project will cover personnel costs, computational resources, data access fees, and dissemination costs. Personnel costs will include salaries for researchers, data analysts, and administrative support. Computational resources will include high-performance computing facilities and software licenses. Data access fees will cover the costs of accessing publicly available datasets. Dissemination costs will include publication fees and conference registration fees. We will also allocate funds for training and professional development of the team members. The resources required for this project include a multidisciplinary team of researchers, access to publicly available data, and computational resources.",,
ai_group_int_gemini_01,ai,group_int,gemini-2.5-pro,The Grammar of Cellular Architecture: Emergent Organization from Multi-modal Imaging and Proteomics Data Synthesis,"A fundamental question in cell biology is whether universal principles govern the spatial organization of organelles and protein complexes, giving rise to robust cellular function. This project posits that a 'grammar of cellular architecture' exists, where local molecular interactions and physical constraints lead to emergent, predictable global organization. We propose to decipher this grammar by synthesizing vast, publicly available datasets that are currently siloed. This working group will bring together cell biologists, computer scientists specializing in vision and AI, biophysicists, and systems biologists to integrate high-content imaging data from repositories like the Image Data Resource (IDR) and the Human Cell Atlas with quantitative proteomics data from PRIDE and BioPlex. Our primary objective is to develop a novel computational framework, 'CellArchitect', that uses deep learning to segment and map the 3D spatial distribution and co-localization of thousands of proteins and organelles across millions of cells. By correlating these spatial maps with protein-protein interaction networks and functional annotations, we will move beyond simple co-localization to build predictive models of subcellular organization. We will employ methods from statistical physics and network theory to identify conserved spatial motifs, quantify organizational entropy, and derive rules that predict how perturbations to a single component cascade to alter the entire cellular architecture. This project is beyond the scope of any single lab due to the immense scale of the data and the required diversity of expertise. The outcomes will include a foundational, queryable 4D atlas of the cell, novel algorithms for multi-modal data integration, and fundamental insights into how cellular form and function emerge from molecular parts. This initiative will train a new generation of scientists at the interface of cell imaging, proteomics, and AI, with all models, code, and integrated data products made fully open-access to the scientific community.",,"Background And Significance

The eukaryotic cell is a paragon of emergent complexity, a highly structured, dynamic entity where precise spatial organization is inextricably linked to function. From the segregation of biochemical reactions within membrane-bound organelles to the assembly of signaling complexes at specific subcellular locales, cellular architecture underpins all life processes. For decades, cell biologists have meticulously cataloged the cell's components and their individual functions. However, a comprehensive, predictive understanding of how these components assemble into a coherent, functioning whole remains one of the most significant unresolved challenges in modern biology. We lack a unifying framework—a 'grammar'—that explains how local molecular interactions give rise to global, emergent cellular form.

The current state of the field is characterized by a wealth of data generated by two parallel technological revolutions. On one hand, advances in microscopy, including super-resolution, light-sheet, and cryo-electron tomography, have provided breathtaking views of the cell's interior. Large-scale initiatives like the Allen Cell Explorer and the Human Protein Atlas, along with public repositories such as the Image Data Resource (IDR), have made petabytes of high-resolution imaging data publicly available. These resources offer unprecedented spatial information on the localization of thousands of proteins and organelles. On the other hand, the proteomics revolution, driven by high-throughput mass spectrometry, has systematically mapped the cell's protein-protein interaction (PPI) networks. Techniques like affinity purification-mass spectrometry (AP-MS) and proximity-dependent labeling (e.g., BioID, APEX), curated in databases like BioPlex and STRING, have generated comprehensive 'parts lists' and wiring diagrams of molecular machinery.

Despite the power of these individual approaches, a critical gap exists: these vast imaging and proteomics datasets remain largely siloed. We have detailed maps of *where* proteins are located and separate maps of *what* proteins interact with, but we lack a systematic, large-scale integration of these two fundamental aspects of cellular organization. Current studies are often limited to a few proteins at a time, or they analyze organization in a static, averaged manner, failing to capture the cell-to-cell variability and dynamic nature of the system. Consequently, our understanding is often descriptive rather than predictive. We can observe that the endoplasmic reticulum makes contact with mitochondria, but we cannot predict from first principles how the density and structure of these contacts will change in response to metabolic stress. This inability to bridge the gap from molecular interactions to cellular architecture represents a major barrier to progress in understanding cellular function in health and disease.

This research is both important and timely for several reasons. First, the sheer volume and quality of publicly available data have reached a critical mass where a large-scale synthesis is not only possible but essential to extract maximal value. Second, recent breakthroughs in artificial intelligence, particularly in deep learning for image segmentation (e.g., CellPose, U-Net) and graph neural networks for analyzing complex relationships, provide the necessary computational tools to tackle this challenge at an unprecedented scale. Third, addressing this fundamental question has profound implications. A predictive model of cellular organization would transform our ability to understand diseases characterized by architectural defects, such as cancer, neurodegeneration, and ciliopathies. It would also provide a foundational blueprint for synthetic biology, enabling the rational design of cells with novel functions. By synthesizing existing data to uncover the universal principles of cellular self-organization, this project will address a grand challenge in biology, moving the field from a descriptive to a predictive science of the cell.

Research Questions And Hypotheses

This project is founded on the central hypothesis that the spatial organization of the eukaryotic cell is governed by a discoverable set of rules—a 'grammar'—where local protein-protein interactions and biophysical constraints give rise to emergent, predictable, and functionally optimized global architectures. We will test this overarching hypothesis through three specific, interconnected research aims.

**Aim 1: To develop a multi-modal computational framework, 'CellArchitect', for integrating large-scale imaging and proteomics data to create a unified, spatially-resolved map of cellular organization.** This aim addresses the foundational challenge of data integration, creating the resource upon which subsequent discoveries will be built.
*   **Research Question 1.1:** Can a unified deep learning pipeline accurately and automatically segment organelles and determine protein distributions from heterogeneous, multi-source public imaging data, thereby creating a standardized spatial coordinate system for the cell?
*   **Hypothesis 1.1:** We hypothesize that a federated learning model, leveraging a 3D U-Net architecture pre-trained on diverse datasets (e.g., Human Protein Atlas, OpenCell), can be fine-tuned to achieve robust, cross-dataset segmentation of at least ten major organelles and protein clusters with an average Dice coefficient exceeding 0.85. This will enable the mapping of thousands of proteins onto a canonical cellular reference frame, normalizing for variations in cell size, shape, and imaging modality.
*   **Research Question 1.2:** How can we probabilistically fuse non-spatial protein-protein interaction (PPI) networks with spatial co-localization data to generate a high-confidence, spatially-aware interactome?
*   **Hypothesis 1.2:** We hypothesize that a Bayesian integration model, which combines biochemical evidence from proteomics (e.g., AP-MS scores from BioPlex) with spatial co-occurrence statistics (e.g., voxel-level correlation from our imaging pipeline), will significantly outperform either data type alone in predicting functionally related protein modules. This will result in a spatially-resolved interactome where interaction probabilities are conditional on subcellular location.

**Aim 2: To identify and characterize conserved spatial motifs and organizational principles that constitute the 'grammar' of cellular architecture.** This aim seeks to extract fundamental rules from the integrated data map created in Aim 1.
*   **Research Question 2.1:** Do recurrent, statistically significant spatial arrangements of proteins and organelles—'supramolecular motifs'—exist, and are they conserved across different cell types and functional states?
*   **Hypothesis 2.1:** We hypothesize that graph-based mining of the spatially-resolved interactome will reveal a finite set of conserved motifs (e.g., a specific geometric arrangement of metabolic enzymes around a mitochondrial crista, or a defined sequence of signaling proteins at the plasma membrane) that occur far more frequently than predicted by random chance. We predict these motifs will represent fundamental functional units of cellular organization.
*   **Research Question 2.2:** Can we use principles from information theory and statistical physics to quantify the complexity, robustness, and efficiency of cellular organization?
*   **Hypothesis 2.2:** We hypothesize that cellular states can be characterized by a quantitative 'organizational entropy'. We predict that pluripotent stem cells will exhibit higher entropy (more organizational plasticity), while terminally differentiated cells will have lower entropy (a more fixed, optimized architecture). Furthermore, we predict that disease states, such as cancer, will be associated with an increase in organizational entropy, reflecting a breakdown of regulatory control.

**Aim 3: To build and validate a predictive model of cellular architecture that can simulate the systemic effects of local perturbations.** This aim represents the ultimate test of our derived 'grammar'.
*   **Research Question 3.1:** Can the organizational rules derived in Aim 2 be formalized into a computational model that accurately predicts the global reorganization of the cell in response to the removal or alteration of a single component?
*   **Hypothesis 3.1:** We hypothesize that a generative model, such as a graph neural network (GNN) trained on the CellArchitect atlas, can predict the new steady-state spatial distributions of key proteins following the in silico knockout of a network hub protein. We predict the model's output will show a high degree of concordance (e.g., Earth Mover's Distance below a validated threshold) with experimental imaging data from corresponding CRISPR-mediated knockout cell lines available in public repositories like the IDR. This will demonstrate a truly predictive understanding of cellular self-organization.

Methods And Approach

This project will be executed through a phased, multi-year approach, integrating expertise from computer science, cell biology, proteomics, and biophysics. Our methodology is designed to be rigorous, reproducible, and entirely based on the synthesis of publicly available data.

**Phase 1: Data Acquisition, Curation, and Harmonization (Months 1-9)**
Our foundation will be a meticulously curated collection of multi-modal data. 
*   **Imaging Data Sources:** We will systematically aggregate 3D imaging datasets from the Image Data Resource (IDR), the Allen Cell Explorer, the Human Protein Atlas (HPA), and the Chan Zuckerberg Institute's OpenCell project. Our selection criteria will prioritize datasets with high-resolution 3D stacks, multiple fluorescent channels, clear metadata (cell type, genetic background, treatment), and correspondence to cell lines with rich proteomics data (e.g., HEK293, HeLa, U2OS, A549). We will target an initial corpus of over 100 distinct studies, comprising millions of individual cell images.
*   **Proteomics Data Sources:** We will compile a comprehensive human interactome from primary repositories like BioPlex (AP-MS), HuRI (Y2H), and proximity-labeling data curated from the PRIDE archive. We will supplement this with functional annotations from Gene Ontology (GO) and pathway information from KEGG and Reactome. 
*   **Standardization Pipeline:** A critical first step is to develop an automated pipeline to convert all data into a unified format. Imaging data will be converted to the OME-Zarr cloud-optimized format, which supports scalable, parallel access. We will apply standardized intensity normalization and metadata mapping to create an analysis-ready collection. Proteomics data will be integrated into a single graph structure, with edges weighted by a composite score reflecting the strength and type of evidence for each interaction.

**Phase 2: The CellArchitect Framework - Integration and Mapping (Months 6-18)**
This phase focuses on the core technical development outlined in Aim 1.
*   **Module 1: Unified Image Analysis Pipeline:** We will develop a multi-scale, deep learning-based segmentation engine. First, a robust instance segmentation model (e.g., CellPose) will identify individual cells and nuclei. Second, within each cell, a 3D U-Net model, trained on manually annotated data from HPA and Allen Cell, will perform semantic segmentation of major organelles (e.g., nucleus, mitochondria, ER, Golgi, lysosomes). Third, for fluorescently tagged proteins, a dedicated convolutional neural network will classify their localization patterns (e.g., punctate, diffuse, filamentous) and quantify their distribution relative to the segmented organelles. To enable cross-cell comparison, we will implement a canonical coordinate system transformation for each cell, aligning them based on the nuclear centroid and principal axes.
*   **Module 2: Spatially-Resolved Interactome Construction:** We will bridge the imaging and proteomics data using a probabilistic framework. For every protein pair with evidence of a physical interaction from our proteomics graph, we will compute a suite of spatial co-occurrence metrics from the imaging data (e.g., Pearson's correlation, Manders' overlap coefficient, radial distribution functions). These spatial scores will be integrated with the biochemical interaction scores using a Bayesian network to yield a final probability for a 'spatially-active interaction'. The output will be a multi-layered graph representing the cell's spatially-resolved molecular network.

**Phase 3: Discovering the 'Grammar' of Organization (Months 15-30)**
With the integrated atlas, we will address Aim 2.
*   **Motif Discovery:** We will apply subgraph mining algorithms (e.g., gSpan) to the spatially-resolved interactome to identify recurrent patterns or 'motifs'. The statistical significance of these motifs will be rigorously tested against a null model generated by spatial permutation of protein locations, allowing us to distinguish true architectural principles from random co-occurrence.
*   **Quantification of Organizational Principles:** We will implement algorithms to compute the 'organizational entropy' for each cell based on the predictability of protein and organelle locations within the voxelized cell volume. We will use statistical methods (e.g., ANOVA, t-tests) to compare entropy across different cell types, cell cycle stages, and perturbation conditions, testing our hypotheses about organizational complexity.
*   **Rule Derivation:** Using machine learning models (e.g., Gradient Boosted Trees), we will build a classifier that learns the relationship between a protein's intrinsic properties (e.g., domain structure, network degree) and its emergent spatial properties. This will allow us to extract human-interpretable rules, such as 'Proteins containing a PX domain that interact with PI3P-binding proteins are localized to endosomes with 95% probability'.

**Phase 4: Predictive Modeling and Validation (Months 24-36)**
This final phase will test the predictive power of our derived grammar (Aim 3).
*   **Generative Model Development:** We will construct a predictive model using a Graph Neural Network (GNN) architecture. The model will take the PPI network and the state of a subset of proteins as input and learn to generate the 3D spatial coordinates for all other proteins in the cell. 
*   **In Silico Perturbations and Validation:** The model will be trained on the thousands of wild-type cells in our atlas. We will then validate its predictive power by performing in silico 'knockouts' (removing a node from the input graph) and comparing the model's predicted cellular reorganization to actual experimental data from public datasets featuring the corresponding gene knockout (e.g., from IDR). Quantitative comparison will be performed using metrics like the Wasserstein distance between predicted and observed protein distributions.

**Timeline and Milestones:**
*   **Year 1:** Complete data curation (M9). Release of v1.0 of the segmentation and feature extraction pipeline (M12).
*   **Year 2:** Release of the first integrated spatially-resolved interactome for three cell lines (M18). Publication of the CellArchitect framework and initial findings on organizational entropy (M24).
*   **Year 3:** Complete motif discovery and rule derivation (M30). Validate the predictive GNN model against at least five different knockout datasets (M33). Public release of the full, queryable CellArchitect Atlas and all associated software tools (M36).

Expected Outcomes And Impact

The successful completion of this project will yield transformative outcomes, providing foundational resources, novel methodologies, and fundamental biological insights that will have a broad and lasting impact on the molecular and cellular biosciences.

**Intellectual Merit and Contributions to the Field:**
1.  **A Foundational, Queryable 4D Atlas of the Cell:** The primary deliverable will be the 'CellArchitect Atlas,' a public, web-accessible resource that integrates spatial and molecular interaction data for thousands of proteins across millions of cells. This will be a paradigm-shifting resource, analogous to the reference human genome. It will empower researchers to ask complex questions that are currently intractable, such as, 'What is the consensus spatial arrangement of all components of the mTOR signaling pathway in response to nutrient starvation?' or 'Which protein-protein interactions are most significantly altered in their spatial context between a normal and a cancerous cell line?'. This atlas will serve as a central hub for hypothesis generation and data exploration for the entire cell biology community.
2.  **Novel, Open-Source Computational Tools:** We will develop and disseminate a suite of powerful, open-source software tools for the large-scale, integrative analysis of multi-modal biological data. The CellArchitect pipeline for automated image segmentation, feature extraction, and spatial-proteomic integration will be a significant methodological contribution, applicable to a wide range of biological imaging and systems biology problems. These tools will lower the barrier to entry for complex data synthesis, democratizing this research area.
3.  **Discovery of Fundamental Principles of Cellular Organization:** This project will move cell biology from a largely descriptive to a predictive science. By identifying conserved spatial motifs and deriving the 'grammatical rules' of cellular architecture, we will provide a new conceptual framework for understanding how robust cellular form and function emerge from molecular parts. This will represent a fundamental advance in our understanding of self-organization in living systems, a central theme in biology.

**Broader Impacts and Applications:**
1.  **Advancing Human Health:** A predictive understanding of cellular architecture has profound implications for medicine. The organizational entropy metric we propose could serve as a novel, quantitative biomarker for disease states like cancer, where cellular disorganization is a hallmark. Our predictive models could be used to screen for therapeutic compounds that restore normal cellular architecture or to understand the mechanisms of drug action at a systems level. This work will provide a new lens through which to view diseases of cellular organization, including neurodegeneration, metabolic disorders, and viral infections.
2.  **Enabling Synthetic Biology and Bioengineering:** The rational design of synthetic cells and biological circuits is currently limited by our incomplete understanding of spatial organization. The rules and models generated by this project will provide a 'design blueprint' for synthetic biologists, enabling them to engineer cells with novel functions by precisely controlling the spatial arrangement of components.
3.  **Training the Next Generation of Data-Savvy Scientists:** As mandated by the research call, this project is intrinsically designed for training. Graduate students and postdoctoral fellows will work at the intersection of cell biology, AI, and biophysics, acquiring a unique and highly sought-after skillset. Through collaborative workshops and open-source software development, they will become leaders in the emerging field of quantitative, data-driven cell biology, directly contributing to the development of a future-ready scientific workforce.

**Dissemination and Open Science:**
Our commitment to open science is absolute. All curated data, source code for all models and pipelines, and the final CellArchitect Atlas will be made publicly available under permissive licenses (e.g., MIT for code, CC-BY for data). We will publish our findings in high-impact, open-access journals. We will actively disseminate our work through presentations at major international conferences (e.g., ASCB, ISMB, NeurIPS) and will host annual workshops to train the broader scientific community on how to use our data resources and computational tools. The project's long-term vision is to establish the CellArchitect Atlas as a living, community-driven resource that will be continuously updated and expanded, ensuring its sustained impact on the biosciences.

Budget And Resources

The proposed research represents a community-scale synthesis project that is beyond the scope of any single research laboratory or standard funding mechanism. The immense scale of the data, the requirement for diverse and deep expertise, and the need for dedicated computational infrastructure necessitate the support and resources provided by the NCEMS. This project requires a collaborative working group that brings together world leaders in cell imaging, computational proteomics, machine learning, and biophysical modeling. The budget reflects the personnel and resources required to coordinate this transdisciplinary team and achieve our ambitious goals over a three-year period.

**Personnel: $985,000**
*   **Senior Personnel (4 Co-PIs):** Requesting 1.0 month of summer salary per year for each of the four Co-PIs. This will provide protected time for project leadership, scientific oversight, coordination of the working group, and mentorship of trainees. ($180,000)
*   **Postdoctoral Fellows (3):** Requesting three full-time postdoctoral fellows for the 3-year project duration. Each will have a distinct specialization: (1) Computer Vision & AI for the image analysis pipeline; (2) Bioinformatics & Systems Biology for proteomics integration and network analysis; (3) Computational Biophysics for modeling and simulation. They will form the core research team driving the project's day-to-day progress. ($585,000 including fringe benefits)
*   **Data Scientist/Software Engineer (1):** Requesting support for one full-time professional staff member. This individual is critical for building and maintaining the robust, scalable data processing pipeline, developing the public-facing web portal for the CellArchitect Atlas, and ensuring all software is well-documented and user-friendly. This role ensures the project's outputs are sustainable and accessible. ($220,000 including fringe benefits)

**Computational Resources: $150,000**
*   **Cloud Computing:** A significant budget is required for cloud computing resources (e.g., AWS, Google Cloud). This will cover the substantial costs of GPU time for training and refining deep learning models on millions of images, as well as CPU clusters for large-scale data processing and statistical analysis. ($120,000)
*   **Data Storage:** Funds are allocated for robust, long-term cloud storage of petabytes of raw and processed data, ensuring data integrity and accessibility for the duration of the project and beyond. ($30,000)

**Travel: $60,000**
*   **Working Group Meetings:** To foster deep collaboration, we will hold bi-annual, in-person workshops for the entire team (PIs, postdocs, and affiliated students). These intensive, multi-day meetings are essential for brainstorming, problem-solving, and cross-training. ($36,000)
*   **Conference Travel:** To ensure broad dissemination of our work and to keep the team at the forefront of the field, funds are allocated for trainees and PIs to present at key national and international conferences. ($24,000)

**Publication and Dissemination: $15,000**
*   Funds are requested to cover open-access publication fees, ensuring that all research articles resulting from this project are freely available to the global community, in line with our commitment to open science.

**Total Direct Costs: $1,210,000**
**Indirect Costs (F&A at 55%): $665,500**
**Total Requested Budget: $1,875,500**",,
ai_group_int_gemini_02,ai,group_int,gemini-2.5-pro,Viral Hijacking and Host Response: A Pan-viral Synthesis of Host-Pathogen Interactomes,"Viruses are master manipulators of cellular machinery, but the common principles and evolutionary trade-offs governing their strategies remain poorly understood. This project will address a long-standing puzzle: what are the conserved molecular 'choke points' in the host cell that are targeted by diverse viruses, and how do host defense networks evolve to counter these attacks? We will assemble a multidisciplinary team of virologists, immunologists, bioinformaticians, and evolutionary biologists to conduct a community-scale synthesis of all publicly available host-pathogen interaction data. Our working group will integrate disparate datasets including transcriptomics (GEO, SRA), proteomics (PRIDE), and protein-protein interaction data (BioGRID, IntAct) spanning hundreds of different viral infections in human and other host species. The core of our approach is to build a multi-layered, pan-viral interaction network. We will develop novel graph-based machine learning algorithms to identify 'viral hijacking modules'—sets of host proteins and pathways consistently targeted by unrelated viruses—and corresponding 'emergent host defense modules'—conserved transcriptional and signaling responses that constitute the core of the intrinsic immune system. By comparing the strategies of DNA vs. RNA viruses, or acute vs. persistent viruses, we will uncover the evolutionary logic behind different hijacking strategies. This synthesis is impossible for a single lab, requiring expertise in virology to curate data, computer science to build the network models, and evolutionary biology to interpret the results. The project will produce a comprehensive, open-access 'Viral Interactome Atlas,' providing an invaluable resource for predicting the cellular impact of emerging viruses and identifying novel, broad-spectrum antiviral targets. Trainees will gain unique cross-disciplinary skills in data integration, network biology, and computational virology.",,"Background And Significance

Viruses, as obligate intracellular parasites, engage in a complex and dynamic interplay with their hosts, representing a quintessential example of a co-evolutionary arms race. To replicate, viruses must commandeer the host's molecular machinery for transcription, translation, and energy production while simultaneously evading sophisticated immune surveillance systems. This intimate relationship has been the subject of intense study for decades, yielding deep insights into the infection cycles of individual pathogens. Seminal works have elucidated how influenza virus's NS1 protein antagonizes interferon signaling, how HIV's Vif protein degrades the host antiviral factor APOBEC3G, and how human papillomavirus E6 and E7 proteins subvert cell cycle control by targeting p53 and pRb. These studies, while foundational, have predominantly focused on a single virus or a single family of viruses. Consequently, our understanding of host-pathogen interactions remains highly fragmented, resembling a collection of detailed but disconnected case studies rather than a unified theoretical framework. The central gap in our knowledge is the absence of a systems-level, pan-viral perspective. We lack a comprehensive understanding of the common principles and convergent strategies employed by diverse, unrelated viruses to manipulate the host cell. Are there universal cellular vulnerabilities—molecular 'choke points'—that are repeatedly exploited? Conversely, does the host mount a conserved, core defense response that represents an emergent property of the cellular network, independent of the specific viral trigger? Answering these questions has been historically intractable due to methodological and data limitations. However, the landscape has been transformed by two key developments. First, the explosion of high-throughput 'omics' technologies has led to an unprecedented accumulation of publicly available data. Repositories like the Gene Expression Omnibus (GEO), the Sequence Read Archive (SRA), and the PRIDE Archive now house thousands of datasets detailing the transcriptomic and proteomic consequences of viral infections across a vast array of viruses and host systems. Concurrently, databases such as BioGRID and IntAct have systematically curated tens of thousands of individual virus-host protein-protein interactions (PPIs). This wealth of data represents a massively underutilized resource for discovering higher-order biological principles. Second, recent advances in computer science, particularly in network biology and machine learning, provide the analytical tools necessary to integrate these vast, heterogeneous datasets and extract meaningful patterns. Previous attempts at meta-analysis have been limited in scope, often focusing on a single data type (e.g., PPIs only) or a small subset of viruses, failing to capture the multi-faceted nature of the host response. This project is therefore both important and timely. The recent COVID-19 pandemic served as a stark reminder of the threat posed by emerging viral pathogens and underscored the urgent need for strategies to rapidly understand and combat novel viruses. By synthesizing the entirety of available public data, we can move beyond virus-specific details to uncover the fundamental rules of engagement in the host-virus conflict. This community-scale effort, which requires the combined expertise of virologists, immunologists, computational biologists, and evolutionary theorists, is perfectly aligned with the call to address fundamental questions through data synthesis. It will not only solve a long-standing puzzle in molecular and cellular biology but also provide a powerful new resource for predicting the impact of future pandemics and identifying the next generation of broad-spectrum antiviral therapies.

Research Questions And Hypotheses

The overarching goal of this project is to define the conserved principles of viral manipulation and host defense by synthesizing the global corpus of host-pathogen interaction data. We will move beyond single-pathogen studies to address fundamental questions about the emergent properties of these complex biological systems. Our research is structured around four specific, interconnected questions, each with testable hypotheses.

**Research Question 1: What are the conserved molecular modules within the host cell that are convergently targeted by phylogenetically diverse viruses?**
We posit that the intricate and interconnected nature of the host cellular network creates inherent vulnerabilities, or 'choke points', that diverse viruses have independently evolved to exploit. 
*   **Hypothesis 1 (H1): Viral Convergence on Cellular Hubs.** Viruses from distinct families will convergently target a limited set of host proteins and pathways that are topologically central or functionally critical within the host interactome. These 'viral hijacking modules' are not random but are enriched for specific cellular functions essential for viral replication and immune evasion.
*   **Prediction & Validation:** We predict that our pan-viral network analysis will identify a statistically significant over-representation of viral interactions with host modules involved in core processes like mRNA translation (e.g., ribosomal subunits, eIF4F complex), nucleocytoplasmic transport (e.g., importins, nuclear pore components), protein degradation (e.g., ubiquitin ligases, proteasome subunits), and central metabolism (e.g., glycolysis, pentose phosphate pathway). We will validate these computationally identified modules by assessing their enrichment for known broad-spectrum antiviral drug targets and host dependency factors identified in genome-wide CRISPR screens.

**Research Question 2: Can we define a core, 'pan-viral host defense program' that represents an emergent, conserved response to diverse viral infections?**
While each virus elicits unique cellular responses, we hypothesize that an underlying, conserved defense network is activated as a general anti-pathogen state.
*   **Hypothesis 2 (H2): Emergence of a Core Defense Network.** Integration of multi-omic data will reveal a core set of host genes, proteins, and signaling pathways whose activity is consistently perturbed across a majority of viral infections, forming a robust, emergent host defense module. 
*   **Prediction & Validation:** We predict this module will include, but extend beyond, the canonical interferon-stimulated genes (ISGs). It will encompass specific stress response pathways (e.g., integrated stress response, UPR), metabolic reprogramming signatures (e.g., shifts away from anabolic pathways), and post-translational modifications (e.g., phosphorylation cascades) that are consistently observed regardless of the infecting virus's family. We will test this by comparing the transcriptional and proteomic signatures across hundreds of infection datasets and using permutation testing to define a statistically robust core response.

**Research Question 3: How do viral hijacking strategies correlate with fundamental viral characteristics, such as genome type, replication site, and chronicity?**
The evolutionary trade-offs faced by a virus are shaped by its basic biology. We hypothesize that these constraints dictate the specific sets of host modules it targets.
*   **Hypothesis 3 (H3): Biological Constraints Shape Hijacking Strategy.** A virus's hijacking strategy is non-random and predictable based on its biological properties. For example, RNA viruses will preferentially target host RNA-binding proteins and splicing machinery, while persistent viruses (e.g., herpesviruses) will target apoptotic and cell cycle checkpoints more subtly than acute, lytic viruses (e.g., influenza).
*   **Prediction & Validation:** We will classify all viruses in our dataset and perform comparative analyses. We predict that statistical tests (e.g., enrichment analysis, MANOVA) will reveal significant differences in the host modules targeted by DNA vs. RNA viruses, nuclear vs. cytoplasmic replicating viruses, and acute vs. persistent viruses. For instance, we expect to find that persistent viruses are significantly enriched for interactions with host anti-apoptotic proteins (e.g., Bcl-2 family) and immune modulators (e.g., MHC class I pathway components).

**Research Question 4: Can the identified modules be leveraged to predict the cellular impact of emerging viruses and identify novel, broad-spectrum antiviral targets?**
A key outcome of this synthesis is the creation of a predictive framework.
*   **Hypothesis 4 (H4): Network Modules Have Predictive Power.** The identified hijacking and defense modules constitute a functional map of the host-virus interface that can be used to predict which host factors a novel virus is likely to target and to prioritize host proteins as high-confidence, broad-spectrum antiviral drug targets.
*   **Prediction & Validation:** We will build a machine learning model trained on our network to predict host targets based on viral protein sequence features. We will test its performance on a hold-out set of viruses not used in training. We will predict that proteins central to multiple hijacking modules, but with low connectivity in the uninfected host network, represent ideal broad-spectrum targets, as their inhibition would be highly disruptive to many viruses but potentially less toxic to the host. These predictions will generate a prioritized list of targets for future experimental validation.

Methods And Approach

This project is a community-scale computational synthesis effort that will proceed in three integrated phases. Our multidisciplinary team has the requisite expertise in virology, bioinformatics, machine learning, and network biology to execute this ambitious plan.

**Phase 1: Data Aggregation, Curation, and Harmonization (Months 1-12)**
This foundational phase focuses on building the comprehensive data resource that will underpin all subsequent analyses. This task is beyond the scope of a single lab and requires a coordinated working group.
*   **Data Sources:** We will systematically mine all relevant publicly available data. 
    *   **Protein-Protein Interactions (PPIs):** We will aggregate data from major databases including BioGRID, IntAct, MINT, and the virus-specific VirHostNet. We will capture both virus-host and host-host interactions, along with associated experimental evidence codes.
    *   **Transcriptomics:** Using the NCBI API and custom scripts, we will query GEO, SRA, and ArrayExpress for all datasets related to viral infection in human and key model organisms (e.g., mouse, macaque). We will develop a rigorous set of inclusion criteria, requiring, for example, the presence of mock-infected controls, sufficient biological replicates (n≥3), and detailed experimental metadata.
    *   **Proteomics and Post-Translational Modifications (PTMs):** We will retrieve datasets from the PRIDE archive and PhosphoSitePlus, focusing on studies that quantify changes in protein abundance or phosphorylation status upon infection.
*   **Standardized Processing Pipeline:** To ensure comparability and minimize batch effects, all raw transcriptomic data (RNA-seq) will be reprocessed through a single, containerized pipeline (e.g., using Nextflow). This pipeline will include quality control (FastQC), adapter trimming (Trimmomatic), alignment to a reference genome (STAR), and quantification (RSEM). Differential expression analysis will be consistently performed using DESeq2. All genes, proteins, and viruses will be mapped to stable, standardized identifiers (Ensembl, UniProt, NCBI Taxonomy).

**Phase 2: Multi-Layer Network Construction and Module Discovery (Months 10-24)**
This phase involves the core intellectual and methodological innovation of the project.
*   **Network Construction:** We will construct a heterogeneous, multi-layered network. Nodes in the network will represent host proteins/genes and viral proteins. Edges will represent different types of biological relationships, each forming a distinct layer: 1) physical PPIs, 2) regulatory interactions (inferred from consistent differential gene expression), and 3) post-translational modifications (e.g., kinase-substrate relationships inferred from phosphoproteomics). Edges will be weighted based on the strength and consistency of evidence across multiple datasets.
*   **Novel Module Discovery Algorithm:** Standard community detection algorithms are ill-suited for identifying 'targeted' modules in a multi-layered, bipartite network. We will develop a novel graph-based machine learning approach, likely a Graph Attention Network (GAT) or a similar Graph Neural Network (GNN) architecture. This algorithm will be trained to learn node embeddings that capture both the topological properties within the host network and the interaction patterns with diverse viral proteins. 'Viral hijacking modules' will be identified as communities of host nodes that are recurrently and significantly targeted by proteins from phylogenetically diverse viruses. 'Emergent host defense modules' will be identified as densely connected host nodes that exhibit consistent activation signatures (e.g., upregulation, phosphorylation) across a wide range of viral infections.

**Phase 3: Comparative Analysis, Hypothesis Testing, and Resource Development (Months 18-36)**
In this final phase, we will use the constructed network and identified modules to test our hypotheses and create a lasting public resource.
*   **Hypothesis Testing:** We will employ a rigorous statistical framework. For H1 (Convergence), we will use permutation testing to assess whether the observed targeting of specific modules by diverse viruses is greater than expected by chance. For H2 (Core Defense), we will define a core response set and analyze its functional and topological properties. For H3 (Strategy Correlation), we will use multivariate statistical methods to correlate viral traits (e.g., genome type) with the specific sets of host modules they target. For H4 (Prediction), we will use the network to train a predictive model and validate it on held-out data.
*   **The Viral Interactome Atlas:** A key deliverable will be a public, web-based portal. This resource, built using modern web frameworks (e.g., React, D3.js), will allow users to visualize the pan-viral network, query specific viruses or host proteins, explore the identified modules, and download all underlying data and analysis results. 
*   **Timeline and Milestones:**
    *   **Year 1:** Complete data curation pipeline; release harmonized dataset v1.0. Develop and benchmark module discovery algorithm on a pilot dataset. 
    *   **Year 2:** Construct the full pan-viral multi-layer network. Identify and functionally annotate the first draft of hijacking and defense modules. Launch beta version of the Viral Interactome Atlas for community feedback. Submit methods paper.
    *   **Year 3:** Complete all comparative analyses and hypothesis testing. Finalize and publicly launch the Viral Interactome Atlas v1.0. Submit primary research articles. Host a community-wide dissemination workshop.

Expected Outcomes And Impact

This project is poised to make transformative contributions to molecular and cellular biology, with far-reaching impacts on biomedical research and public health. Our expected outcomes are organized around advancing fundamental knowledge, developing novel resources and methods, training the next generation of scientists, and providing a direct pathway to new therapeutic strategies.

**Advancement of Scientific Knowledge:**
The primary outcome will be a paradigm shift from a virus-centric to a systems-level, pan-viral understanding of host-pathogen interactions. By identifying conserved 'viral hijacking modules,' we will uncover the fundamental cellular vulnerabilities that have been repeatedly exploited throughout evolutionary history. This will resolve the long-standing puzzle of whether common principles govern viral infection. Similarly, the characterization of an 'emergent host defense module' will provide a definitive, data-driven definition of the core intrinsic immune response, revealing the fundamental logic of how cells sense and react to a generic viral threat. These findings will provide a new conceptual framework for virology, cell biology, and immunology, with direct implications for understanding the emergent properties of complex biological networks.

**Development of a Lasting Community Resource:**
A major tangible outcome is the 'Viral Interactome Atlas.' This will not be a static data release but a dynamic, open-access web portal and knowledge base. It will serve as a central resource for the global research community, enabling any researcher to query their virus or host protein of interest and place it within the context of the entire known virus-host interactome. This will democratize access to large-scale systems analysis and will catalyze countless new hypotheses and research directions. For example, a researcher studying a newly discovered virus could use the Atlas to generate immediate, data-driven hypotheses about its mechanism of action, dramatically accelerating the research cycle. The underlying code, data, and network models will be openly shared, fostering reproducibility and further methodological innovation.

**Broader Impacts on Human Health and Pandemic Preparedness:**
The impact of this work extends directly to public health. The identified hijacking modules are, by definition, critical for the replication of many different viruses. The host proteins within these modules therefore represent a rich source of high-confidence targets for the development of broad-spectrum antiviral drugs. Such host-targeted therapies are less prone to the development of viral resistance and could be deployed against newly emerging pathogens for which specific drugs do not yet exist. The Atlas will become a critical tool for pandemic preparedness, allowing for rapid in silico analysis of novel pathogens to predict their cellular targets and suggest potential therapeutic interventions.

**Training and Workforce Development:**
This project is an ideal training vehicle for the next generation of data-savvy scientists. Trainees (graduate students and postdocs) will be co-mentored by experts in disparate fields, gaining unique cross-disciplinary skills in computational biology, data integration, machine learning, network science, and virology. They will learn to manage large-scale collaborative projects, adhere to open science principles, and communicate effectively across disciplines. Through hands-on participation in working group meetings and workshops, they will build a professional network that will serve them throughout their careers. This training directly addresses the critical need for a workforce that can leverage the growing deluge of biological data to solve complex problems.

**Dissemination and Long-Term Sustainability:**
We will disseminate our findings through high-impact publications, presentations at major international conferences, and the public launch of the Atlas. We will host a final workshop to share our results with the broader community and foster new collaborations. The project is designed for long-term sustainability; the Atlas will be built with a framework for community-driven updates. We will seek follow-on funding to maintain and expand the resource, ensuring it remains a valuable and up-to-date hub for the systems virology community long after the initial funding period concludes.

Budget And Resources

The proposed research represents a large-scale, community-level synthesis effort that requires significant and coordinated resources beyond the capacity of any single lab or existing collaboration. The budget is designed to support a distributed team of ten PIs and their trainees over a three-year period, with a focus on personnel, collaboration, and computational infrastructure.

**1. Personnel (Approximately 70% of total budget):**
*   **Principal Investigators (10 PIs):** We request 1.0 month of summer salary per year for each PI. This is essential to provide the dedicated time required for project leadership, intensive data analysis and interpretation, trainee mentorship, and coordination of the working group.
*   **Postdoctoral Fellows (4 FTEs):** Four full-time postdocs are the core engine of this project. Two will have strong computational backgrounds, leading the development of the data processing pipelines and novel machine learning algorithms. Two will have deep expertise in virology and molecular biology, leading the critical tasks of data curation, functional annotation of modules, and biological interpretation of results. 
*   **Graduate Students (4 FTEs):** Four graduate students will be supported to work on specific sub-projects, such as implementing components of the web portal, performing comparative evolutionary analyses, or applying the network model to specific viral families. This is a cornerstone of our training plan.
*   **Project Manager/Data Scientist (0.5 FTE):** A half-time professional will be hired to manage the complex logistics of a 10-lab collaboration, ensure milestones are met, oversee compliance with open science policies, and manage the data repository.

**2. Travel (Approximately 10%):**
*   **Working Group Meetings:** We request funds for the entire team (PIs, postdocs, students) to meet in person twice annually. These intensive, multi-day workshops are indispensable for fostering genuine collaboration, resolving complex analytical challenges, and providing an immersive training experience for junior researchers. 
*   **Conference Travel:** Funds are allocated for each trainee and one PI per lab to attend one major international conference per year (e.g., ASV, ISMB) to present our findings, disseminate the resource, and receive community feedback.

**3. Computational Resources and Dissemination (Approximately 15%):**
*   **Cloud Computing:** Significant funds are budgeted for cloud computing services (e.g., AWS S3 for storage, EC2 for computation). Reprocessing thousands of public RNA-seq datasets and training complex graph neural network models on a massive network requires computational power that exceeds standard university-provided resources.
*   **Web Server and Data Hosting:** We request funds for a dedicated server to host the Viral Interactome Atlas, ensuring high availability and performance for the global community. This also covers long-term data archiving costs on platforms like Zenodo.
*   **Publication Fees:** A budget is included to cover open-access publication fees for an anticipated 4-5 peer-reviewed articles, ensuring our work is freely accessible in accordance with open science principles.

**4. Indirect Costs (F&A) (Calculated based on lead institution's federally negotiated rate):**
This budget is critically dependent on NCEMS support. The scale of the data integration, the need for novel methodological development, and the requirement for a highly diverse, multi-disciplinary team place this project squarely within the scope of a community-scale synthesis project that cannot be accomplished otherwise.",,
ai_group_int_gemini_03,ai,group_int,gemini-2.5-pro,Metabolic Symbiosis in the Tumor Microenvironment: A Spatially-Resolved Multi-Omics Data Synthesis,"A tumor is not a monolith of cancer cells, but a complex ecosystem where diverse cell types—cancer, immune, and stromal—interact dynamically. A key emergent property of this ecosystem is metabolic symbiosis, where cells exchange metabolites to support collective growth, proliferation, and drug resistance. However, mapping these metabolic exchanges remains a major challenge. This project will tackle this challenge by synthesizing publicly available multi-omics data to build the first comprehensive, spatially-resolved metabolic map of the tumor microenvironment (TME). Our working group, comprising cancer biologists, biochemists, computational modelers, and data scientists, will integrate genomic and transcriptomic data from The Cancer Genome Atlas (TCGA), proteomic data from CPTAC, and a growing number of public single-cell and spatial transcriptomics datasets. The central innovation will be the development of a new analytical pipeline that uses machine learning for cellular deconvolution of bulk data and integrates it with single-cell resolution data to assign metabolic pathways to specific cell types within the TME. We will then use constraint-based modeling (e.g., flux balance analysis) to predict the flow of metabolites between these cell populations, identifying critical symbiotic dependencies. This large-scale integration and modeling effort requires a collaborative team to handle the data heterogeneity and develop the sophisticated computational tools needed. The project will reveal how the collective metabolic network of the TME emerges from individual cell behaviors and how this emergent property contributes to cancer progression and therapy failure. Our findings will be disseminated through an open, interactive web portal, providing a powerful new tool for identifying novel therapeutic targets aimed at disrupting this deadly symbiosis. The project will also serve as a training ground for students and postdocs in the burgeoning field of computational systems oncology.",,"Background And Significance

The tumor microenvironment (TME) is now understood not as a passive scaffold for malignant cells, but as a complex, adaptive ecosystem whose emergent properties dictate cancer progression, metastasis, and therapeutic response. This ecosystem comprises a heterogeneous consortium of cancer cells, cancer-associated fibroblasts (CAFs), endothelial cells, and a diverse array of immune cells. The collective behavior of this system arises from intricate, spatially-defined intercellular communication networks. A critical axis of this communication is metabolism. The century-old observation of the Warburg effect, where cancer cells favor glycolysis even in the presence of oxygen, has evolved into a more nuanced understanding of metabolic plasticity and interdependence. It is now clear that a tumor's metabolic phenotype is a collective property, driven by a phenomenon known as metabolic symbiosis, where different cell populations exchange metabolites to optimize nutrient utilization and support mutual survival and growth. Seminal studies have illuminated pairwise symbiotic relationships. For instance, the 'reverse Warburg effect' describes how glycolytic CAFs secrete lactate, which is then taken up and utilized as a primary fuel source by oxidative cancer cells, thereby sparing glucose for other anabolic processes. This lactate shuttle, mediated by monocarboxylate transporters (MCTs), not only fuels cancer proliferation but also profoundly shapes the TME by inducing angiogenesis and suppressing immune function; high lactate levels are known to inhibit T-cell and natural killer cell activity. Beyond lactate, other metabolites like glutamine, ammonia, and lipids are actively exchanged between cancer cells and various stromal and immune populations, creating a web of metabolic dependencies. However, our current understanding of this metabolic web is fragmented and incomplete. The vast majority of studies have focused on simplified co-culture systems or have examined single metabolic pathways in isolation. This reductionist approach fails to capture the complexity and emergent nature of the TME's metabolic network. A key limitation has been the reliance on bulk-level analyses of tumor tissue. Bulk omics data, while powerful, averages the molecular signals from all constituent cell types, obscuring the cell-type-specific metabolic programs and the crucial intercellular exchanges that define the ecosystem. We lack a systems-level, spatially-resolved map of metabolic flux within the in-vivo TME. This knowledge gap represents a major barrier to developing effective metabolic therapies. Without understanding the full network of dependencies, therapeutic interventions targeting a single pathway may be circumvented by the system's metabolic plasticity. This project is both important and timely due to a confluence of factors. First, the explosion of publicly available multi-omics data, including thousands of tumor profiles from The Cancer Genome Atlas (TCGA) and the Clinical Proteomic Tumor Analysis Consortium (CPTAC), provides an unprecedented substrate for data synthesis. Second, the recent surge in single-cell and spatial transcriptomics datasets offers the potential to deconvolve the cellular heterogeneity of bulk tissues with ever-increasing resolution. Third, advances in computational systems biology, particularly in constraint-based modeling, provide the formalisms necessary to integrate these disparate data types and simulate metabolic function. By synthesizing these vast public data resources, this project will address a fundamental question in cancer biology: how does the collective metabolic network of a tumor emerge from the interactions of its constituent parts? Answering this question is beyond the scope of any single lab, requiring a transdisciplinary working group of cancer biologists, computational scientists, and systems modelers to integrate heterogeneous data and develop novel analytical strategies, perfectly aligning with the mission of this research call.

Research Questions And Hypotheses

The overarching goal of this research is to construct the first comprehensive, spatially-resolved atlas of metabolic symbiosis in the tumor microenvironment across multiple human cancers. By synthesizing a massive corpus of public multi-omics data, we aim to move beyond pairwise interactions and map the emergent, system-level metabolic network that drives tumor progression and therapeutic resistance. This goal is structured around four specific, interconnected research questions, each with testable hypotheses. 

**Research Question 1: How do the metabolic phenotypes of distinct cell populations (cancer subclones, fibroblast subtypes, endothelial cells, immune infiltrates) co-vary within the TME, and how are they shaped by their local cellular neighborhood?**
This question addresses the fundamental issue of metabolic plasticity as a function of cellular context. We hypothesize that a cell's metabolic state is not an intrinsic, static property but is dynamically programmed by signals and metabolites from adjacent cells. 
*   **Hypothesis 1.1:** The metabolic activity of cancer cells is a direct emergent property of their spatial proximity to specific stromal and immune cells. 
    *   **Prediction:** Our spatially-aware models will predict that cancer cells physically adjacent to lactate-secreting CAFs will exhibit significantly higher flux through oxidative phosphorylation pathways compared to cancer cells in fibroblast-poor regions. Conversely, cancer cells in immune-cell-rich 'hot' regions will exhibit distinct metabolic profiles compared to those in 'cold' regions. 
    *   **Validation:** We will validate this by correlating the predicted metabolic fluxes with the expression of metabolic pathway markers in spatially-resolved transcriptomics datasets. We will test for statistically significant associations between cell-type co-localization patterns and metabolic gene expression signatures across thousands of TCGA samples. 

**Research Question 2: What are the principal metabolic exchange networks that define symbiotic and competitive relationships within the TME, and which metabolites serve as the primary currencies of exchange?**
We aim to identify the key metabolic highways that connect different cell populations. 
*   **Hypothesis 2.1:** A conserved set of metabolic exchange networks, centered on a lactate-glutamine-ammonia axis, forms the backbone of TME symbiosis across diverse solid tumor types (e.g., pancreatic, breast, lung). 
    *   **Prediction:** Our multi-cellular flux balance analysis will consistently predict high rates of lactate export from CAFs, lactate import by cancer cells, and a coupled exchange of glutamine and its byproducts between these and other cell types. 
    *   **Validation:** We will seek evidence for these predicted fluxes by examining the coordinated expression of key metabolite transporters (e.g., MCT1, MCT4, ASCT2) and metabolic enzymes in our deconvoluted TCGA data. A strong positive correlation in the expression of a CAF lactate exporter and a cancer cell lactate importer would support our prediction. 

**Research Question 3: How does the spatial architecture of the TME constrain the topology of the metabolic interaction network and create localized metabolic niches?**
This question links physical structure to metabolic function. 
*   **Hypothesis 3.1:** The TME is organized into distinct metabolic niches, such as immunosuppressive niches characterized by high lactate and low glucose, which spatially exclude effector T-cells. 
    *   **Prediction:** Our models will identify spatial 'hotspots' of immunosuppressive metabolite production (e.g., lactate, kynurenine) that spatially anti-correlate with the predicted activity and infiltration of cytotoxic T-lymphocytes. 
    *   **Validation:** We will use publicly available spatial proteomics (e.g., MIBI, CODEX) and transcriptomics data to confirm the predicted spatial segregation of immune cells from these metabolically hostile niches. 

**Research Question 4: Can we identify conserved metabolic dependencies within these symbiotic networks that represent robust, pan-cancer therapeutic vulnerabilities?**
This is the translational thrust of our project. 
*   **Hypothesis 4.1:** The metabolic network of the TME contains critical 'choke points'—enzymes or transporters essential for maintaining the symbiotic state—whose inhibition would cause a systemic collapse of the tumor ecosystem. 
    *   **Prediction:** In-silico gene/reaction knockout simulations in our community metabolic models will identify specific targets (e.g., a transporter on a stromal cell) whose removal leads to a significant reduction in cancer cell biomass production. 
    *   **Validation:** We will prioritize predicted targets by cross-referencing them with cancer dependency maps (e.g., DepMap) and clinical data to assess their potential as viable therapeutic targets. The top-ranked predictions will form the basis for future experimental collaborations.

Methods And Approach

This project will synthesize vast, publicly available datasets through a novel, multi-stage computational pipeline. Our approach is designed to systematically deconstruct the complexity of the TME, reconstruct its metabolic network, and simulate its behavior to uncover emergent properties. The project is organized into three synergistic Aims.

**Aim 1: Comprehensive Curation and Harmonization of Public Multi-Omics Data.**
The foundation of this synthesis project is the rigorous aggregation and processing of diverse data types. This effort requires significant bioinformatic expertise and is a key area where NCEMS support is critical.
*   **Data Sources:** We will leverage several major public data repositories. 
    1.  **Bulk Genomics/Proteomics:** We will download and process RNA-sequencing, copy number variation, mutation, and clinical data for all available solid tumor cohorts from The Cancer Genome Atlas (TCGA) (~33 cancer types, >11,000 patients). This will be complemented by quantitative proteomics data from the Clinical Proteomic Tumor Analysis Consortium (CPTAC) for overlapping cohorts, providing a crucial layer of protein-level validation.
    2.  **Single-Cell Transcriptomics (scRNA-seq):** We will compile a comprehensive database of publicly available TME scRNA-seq studies from repositories like the Gene Expression Omnibus (GEO) and the Human Cell Atlas. We will initially target major solid tumors (e.g., breast, lung, pancreatic, colorectal, melanoma), curating data from over 100 independent studies encompassing millions of cells. 
    3.  **Spatial Transcriptomics:** We will gather all available spatial transcriptomics datasets (e.g., 10x Visium, Slide-seq) for human tumors. This data is critical for providing the spatial context and ground truth for our models.
*   **Data Harmonization:** A major challenge is the heterogeneity of these datasets. We will develop a standardized processing pipeline to harmonize them. This includes: uniform alignment and quantification of sequencing data, batch correction across studies using algorithms like ComBat-seq, standardization of gene and protein nomenclature, and consistent clinical data annotation.

**Aim 2: Development and Application of a Spatially-Informed Cellular Deconvolution Framework.**
This Aim focuses on computationally dissecting bulk tumor data into its constituent cell-type-specific components, a key innovation of our proposal.
*   **Step 1: Building a TME Cell Type Signature Matrix:** Using the harmonized scRNA-seq data from Aim 1, we will perform unsupervised clustering and expert-guided annotation to identify all major cell populations and their subtypes (e.g., cancer epithelial, myofibroblastic vs. inflammatory CAFs, M1 vs. M2 macrophages, T-cell subtypes). We will then use statistical methods to define a robust, context-specific gene expression signature for each cell type.
*   **Step 2: Deconvolution of Bulk TCGA/CPTAC Data:** We will employ and enhance machine learning-based deconvolution algorithms (e.g., CIBERSORTx, BayesPrism) to estimate the relative abundance and infer the cell-type-specific expression profiles for every cell type in each of the >11,000 TCGA tumors. This will transform the bulk data into a pseudo-multi-cellular dataset.
*   **Step 3: Integrating Spatial Constraints:** We will use the spatial transcriptomics data to build a probabilistic model of cell-cell co-occurrence. This model will learn which cell types are likely to be physically adjacent. This spatial prior will then be integrated into our deconvolution framework to refine the cell-type-specific expression profiles, making them spatially aware.

**Aim 3: Spatially-Resolved Community Metabolic Modeling and Simulation.**
This Aim will use the outputs from Aim 2 to construct and analyze predictive models of TME metabolism.
*   **Step 1: Contextualizing Genome-Scale Metabolic Models (GEMs):** We will use the inferred cell-type-specific gene and protein expression profiles to customize a human GEM (e.g., Recon3D). For each cell type in each tumor, we will use algorithms like GIMME or iMAT to generate a specific metabolic model that reflects its likely metabolic activity.
*   **Step 2: Assembling Multi-Cellular Community Models:** We will combine the individual cell-type GEMs into a single community model for each tumor. Critically, we will model the exchange of metabolites through a shared extracellular compartment. The maximum rate of exchange between any two cell types will be constrained by their estimated spatial proximity from Aim 2. This ensures that only adjacent cells can efficiently exchange metabolites, a key feature missing from previous models.
*   **Step 3: Simulation and Analysis:** We will use constraint-based methods, primarily Flux Balance Analysis (FBA), to simulate metabolic activity. We will set the objective function to maximize cancer cell proliferation while other cells perform ATP maintenance. By simulating thousands of these tumor-specific community models, we will identify common metabolic flux patterns, predict key metabolite exchanges, and perform in-silico knockout simulations to identify therapeutic vulnerabilities (choke points).

**Timeline and Milestones:**
*   **Year 1:** Complete Aim 1 (data acquisition/harmonization). Develop and validate the deconvolution pipeline (Aim 2). Publish the pipeline as an open-source tool.
*   **Year 2:** Apply the pipeline to all TCGA cohorts. Construct and simulate community metabolic models for 5 priority cancer types (Aim 3). Develop beta version of the web portal.
*   **Year 3:** Complete simulations for all cohorts. Perform pan-cancer analysis to identify conserved vulnerabilities. Launch and publicize the final interactive web portal. Submit primary manuscripts.

Expected Outcomes And Impact

This project is poised to make transformative contributions to cancer biology, computational systems biology, and translational oncology. The outcomes will extend far beyond the immediate findings, providing the scientific community with new paradigms, tools, and resources that will catalyze future research. The collaborative, data-synthesis nature of this work directly addresses the core tenets of the NCEMS research call.

**Intellectual Merit and Contribution to the Field:**
1.  **A Paradigm Shift in Cancer Metabolism:** The primary outcome will be a fundamental shift from a cell-centric view of tumor metabolism to a systems-level, ecological perspective. We will produce the first comprehensive, spatially-resolved atlas of metabolic symbiosis in the TME. This will reveal how system-level properties like aggressive growth and drug resistance emerge from local, multi-cellular interactions. This atlas will provide a foundational resource for understanding the metabolic principles that govern tumor progression.
2.  **Methodological Innovation in Computational Biology:** We will develop and disseminate a novel, open-source computational framework for the integrative analysis of bulk, single-cell, and spatial omics data. This spatially-aware deconvolution and metabolic modeling pipeline will be a significant methodological advance, applicable not only to cancer but to any complex, heterogeneous tissue (e.g., in neuroscience, immunology, or developmental biology). This addresses the call's goal to develop innovative analytical strategies.
3.  **Generation of Novel, Testable Hypotheses:** Our simulations will generate hundreds of specific, high-priority hypotheses about metabolic dependencies and vulnerabilities in the TME. For example, we might hypothesize that a specific amino acid transporter on endothelial cells is essential for fueling cancer cell growth in glioblastoma. These data-driven hypotheses will provide a rich substrate for experimental validation by the broader research community, accelerating the pace of discovery.

**Broader Impacts and Applications:**
1.  **Identification of Novel Therapeutic Targets:** By identifying conserved metabolic 'choke points' within the symbiotic network, this project will provide a rational basis for a new generation of cancer therapies. Targeting the metabolic support infrastructure of the tumor (e.g., stromal cell metabolism) rather than the genetically unstable cancer cell is a promising strategy to overcome acquired drug resistance. Our ranked list of vulnerabilities will be a valuable starting point for pharmaceutical development.
2.  **Development of a Community-Wide Resource:** A key deliverable is the creation of an open-access, interactive web portal. This portal will allow researchers worldwide, regardless of their computational expertise, to explore our results. A biologist could, for instance, query the predicted metabolic flux of their favorite gene in a specific cell type in breast cancer, or a clinician could explore the metabolic differences between responder and non-responder patient tumors. This resource will democratize access to complex data synthesis and significantly amplify the project's impact.
3.  **Training the Next Generation of Scientists:** As mandated by the research call, this project is an ideal training vehicle. Graduate students and postdoctoral fellows will work at the cutting edge of data science, cancer biology, and systems modeling. They will gain invaluable cross-disciplinary skills in large-scale data management, machine learning, and computational modeling, preparing them to be leaders in the future data-savvy biomedical workforce.

**Dissemination and Open Science:**
Our commitment to open science is unwavering. All software developed will be released on GitHub with permissive open-source licenses. All processed data, models, and results will be deposited in public repositories (e.g., Zenodo, Figshare). We will publish our findings in high-impact, open-access journals and present our work at major international conferences (e.g., AACR, ISMB, RECOMB). This multi-pronged approach ensures that our methods, data, and discoveries are immediately and broadly available, maximizing their utility and impact for the scientific community and the public.

Budget And Resources

The proposed research represents a community-scale synthesis project whose scope, complexity, and transdisciplinary nature far exceed the capabilities of a single research laboratory or existing collaboration. The integration of petabyte-scale heterogeneous datasets, development of novel machine learning algorithms, and large-scale computational modeling require a dedicated, coordinated team with diverse expertise and significant computational resources. Therefore, support from the NCEMS is essential for the success of this ambitious project.

**Justification for NCEMS Support:**
This project is uniquely suited for the NCEMS program. It is not a data generation project but a pure synthesis effort, leveraging vast public data repositories (TCGA, CPTAC, GEO). The core challenge lies in the sophisticated integration and modeling of this data, requiring a team of cancer biologists, bioinformaticians, computer scientists, and systems modelers to work in concert. Funding is required to support the protected time for these experts to collaborate, develop novel software, and perform analyses that are too large and complex for standard institutional resources. Furthermore, the development and long-term maintenance of a high-quality, public-facing web portal is a significant software engineering task that requires dedicated personnel and resources not typically covered by traditional research grants.

**Budget Breakdown (3-Year Total):**

**1. Personnel ($650,000):** This is the largest budget component, reflecting the project's reliance on specialized human expertise.
*   **Postdoctoral Fellows (2.0 FTE x 3 years):** $390,000. One postdoc will specialize in computational systems biology, leading the construction and simulation of metabolic models. The second will have expertise in machine learning and bioinformatics, leading the development of the data harmonization and cellular deconvolution pipeline.
*   **Graduate Students (2.0 FTE x 3 years):** $180,000. Two students will be trained through this project, assisting with data curation, running computational pipelines, and performing validation analyses. This directly supports the NCEMS goal of training a data-savvy workforce.
*   **Data Scientist/Software Engineer (0.5 FTE x 3 years):** $80,000. This part-time position is critical for managing the cloud-based data infrastructure and leading the design, implementation, and maintenance of the public web portal.

**2. Computational Resources ($60,000):**
*   **Cloud Computing Credits (AWS/Google Cloud):** $45,000. For storage of terabytes of processed data and for scalable computation during the machine learning and data deconvolution phases.
*   **HPC Cluster Access Fees:** $15,000. For running thousands of computationally intensive flux balance analysis simulations on institutional or national high-performance computing clusters.

**3. Travel ($30,000):**
*   **Working Group Meetings:** $18,000. To support twice-yearly, in-person meetings for the entire working group. These meetings are vital for fostering deep collaboration, resolving complex technical challenges, and strategic planning.
*   **Conference Travel:** $12,000. To enable trainees and PIs to disseminate findings at key national and international conferences (e.g., AACR, ISMB).

**4. Publication and Dissemination ($15,000):**
*   Funds to cover open-access publication fees in high-impact journals, ensuring our findings are freely accessible to all, in line with our commitment to open science.

**5. Indirect Costs (F&A) ($377,500):**
*   Calculated at a negotiated institutional rate of 50% of modified total direct costs ($755,000).

**Total Requested Budget: $1,132,500**",,
ai_group_int_gemini_04,ai,group_int,gemini-2.5-pro,From Sequence to Interactome: Predicting Emergent Cellular Functions of Intrinsically Disordered Proteins,"Intrinsically disordered proteins (IDPs), which lack a stable three-dimensional structure, represent a major puzzle in molecular biology. They are key players in cellular signaling and regulation, often functioning by forming dynamic, multivalent interactions that lead to the emergence of membraneless organelles through liquid-liquid phase separation. This project aims to create a predictive framework that can decipher the 'interaction grammar' of IDPs from their amino acid sequence alone. A transdisciplinary team of structural biologists, polymer physicists, bioinformaticians, and machine learning experts will integrate data from diverse public sources. We will leverage sequence and annotation data from DisProt, structural context from the PDB and AlphaFold DB, known interactions from BioGRID, and post-translational modification (PTM) data from proteomics repositories. The core of our project is to develop a novel deep learning model, inspired by large language models like BERT, that learns the sequence features, motifs, and PTM patterns that determine IDP binding specificity and phase separation propensity. This model will be trained on the integrated dataset to predict IDP interaction partners and the conditions under which they form condensates. The predictions will be used to parameterize polymer physics simulations to explore the emergent material properties of these condensates. This effort is beyond a single lab, requiring the fusion of AI development with deep biophysical and biological domain knowledge. The outcome will be a powerful, open-source tool to predict the functional consequences of mutations in disordered regions, which are frequently implicated in diseases like cancer and neurodegeneration. This will transform our ability to understand how the dynamic, fuzzy interactions of IDPs give rise to the highly organized, emergent behavior of the cell.",,"Background And Significance

The central dogma of molecular biology, for decades, was dominated by the sequence-structure-function paradigm, where a protein's function was inextricably linked to a unique, stable three-dimensional structure. However, the sequencing of eukaryotic genomes revealed a startling reality: a significant fraction of proteomes, over 30% in humans, consists of proteins or regions that lack a fixed structure under physiological conditions. These intrinsically disordered proteins (IDPs) and regions (IDRs) challenged classical structural biology and were initially dismissed as biological noise. We now understand that this conformational heterogeneity is not a bug but a feature, enabling a vast range of functions that are inaccessible to structured proteins. IDPs are central hubs in cellular interaction networks, mediating signal transduction, transcriptional regulation, and chromatin remodeling. Their functional advantage lies in their ability to form dynamic, multivalent, and often transient interactions, creating what has been termed 'fuzzy' complexes. This dynamic binding allows them to act as scaffolds, sensors, and regulators, integrating multiple cellular signals. A key emergent phenomenon driven by IDPs is liquid-liquid phase separation (LLPS), a thermodynamic process where multivalent interactions among IDPs and RNA drive their demixing from the cytoplasm to form membraneless organelles (MLOs). These biomolecular condensates, such as nucleoli, stress granules, and P-bodies, are dynamic compartments that concentrate specific molecules to enhance reaction rates, sequester components, and organize the cellular landscape. The physical principles governing LLPS are rooted in polymer physics, where IDPs are treated as associative polymers. The 'stickers-and-spacers' model provides a conceptual framework, where 'stickers' (e.g., aromatic or charged residues) mediate specific interactions, and flexible 'spacers' determine the polymer's conformational properties and the phase behavior of the system. Post-translational modifications (PTMs) like phosphorylation act as a crucial regulatory layer, altering the 'stickiness' of these motifs and dynamically tuning the formation and dissolution of condensates in response to cellular cues. The importance of IDPs is underscored by their profound link to human disease. The misregulation of IDP interactions or their aberrant phase transitions into irreversible, solid-like aggregates are hallmarks of numerous pathologies. For instance, the hyperphosphorylation of the IDP Tau is linked to the formation of neurofibrillary tangles in Alzheimer's disease, while mutations in the disordered regions of FUS and TDP-43 promote their aggregation in amyotrophic lateral sclerosis (ALS). In oncology, oncogenic IDPs like c-Myc and p53 are master regulators whose disordered regions are critical for their function and dysregulation in cancer. Despite this progress, a fundamental gap persists in our understanding: we lack a predictive framework that can translate the primary amino acid sequence of an IDP into its functional interactome and emergent phase behavior. Current computational tools are fragmented. Some predictors, like IUPred2A or PONDR, can identify disordered regions with reasonable accuracy. Others, such as those searching for short linear motifs (SLiMs), can predict potential binding sites but often suffer from high false-positive rates and lack context. Similarly, algorithms like FuzDrop and PSPredictor estimate LLPS propensity based on general sequence features but cannot predict specific interaction partners or the material properties of the resulting condensates. This fragmentation prevents a holistic understanding of how sequence encodes function. The time is ripe to address this challenge. We are at a unique confluence of massive, publicly available biological data—from genomic and proteomic sequences (UniProt), curated disorder annotations (DisProt), interaction networks (BioGRID), structural snapshots (PDB, AlphaFold DB), and PTM atlases (PhosphoSitePlus)—and revolutionary advances in artificial intelligence, particularly deep learning models like transformers. These models have demonstrated an unparalleled ability to learn context and long-range dependencies in sequential data, as exemplified by AlphaFold2's success in protein structure prediction. By synthesizing these vast datasets through a purpose-built deep learning architecture, we can begin to decipher the 'interaction grammar' of IDPs, creating a unified model that bridges the scales from sequence to emergent cellular function.

Research Questions And Hypotheses

This project is driven by a central, overarching goal: to develop a computational framework that can predict the emergent functional landscape of an intrinsically disordered protein directly from its amino acid sequence. To achieve this, we have formulated three specific, interconnected research questions, each with testable hypotheses that will guide our data synthesis and modeling efforts. Our approach is designed to move beyond simple classification (e.g., disordered/ordered) towards a quantitative, mechanistic understanding of IDP function.

**Research Question 1: Can a unified 'interaction grammar' be learned from IDP sequences to accurately predict specific binding partners and the impact of post-translational modifications (PTMs)?**
This question addresses the fundamental challenge of mapping sequence to specific molecular interactions. While we know certain motifs are important, we hypothesize that the context in which these motifs appear, including flanking sequences, global amino acid composition, and PTM status, is critical for determining binding specificity.
*   **Hypothesis 1a:** A multi-modal deep learning model, trained on integrated data encompassing protein sequences, known interactions, structural contexts, and PTM sites, can learn the complex sequence features that govern IDP binding specificity with significantly higher fidelity than current motif-based or co-expression-based methods.
*   **Prediction:** Our model, which we term 'IDP-BERT', will be able to distinguish true interaction partners from non-partners in a held-out test set with an Area Under the Receiver Operating Characteristic Curve (AUC) greater than 0.85. Furthermore, the model's attention mechanisms will highlight specific residues and regions ('stickers') critical for binding, recapitulating known short linear motifs (SLiMs) and discovering novel ones.
*   **Validation:** We will validate predictions against a manually curated benchmark dataset of high-confidence, experimentally verified IDP interactions not included in the training data. We will perform *in silico* saturation mutagenesis on well-characterized IDP interaction pairs (e.g., p53-MDM2) to test if our model correctly predicts the loss or gain of function associated with known mutations.

**Research Question 2: How do sequence-encoded features and PTMs collectively determine an IDP's propensity to undergo liquid-liquid phase separation (LLPS) and form biomolecular condensates?**
This question links molecular interactions to the next level of organization: the formation of mesoscale assemblies. We posit that the same sequence grammar that dictates one-on-one interactions also governs the multivalent interactions driving LLPS.
*   **Hypothesis 2a:** The features learned by IDP-BERT for specific binding can be leveraged in a multi-task learning framework to predict a protein's intrinsic propensity to phase separate. The model will learn how the distribution, type, and PTM-state of 'stickers' across the sequence collectively determine the valency and interaction strength required for LLPS.
*   **Prediction:** The model will accurately classify proteins from curated LLPS databases (e.g., PhaSepDB) with high precision and recall. It will also predict the direction of change in LLPS propensity upon specific PTMs, such as predicting that phosphorylation of FUS's low-complexity domain will decrease its tendency to phase separate, consistent with experimental observations.
*   **Validation:** We will systematically compare our model's LLPS propensity scores against experimental data from the literature, including saturation concentrations and phase diagrams for a set of well-studied proteins (e.g., FUS, TDP-43, LAF-1). We will also test its ability to predict the effects of known ALS-associated mutations on the phase behavior of FUS.

**Research Question 3: Can sequence-level predictions be systematically bridged to predict the emergent, physical material properties of IDP-driven condensates?**
This is the most ambitious question, aiming to connect our predictive model to the principles of polymer physics to understand the emergent behavior of the cell. The 'fuzziness' of IDP interactions leads to condensates with diverse material states, from dynamic liquids to viscous gels, and we hypothesize this is predictable.
*   **Hypothesis 3a:** The interaction probabilities and binding site locations predicted by IDP-BERT can be directly translated into effective interaction parameters for coarse-grained molecular simulations, enabling the prediction of mesoscale properties like condensate viscosity, surface tension, and internal component dynamics.
*   **Prediction:** Simulations parameterized by our model will recapitulate known differences in material properties between different MLOs. For example, simulations of P-granule components will yield condensates with higher fluidity (lower viscosity) than simulations of stress granule components.
*   **Validation:** We will compare quantitative outputs from our simulations (e.g., diffusion coefficients, droplet fusion timescales) with experimental measurements from public datasets and literature, such as data from fluorescence recovery after photobleaching (FRAP) experiments. We will validate that our simulation pipeline can reproduce the experimentally observed impact of changing salt concentration or temperature on condensate stability.

**Expected Deliverables:** The project will yield: (1) A publicly available, integrated, and version-controlled database of IDP features. (2) The open-source code for the IDP-BERT model and all training pipelines. (3) A user-friendly web server for the community to analyze their proteins of interest. (4) A validated pipeline for parameterizing coarse-grained simulations from model outputs.

Methods And Approach

Our research plan is structured into three synergistic aims that integrate data curation, machine learning model development, and biophysical simulation. This transdisciplinary approach is essential to bridge the scales from primary sequence to emergent cellular function. The entire project will adhere to FAIR (Findable, Accessible, Interoperable, and Reusable) principles for data and open science practices for software.

**Aim 1: Assemble a Large-Scale, Multi-Modal Integrated Data Resource for IDP Function.**
This foundational aim focuses on aggregating and harmonizing diverse public datasets into a unified resource suitable for training a sophisticated deep learning model. This task requires significant bioinformatic expertise and is critical for the project's success.
*   **Data Sources:** We will synthesize data from a comprehensive list of public repositories. 
    *   **Sequence and Disorder Annotations:** Protein sequences will be sourced from UniProt (Swiss-Prot). Experimentally validated disordered regions will be extracted from DisProt and MobiDB. Computationally predicted disorder scores from multiple algorithms (e.g., IUPred2A, Espritz) will be included as input features.
    *   **Protein-Protein Interactions (PPIs):** High-confidence physical interactions will be aggregated from BioGRID, IntAct, and STRING. We will use stringent filtering criteria, retaining only interactions supported by at least two independent experiments or those from manually curated datasets to minimize noise.
    *   **Structural Context:** For IDPs that form complexes with structured proteins, we will extract atomic coordinates from the Protein Data Bank (PDB). For all proteins, we will use the AlphaFold Protein Structure Database to obtain high-quality structural models of any folded domains flanking or embedded within IDRs.
    *   **Post-Translational Modifications (PTMs):** Experimentally verified PTM sites (e.g., phosphorylation, ubiquitination, acetylation) will be sourced from PhosphoSitePlus, dbPTM, and recent large-scale proteomics studies. We will map these sites precisely onto our canonical UniProt sequences.
    *   **Phase Separation Data:** A ground-truth dataset of phase-separating proteins will be compiled from specialized databases like PhaSepDB, DrLLPS, and PhaSePro, along with manual curation from the literature.
*   **Data Integration and Preprocessing:** A robust pipeline using Python (Biopython, Pandas) and custom scripts will be developed to process and integrate these data. All entries will be mapped to unique UniProtKB identifiers. The final integrated dataset will be structured as a graph, where nodes represent proteins and their attributes (sequence, disorder, PTMs) and edges represent interactions. This resource will be version-controlled and made publicly available via Zenodo.

**Aim 2: Develop, Train, and Validate 'IDP-BERT', a Transformer-Based Model for Predicting IDP Interactomes and Phase Behavior.**
This aim constitutes the core computational modeling effort of the project.
*   **Model Architecture:** We will build upon the Transformer architecture, which has proven highly effective for sequential data. Our model, 'IDP-BERT', will take a protein's amino acid sequence as input. We will develop a custom tokenization scheme that represents not only the 20 standard amino acids but also incorporates information about PTMs at specific sites. This allows the model to learn how modifications like phosphorylation alter the 'meaning' of a sequence.
*   **Training Strategy:** We will employ a two-stage training process.
    1.  **Self-Supervised Pre-training:** The model will first be pre-trained on the entire human proteome from UniProt. Using a masked language model objective, it will learn the fundamental statistical patterns and long-range dependencies inherent in protein sequences, creating powerful, context-aware embeddings for each amino acid.
    2.  **Supervised Fine-tuning:** The pre-trained model will then be fine-tuned on our integrated dataset from Aim 1 using a multi-task learning approach. The model will have multiple output 'heads', each trained for a specific task: a) an **Interaction Head** that predicts the probability of interaction between two proteins; b) an **LLPS Head** that predicts the propensity of a single protein to phase separate; and c) a **Binding Site Head** that predicts which specific residues are likely to mediate interactions. This multi-task setup allows the model to learn a shared representation that captures the underlying principles common to both specific binding and multivalent phase separation.
*   **Validation and Interpretation:** The model will be rigorously validated using k-fold cross-validation and on held-out test sets. We will analyze the model's internal attention weights to interpret its predictions, identifying the sequence motifs and long-range contacts it deems important. This will provide novel biological insights into the 'rules' of IDP interactions.

**Aim 3: Bridge Sequence-Level Predictions to Mesoscale Simulations of Condensate Material Properties.**
This aim connects the machine learning predictions to physical models to explore emergent properties.
*   **Simulation Parameterization:** We will develop a pipeline to translate the outputs of IDP-BERT into parameters for coarse-grained (CG) molecular dynamics simulations. The predicted interaction probabilities between different sequence regions ('stickers') will be converted into effective pairwise interaction energies in a CG model (e.g., a 'stickers-and-spacers' representation).
*   **Simulation Engine:** We will utilize highly efficient, open-source simulation packages designed for polymer physics and phase separation, such as LaSSI or HOOMD-blue, running on GPU-accelerated hardware.
*   **Analysis of Emergent Properties:** We will run large-scale simulations of single- and multi-component systems to predict phase diagrams as a function of protein concentration and other variables. By analyzing the simulation trajectories, we will calculate key material properties, including condensate density, surface tension, and viscosity (via Green-Kubo relations), and the diffusion rates of molecules within the condensate.

**Timeline and Milestones:**
*   **Year 1:** Completion of the integrated data resource (M6). Development and pre-training of the first IDP-BERT prototype (M9). Initial fine-tuning and validation for PPI prediction (M12).
*   **Year 2:** Full multi-task model development, including LLPS and PTM effects (M18). Establishment of the simulation parameterization pipeline (M21). First comparative simulations of different IDP systems (M24).
*   **Year 3:** Rigorous validation of the full model and simulation pipeline against experimental data (M30). Development and deployment of the public web server (M33). Submission of manuscripts and release of all data and software (M36).

Expected Outcomes And Impact

This project is poised to deliver transformative outcomes that will significantly advance the fields of molecular and cellular biology, biophysics, and computational biology. By creating a unified predictive framework for IDP function, we will move the field from a descriptive to a predictive science, enabling researchers to generate and test hypotheses about the complex organizational principles of the cell. The impact will be felt across basic science, translational medicine, and biotechnology.

**Intended Contributions to the Field:**
1.  **A Unified Predictive Tool for IDP Function:** The primary outcome will be 'IDP-BERT', a powerful, open-source computational tool and web server. For the first time, researchers will be able to input an amino acid sequence and receive a comprehensive functional prediction, including likely interaction partners, propensity for phase separation, key functional sites, and the probable impact of PTMs. This will democratize the study of IDPs, making sophisticated computational analysis accessible to the entire biological community.
2.  **Deciphering the 'Molecular Grammar' of IDPs:** Our work will provide fundamental insights into the sequence-encoded rules that govern IDP interactions. By interpreting the features learned by our model, we expect to discover novel interaction motifs (SLiMs) and uncover the complex interplay between local motifs and global sequence context. This will provide a new 'dictionary' for understanding how information is encoded in the disordered proteome.
3.  **A Quantitative Bridge Between Sequence and Mesoscale Physics:** The project will establish a novel, validated workflow for connecting sequence-level information to the emergent material properties of biomolecular condensates. This will provide a powerful *in silico* microscope to explore how mutations or PTMs alter the liquidity, viscosity, and dynamics of membraneless organelles, offering mechanistic explanations for their function and dysfunction.
4.  **A Rich, Integrated Data Resource:** The curated, multimodal dataset assembled in Aim 1 will be a valuable community resource in its own right. By harmonizing data from disparate sources, we will provide a high-quality, benchmark dataset that can be used by other groups to develop and test new computational methods.

**Broader Impacts and Applications:**
*   **Translational Medicine and Disease:** The ability to predict the functional consequences of mutations in disordered regions has profound clinical implications. Our tool can be used to rapidly assess the pathogenicity of variants of unknown significance (VUS) found in patient genomes, particularly for neurodegenerative diseases (e.g., ALS, Alzheimer's) and cancers where IDPs are frequently mutated. This can accelerate diagnostics and guide the development of targeted therapies aimed at modulating IDP interactions or phase behavior.
*   **Synthetic Biology and Bioengineering:** A predictive understanding of IDP grammar will empower the rational design of novel proteins. Engineers will be able to design synthetic IDPs with bespoke interaction partners and phase separation properties to create artificial cellular compartments, novel biosensors, or therapeutic proteins with precisely tuned behaviors.
*   **Training and Workforce Development:** This project is an ideal training vehicle for the next generation of data-savvy bioscientists. Graduate students and postdocs will work at the cutting edge of machine learning, biophysics, and cell biology, acquiring a uniquely transdisciplinary skillset that is in high demand. Through workshops and collaborative coding sessions, we will foster a team-oriented and open-science research culture.

**Dissemination Plan and Open Science Commitment:**
We are fully committed to open science principles. All software will be developed under a permissive open-source license (e.g., MIT) and hosted on GitHub. The trained models and the integrated dataset will be deposited in public repositories like Zenodo and Model Zoo. We will develop a user-friendly web server to ensure broad accessibility of our predictive tool. Our findings will be disseminated through high-impact, open-access publications in journals such as *Nature Methods*, *Cell Systems*, or *PNAS*, and through presentations at major international conferences (e.g., ISMB, ASCB, BPS). We will also organize a workshop in the final year to train other researchers in the use of our tools and methods.

**Long-Term Vision:** This project lays the groundwork for a long-term vision of creating a 'virtual cell' where the emergent organization of the cytoplasm can be predicted from genomic information. Future iterations could incorporate cellular localization data, RNA interactions, and metabolic information to build an increasingly comprehensive model of cellular life. This work will fundamentally change our ability to interpret genomes, moving from a list of parts to a dynamic blueprint of the living cell.

Budget And Resources

The proposed research represents a large-scale data synthesis and modeling effort that requires a dedicated, transdisciplinary team and significant computational resources, placing it beyond the scope of a single research lab or a standard collaborative grant. The NCEMS Working Group mechanism is ideally suited to provide the necessary support and collaborative infrastructure. The budget is requested for a period of three years.

**1. Personnel (Total: $980,000, ~70% of direct costs):**
This is the largest budget component, reflecting the collaborative, human-capital-intensive nature of the project.
*   **Principal Investigators:** 1.0 month of summer salary per year for each of the four key PIs (specializing in Computer Science/AI, Cell Biology, Polymer Physics, and Bioinformatics). This ensures dedicated time for project leadership, mentorship, and intellectual integration. (4 PIs x 1 mo/yr x 3 yrs = $240,000)
*   **Postdoctoral Fellows:** Two full-time postdoctoral fellows for three years. Postdoc 1 will have expertise in machine learning and will lead the development of the IDP-BERT model. Postdoc 2 will be a computational biophysicist responsible for the coarse-grained simulations and their integration with the ML model. Their salaries, including fringe benefits, are budgeted at a competitive level. (2 Postdocs x 3 yrs = $450,000)
*   **Graduate Students:** Support for three graduate students for three years, covering stipends, tuition, and health benefits. Students will be embedded in the PIs' labs and will work collaboratively on data curation, model validation, and application of the tools to specific biological systems. (3 Students x 3 yrs = $270,000)
*   **Project Coordinator (0.25 FTE):** A part-time coordinator to manage logistics for the working group meetings, maintain the project website, and ensure compliance with data sharing and reporting requirements. ($20,000)

**2. Computational Resources (Total: $210,000, ~15% of direct costs):**
As this project does not generate experimental data, computational resources are our primary 'equipment'.
*   **High-Performance Computing (HPC):** Access to a GPU-enabled HPC cluster is critical for training large transformer models. We request funds to purchase a dedicated block of 25,000 GPU-hours per year on an NCEMS-affiliated or commercial cloud cluster. ($150,000)
*   **Data Storage and Web Hosting:** Funds for robust, backed-up data storage for the multi-terabyte integrated dataset. Additional funds are allocated for cloud services (e.g., AWS, GCP) to host the public-facing web server and database for the final two years of the project and two years beyond. ($60,000)

**3. Travel (Total: $70,000, ~5% of direct costs):**
*   **Working Group Meetings:** Funds to support twice-yearly, in-person meetings for the entire team (PIs, postdocs, students). These intensive, multi-day 'hackathon' style meetings are essential for fostering deep collaboration, resolving technical challenges, and cross-training personnel. ($40,000)
*   **Conference Travel:** Support for students and postdocs to travel to one major international conference per year to present their work, disseminate findings, and network with the broader scientific community. ($30,000)

**4. Publications and Dissemination (Total: $40,000, ~3% of direct costs):**
*   Funds are allocated to cover open-access publication fees for an anticipated 3-4 major manuscripts in high-impact journals. ($25,000)
*   Support for developing training materials, tutorials, and hosting a final-year workshop to disseminate our tools and methods to the community. ($15,000)

**Total Direct Costs:** $1,300,000
**Indirect Costs (F&A):** To be calculated based on the negotiated rates of the participating institutions.

**Justification for NCEMS Support:** The ambitious scale of this project—integrating vast, heterogeneous datasets, developing a novel deep learning architecture, and coupling it with advanced biophysical simulations—requires a critical mass of diverse expertise that no single lab possesses. The NCEMS framework is essential for assembling and supporting this 'dream team' of scientists and providing the collaborative environment needed to tackle this fundamental question in cellular emergence.",,
ai_group_int_gemini_05,ai,group_int,gemini-2.5-pro,The Deep Homology of Gene Regulatory Networks: Uncovering the Emergent Logic of Multicellularity,"The evolution of complex multicellularity occurred independently multiple times across the tree of life, presenting a profound puzzle: are there universal design principles governing the gene regulatory networks (GRNs) that orchestrate this transition? This project proposes to answer this question by performing the first large-scale comparative analysis of GRNs from a wide phylogenetic range of unicellular and multicellular organisms. Our working group will unite evolutionary biologists, developmental biologists, network scientists, and computational biologists to synthesize a massive collection of public genomic, epigenomic, and transcriptomic data (e.g., from NCBI, ENCODE, and single-cell developmental atlases). We will develop a standardized computational pipeline to reconstruct GRNs for dozens of species, from choanoflagellates and fungi to plants and animals. Using tools from network theory, information theory, and topological data analysis, we will compare these networks to identify conserved architectural features, motifs, and dynamic properties associated with the emergence of multicellular life. We will test specific hypotheses, such as whether the evolution of multicellularity is consistently preceded by an expansion of transcription factor families or the appearance of specific network motifs that enable robust cell differentiation. This community-scale synthesis is essential because it requires a vast amount of data processing and a unique combination of expertise in evolutionary theory and computational network analysis. The project will yield fundamental insights into the 'syntactical' rules that allow simple genetic toolkits to generate the emergent complexity of a multicellular organism. We will create a public 'GRN-Zoo' database and visualization platform, providing a foundational resource for the community and training young scientists in the principles of evolutionary systems biology.",,"Background And Significance

The emergence of complex multicellular organisms from unicellular ancestors is one of the most profound and consequential transitions in the history of life. This evolutionary leap, which occurred independently in at least six eukaryotic lineages including animals, plants, and fungi, enabled the evolution of macroscopic life and the vast ecological complexity we observe today. A central, unanswered question in biology is whether these independent origins of multicellularity followed convergent evolutionary paths governed by universal principles. While the specific genes and proteins involved—the 'hardware'—differ between lineages, we hypothesize that the underlying 'software'—the logic and architecture of the gene regulatory networks (GRNs) that control cell differentiation, communication, and spatial organization—may exhibit deep homologies or convergent features. Understanding this regulatory grammar is fundamental to explaining how emergent complexity arises from a finite set of genetic components. 

Current research, largely from the field of evolutionary developmental biology (evo-devo), has provided critical foundational insights. Studies on the unicellular relatives of animals, such as choanoflagellates, have revealed that a significant portion of the 'developmental toolkit' genes—including transcription factors, signaling pathway components, and adhesion molecules—were already present in their single-celled ancestors (King et al., 2008; Sebé-Pedrós et al., 2017). This seminal discovery shifted the focus from the origin of new genes to the rewiring of pre-existing regulatory connections as the primary driver of multicellular innovation. Similarly, comparative genomics in plants and fungi has shown that unicellular algae and fungi possess orthologs of key regulators of multicellular development (e.g., MADS-box genes in plants). These findings strongly imply that the transition to multicellularity was primarily a problem of information processing and network engineering.

Systems biology has provided the conceptual framework for studying GRNs as complex networks. Detailed GRN models have been painstakingly constructed for specific developmental processes in model organisms like the sea urchin endomesoderm (Davidson et al., 2002) and Drosophila segmentation (Jaeger et al., 2004). These studies demonstrate how network architecture gives rise to precise spatiotemporal patterns of gene expression and robust developmental outcomes. However, these efforts have been largely confined to single species or closely related groups, providing limited insight into the macroevolutionary patterns of GRN evolution across deep time.

This leaves a critical gap in our knowledge. We lack a systematic, cross-kingdom comparative analysis of GRNs. Previous comparative studies have been limited in phylogenetic scope or have focused on the evolution of individual transcription factors or small motifs rather than whole-network architectures. Furthermore, the methods used to infer GRNs are diverse and often not directly comparable, creating a methodological fragmentation that has prevented a unified synthesis. The sheer scale of the data required—integrating genomics, transcriptomics, and epigenomics from dozens of species—and the analytical complexity of comparing large, heterogeneous networks have placed such a project beyond the capacity of any single research laboratory. 

This project is both important and timely due to a confluence of factors. First, the explosion of publicly available data, including hundreds of high-quality genomes and vast repositories of transcriptomic (e.g., SRA, GEO) and epigenomic (e.g., ENCODE) data, makes a large-scale synthesis project feasible for the first time. The recent advent of single-cell atlases provides unprecedented resolution into the cell-type-specific regulatory states that define multicellularity. Second, advances in computational biology, including machine learning algorithms for network inference and novel analytical tools from network science and topological data analysis, provide the necessary power to extract meaningful patterns from this data deluge. By uniting a multidisciplinary team of experts to tackle this challenge, we can move beyond species-specific descriptions to uncover the fundamental, and potentially universal, principles of regulatory logic that enabled one of life's greatest innovations.

Research Questions And Hypotheses

This project is guided by the overarching question: Are there universal architectural principles and dynamic properties of Gene Regulatory Networks (GRNs) that convergently evolved to enable the transition from unicellular to complex multicellular life? To deconstruct this central question, we have formulated four specific, interconnected research questions (RQs), each associated with a set of testable hypotheses and clear, falsifiable predictions.

**RQ1: How did the global architecture and topology of GRNs change during the transition to multicellularity?**
This question addresses the macro-level structural changes in GRNs. We hypothesize that the demands of coordinating multiple cell types in a stable body plan imposed strong selection for more complex and organized network structures.
*   **Hypothesis 1 (The Hierarchy and Modularity Hypothesis):** The evolution of multicellularity is consistently associated with an increase in both the hierarchical organization and modularity of GRNs.
    *   *Prediction 1a:* Using network metrics, we will find that GRNs from multicellular organisms exhibit significantly higher global reaching centrality and a more layered, acyclic structure compared to their unicellular sister taxa. This reflects the emergence of master regulators controlling downstream developmental cascades.
    *   *Prediction 1b:* We predict that multicellular GRNs will have a higher modularity score, with identified modules corresponding to distinct cell-type-specific or tissue-specific developmental programs. We will test this by assessing the functional enrichment (GO terms) of genes within computationally defined network modules.

**RQ2: Are specific, functional network motifs convergently acquired or enriched in independent multicellular lineages?**
This question focuses on the micro-level circuitry of GRNs. We posit that certain computational tasks essential for multicellularity, such as robust cell fate decisions and noise filtering, are solved by a limited set of optimal network motifs.
*   **Hypothesis 2 (The Convergent Motif Hypothesis):** Network motifs that function as bistable switches, oscillators, and noise-dampening filters are significantly and convergently enriched in the GRNs of multicellular organisms.
    *   *Prediction 2a:* A systematic search will reveal a statistically significant over-representation of motifs like the toggle switch (two mutually repressing genes) and the coherent feed-forward loop (FFL) in all analyzed multicellular lineages (animals, plants, fungi) relative to their unicellular relatives.
    *   *Prediction 2b:* The genes participating in these enriched motifs will be disproportionately involved in cell fate specification, cell cycle control, and intercellular signaling pathways, as determined by functional annotation.

**RQ3: How do the potential dynamic properties of GRNs differ between unicellular and multicellular organisms?**
This question moves from static network structure to the functional consequences for cellular behavior. We hypothesize that multicellular GRNs are structured to support a larger and more stable repertoire of cellular states.
*   **Hypothesis 3 (The Expanded Attractor Landscape Hypothesis):** GRNs from multicellular organisms possess a richer landscape of stable states (attractors), corresponding to distinct cell types, compared to the simpler landscapes of their unicellular ancestors.
    *   *Prediction 3a:* Using Boolean network modeling on core regulatory sub-networks, we will find that multicellular GRNs have a significantly larger number of stable point attractors and stable cyclic attractors than unicellular GRNs.
    *   *Prediction 3b:* The attractors identified in our models will correspond to known cell-type-specific gene expression profiles derived from single-cell RNA-seq data, validating their biological relevance. The transitions between these attractors will require specific input signals, mirroring developmental signaling pathways.

**RQ4: What are the genomic and epigenomic correlates of the evolutionary transition to multicellular GRN architectures?**
This question links network evolution to changes in the genome itself. We propose that network rewiring is scaffolded by specific changes in the genetic toolkit and the regulatory landscape.
*   **Hypothesis 4 (The Regulatory Scaffolding Hypothesis):** The emergence of complex multicellular GRNs is preceded or accompanied by the expansion of specific transcription factor (TF) families and an increase in the complexity of the cis-regulatory landscape.
    *   *Prediction 4a:* Phylogenetic analysis will show that major expansions of lineage-specific TF families (e.g., bHLH, Homeobox in animals; MADS-box in plants) correlate tightly with the phylogenetic nodes where multicellularity arose.
    *   *Prediction 4b:* These expanded TF families will be identified as key hub nodes in the multicellular GRNs. Furthermore, we predict a significant increase in the density of conserved non-coding elements (potential cis-regulatory sites) and regions of open chromatin (from ATAC-seq data) in the genomes of multicellular organisms compared to their unicellular relatives.

Methods And Approach

This project will be executed in four integrated phases by a transdisciplinary working group with expertise spanning evolutionary biology, network science, computational biology, and data science. Our approach is designed to be rigorous, reproducible, and open, leveraging only publicly available data as stipulated by the research call.

**Phase 1: Community-Scale Data Curation and Harmonization (Months 1-9)**
This foundational phase addresses the critical challenge of integrating vast and heterogeneous public datasets.
*   **Species Selection:** We will select a phylogenetically balanced set of approximately 50 species, capturing at least five independent origins of multicellularity (Metazoa, Land Plants, Florideophyte red algae, and two fungal lineages). For each origin, we will sample a triad of species: a unicellular outgroup, a simple multicellular relative, and a complex multicellular organism (e.g., *Monosiga brevicollis* -> *Amphimedon queenslandica* -> *Drosophila melanogaster*). This comparative phylogenetic framework is essential for inferring the direction of evolutionary change.
*   **Data Aggregation:** We will systematically mine major public repositories, including NCBI (genomes, SRA), EBI (ArrayExpress, Expression Atlas), and ENCODE/modENCODE. We will prioritize species with high-quality genome assemblies, annotations, and extensive transcriptomic (bulk and single-cell RNA-seq) and epigenomic (ATAC-seq, ChIP-seq for histone marks) data.
*   **Standardized Processing Pipeline:** To ensure comparability across species, all raw data will be processed through a single, harmonized pipeline. This pipeline, built using Nextflow for scalability and portability, and containerized with Docker/Singularity for reproducibility, will perform tasks such as: uniform quality control of sequencing reads, mapping to reference genomes, transcript quantification (e.g., Salmon), and peak calling for epigenomic data (e.g., MACS2). All processed data and metadata will be stored in a centralized, versioned repository.

**Phase 2: Multi-Modal Gene Regulatory Network Inference (Months 6-18)**
Recognizing that no single GRN inference algorithm is infallible, we will employ an ensemble, integrative approach to generate a consensus, high-confidence network for each species.
*   **Inference Methods:** Our pipeline will integrate three distinct lines of evidence:
    1.  **Expression-based inference:** We will use mutual information-based methods (e.g., ARACNe, GENIE3) and Bayesian network models to infer regulatory relationships from large compilations of gene expression data.
    2.  **Sequence-based inference:** We will perform genome-wide scans for transcription factor binding motifs (using databases like JASPAR and CIS-BP) in promoter and enhancer regions (identified via chromatin accessibility data) to predict physical TF-target gene interactions.
    3.  **Chromatin-based inference:** We will leverage ATAC-seq and histone mark ChIP-seq data to identify active regulatory elements and link them to target genes based on proximity and chromatin conformation data (where available), thus refining the set of potential regulatory interactions.
*   **Network Integration and Validation:** The outputs of these methods will be integrated using a weighted scoring scheme to produce a final, confidence-scored GRN for each species. The pipeline's performance and scoring thresholds will be calibrated by benchmarking against gold-standard, experimentally validated GRNs from model organisms like *E. coli*, *S. cerevisiae*, and *D. melanogaster*.

**Phase 3: Comparative Network Analysis and Hypothesis Testing (Months 15-30)**
This phase constitutes the core scientific inquiry of the project.
*   **Topological Analysis (RQ1, H1):** We will compute a suite of global and local network metrics for all 50 GRNs using libraries like NetworkX and igraph. We will use Phylogenetic Generalized Least Squares (PGLS) to test for statistically significant correlations between network properties (e.g., modularity, hierarchy) and the presence of multicellularity, while controlling for phylogenetic non-independence.
*   **Motif Analysis (RQ2, H2):** We will use specialized algorithms (e.g., FANMOD) to determine the statistical significance (Z-score) of all 3- and 4-node motifs in each network compared to a null model. We will then compare motif significance profiles across the phylogenetic tree to identify convergently enriched motifs.
*   **Dynamical Modeling (RQ3, H3):** For key sub-networks involved in cell fate decisions, we will construct and analyze Boolean network models. By simulating the network dynamics, we will identify the number and characteristics of their attractors (stable states). We will test if the number of attractors correlates with the number of cell types in the organism.
*   **Innovative Approaches:** We will employ Topological Data Analysis (TDA), specifically persistent homology, to capture higher-order structural features of the GRNs. This novel approach may reveal complex organizational principles (e.g., loops, voids in network space) that are invisible to standard metrics and may be signatures of multicellularity.

**Phase 4: Resource Dissemination and Community Training (Months 24-36)**
*   **The GRN-Zoo:** We will develop and launch a public, web-accessible database, the 'GRN-Zoo'. This resource will feature an intuitive interface for browsing, searching, visualizing (using Cytoscape.js), and downloading all inferred GRNs, associated metadata, and analysis results. It will serve as a lasting contribution to the scientific community.
*   **Open Science Commitment:** All analysis scripts, workflows, and pipelines will be open-source and available on GitHub. All data products will be deposited in public repositories like Zenodo. We will publish our findings in open-access journals.
*   **Timeline and Milestones:**
    *   Year 1: Data curation for all species complete. GRN inference pipeline v1.0 deployed. Pilot GRNs for 10 species reconstructed and analyzed.
    *   Year 2: All 50 GRNs reconstructed. Comparative analysis framework finalized. GRN-Zoo beta version launched. First major manuscript submitted.
    *   Year 3: In-depth hypothesis testing, including dynamical modeling and TDA, completed. GRN-Zoo public release with full functionality. Final manuscripts submitted. Host first community training workshop.

Expected Outcomes And Impact

This community-scale synthesis project is poised to deliver transformative outcomes, significantly advancing the fields of evolutionary, developmental, and systems biology. The impact will be felt through the generation of fundamental new knowledge, the creation of a lasting community resource, the development of novel analytical strategies, and the training of a new generation of data-savvy biologists.

**Intended Contributions to the Field:**
1.  **Discovery of Universal Principles of Biological Emergence:** The primary intellectual contribution will be the identification of a core set of principles—a 'regulatory grammar'—that govern the evolution of multicellularity. By moving beyond the specifics of individual lineages, we aim to uncover the convergent logic that enables the emergence of complex biological form and function. This would represent a major step towards a predictive theory of developmental evolution and would directly address the funding call's focus on emergence phenomena.
2.  **A Foundational Community Resource: The GRN-Zoo:** We will deliver the first large-scale, cross-kingdom comparative atlas of gene regulatory networks. The 'GRN-Zoo' database and visualization platform will be a durable resource for the entire biological community, analogous to how GenBank or the Protein Data Bank have catalyzed research. It will empower countless new investigations by enabling researchers to easily query and compare regulatory architectures across the tree of life, ask new questions, and formulate novel hypotheses.
3.  **Methodological Advancement and Standardization:** Our project will produce and disseminate a robust, reproducible, and open-source computational pipeline for integrating diverse 'omics' data to infer and compare GRNs. This standardized framework will help overcome the methodological fragmentation in the field, promoting higher standards of rigor and comparability in future systems biology research.

**Broader Impacts and Applications:**
*   **Informing Synthetic Biology:** The design principles we uncover will provide a blueprint for the forward engineering of synthetic multicellular systems. Understanding how nature builds robust, complex systems can guide the creation of engineered tissues for regenerative medicine, programmable microbial consortia for bioremediation, and sophisticated cellular circuits for bioproduction.
*   **New Perspectives on Disease:** Many human diseases, most notably cancer, can be conceptualized as a breakdown of the regulatory programs that maintain multicellular cooperation. Cancer cells often reactivate ancestral unicellular behaviors like proliferation and migration. Our research into the GRNs that stabilize multicellularity could reveal network-level vulnerabilities and identify novel therapeutic targets aimed at reinforcing the 'multicellular contract'.
*   **Advancing STEM Education and Training:** This project is intrinsically multidisciplinary and will serve as an exceptional training environment. Graduate students and postdoctoral fellows will gain unique expertise at the intersection of evolutionary biology, computational science, and network theory. Through our planned annual workshops, we will disseminate these skills to the broader community, directly contributing to the development of the future data-savvy workforce as mandated by the research call.

**Dissemination Plan and Long-Term Vision:**
Our dissemination strategy is multi-faceted. We will publish our primary findings in high-impact, open-access journals such as *Nature*, *Science*, or *Cell*, with more specialized methodological and analytical papers submitted to journals like *PLoS Computational Biology* and *Molecular Biology and Evolution*. We will actively present our work at major international conferences (e.g., ISMB, SMBE, SDB) to engage with the community. The GRN-Zoo will be the central hub for dissemination, providing open access to all data and results. Our long-term vision is for the GRN-Zoo to become a living database, sustained by follow-on funding and community contributions. The working group established by this project will form the nucleus of a durable collaborative network, fostering future projects that build upon this foundational work to incorporate new data types like spatial transcriptomics and proteomics, further unraveling the emergent logic of life.

Budget And Resources

The proposed research represents a community-scale synthesis project that is beyond the capabilities of a single research lab or a standard collaborative grant. The immense scale of data aggregation and processing across ~50 species, the development of a sophisticated and robust computational pipeline, the deep, multidisciplinary expertise required for analysis, and the creation of a lasting public resource (the GRN-Zoo) necessitate the support and structure provided by the NCEMS Working Group program. This project requires a dedicated team of specialists in evolutionary genomics, developmental biology, network science, computer science, and statistics to work in a deeply integrated fashion over three years, a mode of collaboration that this funding mechanism is uniquely designed to foster.

**Budget Justification (Total Request: $1,450,000 over 3 years)**

*   **A. Personnel ($945,000 - 65%):** The bulk of the budget is allocated to personnel who will drive the project's day-to-day research and development.
    *   **Postdoctoral Fellows (3):** $225,000 (3 x $75,000/yr avg salary+fringe). Three postdocs will form the core research team. One will specialize in data curation and pipeline development, a second in network inference and comparative genomics, and a third in network analysis, modeling, and TDA.
    *   **Graduate Students (4):** $240,000 (4 x $60,000/yr stipend+tuition). Supporting four students in the PIs' labs is central to our training mission. They will be mentored by the working group and contribute to all aspects of the project.
    *   **Data Scientist/Software Engineer (1):** $330,000 (1 x $110,000/yr salary+fringe). A dedicated professional is essential for the robust development, maintenance, and user support of the GRN-Zoo database and web portal.
    *   **PI Summer Salary:** $150,000 (10 PIs x 0.5 months/yr x 3 yrs). To compensate PIs for their significant time commitment to project management, mentorship, and intellectual leadership.

*   **B. Travel ($145,000 - 10%):** Collaboration and dissemination are key.
    *   **Annual Working Group Meeting:** $90,000 ($30,000/yr). To bring all 10 PIs and trainees together for an intensive 3-day workshop to facilitate data integration, resolve challenges, and synthesize results.
    *   **Conference Travel:** $55,000. To support trainees and PIs in presenting project findings at major national and international conferences, ensuring broad dissemination.

*   **C. Computational Resources ($145,000 - 10%):**
    *   **Cloud Computing:** $100,000. For on-demand, high-performance computing resources (AWS/Google Cloud) required for processing terabytes of sequencing data and running large-scale network inference algorithms.
    *   **Data Storage & Server Hosting:** $45,000. For long-term data archiving and for hosting the GRN-Zoo web server.

*   **D. Materials & Supplies ($75,000 - 5%):**
    *   **Publication Costs:** $45,000. To cover open-access fees for an estimated 6-8 publications.
    *   **Workshop Costs:** $30,000. For materials and logistical support for hosting one community training workshop in Year 3.

*   **E. Indirect Costs (F&A) ($140,000 - ~10%):** Calculated based on the lead institution's negotiated rate on a modified total direct cost base. This covers the administrative and facilities support essential for a project of this scale.",,
ai_group_int_gemini_06,ai,group_int,gemini-2.5-pro,CellStateNet: A Foundational Model of Cellular Perturbations to Predict Emergent Phenotypes,"Predicting cellular response to novel perturbations—such as drugs, mutations, or environmental stimuli—is a grand challenge in biology and medicine. Current approaches are often piecemeal. We propose to build 'CellStateNet,' a foundational AI model for molecular and cellular biology, trained on the entirety of public perturbation-response data. This project will assemble a team of machine learning experts, systems biologists, and pharmacologists to tackle this ambitious goal. The working group will aggregate and harmonize massive datasets, including transcriptomic data from LINCS L1000 and GEO, genetic dependency maps from DepMap and Achilles, and proteomic perturbation data from CPTAC. The core technical innovation will be the development of a multi-modal, self-supervised deep learning architecture that learns a unified, latent representation of cellular state. This model will be trained to predict the outcome of one data modality (e.g., proteomics) from another (e.g., transcriptomics) under a given perturbation. Once trained, CellStateNet will function as a powerful in-silico laboratory. It will be used to predict the effects of novel drug combinations, forecast the phenotypic consequences of uncharacterized genetic variants, and identify synergistic therapeutic strategies. The emergent cellular phenotype is a complex output of the underlying molecular network; our model aims to learn this mapping directly from data at an unprecedented scale. The sheer volume of data and the complexity of the required AI models make this project intractable for a single research group. The resulting pre-trained model, along with all associated code and training pipelines, will be made publicly available, empowering the broader research community to address a vast range of biological questions and accelerating the pace of therapeutic discovery.",,"Background And Significance

The living cell is a quintessential complex adaptive system. Its state is defined by an intricate, multi-layered network of interactions between genes, proteins, and other molecules. The response of this system to perturbations—be it a therapeutic drug, a disease-causing mutation, or an environmental stressor—is an emergent phenomenon that is notoriously difficult to predict. This predictive challenge represents a fundamental bottleneck in virtually every aspect of modern biology and medicine, from designing effective cancer therapies to understanding the mechanisms of genetic disorders. For decades, the dominant paradigm for studying cellular responses has been reductionist, focusing on individual pathways or a handful of molecules. While invaluable, this approach struggles to capture the system-level properties that govern cellular behavior. The advent of high-throughput omics technologies has enabled a more holistic view, generating vast datasets that profile cellular states at unprecedented resolution. Landmark public resources such as the Gene Expression Omnibus (GEO), the Library of Integrated Network-based Cellular Signatures (LINCS L1000), the Dependency Map (DepMap), and the Clinical Proteomic Tumor Analysis Consortium (CPTAC) now house millions of molecular profiles of cells under a wide array of chemical and genetic perturbations. These datasets represent a monumental community achievement and an unparalleled resource for understanding cellular logic. However, they remain largely fragmented and underutilized. Current computational approaches to model this data are often limited in scope. Mechanistic models based on ordinary differential equations (ODEs) can provide deep insights but are restricted to well-characterized pathways and do not scale to the genome level. Conversely, statistical and machine learning models have shown promise in specific tasks, such as predicting drug sensitivity from baseline gene expression. Yet, these models are typically trained on a single data modality for a single predictive task (e.g., predicting cell viability). They often lack generalizability across different cell types, perturbation classes, or molecular readouts. A key gap in the field is the absence of a unified framework that can integrate these disparate, multi-modal datasets to learn a general, transferable model of cellular response. Recent breakthroughs in artificial intelligence, particularly the development of 'foundational models' in natural language processing (e.g., BERT, GPT-3) and computer vision, offer a new paradigm. These models are pre-trained on massive, diverse datasets using self-supervised objectives and learn powerful, general-purpose representations that can be adapted to a multitude of downstream tasks. The application of this paradigm to molecular and cellular biology is both timely and transformative. Early efforts have demonstrated the potential of deep learning to predict single-cell transcriptomic responses to perturbations (Lotfollahi et al., 2022), but these have been limited to a single data modality and a narrow set of perturbations. This project, CellStateNet, proposes to take a quantum leap forward. We will be the first to construct a true foundational model for cellular biology by integrating the world's public perturbation-response data across transcriptomics, proteomics, and functional genomics. The importance of this research cannot be overstated. Such a model would function as a universal 'in-silico cell,' allowing researchers to perform virtual experiments at a scale and speed unattainable in the laboratory. It would enable the systematic exploration of the vast combinatorial space of drug mixtures, the functional interpretation of uncharacterized genetic variants, and the deconvolution of complex disease mechanisms. The sheer scale of the data integration effort, the computational complexity of training a multi-modal foundational model, and the transdisciplinary expertise required—spanning machine learning, bioinformatics, systems biology, and pharmacology—place this project far beyond the capacity of any single research lab. It is precisely the type of community-scale synthesis project that this research call is designed to catalyze, promising to create a public resource that will empower the entire biomedical research community and accelerate the pace of discovery for years to come.

Research Questions And Hypotheses

This project is guided by a central, ambitious question: Can a single, unified computational model learn the fundamental 'rules' of cellular response by integrating the full breadth of public, multi-modal perturbation data? To address this, we have formulated three specific research questions (RQs), each with testable hypotheses that will structure our investigation. 

**RQ1: Can a multi-modal deep learning architecture learn a shared, biologically meaningful latent representation of cellular state that is predictive across diverse omics layers and perturbation types?**
The central premise of CellStateNet is that a unified representation of cellular state exists and can be learned from data. This latent space should capture the underlying biological processes that are perturbed, regardless of whether the perturbation is chemical or genetic, or whether the readout is transcriptomic or proteomic.
*   **Hypothesis 1a:** A self-supervised model, trained on the cross-modal task of predicting the proteomic state of a cell from its transcriptomic state (and vice versa) under a given perturbation, will learn a latent space whose dimensions are significantly enriched for canonical biological pathways and gene ontology terms. 
*   **Hypothesis 1b:** The latent embeddings of perturbations learned by the model will cluster according to their known mechanism of action (MoA), demonstrating that the model captures functional similarities between distinct chemical and genetic interventions.
*   **Hypothesis 1c:** A multi-modal model will outperform single-modality autoencoders in downstream predictive tasks, as the integration of multiple data types provides a more constrained and robust representation of cellular state.
*   **Validation:** We will test H1a by performing systematic enrichment analyses (e.g., GSEA) on genes highly weighted in each latent dimension. For H1b, we will use clustering metrics (e.g., silhouette score) to quantify the separation of perturbations by MoA in the learned embedding space. For H1c, we will establish a suite of benchmark tasks (e.g., predicting cell viability) and compare the performance of CellStateNet's representations against those from models trained on transcriptomics or proteomics alone.

**RQ2: Can the trained CellStateNet model accurately predict the molecular and phenotypic outcomes of unseen perturbations, including novel drug combinations and uncharacterized genetic variants?**
A truly foundational model must possess predictive power beyond its training data, demonstrating true generalization. We will rigorously test the model's ability to make *de novo* predictions for perturbations it has not seen during training.
*   **Hypothesis 2a:** CellStateNet can predict the full transcriptomic profile of a cell's response to a combination of two drugs, given only data on the effects of each drug individually. The model's predictions will accurately classify the interaction as synergistic, additive, or antagonistic when compared to experimental ground truth.
*   **Hypothesis 2b:** The model can predict the phenotypic consequences (e.g., cell fitness score from DepMap) of introducing an uncharacterized, non-coding genetic variant by first predicting its impact on the expression of nearby genes and then propagating this molecular change through the model to a final phenotype.
*   **Validation:** For H2a, we will employ a temporal hold-out strategy, training the model on public data up to a specific year and testing its ability to predict the results of combination screens published subsequently. We will compare predicted gene expression vectors using cosine similarity and correlation, and synergy scores using established metrics. For H2b, we will leverage data from massively parallel reporter assays (MPRAs) and variant effect mapping studies (e.g., MaveDB) as ground truth, assessing the correlation between our model's predicted fitness effects and experimentally measured ones for held-out variants.

**RQ3: Can CellStateNet function as an *in silico* discovery engine to generate novel, testable hypotheses about synergistic therapeutic strategies and key drivers of cellular state transitions?**
The ultimate utility of CellStateNet lies in its application as a tool for scientific discovery. We will use the trained model to probe complex biological systems and generate high-priority hypotheses for experimental validation.
*   **Hypothesis 3a:** An *in silico* screen using CellStateNet across millions of virtual compound pairs against a specific cancer cell line model will identify novel synergistic combinations at a rate significantly higher than random chance.
*   **Hypothesis 3b:** Model interpretability techniques (e.g., integrated gradients, attention analysis) applied to the model's prediction of a transition from a healthy to a diseased cellular state can identify key genes and pathways that act as critical drivers of the transition.
*   **Validation:** For H3a, we will generate a ranked list of predicted synergistic pairs for a well-studied cancer model (e.g., MCF-7 breast cancer) and assess the enrichment of known synergies and plausible biological mechanisms in the top predictions by cross-referencing literature and pathway databases. For H3b, we will simulate a disease transition (e.g., oncogenic transformation) and compare the model-identified driver genes with known oncogenes and tumor suppressors from databases like COSMIC, testing for significant overlap.

**Expected Deliverables:** This research will produce: (1) A comprehensive, harmonized, and analysis-ready database of public perturbation-response data. (2) The open-source code for the CellStateNet model architecture and training pipeline. (3) The pre-trained CellStateNet model weights. (4) A user-friendly web portal and API for querying the model. (5) A series of high-impact publications detailing the model and its applications.

Methods And Approach

Our methodology is structured into three synergistic aims, designed to create a robust, validated, and accessible foundational model. This project is exclusively computational and will synthesize existing public data, aligning perfectly with the research call.

**Aim 1: Community-Scale Data Aggregation, Harmonization, and Curation.**
This foundational aim addresses the critical challenge of data fragmentation. Our working group will develop a scalable, reproducible pipeline to process and unify the world's major cellular perturbation datasets.
*   **Data Sources:** We will integrate data from multiple modalities: 
    *   **Transcriptomics:** The LINCS L1000 dataset (~1.3M profiles of ~20,000 small molecules and ~20,000 genetic perturbations across ~100 cell lines) will form the core. We will supplement this with curated, large-scale perturbation studies from GEO and ArrayExpress, including single-cell datasets (e.g., sci-Plex, Perturb-seq) to capture cell-to-cell heterogeneity.
    *   **Functional Genomics:** Genome-scale CRISPR-Cas9 knockout screens from the DepMap and Achilles projects, providing fitness data for ~18,000 gene knockouts across >1000 cancer cell lines.
    *   **Proteomics & Phosphoproteomics:** Datasets from CPTAC detailing proteomic changes in response to drugs in cancer cell lines, and relevant studies from the PRIDE repository.
    *   **Epigenomics:** We will incorporate ATAC-seq and ChIP-seq data from ENCODE and GEO that profile chromatin accessibility and histone modifications following specific perturbations, providing another layer of cellular state information.
*   **Harmonization and Curation Pipeline:** This is a major undertaking requiring diverse expertise. (1) **Standardized Metadata:** We will develop a unified schema to capture critical metadata, mapping cell lines to Cellosaurus IDs, chemical compounds to PubChem/ChEMBL IDs, and genes/proteins to Ensembl/HGNC/UniProt IDs. (2) **Standardized Data Processing:** We will create and publish containerized (e.g., Docker/Singularity) workflows for processing each data type from its raw form to an analysis-ready matrix, ensuring reproducibility. For example, RNA-seq data will be uniformly processed through a Kallisto/STAR pipeline. (3) **Quality Control:** Automated QC metrics will be implemented to flag and potentially exclude low-quality samples, and computational methods like ComBat will be used to mitigate batch effects where appropriate. (4) **Unified Data Structure:** The final output will be a single, cohesive data object (e.g., stored in HDF5 or Zarr format) containing all data modalities and metadata, ready for model training.

**Aim 2: Development and Training of the CellStateNet Foundational Model.**
This aim constitutes the core technical innovation of the project.
*   **Model Architecture:** CellStateNet will be a multi-modal, deep generative model. Its architecture will consist of three main components:
    1.  **Modality-Specific Encoders:** Each data type (transcriptome, proteome, etc.) will be fed into a dedicated encoder (e.g., a Transformer or a Residual Network) that learns to project the high-dimensional input into a common, lower-dimensional latent space.
    2.  **Perturbation Encoder:** Perturbations will be encoded into the same latent space. Small molecules will be represented by their graph structure using a Graph Convolutional Network (GCN), while genetic perturbations will be represented by learned embeddings for each gene.
    3.  **Integration Core:** A central cross-attention-based Transformer will form the core of the model. It will take the latent representation of the baseline cellular state and the perturbation embedding as input and predict the latent representation of the post-perturbation state. This predicted latent state is then passed to modality-specific decoders to generate the final molecular profiles.
*   **Self-Supervised Training:** The model will be trained on a multi-faceted, self-supervised objective function without requiring labeled data. The primary task will be **cross-modal prediction**: given a cell's baseline state and a perturbation, the model will be trained to predict the post-perturbation state in one modality (e.g., proteomics) using the post-perturbation state from another modality (e.g., transcriptomics) as input. This forces the model to learn the intricate relationships between different molecular layers. We will supplement this with a contrastive loss to ensure that similar perturbations result in similar latent space trajectories.
*   **Computational Scale:** Training a model of this complexity on terabytes of data is computationally intensive and requires resources beyond a single lab. We will leverage national HPC resources or cloud computing platforms, requiring an estimated 500,000 GPU-hours on NVIDIA A100 or equivalent hardware. This demonstrates a clear need for NCEMS support.

**Aim 3: Rigorous Model Validation and Application for Hypothesis Generation.**
*   **Validation Strategy:** We will employ a multi-pronged validation approach. **Intrinsic validation** will assess the model's accuracy on its self-supervised training objectives using a held-out test set of perturbations and cell lines. **Extrinsic validation** will evaluate the model's performance on a series of predefined downstream tasks that it was not explicitly trained on, as detailed in RQ2. This includes predicting drug synergy, forecasting variant effects, and classifying drug mechanism-of-action.
*   **Application Case Studies:** To demonstrate the model's utility, we will conduct two in-depth case studies: (1) **Oncology:** We will perform a large-scale *in silico* screen for synergistic drug combinations to overcome resistance to KRAS inhibitors in lung adenocarcinoma models. (2) **Genetic Disease:** We will predict the molecular consequences of Variants of Uncertain Significance (VUS) in the CFTR gene, aiming to stratify them by their likely functional impact and identify potential patient-specific therapeutic avenues.

**Timeline and Milestones:**
*   **Year 1:** Finalize data sources; build and execute the data harmonization pipeline (Deliverable: Harmonized dataset v1.0). Develop and benchmark prototype model architectures.
*   **Year 2:** Complete the first full-scale training of CellStateNet v1.0. Perform intrinsic validation and begin extrinsic validation benchmarks. Develop the prototype web portal. (Deliverable: Pre-trained model v1.0 and associated open-source code).
*   **Year 3:** Refine model architecture and retrain (v2.0) based on validation results. Complete all extrinsic validation and application case studies. Finalize and launch the public web portal. Disseminate results through publications and conference presentations. (Deliverable: Final model, web portal, and primary manuscript).

Expected Outcomes And Impact

The CellStateNet project is poised to deliver transformative outcomes and exert a profound impact on the molecular and cellular biosciences, directly aligning with the goals of the NCEMS program to catalyze synthesis-driven discovery.

**Intended Contributions to the Field:**
Our primary contribution will be a paradigm shift in how cellular systems are modeled computationally. We will move the field from a collection of bespoke, task-specific models to a single, general-purpose foundational model. This is analogous to the impact of models like AlphaFold and GPT-3 in their respective domains. 
1.  **A Unified, Predictive Model of the Cell:** CellStateNet will be the first computational model to learn the relationships between diverse molecular layers (transcriptome, proteome, etc.) and perturbation types (chemical, genetic) from the ground up, using the entirety of the world's relevant public data. It will provide a quantitative, system-level framework for understanding how cellular networks process information and respond to stimuli.
2.  **A Powerful Engine for Hypothesis Generation:** By enabling rapid, large-scale *in silico* experimentation, CellStateNet will dramatically accelerate the scientific discovery cycle. Biologists will be able to prioritize experiments, screen vast combinatorial spaces of perturbations that are experimentally intractable, and generate novel, data-driven hypotheses about disease mechanisms and therapeutic interventions.
3.  **A Community-Wide Resource:** We are not just building a model; we are building an ecosystem. The project will deliver a meticulously harmonized database, a pre-trained model, open-source software, and an accessible web portal. This comprehensive suite of resources will empower researchers globally, regardless of their computational expertise, to leverage petabytes of public data to answer their own specific biological questions.

**Broader Impacts and Applications:**
The impact of CellStateNet will extend far beyond basic science, with significant potential for translational and clinical applications.
*   **Accelerating Therapeutic Development:** In pharmacology and drug discovery, CellStateNet can be used to predict the efficacy and off-target effects of novel compounds, identify synergistic drug combinations to combat drug resistance, and rationally design polypharmacology strategies. This could significantly reduce the cost and failure rate of preclinical drug development.
*   **Enabling Precision Medicine:** The model can be conditioned on specific genetic backgrounds, allowing for personalized predictions of a patient's response to various therapies based on their tumor's mutational profile or their personal genome. This is a critical step towards true data-driven precision medicine.
*   **Deciphering Complex Diseases:** For complex genetic diseases, CellStateNet can help elucidate the functional consequences of Variants of Uncertain Significance (VUS), providing a powerful tool for genetic diagnosis and for understanding the molecular etiology of disease.

**Dissemination, Data Sharing, and Open Science:**
Our working group is deeply committed to the principles of open, team, and reproducible science. All products of this research will be made promptly and freely available to the public.
*   **Publications:** We will target high-impact journals like *Nature*, *Science*, or *Cell* for the primary manuscript describing the model and its validation. Methodological advancements and specific applications will be published in leading specialized journals (*Nature Methods*, *Nature Biotechnology*).
*   **Open-Source Software:** All code for data processing, model training, and analysis will be version-controlled and hosted on GitHub under a permissive license (e.g., MIT).
*   **Open Data and Models:** The complete, harmonized dataset will be deposited in a permanent public repository (e.g., Zenodo). The final pre-trained model weights and an easy-to-use Python API will be released to facilitate fine-tuning and downstream applications by the community.
*   **Training and Outreach:** We will develop tutorials, documentation, and run workshops at major computational biology conferences (e.g., ISMB, RECOMB) to train the next generation of scientists in using CellStateNet. The creation of a public web portal is a key part of our strategy to ensure the model's accessibility to the entire biomedical community.

**Long-Term Vision and Sustainability:**
CellStateNet is envisioned not as a static, one-time product, but as a living resource for the community. The open-source framework we develop will be extensible, allowing for the future incorporation of new data modalities (e.g., metabolomics, spatial omics, high-content imaging) as they become widely available. The success of this project will establish a new standard for data synthesis in biology and will seed a vibrant ecosystem of follow-up research, where scientists around the world use, extend, and build upon our foundational model to probe countless biological questions.

Budget And Resources

The proposed research is a large-scale, computationally intensive effort that requires a diverse, collaborative team and significant resources beyond the capacity of a single institution. The budget reflects the personnel, computational power, and collaborative infrastructure necessary to achieve our ambitious goals over a three-year period. The total requested budget is $2,450,000.

**1. Personnel ($1,500,000):**
The success of this project hinges on assembling a team with deep, cross-disciplinary expertise. 
*   **Principal Investigators (3):** Requesting 1 month of summer salary per year for each of the three PIs to oversee the project, guide research directions, and manage the working group.
*   **Postdoctoral Fellows (3 FTEs):** We request support for three full-time postdocs who will be the primary drivers of the research. Their roles are specialized: 
    *   *Postdoc 1 (Machine Learning):* Will lead the design, implementation, and training of the deep learning architecture.
    *   *Postdoc 2 (Bioinformatics/Data Science):* Will lead the development and execution of the massive data aggregation and harmonization pipeline.
    *   *Postdoc 3 (Systems Biology/Validation):* Will lead the biological interpretation of the model, design and execute the validation benchmarks, and oversee the application case studies.
*   **Graduate Students (3 FTEs):** Support for three graduate students who will work closely with the postdocs on all aspects of the project, providing an unparalleled training opportunity.
*   **Software Engineer (1 FTE):** A dedicated software engineer is critical for a project of this scale to ensure the creation of robust, scalable, and maintainable code, manage the database infrastructure, and develop the public-facing web portal. This role is essential for the project's long-term impact and usability.

**2. Computational Resources ($600,000):**
Training a foundational model of this size is the primary cost driver and a key justification for NCEMS support.
*   **Cloud Computing/HPC Access:** We estimate the need for approximately 500,000 GPU-hours on high-end accelerators (e.g., NVIDIA A100/H100). This budget provides flexibility to use commercial cloud providers (e.g., AWS, GCP) or purchase dedicated time on national supercomputing clusters. This cost is prohibitive for standard research grants.
*   **Data Storage:** Funds are allocated for robust, long-term storage of terabytes of raw, processed, and harmonized data, as well as model checkpoints and outputs.

**3. Travel ($150,000):**
Collaboration is central to the working group model.
*   **Working Group Meetings:** Funds to support twice-yearly, in-person meetings for the entire team to facilitate deep collaboration, brainstorming, and project planning.
*   **Conference Travel:** Support for all trainees and key personnel to attend and present their work at one major international conference per year (e.g., ISMB, NeurIPS, AACR), ensuring broad dissemination of our findings.

**4. Other Direct Costs ($50,000):**
*   **Publication Fees:** Budgeted to cover open-access publication costs in high-impact journals.
*   **Software Licenses & Subscriptions:** For any specialized software required for data analysis or visualization.

**5. Indirect Costs (F&A):**
Indirect costs are calculated based on the negotiated rates of the participating institutions and are included in the total budget request. 

**Justification for NCEMS Support:**
This project is uniquely suited for the NCEMS working group mechanism. The required synthesis of disparate, community-scale datasets, the immense computational resources needed for model training, and the essential collaboration between experts in machine learning, bioinformatics, and systems biology create a project that is impossible for a single lab or a small collaboration to undertake. NCEMS support is critical to provide the necessary computational infrastructure, personnel coordination, and collaborative environment to build a truly foundational resource for the entire biosciences community.",,
ai_group_int_gemini_07,ai,group_int,gemini-2.5-pro,The Living Material: A Data-Driven Framework for Understanding the Emergent Mechanics of the Cytoskeleton,"The cell's ability to maintain its shape, move, and divide is an emergent property of the cytoskeleton, a dynamic network of protein polymers, motors, and cross-linkers. While we understand the individual components, how they collectively generate robust, cell-scale mechanical behavior remains a key question. This project will bridge the gap between molecular components and cellular mechanics by creating a multi-scale, data-driven modeling framework. We will form a working group of biophysicists, materials scientists, cell biologists, and computer vision experts to synthesize diverse public data types. We will integrate 3D ultrastructural data of cytoskeletal networks from cryo-electron tomography (EMPIAR), dynamic live-cell imaging data of cytoskeletal components (IDR), quantitative proteomics of cytoskeleton-associated proteins (PRIDE), and cell mechanics measurements from atomic force microscopy (AFM) repositories. Our innovative approach involves using AI-powered segmentation to extract detailed architectural parameters (e.g., filament length, orientation, cross-link density) from tomograms. These structural 'ground truths' will be correlated with protein composition from proteomics data. This integrated information will then be used to parameterize and validate a new generation of agent-based and continuum mechanics models that predict emergent properties like cell stiffness, viscosity, and force generation. This multi-modal data integration and multi-scale modeling is far beyond the capabilities of a single lab. The project will deliver a powerful, predictive model of the cell as a 'living material,' enabling us to understand how molecular-level mutations found in diseases like cancer and muscular dystrophy lead to defects in emergent cellular mechanics. All models and integrated datasets will be shared openly, providing a foundational resource for the mechanobiology community.",,"Background And Significance

The cytoskeleton is a complex, dynamic network of protein filaments—actin, microtubules, and intermediate filaments—that permeates the cytoplasm of eukaryotic cells. This intricate scaffold is not static; it is an active, living material, constantly remodeled by a host of associated proteins, including molecular motors that generate force and cross-linkers that modulate network architecture. The emergent mechanical properties of this network govern fundamental cellular processes, from cell division and migration to mechanotransduction and tissue morphogenesis. A central challenge in cell biology is to understand how the collective interactions of these molecular components give rise to the robust, adaptable mechanical behavior observed at the cellular scale. For decades, research has largely followed two parallel tracks. On one hand, cell biologists have used fluorescence microscopy to visualize the dynamics of specific proteins, providing crucial insights into their localization and function. On the other hand, biophysicists have employed techniques like atomic force microscopy (AFM) and optical tweezers to probe the mechanical properties of cells and reconstituted cytoskeletal networks. Seminal work on reconstituted actin networks, for example, has revealed how cross-linker density and filament length control the viscoelastic properties of the material, establishing the cytoskeleton as a prime example of soft condensed matter. Similarly, theoretical frameworks such as active gel theory have provided powerful phenomenological models that capture the behavior of the cytoskeleton as a fluid-like material driven by internal stresses generated by motor proteins. However, a significant gap persists between our understanding of individual components and the integrated, emergent mechanics of the cell. Models are often parameterized using data from simplified in vitro systems, which lack the molecular crowding, spatial confinement, and compositional complexity of the native cellular environment. While techniques like super-resolution microscopy have improved our view of the cytoskeleton, they often lack the 3D isotropic resolution and molecular identification needed to map the complete network architecture. Cryo-electron tomography (cryo-ET) has emerged as a transformative technology, providing nanometer-resolution 3D snapshots of the cytoskeleton within vitrified, near-native state cells. These tomograms contain a wealth of architectural information—filament lengths, orientations, branching angles, and cross-linker distributions—that has been largely untapped due to the immense challenge of manual segmentation and analysis. The recent explosion of publicly available data presents an unprecedented opportunity to bridge this gap. Large-scale repositories like the Electron Microscopy Public Image Archive (EMPIAR), the Image Data Resource (IDR), and the PRoteomics IDEntifications (PRIDE) database now house a critical mass of ultrastructural, dynamic, and compositional data from a wide range of cell types and conditions. This project is timely because recent advances in artificial intelligence, particularly deep learning-based image segmentation, provide the necessary tools to automate the extraction of quantitative architectural information from cryo-ET data at scale. By synthesizing these disparate data modalities—ultrastructure, composition, dynamics, and mechanics—we can construct a new generation of data-driven, multi-scale models. Such a framework is essential for addressing fundamental questions in mechanobiology and disease. For instance, in cancer, altered cell mechanics are a hallmark of metastasis, but the underlying changes in cytoskeletal architecture and composition are poorly understood. Similarly, many congenital myopathies and cardiomyopathies are caused by mutations in cytoskeletal proteins, but how these molecular defects propagate to cause tissue-level mechanical failure remains unclear. This proposal outlines a collaborative, transdisciplinary effort to build a predictive framework of the cell as a living material, directly linking molecular-level details to emergent cellular function and dysfunction.

Research Questions And Hypotheses

This project is guided by the overarching question: How do the specific 3D architecture, molecular composition, and dynamic remodeling of the cytoskeleton collectively determine the emergent mechanical properties of the cell? To deconstruct this complex problem, we have formulated four specific, interconnected research questions (RQs) and their corresponding testable hypotheses (H). 

RQ1: What are the fundamental quantitative principles governing the 3D architecture of distinct cytoskeletal assemblies (e.g., lamellipodial actin networks, stress fibers, cortical actin) in their native cellular context? 
H1: We hypothesize that different functional assemblies of the cytoskeleton are characterized by unique and quantifiable architectural signatures. Specifically, we predict that lamellipodial networks will exhibit a high density of short, branched filaments with a narrow orientation distribution, consistent with dendritic nucleation, whereas stress fibers will be defined by long, aligned filaments with a high density of bundling cross-linkers. We further hypothesize that these architectural parameters are not random but follow specific statistical distributions (e.g., exponential length distributions) that reflect the underlying polymerization and severing dynamics. This hypothesis will be tested by applying our AI-powered segmentation pipeline to public cryo-ET tomograms of various cell types and subcellular regions, extracting a multi-dimensional vector of architectural features (filament density, length distribution, orientation tensor, cross-link density, mesh size) for each region. 

RQ2: How does the local proteomic composition of the cytoskeleton correlate with, and potentially determine, its local 3D architecture? 
H2: We hypothesize that a direct, predictive relationship exists between the relative abundance of key cytoskeleton-associated proteins and the observed network architecture. For example, we predict that the local ratio of Arp2/3 complex to formins, inferred from whole-cell proteomics data, will strongly correlate with the degree of filament branching versus alignment observed in cryo-ET. Similarly, the ratio of bundling proteins (e.g., fascin, alpha-actinin) to cross-linking proteins (e.g., filamin) will correlate with the prevalence of filament bundles versus isotropic networks. We will test this by integrating quantitative proteomics data (from PRIDE) with our architectural measurements from cryo-ET for matched cell types. We will build a statistical correlation model to identify key protein determinants of specific architectural motifs. 

RQ3: Can a multi-scale computational model, parameterized directly with integrated architectural and proteomic data, accurately predict the emergent, cell-scale mechanical properties measured experimentally? 
H3: We hypothesize that an agent-based model (ABM) of the cytoskeleton, whose parameters (e.g., filament lengths, cross-linker densities, motor protein concentrations) are directly sampled from the distributions derived from cryo-ET and proteomics data, will quantitatively reproduce the bulk mechanical responses (e.g., Young's modulus, viscoelastic loss and storage moduli) of cells as measured by AFM. We predict that our data-driven model will outperform models parameterized with generic or in vitro-derived values. Validation will be achieved by simulating AFM indentation on our virtual cytoskeletons and comparing the resulting force-displacement curves with publicly available experimental data for the same cell types. The model's success will be quantified by the root-mean-square error between simulated and experimental results. 

RQ4: How do disease-associated perturbations in cytoskeletal proteins alter network architecture and, consequently, the emergent cellular mechanics? 
H4: We hypothesize that our validated framework can predict the mechanical consequences of molecular perturbations. Specifically, we predict that simulating a cancer-associated upregulation of the severing protein cofilin (by decreasing the average filament length in the ABM's parameter set) will result in a significant decrease in predicted cell stiffness, consistent with the known phenotype of increased deformability in metastatic cells. Conversely, simulating a mutation that impairs the function of a cross-linking protein implicated in muscular dystrophy will lead to a predicted decrease in the network's ability to bear sustained loads. These predictions will be tested against public AFM datasets from cell lines expressing these specific mutations, providing a powerful validation of our model's predictive capability. 

Expected Deliverables: The primary deliverables will be: (1) A comprehensive, integrated database linking quantitative cytoskeletal architectures with proteomic profiles and mechanical properties for multiple cell types. (2) A suite of open-source, AI-driven software tools for segmenting and analyzing cytoskeletal networks in cryo-ET data. (3) A validated, multi-scale, and open-source computational model of the cytoskeleton that serves as a community resource. (4) A set of predictive 'phase diagrams' that map molecular composition and architecture to emergent mechanical phenotypes.

Methods And Approach

This project is a community-scale synthesis effort that requires a transdisciplinary working group and is organized into three synergistic phases. Our team comprises experts in cell biology (Dr. Lena Weber, microscopy data interpretation), biophysics (Dr. David Chen, modeling lead), computer science (Dr. Aisha Khan, AI/ML lead), materials science (Dr. Samuel Jones, continuum mechanics), proteomics (Dr. Maria Garcia), cryo-electron tomography (Dr. Ben Carter), and data science (Dr. Emily Sato), ensuring comprehensive expertise.

**Phase 1: Multi-Modal Data Curation, Integration, and Feature Extraction (Months 1-15)**
This foundational phase focuses on aggregating and harmonizing diverse public datasets. 
*   **Data Sources:** We will systematically mine major public repositories. 
    *   **Ultrastructure (Cryo-ET):** We will source tomograms from EMPIAR, targeting high-resolution datasets of intact lamellipodia, stress fibers, and cortical regions from cell types like HeLa (e.g., EMPIAR-10491), U2OS, and primary neurons (e.g., EMPIAR-10164). We will select a minimum of 20 high-quality tomograms per cell type/condition to ensure statistical power.
    *   **Proteomics (Composition):** We will query the PRIDE archive for deep, quantitative mass spectrometry datasets from matching cell lines (e.g., PXD028842 for U2OS cells). We will focus on datasets that provide absolute or relative quantification of cytoskeletal and associated proteins.
    *   **Mechanics (Properties):** We will collate AFM indentation data from repositories like BioStudies, Dryad, and the AFM Data Bank. We will target datasets that provide force-indentation curves or reported Young's moduli for our chosen cell types under comparable culture conditions.
    *   **Dynamics (Live-cell Imaging):** We will use data from the Image Data Resource (IDR) (e.g., idr0075) showing dynamics of fluorescently-tagged cytoskeletal components. This data will not be used for initial parameterization but will be crucial for validating the dynamic aspects of our models in Phase 3 (e.g., turnover rates, filament growth speeds).
*   **Data Harmonization:** A critical task will be to develop a unified metadata schema to link these disparate datasets. We will create a relational database that maps datasets by cell line, genotype, and experimental conditions, enabling robust cross-modal analysis.
*   **AI-Powered Segmentation and Analysis:** This is the core innovation of Phase 1. We will develop a deep learning pipeline for automated segmentation of cytoskeletal elements from cryo-ET volumes. We will employ a 3D U-Net architecture, a type of convolutional neural network (CNN) adept at biomedical image segmentation. The model will be trained on a 'ground truth' dataset of ~5-10 tomograms manually segmented by our cryo-ET expert. The trained model will then be applied to the full dataset to segment filaments (actin, microtubules), cross-links, and other structures. From these segmented volumes, a custom Python-based analysis toolkit will extract a rich set of quantitative architectural parameters: filament length and tortuosity distributions, orientation tensors, branching densities and angles, cross-linker/bundler densities, and network mesh size distributions.

**Phase 2: Multi-Scale Computational Modeling (Months 12-27)**
Using the quantitative data from Phase 1, we will develop a hierarchical modeling framework.
*   **Agent-Based Model (ABM):** At the microscale, we will use the open-source platform CytoSim to build a detailed ABM. Individual filaments will be modeled as semi-flexible polymers. Cross-linkers and motors will be modeled as two-headed spring-like elements with stochastic binding/unbinding kinetics. Crucially, instead of using generic parameters, the model will be instantiated by directly sampling from the statistical distributions of architectural parameters extracted in Phase 1. For example, the initial filament lengths in the simulation will be drawn from the experimentally measured length distribution. The concentrations of different agents (e.g., fascin vs. filamin) will be set by the relative abundances measured in the proteomics data.
*   **Continuum Mechanics Model:** To bridge to the cellular scale, we will develop a coarse-grained continuum model based on active gel theory. This model describes the cytoskeleton as a viscoelastic material with internal stresses. The key innovation is that the phenomenological parameters of this continuum model (e.g., elastic modulus, viscosity, active stress coefficient) will not be arbitrarily fitted. Instead, they will be systematically derived by performing numerical homogenization on large-scale simulations of the ABM. This creates a direct, mechanistic link between the microscale architecture and the macroscale material properties.

**Phase 3: Model Validation, Prediction, and Dissemination (Months 24-36)**
*   **Validation:** The framework's predictive power will be rigorously tested. We will simulate AFM indentation in both the ABM and continuum models. The resulting force-indentation curves and calculated Young's moduli will be compared directly against the experimental AFM data curated in Phase 1. We will also validate dynamic aspects by simulating processes like fluorescence recovery after photobleaching (FRAP) and comparing the recovery timescales to experimental data from the IDR.
*   **Predictive Simulations:** Once validated, the model will be used to test hypotheses about disease states (as outlined in RQ4). We will systematically alter specific model parameters to mimic known mutations (e.g., change cross-linker binding affinity, alter motor protein velocity) and predict the resulting changes in network architecture and bulk mechanics.
*   **Timeline and Milestones:**
    *   M6: Curation and harmonization of initial 20 datasets across four modalities.
    *   M12: Version 1.0 of the AI segmentation pipeline released on GitHub.
    *   M18: Complete architectural parameter database for three cell types.
    *   M24: First validated ABM for a single cell type, linking architecture to mechanics.
    *   M30: Continuum model parameterized and validated; predictive simulations of disease mutations initiated.
    *   M36: Final integrated database, all models, and analysis tools publicly released with comprehensive documentation and tutorials.

Expected Outcomes And Impact

This project will generate transformative outcomes and have a profound impact on the fields of cell biology, biophysics, and computational biology. Our contributions are designed to be foundational, enabling new avenues of research for the entire scientific community.

**Intellectual Merit and Contributions to the Field:**
1.  **A First-of-its-Kind Integrated Data Resource:** The primary outcome will be a publicly accessible, cross-modal database that quantitatively links 3D cytoskeletal ultrastructure, proteomic composition, and cell mechanical properties. This resource will be unique in its scope and level of detail, moving beyond qualitative descriptions to provide a quantitative, multi-scale view of the cell's mechanical machinery. It will serve as a gold-standard benchmark for the community, enabling researchers to formulate and test new hypotheses without the need for new data generation.
2.  **Novel, Open-Source Analytical Tools:** We will develop and disseminate a powerful, AI-driven software pipeline for the automated segmentation and quantitative analysis of cryo-ET data. This will break a major bottleneck in the field, democratizing the ability to extract rich architectural information from complex 3D imaging data. By making this tool open-source and providing extensive documentation, we will empower other researchers to apply it to their own data, vastly accelerating discovery in structural cell biology.
3.  **A Predictive, Multi-Scale Model of Cellular Mechanics:** Our core scientific contribution will be the creation of a validated, data-driven modeling framework that mechanistically connects molecular-level details to emergent, cell-scale mechanical behavior. This moves the field beyond correlative studies and phenomenological models, providing a truly predictive tool. This 'virtual cytoskeleton' will allow researchers to perform in silico experiments that are difficult or impossible to conduct in living cells, such as systematically titrating the concentration of a specific protein or altering its biophysical properties to observe the impact on cell stiffness or force generation.
4.  **Fundamental Insights into Cytoskeletal Design Principles:** By analyzing the relationships between composition, architecture, and mechanics across different cell types and functional assemblies, our project will uncover fundamental 'design principles' of the cytoskeleton. We expect to reveal how cells robustly tune their mechanical properties by modulating specific architectural features, providing deep insights into the evolution and function of this essential cellular system.

**Broader Impacts and Applications:**
*   **Biomedical Relevance:** The framework will have direct implications for understanding human disease. By modeling how mutations found in cancers, muscular dystrophies, or cardiomyopathies alter the cytoskeleton's mechanical properties, our work will provide a mechanistic basis for disease pathology. This could ultimately inform the development of novel diagnostics based on cell mechanics or therapeutics that target the cytoskeleton.
*   **Training the Next Generation:** This project is intrinsically designed to train graduate students and postdocs at the interface of biology, physics, and computer science. Trainees will gain invaluable hands-on experience in data synthesis, machine learning, computational modeling, and collaborative, open-team science. We will host an annual virtual 'data-thon' for the broader community, providing training on our tools and fostering a data-savvy workforce, directly aligning with the research call's goals.
*   **Advancing Open Science:** We are fully committed to open science principles. All curated data, analysis code, simulation models, and results will be made immediately and freely available through established repositories (e.g., GitHub, Zenodo, EMPIAR-S). This commitment ensures our outputs are a lasting community resource that is Findable, Accessible, Interoperable, and Reusable (FAIR).
*   **Dissemination and Community Engagement:** We will disseminate our findings through high-impact publications, presentations at international conferences (e.g., ASCB, Biophysical Society), and a dedicated project website with tutorials and documentation. We will organize workshops at conferences to train researchers in the use of our tools, ensuring broad adoption and maximizing the project's impact.

**Long-Term Vision:** This project will establish a durable, collaborative network and a foundational framework that is extensible. In the future, this 'living material' model can be expanded to incorporate other cellular components, such as the cell membrane and nucleus, and other physical phenomena, like mechanochemical signaling. It will serve as a cornerstone for a future 'whole-cell' mechanical model, transforming our ability to understand and engineer cellular behavior.

Budget And Resources

The proposed research is a large-scale data synthesis and modeling effort that relies on personnel with diverse expertise rather than new equipment. The budget is designed to support a collaborative, distributed working group over a 3-year period. The requested funds are essential and beyond the scope of a single investigator grant, reflecting the community-scale nature of the project as specified in the research call.

**1. Personnel (Total: $XXX,XXX)**
This is the largest budget category, reflecting the project's focus on data analysis, software development, and computational modeling.
*   **Postdoctoral Fellows (2.0 FTE/year):** We request support for two full-time postdocs. 
    *   Postdoc 1 (Computer Science/AI): Will lead the development, training, and validation of the AI segmentation pipeline and the architectural analysis toolkit. 
    *   Postdoc 2 (Biophysics/Modeling): Will lead the development, parameterization, and validation of the agent-based and continuum mechanics models.
*   **Graduate Students (3.0 FTE/year):** Support for three graduate students is requested. They will be embedded with different PIs and will work on specific aspects of the project, such as data curation, model validation, and applying the framework to specific biological questions, ensuring they receive interdisciplinary training.
*   **Data Manager / Software Engineer (0.5 FTE/year):** A part-time professional is crucial for maintaining the integrated database, ensuring data integrity and FAIR compliance, and managing the open-source code repositories and documentation. This ensures the long-term sustainability of the project's outputs.
*   **Principal Investigators (1.0 summer month/year for 3 PIs):** Summer salary is requested for the three lead PIs to dedicate protected time for project management, scientific oversight, and trainee mentorship during the summer months.

**2. Travel ($XX,XXX)**
*   **Annual Working Group Meeting:** Funds are allocated for one in-person, 3-day meeting per year for all PIs, postdocs, and students. This is critical for fostering collaboration, resolving technical challenges, and strategic planning.
*   **Conference Travel:** Funds are included for each trainee and one PI to attend one major international conference (e.g., ASCB, BPS) per year to present their work, disseminate findings, and receive feedback from the community.

**3. Training and Dissemination ($XX,XXX)**
*   **Annual Workshop/Data-thon:** We request funds to host one virtual, 2-day workshop per year. This event will be open to the broader research community and will provide hands-on training for using our software tools and models, directly addressing the call's goal of training a data-savvy workforce.
*   **Publication Costs:** Funds are budgeted to cover open-access publication fees in high-impact journals, ensuring our findings are freely accessible to all.

**4. Computational Resources ($XX,XXX)**
*   **Cloud Computing:** We request funds for cloud computing services (e.g., AWS or Google Cloud Platform). These resources are essential for training the large deep learning models on GPU clusters and for running the thousands of CPU-hours required for the large-scale agent-based simulations.

**5. Indirect Costs (F&A) ($XXX,XXX)**
Indirect costs are calculated based on the federally negotiated rates for each participating institution.

**Justification for NCEMS Support:** The proposed project is impossible for a single lab or a small collaboration to undertake. It requires the deep integration of expertise from cell biology, computer vision, proteomics, and theoretical physics. The sheer scale of data curation, the development of novel AI tools, and the creation of a community-wide modeling platform necessitate the coordinated, resource-intensive working group structure that this NCEMS program is uniquely designed to support.",,
ai_group_int_gemini_08,ai,group_int,gemini-2.5-pro,Decoding the Chemical Lexicon of the Gut Microbiome: Emergent Host-Microbe Communication,"The gut microbiome profoundly influences host health and disease through a complex chemical dialogue, yet the 'words' of this language—the specific microbial molecules and their effects on host cells—are largely unknown. This project aims to systematically decode this chemical lexicon by integrating public metagenomic, metabolomic, and host transcriptomic data. Our multidisciplinary working group will include microbial ecologists, chemists, bioinformaticians, and immunologists. We will synthesize three key data types: 1) metagenomic data from SRA and MG-RAST to identify biosynthetic gene clusters (BGCs) within microbial genomes, predicting their chemical products; 2) untargeted metabolomics data from MetaboLights and GNPS to identify the actual small molecules present in the gut; and 3) host transcriptomic and proteomic data (GEO, TCGA) from relevant clinical and gnotobiotic animal studies. The central innovation is a novel computational pipeline that probabilistically links BGCs to observed metabolites and correlates the presence of these microbe-derived molecules with specific host gene expression signatures in intestinal and immune cells. Using network inference and machine learning, we will construct a 'Microbe-Metabolite-Host Gene' atlas. This will allow us to understand how community-level changes in the microbiome's genetic potential lead to an emergent chemical environment that, in turn, orchestrates host physiology. The scale of data integration and the need for expertise spanning from microbial genetics to host immunology make this a quintessential community-scale project. The resulting 'Chemical Lexicon Atlas' will be a publicly accessible resource to generate hypotheses about how microbial metabolites drive emergent states of health and diseases like inflammatory bowel disease and colorectal cancer, paving the way for next-generation microbiome-based therapeutics.",,"Background And Significance

The community of microorganisms residing in the human gut, collectively known as the microbiome, functions as a complex, adaptive ecosystem that is integral to host physiology. This intricate relationship is not merely commensal; it is a deeply symbiotic partnership where host health emerges from a dynamic interplay of microbial community structure, metabolic function, and host response. The primary language of this host-microbe dialogue is chemical. Microbes produce a vast and diverse arsenal of small molecules—from simple short-chain fatty acids (SCFAs) to complex polyketides and non-ribosomal peptides—that directly interact with host cells, shaping mucosal immunity, metabolic regulation, and even neurobehavioral development. Landmark studies have identified key examples of this chemical communication. For instance, the SCFA butyrate, produced by Firmicutes bacteria, serves as a primary energy source for colonocytes and acts as a histone deacetylase (HDAC) inhibitor, epigenetically regulating host gene expression to promote anti-inflammatory responses and fortify the gut barrier. Similarly, secondary bile acids, produced through multi-step microbial transformations, act as signaling molecules for nuclear receptors like FXR, influencing lipid metabolism and innate immunity. Despite these foundational discoveries, our understanding of this chemical lexicon remains rudimentary. The known microbial metabolites represent a tiny fraction of the chemical space within the gut. Untargeted metabolomics studies consistently reveal thousands of molecular features in fecal and intestinal samples, the vast majority of which are of unknown structure and origin—the 'dark matter' of the gut metabolome. Concurrently, advances in genomics have unveiled the immense biosynthetic potential encoded within microbial genomes. Computational tools like antiSMASH can predict tens of thousands of unique biosynthetic gene clusters (BGCs) from public metagenomic data, each potentially producing a novel bioactive molecule. This creates two major knowledge gaps that impede progress in the field. The first is the 'gene-to-metabolite' gap: we cannot systematically link the predicted BGCs to the molecules they produce in the complex in vivo environment. The second is the 'metabolite-to-function' gap: for the thousands of unknown molecules we can detect, we lack any knowledge of their biological effects on the host. Current research efforts typically focus on one aspect of this triad—microbial composition, metabolomic profiles, or host gene expression—in isolation. While multi-omics studies are becoming more common, they often rely on simple correlation analyses between a few data types and are limited to the scale of a single lab or cohort. A systematic, integrative approach to connect all three layers—microbial genetic potential, the resulting chemical environment, and the host's cellular response—has not been undertaken at a community-wide scale. This project is both important and timely because we are at a unique inflection point. The exponential growth of publicly available data in repositories like NCBI's Sequence Read Archive (SRA), the MetaboLights database, and the Gene Expression Omnibus (GEO) has created an unprecedented, yet underutilized, resource. Simultaneously, advances in machine learning, network theory, and computational chemistry provide the necessary tools to synthesize these disparate datasets. By framing host physiology as an emergent property of the microbiome's collective chemical output, we can move beyond descriptive cataloging of microbes. This research will establish a new paradigm for understanding host-microbe symbiosis, treating it as a complex system where community-level microbial genetics gives rise to a chemical milieu that orchestrates host cellular behavior. Decoding this lexicon is critical for translating microbiome science into predictable, mechanism-based therapeutics for a wide range of human diseases, including inflammatory bowel disease (IBD), colorectal cancer (CRC), metabolic syndrome, and autoimmune disorders.

Research Questions And Hypotheses

This project is organized around three central aims, each designed to bridge a critical gap in our understanding of host-microbe chemical communication. By systematically integrating public multi-omics data, we will construct a comprehensive atlas that maps the flow of information from microbial genes to microbial chemicals to host cellular responses, thereby elucidating the emergent properties of this complex system.

**Aim 1: To construct a probabilistic linkage map between microbial biosynthetic gene clusters (BGCs) and observed gut metabolites.**
This aim addresses the fundamental 'gene-to-metabolite' gap. While we can predict the biosynthetic potential of the microbiome, we lack a systematic method to identify the actual molecular products in vivo.
*   **Research Question 1.1:** Can we statistically associate the presence and abundance patterns of specific BGCs across thousands of public metagenomes with the presence and abundance of specific metabolite features in corresponding public metabolomes?
*   **Hypothesis 1.1:** The co-occurrence patterns of a BGC and its true metabolic product across diverse human cohorts will exhibit a significantly stronger statistical dependency than random BGC-metabolite pairs. We hypothesize that a multi-modal machine learning framework, integrating correlation metrics, mutual information, and probabilistic graphical models, can identify these true pairings with a quantifiable confidence score.
*   **Testing and Validation:** We will test this hypothesis by first applying our pipeline to a 'gold standard' set of known BGC-metabolite pairs (e.g., those for butyrate synthesis, secondary bile acid conversion, salinomycin). Our model must recover these known links with high probability. For novel predictions, we will perform in silico validation by comparing the predicted elemental formula from the BGC's biosynthetic pathway with the formula derived from high-resolution mass spectrometry data of the linked metabolite. The expected outcome is a comprehensive, weighted BGC-Metabolite map, representing the first large-scale dictionary linking microbial genetic potential to chemical output.

**Aim 2: To identify host cellular pathways that are significantly modulated by specific microbe-derived metabolites.**
This aim tackles the 'metabolite-to-function' gap by connecting the chemical environment of the gut to the host's physiological response at the molecular level.
*   **Research Question 2.1:** Which host gene expression signatures in intestinal epithelial cells and gut-resident immune cells consistently co-vary with the abundance of specific microbial metabolites across diverse health and disease states (e.g., IBD vs. healthy, CRC vs. healthy)?
*   **Hypothesis 2.1:** Specific microbial metabolites, or structurally related families of metabolites, will be significantly and reproducibly correlated with the coordinated up- or down-regulation of distinct host biological pathways. For example, we predict that a class of unidentified polyketides will correlate with the activation of the aryl hydrocarbon receptor (AHR) pathway, while certain bacterially-modified lipids will correlate with inflammasome activation signatures in host immune cells.
*   **Testing and Validation:** We will leverage public datasets from gnotobiotic mouse experiments, where the host is colonized with defined microbial communities, as a validation cohort. In these controlled systems, the links between specific microbes, their metabolites, and host response are less confounded. We will test if the metabolite-pathway associations discovered in human data can predict host gene expression changes in these gnotobiotic models. Furthermore, we will cross-reference our findings with pharmacological databases (e.g., STITCH, DrugBank) to see if our predicted bioactive metabolites are structurally similar to known drugs that target the implicated host pathways.

**Aim 3: To build and query an integrated 'Microbe-Metabolite-Host Gene' atlas to model emergent host phenotypes.**
This aim synthesizes the findings from Aims 1 and 2 into a unified, multi-layered network model to understand the system-level properties of host-microbe communication.
*   **Research Question 3.1:** Can a multi-scale network model, representing the flow of information from microbial community genetics to the chemical environment and ultimately to the host's transcriptional state, reveal principles of emergent host-microbe homeostasis and dysbiosis?
*   **Hypothesis 3.1:** The topology of the integrated network will reveal 'keystone' metabolites and 'communication hubs'—molecules that are central to mediating the microbiome's influence on the host. We hypothesize that the state of this integrated network, captured using graph-based machine learning features, will be more predictive of host phenotype (e.g., IBD diagnosis or treatment response) than any single data modality alone.
*   **Testing and Validation:** The predictive power of our integrated atlas will be rigorously tested on held-out datasets not used in model construction. We will build machine learning classifiers (e.g., Random Forest, Gradient Boosting, Graph Convolutional Networks) to predict disease status using features derived from: (1) metagenomics only, (2) metabolomics only, (3) host transcriptomics only, and (4) our integrated network. Our hypothesis will be supported if the integrated model demonstrates significantly superior performance (e.g., higher AUROC and AUPRC). The primary deliverable will be the 'Chemical Lexicon Atlas,' an open-access, interactive web portal for the scientific community to explore these multi-omic relationships.

Methods And Approach

**Data Acquisition, Curation, and Harmonization**
This project exclusively utilizes publicly available data. Our first task is to build a comprehensive, curated, and harmonized data repository. 
*   **Data Sources:** We will systematically query major public repositories including NCBI Sequence Read Archive (SRA) and MG-RAST for metagenomic data; MetaboLights, GNPS, and Metabolomics Workbench for untargeted mass spectrometry-based metabolomics data; and Gene Expression Omnibus (GEO), ArrayExpress, and The Cancer Genome Atlas (TCGA) for host transcriptomic data (RNA-seq). We will prioritize studies with multiple data types from the same subjects, such as the Human Microbiome Project (HMP) and the Inflammatory Bowel Disease Multi'omics Database.
*   **Inclusion Criteria:** We will select datasets based on rigorous criteria: (1) availability of raw data (e.g., FASTQ, mzML); (2) sufficient sequencing depth or instrument resolution; (3) comprehensive and standardized metadata (e.g., diagnosis, sample type, age, sex); (4) studies focusing on human gut samples or relevant gnotobiotic mouse models. We anticipate aggregating over 10,000 metagenomes, 5,000 metabolomes, and 5,000 transcriptomes.
*   **Standardized Re-processing:** To mitigate batch effects and ensure comparability, all raw data will be re-processed through standardized, containerized (Docker/Singularity) pipelines. Metagenomic reads will be quality-controlled with FastQC/Trimmomatic and assembled with MEGAHIT. Metabolomic data will be processed using a uniform MZmine 2 workflow for feature detection and alignment. RNA-seq data will be processed via a STAR/RSEM pipeline for alignment and quantification.

**Aim 1: BGC-Metabolite Linkage Pipeline**
1.  **BGC Prediction and Grouping:** Assembled metagenomic contigs will be analyzed with antiSMASH (v6.0) to identify BGCs. The predicted BGCs will then be grouped into Gene Cluster Families (GCFs) using BiG-SCAPE, which compares BGCs based on domain architecture and sequence similarity. This reduces the complexity from millions of individual BGCs to tens of thousands of GCFs. The output will be a sample-by-GCF abundance matrix.
2.  **Metabolite Feature Networking:** Processed metabolomic features will be organized using molecular networking on the GNPS platform. This method groups ions with similar fragmentation spectra (MS/MS), which often correspond to structurally related molecules. This creates a sample-by-metabolite feature/family abundance matrix.
3.  **Probabilistic Association:** We will employ a multi-pronged statistical approach to link the GCF and metabolite matrices. We will first use Sparse Canonical Correlation Analysis (sCCA) to identify latent variables that explain the maximum shared variance between the two datasets. In parallel, we will use tools like MIMOSA2, which models the conditional probability of a metabolic pathway's (or GCF's) activity given the community composition. Finally, we will construct a probabilistic graphical model to infer direct dependencies. The consensus from these methods will generate a high-confidence list of BGC-metabolite links, each with a quantitative confidence score.

**Aim 2: Metabolite-Host Pathway Correlation Pipeline**
1.  **Host Transcriptome Analysis:** For samples with paired metabolomics and host transcriptomics, we will perform differential expression analysis and Gene Set Enrichment Analysis (GSEA) to identify host pathways that are active in specific conditions. 
2.  **Multi-Omic Correlation:** We will use the WGCNA (Weighted Gene Co-expression Network Analysis) R package to identify modules of highly co-expressed host genes. The eigengene (first principal component) of each module, which represents the module's overall expression pattern, will then be correlated with the abundance of every microbial metabolite feature. This powerful approach reduces multiple testing burdens and identifies metabolites associated with entire biological processes rather than single genes. We will use linear mixed-effects models to account for covariates and repeated measures where applicable.

**Aim 3: Atlas Construction and Predictive Modeling**
1.  **Network Integration:** The results from Aims 1 and 2 will be integrated into a multi-partite graph using network analysis libraries (e.g., NetworkX in Python). The graph will contain three types of nodes (GCFs, Metabolites, Host Genes/Pathways). Edges will connect nodes based on the analyses above, weighted by the statistical significance (e.g., correlation coefficient, probability score) of the association.
2.  **Topological Analysis and Hypothesis Generation:** We will analyze the network's structure to identify key features. Centrality algorithms (e.g., betweenness, degree) will pinpoint 'keystone' metabolites that bridge many microbes to many host functions. Community detection algorithms (e.g., Louvain modularity) will reveal functional modules, such as a group of microbes producing a class of metabolites that collectively modulate a specific immune pathway.
3.  **Graph-Based Machine Learning:** To test the predictive power of the atlas, we will use Graph Convolutional Networks (GCNs). A GCN is a type of neural network that operates directly on graphs, learning to classify nodes or entire graphs. We will train a GCN to classify samples as 'healthy' or 'diseased' (e.g., IBD) using the integrated network structure and node abundances as input. The model's performance will be evaluated on a held-out test set and compared against baseline models using single data types.

**Timeline and Milestones**
*   **Year 1:** Data acquisition, curation, and establishment of standardized processing pipelines. Development and benchmarking of the BGC-Metabolite linkage pipeline. Milestone: A fully operational, containerized workflow for processing all three data types; processing of 50% of target datasets.
*   **Year 2:** Full-scale execution of Aim 1 and Aim 2 analyses. Milestone: Generation of the first draft of the BGC-Metabolite map and a prioritized list of metabolite-host pathway associations. First manuscript in preparation.
*   **Year 3:** Integration of results into the network atlas (Aim 3). Development of the predictive GCN model. Design and implementation of the public-facing interactive web portal. Milestone: A functional beta version of the 'Chemical Lexicon Atlas' web portal.
*   **Year 4:** Model refinement, validation on independent cohorts, and finalization of the Atlas. Dissemination activities, including manuscript submissions, conference presentations, and community training workshops. Milestone: Public release of the Atlas and all associated code; submission of two major manuscripts.

Expected Outcomes And Impact

**Intellectual Merit and Contribution to the Field**
This project is poised to make transformative contributions to molecular and cellular biosciences by fundamentally shifting how we study host-microbiome interactions. 
1.  **A New Paradigm for Microbiome Research:** Our primary outcome will be a paradigm shift from descriptive, census-based studies ('who is there?') to a mechanistic, function-centric understanding ('what are they doing and how does it matter?'). By decoding the chemical language of the microbiome, we will provide a framework for understanding how emergent host phenotypes, such as immune homeostasis or disease susceptibility, arise from the collective metabolic output of a complex microbial community. This directly addresses the research call's focus on emergence phenomena.
2.  **A Foundational, Community-Wide Resource:** The principal deliverable, the 'Chemical Lexicon Atlas,' will be a first-of-its-kind, publicly accessible resource. Analogous to foundational databases like KEGG for metabolic pathways or the Gene Ontology for gene function, our Atlas will provide a comprehensive, queryable map of microbe-metabolite-host interactions. This will empower researchers, even those without deep computational expertise, to generate novel, data-driven hypotheses. For example, an immunologist studying a specific cytokine could query the Atlas to identify candidate microbial metabolites that regulate its expression, along with the microbes and genes responsible for their production.
3.  **Methodological Innovation in Multi-omics Integration:** The computational pipeline we develop will be a significant methodological advancement. Integrating three distinct and high-dimensional data types (genomics, metabolomics, transcriptomics) poses immense statistical and computational challenges. Our novel framework, combining network science, probabilistic modeling, and graph-based machine learning, will provide a robust and adaptable blueprint for data synthesis that can be applied to other complex biological systems beyond the gut microbiome.

**Broader Impacts and Applications**
Beyond its fundamental scientific contributions, this project will have far-reaching impacts on therapeutic development, workforce training, and the practice of open science.
1.  **Accelerating Therapeutic and Diagnostic Development:** The Atlas will serve as a powerful engine for hypothesis generation, directly informing the development of next-generation microbiome-based diagnostics and therapeutics. Identifying specific anti-inflammatory metabolites could lead to the development of 'postbiotic' drugs for IBD. Conversely, discovering microbial metabolites that promote cell proliferation could yield new non-invasive biomarkers for early detection of colorectal cancer. Our work will provide a mechanistic rationale for the targeted manipulation of the microbiome, moving beyond the trial-and-error approach of fecal microbiota transplantation.
2.  **Training the Next Generation of Data Scientists:** This project is an ideal training environment for the 'future data-savvy workforce' envisioned by the research call. Graduate students and postdoctoral fellows will work in a deeply collaborative, transdisciplinary team, gaining hands-on expertise at the cutting edge of computational biology, bioinformatics, microbiology, and immunology. We will host an annual open-to-the-public workshop to disseminate our methods and train the broader community, thereby amplifying our educational impact.
3.  **Championing Open and Reproducible Science:** Our commitment to open science is unwavering. All computational pipelines will be developed as open-source software with comprehensive documentation and distributed via platforms like GitHub. All processed data and results, including the final Atlas, will be deposited in public repositories and made accessible through our web portal, adhering to FAIR (Findable, Accessible, Interoperable, and Reusable) data principles. This ensures our work is transparent, reproducible, and serves as a lasting resource for the entire scientific community.

**Dissemination and Long-Term Vision**
Our dissemination strategy is multi-faceted, designed to maximize reach and impact. We will publish our primary findings in high-impact, peer-reviewed journals (e.g., Nature Biotechnology, Cell Host & Microbe). We will present our work at key international conferences (e.g., Keystone Symposia on the Microbiome, ISMB). The interactive web portal will be our primary tool for broad dissemination to the research community. Our long-term vision is for the Chemical Lexicon Atlas to become a living resource, continuously updated with new public data and analytical tools, sustained through future funding and community collaboration. This project will lay the foundation for a new era of predictive, systems-level understanding of the microbiome's role in health and disease.

Budget And Resources

**Budget Justification**
This project's scope, requiring the integration of tens of thousands of complex datasets and the development of novel computational tools, is beyond the capacity of any single research lab and necessitates the support of a dedicated Working Group. The budget is designed to support the personnel, computational infrastructure, and collaborative activities essential for its success. As this is a data synthesis project, no funds are requested for experimental data generation.

**1. Personnel (Total: $1,200,000 over 4 years)**
This is the largest and most critical component of the budget. The intellectual work of developing pipelines, analyzing data, and interpreting results will be driven by a team of highly skilled trainees and staff.
*   **Postdoctoral Fellows (4 FTEs):** $75,000/year salary + benefits x 4 fellows x 4 years. Each postdoc will lead a core analytical area: (1) Metagenomics and BGC analysis, (2) Metabolomics data processing and annotation, (3) Host transcriptomics and pathway analysis, and (4) Network integration and machine learning. This structure ensures dedicated expertise for each data modality.
*   **Data Scientist/Software Engineer (1 FTE):** $90,000/year salary + benefits x 1 FTE x 4 years. This position is crucial for building robust, scalable, and reusable computational pipelines and for developing the professional-grade, public-facing web portal for the Atlas. 
*   **Graduate Students (2 FTEs):** $40,000/year stipend + tuition x 2 students x 4 years. Students will support the postdocs, receive interdisciplinary training, and work on specific sub-projects, ensuring the project contributes to training the next generation.
*   **Principal Investigator Support:** 1 month of summer salary per year for each of the 3 lead PIs to cover time dedicated to project management, scientific oversight, and trainee mentorship.

**2. Computational Resources (Total: $150,000 over 4 years)**
Large-scale data synthesis is computationally intensive and requires significant resources.
*   **Cloud Computing Credits (AWS/Google Cloud):** $30,000/year x 4 years. Essential for on-demand, scalable computing for tasks like metagenomic assembly of thousands of samples and training deep learning models. Also used for hosting the final web portal.
*   **Data Storage:** $7,500/year x 4 years. For long-term, secure storage and backup of raw and processed data, totaling many terabytes.

**3. Travel and Collaboration (Total: $80,000 over 4 years)**
Fostering collaboration within our geographically distributed, multidisciplinary team is paramount.
*   **Annual All-Hands Meeting:** $15,000/year x 4 years. Funds to bring the entire working group (PIs, postdocs, students) together for an intensive 3-day in-person workshop to review progress, resolve challenges, and plan future steps.
*   **Conference Travel:** $5,000/year x 4 years. To allow trainees to present their findings at major international conferences, disseminate our work, and network with the broader scientific community.

**4. Dissemination and Training (Total: $50,000 over 4 years)**
*   **Publication Costs:** $5,000/year x 4 years. To cover open-access publication fees in high-impact journals.
*   **Community Workshop:** $7,500/year for years 3 and 4. To host two training workshops for the wider research community on how to use our pipelines and the Atlas resource.

**5. Indirect Costs (F&A)**
Calculated based on the negotiated rates at the participating institutions, applied to the modified total direct costs.

This budget reflects the resources necessary to assemble a collaborative team with diverse expertise to tackle a grand challenge in molecular biosciences, a goal perfectly aligned with the NCEMS mission.",,
ai_group_int_gemini_09,ai,group_int,gemini-2.5-pro,The Cellular Ecology of Aging: A Pan-Tissue Synthesis of Single-Cell Data to Define Emergent Hallmarks of Senescence,"Aging is not simply the decline of individual cells, but an emergent property of deteriorating cellular ecosystems within tissues. Communication breakdown, altered cellular composition, and microenvironmental changes collectively drive organ dysfunction. This project will create a comprehensive, cross-tissue atlas of aging at single-cell resolution to dissect these emergent phenomena. We will assemble a team of gerontologists, computational biologists, and single-cell genomics experts to synthesize the rapidly growing body of public single-cell RNA-seq and ATAC-seq data from multiple tissues and organisms (e.g., Tabula Muris Senis, Human Cell Atlas, GTEx). The primary challenge, which requires a collaborative, community-scale effort, is the development of novel computational methods to harmonize and integrate these heterogeneous datasets, correcting for technical batch effects while preserving true biological variation related to age, tissue, and species. Using this integrated atlas, we will first define robust, consensus 'aging signatures' for every major cell type, moving beyond tissue-specific markers. Second, we will apply advanced algorithms for inferring cell-cell communication networks to map how signaling pathways between different cell populations (e.g., immune-stromal, epithelial-fibroblast) are systematically rewired during aging. This will allow us to identify the key intercellular communication failures that precede functional decline. The project will produce a foundational, open-access 'Single-Cell Atlas of Aging,' providing unprecedented insights into how tissue-level senescence emerges from changes in the composition and interaction of its constituent cells. This resource will be invaluable for identifying novel, pan-tissue targets for pro-longevity interventions and will train a cohort of researchers in the complex analysis of large-scale single-cell data.",,"Background And Significance

Aging is a complex, multifactorial process and the primary risk factor for most chronic human diseases, including cancer, neurodegeneration, and cardiovascular disease. For decades, research has focused on cell-intrinsic mechanisms, elegantly summarized in the 'Hallmarks of Aging' framework (López-Otín et al., 2013, 2023). These hallmarks, such as genomic instability, telomere attrition, and epigenetic alterations, describe molecular damage accumulating within individual cells. While this paradigm has been incredibly fruitful, it provides an incomplete picture. It largely overlooks the fact that tissues are not mere collections of independent cells, but complex, dynamic ecosystems. Organismal aging, we posit, is an emergent property arising from the progressive deterioration of these ecosystems, driven by altered cellular composition and a systemic breakdown in intercellular communication. This project reframes aging from a cell-centric problem to a systems-level challenge in cellular ecology. Evidence for this ecological view is mounting. The concept of 'inflammaging' describes a chronic, low-grade, sterile inflammation that characterizes aged tissues, driven by dysfunctional communication between immune cells and stromal cells (Franceschi et al., 2018). Similarly, the accumulation of senescent cells, which secrete a potent mix of inflammatory factors known as the senescence-associated secretory phenotype (SASP), profoundly remodels the local tissue microenvironment, disrupting homeostasis and promoting age-related pathologies (Coppé et al., 2010). These phenomena highlight that the context and communication between cells are as critical as their individual internal states. The recent explosion in single-cell genomics, particularly single-cell RNA sequencing (scRNA-seq) and single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq), provides an unprecedented opportunity to deconstruct these complex tissue ecosystems at high resolution. Large-scale public consortia like the Human Cell Atlas (HCA), the Genotype-Tissue Expression (GTEx) project, and Tabula Muris Senis have generated vast repositories of single-cell data from diverse tissues across the lifespan of multiple organisms. These datasets represent a monumental, yet largely untapped, resource for understanding aging as a systemic process. The primary bottleneck preventing a holistic synthesis is a formidable computational and logistical challenge. These datasets were generated by hundreds of different laboratories using varied protocols, platforms, and processing pipelines. This heterogeneity introduces profound technical batch effects that can easily mask the subtle biological signals of aging. While numerous computational methods for data integration exist (e.g., Seurat v4, Harmony, Scanpy), they were primarily designed for smaller-scale projects and often struggle to harmonize data across different technologies, species, and tissues without erasing crucial biological variance. A community-scale effort, uniting experts in gerontology, computational biology, and systems biology, is required to overcome this barrier. This research is therefore critically important and timely. The data exists, but the collaborative framework and advanced analytical strategies to synthesize it into a coherent whole are missing. By creating a unified, pan-tissue 'Single-Cell Atlas of Aging,' this project will address a fundamental gap in our knowledge: how do cell-intrinsic changes translate into the emergent, system-wide functional decline of an organism? Answering this question will not only provide a new conceptual framework for aging research but also reveal novel, high-priority targets for interventions aimed at extending human healthspan.

Research Questions And Hypotheses

This project is organized around three central aims, each designed to address fundamental questions about the emergent nature of aging. Our overarching goal is to move beyond tissue-specific descriptions to uncover universal principles of aging at the level of cellular ecosystems. 

**Aim 1: Develop a robust, scalable computational framework for the pan-tissue, cross-species integration of public single-cell aging data.** This aim addresses the primary technical barrier to a systems-level understanding of aging.
*   **Research Question 1.1:** How can we effectively disentangle true biological aging signals from confounding technical variables (e.g., study-specific batch effects, dissociation protocols, sequencing platforms) across hundreds of heterogeneous single-cell datasets?
*   **Hypothesis 1.1:** We hypothesize that a hierarchical, deep learning-based integration strategy will significantly outperform current linear or shallow non-linear methods. By first correcting batch effects within tissues and then using a transfer learning approach to project all data into a shared latent space, our framework will maximally preserve cell-type identity and subtle age-related state transitions while minimizing technical artifacts. We will validate this by demonstrating superior performance on established integration metrics (e.g., Local Inverse Simpson's Index for batch mixing, Adjusted Rand Index for label conservation) compared to standard pipelines.

**Aim 2: Define and validate consensus, cross-tissue molecular signatures of aging for major cell types.** This aim seeks to identify the fundamental, conserved changes that occur in specific cell types, regardless of their tissue environment.
*   **Research Question 2.1:** Do universal aging programs exist within conserved cell lineages (e.g., fibroblasts, T cells, endothelial cells, macrophages) that are independent of their tissue of residence?
*   **Hypothesis 2.1:** We hypothesize that beyond tissue-specific adaptations, common cell types will exhibit a core set of age-associated changes in gene expression and chromatin accessibility related to fundamental cellular processes like metabolic stress, proteostasis failure, and inflammatory response. We predict that our integrated atlas will reveal a 'pan-fibroblast' aging signature, for example, that is conserved across skin, lung, and heart, providing a more fundamental definition of cellular aging.
*   **Research Question 2.2:** How does cellular heterogeneity and plasticity change with age within a given cell type?
*   **Hypothesis 2.2:** We hypothesize that aging leads to a loss of cellular identity and an increase in transcriptional noise, resulting in the emergence of aberrant, dysfunctional cell sub-states. We predict that trajectory inference and heterogeneity analyses will reveal a 'blurring' of distinct functional cell states in aged tissues and the appearance of novel senescent-like or pro-fibrotic subpopulations that contribute to tissue decline. 

**Aim 3: Map the systemic rewiring of intercellular communication networks as an emergent driver of organismal aging.** This aim directly investigates aging as a property of the cellular ecosystem.
*   **Research Question 3.1:** Which specific signaling pathways and ligand-receptor interactions are consistently gained, lost, or altered across disparate aging tissues, and which cell types are the primary drivers of these changes?
*   **Hypothesis 3.1:** We hypothesize that a key emergent property of aging is a systemic shift from tissue-specific, homeostatic signaling to a convergent, chronic pro-inflammatory communication network (inflammaging). We predict that computational inference of cell-cell communication will reveal a conserved gain of signaling mediated by TNF, IL-1, and IL-6, and a concomitant loss of regenerative signaling (e.g., Wnt, FGF), primarily driven by activated immune and stromal cell populations across multiple tissues.
*   **Research Question 3.2:** Can we identify 'keystone' cell types or communication hubs whose dysregulation has a disproportionately large impact on the entire tissue ecosystem?
*   **Hypothesis 3.2:** We hypothesize that specific cell populations, such as senescent fibroblasts or a subset of tissue-resident macrophages, act as central, pathological signaling hubs in aged tissues. We predict that network analysis of the inferred communication graphs will identify these cell types as having the highest centrality scores in aged tissues, and that their specific ligand emissions are the primary drivers of age-related transcriptional changes in neighboring epithelial, endothelial, and immune cells.

Methods And Approach

This project will exclusively synthesize publicly available data, requiring a transdisciplinary team and a robust computational infrastructure that necessitates NCEMS support. Our approach is structured around our three research aims and a detailed project timeline.

**Data Acquisition, Curation, and Standardization**
Our working group will systematically query major public repositories, including the Gene Expression Omnibus (GEO), ArrayExpress, the Single Cell Portal, the Human Cell Atlas (HCA) Data Coordination Platform, and the GTEx Portal. We will focus on scRNA-seq and scATAC-seq datasets from human and mouse tissues with clearly annotated chronological age information (e.g., 'young' vs. 'old' cohorts). A critical first step, requiring a collaborative effort, will be the development of a unified metadata schema. We will manually curate and standardize all available sample- and cell-level information, including species, tissue, age, sex, experimental protocol, and original author-provided cell annotations. This standardized metadata is essential for downstream modeling and interpretation and represents a significant contribution in itself.

**Aim 1: Computational Integration Framework**
Our analytical pipeline will begin with a standardized pre-processing workflow for each dataset, including quality control, normalization (e.g., SCTransform for RNA, TF-IDF for ATAC), and feature selection. For integration, we will develop a novel, three-stage hierarchical approach:
1.  **Intra-Tissue Integration:** For each tissue with multiple available datasets (e.g., mouse lung), we will first use an established method like Harmony to correct for study-specific batch effects, creating a harmonized representation for that single tissue.
2.  **Cross-Tissue, Single-Cell-Type Integration:** We will then develop a more powerful, non-linear integration model based on a conditional variational autoencoder (CVAE). For each major cell type (e.g., fibroblasts), we will train a CVAE to learn a latent space that is conditioned on tissue identity but is invariant to technical batch effects. This allows us to directly compare, for example, a lung fibroblast to a skin fibroblast.
3.  **Pan-Atlas Assembly:** Finally, we will use a transfer learning approach to project all cell-type-specific latent spaces into a global, shared embedding. This final atlas will allow for comparisons across both cell types and tissues.
We will rigorously benchmark this framework against state-of-the-art methods using quantitative metrics for batch correction (kBET, LISI) and biological conservation (cell type ARI, silhouette width). This methodological development is a core component of the project.

**Aim 2: Defining Consensus Aging Signatures**
With the integrated atlas, we will first establish a consistent, cross-dataset cell type ontology. This will involve a combination of automated annotation tools (e.g., Azimuth, SingleR) followed by manual review and refinement by the domain experts within our working group. To identify aging signatures, we will employ pseudo-bulk differential expression (DE) and differential accessibility (DA) analyses, aggregating counts for each cell type within each biological sample. This approach allows the use of robust statistical models like DESeq2 and limma that properly account for inter-individual variability. A 'core' aging signature for a cell type will be defined as genes or chromatin regions that are significantly altered with age in the same direction across at least three distinct tissues. We will use Gene Set Enrichment Analysis (GSEA) and other pathway tools to determine the biological functions of these core signatures. To analyze changes in heterogeneity, we will quantify transcriptional variance and use trajectory inference tools (e.g., scVelo, Palantir) to map age-associated shifts in cell state landscapes.

**Aim 3: Mapping Intercellular Communication Networks**
We will leverage the harmonized gene expression data to infer intercellular signaling. We will apply a suite of algorithms, including CellChat, NicheNet, and scConnect, which use ligand-receptor expression databases to predict the strength and targets of communication pathways between all cell types. By comparing the inferred networks between young and old atlas subsets, we will perform differential network analysis to identify signaling pathways that are consistently gained or lost with age across tissues. We will then use graph theory metrics (e.g., degree centrality, betweenness centrality) to identify 'keystone' cell types that act as major signaling hubs in the aged communication network. The predictions from this analysis will generate specific, testable hypotheses about the drivers of tissue-level decline.

**Timeline and Milestones**
*   **Year 1:** Data acquisition and metadata standardization completed. Development and benchmarking of the integration framework (Aim 1). Release of a pilot atlas integrating 3-4 key tissues. Publication on the integration methodology.
*   **Year 2:** Completion of the full pan-tissue, cross-species atlas. Consensus cell type annotation finalized. Identification and initial characterization of core aging signatures for major cell types (Aim 2). Development of the public web portal begins.
*   **Year 3:** Comprehensive cell-cell communication network analysis (Aim 3). Final synthesis of all findings. Launch of the interactive 'Single-Cell Atlas of Aging' web portal. Submission of the main atlas manuscript to a high-impact journal. Host a community-wide training workshop.

Expected Outcomes And Impact

This project is designed to be transformative, producing novel biological insights, powerful new methodologies, and a foundational resource for the scientific community, directly aligning with the NCEMS mission to catalyze multidisciplinary synthesis research.

**Intended Contributions to the Field**
1.  **A Foundational Public Resource:** The primary deliverable will be the 'Single-Cell Atlas of Aging,' an open-access, interactive web portal. This resource will allow any researcher, regardless of computational expertise, to explore the effects of aging on any major cell type across a wide range of tissues and species. It will serve as a central hypothesis-generation engine for the entire aging research community, analogous to the impact of resources like the Cancer Genome Atlas (TCGA) or the Human Cell Atlas (HCA).
2.  **A Paradigm Shift in Aging Biology:** Our work will provide the first comprehensive, systems-level view of aging as an emergent property of deteriorating cellular ecosystems. By identifying consensus, pan-tissue aging signatures and mapping the systemic rewiring of cell-cell communication, we will move the field beyond a cell-intrinsic focus. This will establish a new conceptual framework for understanding how molecular damage translates into organismal decline, revealing the ecological principles that govern tissue homeostasis and its failure with age.
3.  **Methodological Advancement in Computational Biology:** The hierarchical deep learning framework we develop for data integration will represent a significant advance for the analysis of large-scale, heterogeneous single-cell data. This open-source tool will be broadly applicable to other complex biological questions that require the synthesis of disparate public datasets, extending its impact beyond aging research.

**Broader Impacts and Applications**
*   **Therapeutic Development:** By identifying the most conserved molecular pathways and central 'keystone' cell types that drive aging across multiple organs, our atlas will pinpoint the highest-priority targets for pro-longevity interventions. Therapeutics targeting a pan-tissue mechanism are more likely to have systemic benefits and improve overall healthspan, rather than targeting a single disease in isolation. For example, identifying a specific signaling pathway from senescent fibroblasts that disrupts endothelial function across all tissues would nominate that pathway as a prime target for novel senotherapeutics.
*   **Biomarker Discovery:** The core aging signatures we define for accessible cell types (e.g., immune cells in the blood) can serve as the basis for developing highly sensitive and specific biomarkers of biological age. Such biomarkers are urgently needed to assess the efficacy of anti-aging interventions in clinical trials.

**Dissemination, Open Science, and Sustainability**
We are fully committed to open science principles. All analysis code will be version-controlled and publicly available on GitHub. All processed data objects, standardized metadata, and results will be deposited in FAIR-compliant repositories like Zenodo and Figshare. Our findings will be disseminated through high-impact publications (targeting journals like *Cell*, *Nature*, and *Science* for the main atlas and *Nature Methods* for the computational framework), and presentations at international conferences. The long-term sustainability of the web portal will be ensured through its integration with established platforms and by seeking follow-up funding to maintain and expand the atlas as new data becomes available.

**Training and Collaboration**
The project structure as an NCEMS Working Group is essential for its success. It brings together a diverse team of gerontologists, computational scientists, and systems biologists, fostering the cross-disciplinary collaboration required to tackle this challenge. Graduate students and postdoctoral fellows will be at the heart of this effort, receiving unique, hands-on training at the intersection of data science, genomics, and aging biology, thereby cultivating the next generation of data-savvy biomedical researchers as mandated by the research call.

Budget And Resources

The proposed research represents a community-scale synthesis effort whose scope, complexity, and collaborative nature extend far beyond the capabilities of a single research lab or existing collaboration. Support from NCEMS is therefore essential for its success. The budget is designed to support the personnel, computational infrastructure, and collaborative activities required to build and disseminate this foundational resource.

**Personnel ($650,000)**
The primary cost is for dedicated personnel with the diverse expertise needed for this project. This team will form the core of the Working Group's analytical effort.
*   **Postdoctoral Fellows (2):** $280,000. One fellow with expertise in computational biology and machine learning will lead the development of the integration framework. A second fellow with a background in systems biology and gerontology will lead the biological interpretation, pathway analysis, and cell-cell communication modeling.
*   **Graduate Students (2):** $180,000. Two students will support the massive effort of data acquisition, metadata curation, and execution of specific analytical pipelines, while receiving unparalleled cross-disciplinary training.
*   **Data Scientist/Software Engineer (0.5 FTE):** $120,000. This is a critical role that is not available in most academic labs. This individual will be responsible for building the robust, user-friendly web portal for the 'Single-Cell Atlas of Aging,' managing the underlying database, and ensuring the long-term sustainability of the resource.
*   **Principal Investigators:** $70,000. Summer salary support for the PIs to provide dedicated oversight and scientific direction.

**Computational Resources ($100,000)**
The scale of this project—integrating millions of cells from hundreds of datasets—requires substantial computational power. Training and benchmarking deep learning models is computationally intensive and requires significant GPU resources. While university HPC clusters are available, their queue times and resource limits are not conducive to rapid, iterative model development. We request funds for cloud computing credits (e.g., Amazon Web Services, Google Cloud) to provide on-demand access to GPU nodes for model training and to host the final, publicly accessible web portal. This cost is a primary reason why such a project cannot be undertaken by an individual lab.

**Meetings and Travel ($60,000)**
To foster the deep, synergistic collaboration that is the hallmark of an NCEMS Working Group, we will hold twice-yearly, in-person meetings of the entire team. These meetings are essential for data interpretation, strategic planning, and synthesizing diverse perspectives. Funds are also allocated for travel to one major international conference per year for trainees and PIs to present our findings and engage with the broader scientific community.

**Training and Dissemination ($40,000)**
We have budgeted funds to host an annual open workshop in Year 3 to train the wider research community on how to use the atlas and our analytical tools. This directly supports the NCEMS goal of training a data-savvy workforce. Funds are also included for open-access publication fees to ensure our results are freely and immediately available to all.

**Total Requested Budget: $850,000**",,
ai_group_int_gemini_10,ai,group_int,gemini-2.5-pro,Chromatin as a Programmable Matter: Uncovering the Physical Principles of Genome Folding and Function,"The folding of a two-meter long genome into a micron-sized nucleus is a remarkable feat of self-organization. This 3D architecture is not random; it is a key regulator of gene expression, and its disruption is a hallmark of many diseases. This project will address the fundamental question of how the 1D epigenome sequence 'codes' for the emergent 3D chromatin structure. We propose to build a unified, predictive model of genome folding by synthesizing massive public datasets and fostering a novel collaboration between polymer physicists, bioinformaticians, and molecular biologists. Our working group will integrate genome-wide 3D architecture data (Hi-C, from 4DN and ENCODE), 1D epigenomic data (ChIP-seq, ATAC-seq), and transcriptional data (RNA-seq) across hundreds of cell types and developmental states. The core of the project is to develop a new generation of data-constrained polymer physics models. We will use machine learning to learn the 'rules' that map specific combinations of histone modifications and protein binding sites to physical parameters in the model, such as local stiffness or interaction affinity. This hybrid AI-physics approach will allow us to simulate how the genome folds and predict how specific epigenomic alterations (e.g., in cancer) lead to changes in 3D structure and emergent gene expression patterns. The complexity of the models and the sheer scale of the data require a concerted effort beyond any single lab. This project will deliver a powerful, open-source simulation engine for the 4D nucleome, transforming our understanding of chromatin from a static blueprint to a dynamic, programmable matter. It will provide fundamental insights into the physical basis of gene regulation and train a new generation of scientists fluent in both the language of biology and physics.",,"Background And Significance

The central dogma of molecular biology describes the flow of genetic information, but it is now clear that this process is profoundly regulated by the physical organization of the genome in three-dimensional space. The packaging of a two-meter long DNA polymer into a nucleus mere microns in diameter is not merely a feat of compaction; it is a dynamic, functional architecture that orchestrates gene expression programs essential for cellular identity and function. The advent of chromosome conformation capture technologies, particularly Hi-C, has revolutionized our understanding of this 3D genome, revealing a hierarchical organization of chromosome territories, A/B compartments corresponding to active and inactive chromatin, Topologically Associating Domains (TADs), and specific chromatin loops that connect distal enhancers to their target promoters. Large-scale public consortia, such as the Encyclopedia of DNA Elements (ENCODE) and the 4D Nucleome (4DN) Program, have generated an unprecedented wealth of data, mapping this 3D architecture alongside 1D epigenomic features—such as histone modifications, chromatin accessibility, and protein binding sites—across a vast array of cell types and developmental stages. This data deluge presents a historic opportunity to move beyond descriptive characterizations and address a fundamental, unresolved question: what are the physical principles that govern genome folding? How does the linear sequence of epigenomic marks 'program' the emergent 3D structure? Current theoretical models have provided crucial, yet incomplete, insights. Polymer physics models, such as the Strings and Binders model and the Loop Extrusion model, have successfully explained general features like TAD formation through the action of architectural proteins like CTCF and cohesin. However, these models often rely on a small set of manually-tuned parameters and struggle to capture the cell-type-specific nuances driven by the complex combinatorial code of the epigenome. On the other end of the spectrum, recent machine learning (ML) approaches have demonstrated remarkable accuracy in predicting Hi-C contact maps from 1D sequence and epigenomic data. Models like Akita and Orca can learn complex patterns but operate largely as 'black boxes,' offering limited mechanistic insight into the underlying physical forces. They predict what the structure looks like, but not why it adopts that conformation. This leaves a critical gap in our knowledge: a unified, predictive, and physically interpretable framework that directly links the 'cause' (the 1D epigenome) to the 'effect' (the 3D structure and its functional consequences). We lack a quantitative 'dictionary' that translates specific combinations of histone modifications and protein binding events into effective physical parameters like local compaction, stiffness, or interaction energies. Bridging this gap is not only a grand challenge in fundamental biology but also holds immense translational potential. Misregulation of the 3D genome is increasingly recognized as a driver of human diseases, including cancer and developmental disorders. Many disease-associated genetic variants identified through genome-wide association studies (GWAS) reside in non-coding regions and are thought to function by altering enhancer-promoter loops. Without a predictive model of genome folding, interpreting the functional impact of these variants remains a formidable challenge. This research is therefore exceptionally timely. The availability of massive, harmonized public datasets, coupled with advances in computational power and hybrid AI-physics modeling techniques, creates the perfect conditions for a community-scale synthesis effort. By bringing together a transdisciplinary team of polymer physicists, computer scientists, bioinformaticians, and molecular biologists, this project will tackle this challenge head-on. We aim to build the first generalizable, data-constrained physical model of the genome, transforming our view of chromatin from a static scaffold to a dynamic, programmable matter, and ultimately uncovering the physical basis of gene regulation in health and disease.

Research Questions And Hypotheses

This project is driven by the overarching scientific question: How does the linear, 1D landscape of epigenomic marks and architectural protein binding sites encode the physical rules that govern the emergent, functional 3D folding of the genome? To deconstruct this complex problem, we have formulated three specific, interconnected research questions, each with a corresponding set of testable hypotheses. Our approach is designed to systematically build a predictive framework, validate its accuracy, and apply it to uncover novel biological insights.

**Research Question 1: Can we derive a generalizable, quantitative mapping between local epigenomic states and the effective physical parameters of a polymer model of chromatin?**
This question addresses the core challenge of creating a 'dictionary' that translates biological information into physical properties. We aim to move beyond ad-hoc parameterization and learn these rules directly from data.
*   **Hypothesis 1.1:** A deep learning model, trained on a diverse compendium of cell types, can learn a robust function that maps a high-dimensional vector of 1D epigenomic features (e.g., levels of H3K27ac, H3K27me3, CTCF binding) to a low-dimensional set of physical parameters (e.g., monomer type, interaction energy, local stiffness) for a coarse-grained polymer simulation.
*   **Prediction:** We predict this learned mapping will be biologically interpretable. For instance, genomic regions enriched for active marks like H3K4me3 and H3K27ac will be assigned parameters that promote self-association and phase separation, consistent with the formation of active A compartments. Conversely, regions with repressive marks like H3K27me3 and H3K9me3 will be assigned parameters promoting a different type of self-association, forming B compartments. CTCF binding sites will be mapped to parameters that create strong, directional loop anchors.
*   **Validation:** The generalizability of this mapping will be tested via rigorous cross-validation. The model will be trained on a large subset of cell types (~80%) and its ability to predict the 3D genome architecture of held-out cell types (~20%) from their 1D epigenome alone will be quantified by comparing simulated and experimental Hi-C maps.

**Research Question 2: How accurately can our data-inferred, physics-based model predict de novo changes in 3D genome organization in response to specific molecular perturbations?**
This question tests the predictive power and mechanistic validity of our model. A truly robust model should not only recapitulate static structures but also predict how they change when the system is perturbed.
*   **Hypothesis 2.1:** Our hybrid AI-physics model will accurately predict the structural consequences of depleting key architectural proteins, such as CTCF or cohesin subunits (e.g., RAD21). 
*   **Prediction:** Simulating CTCF depletion (by removing the specific interaction parameters associated with CTCF sites) will result in a quantifiable loss of TAD insulation and a blurring of domain boundaries in the simulated Hi-C maps. This will manifest as an increase in the insulation score and a decrease in intra-TAD contact frequency, mirroring published experimental results from degron-based studies. The model's predictions will be quantitatively superior to those from models with fixed, non-data-driven parameters.
*   **Validation:** We will leverage publicly available Hi-C datasets from experiments where specific proteins were acutely depleted. We will compare the difference between our simulated perturbed and unperturbed contact maps with the difference between the experimental perturbed and unperturbed maps.

**Research Question 3: Can the model reveal how epigenomic dysregulation in disease states, such as cancer, leads to pathogenic alterations in 3D genome architecture and gene expression?**
This question applies our predictive framework to a critical biomedical problem, aiming to link genotype and epigenotype to phenotype through the lens of 3D genome structure.
*   **Hypothesis 3.1:** Our model can predict aberrant enhancer-promoter contacts in cancer cells based on their altered epigenomic landscapes, and these novel contacts will correlate with the misexpression of key oncogenes or tumor suppressors.
*   **Prediction:** For a cancer cell line with a known amplification of a distal enhancer, our model, when provided with the corresponding epigenomic data (e.g., ATAC-seq, H3K27ac ChIP-seq), will predict the formation of a new, strong chromatin loop connecting this enhancer to an oncogene promoter. This predicted structural change will be associated with a corresponding increase in the oncogene's expression level, as observed in matched RNA-seq data.
*   **Validation:** We will use matched epigenomic, 3D genomic, and transcriptomic data from cancer cell lines and primary tumors available through consortia like TCGA and CCLE. We will systematically compare our model's predictions of structural rewiring with observed changes in gene expression, identifying statistically significant correlations that point to novel mechanisms of oncogene activation.

Methods And Approach

Our research plan is structured into three synergistic Aims, designed to systematically collect and harmonize data, develop and train our novel modeling framework, and validate its predictive power. This project exclusively utilizes publicly available data, and its success hinges on the collaborative synthesis of expertise from our multidisciplinary working group.

**Aim 1: Curation, Harmonization, and Integration of a Multi-modal Public Data Compendium.**
The foundation of this project is a comprehensive, consistently processed dataset integrating 1D epigenomics, 3D genomics, and transcriptomics. This large-scale data harmonization effort is beyond the scope of a single lab and is a key justification for the working group structure.
*   **Data Sources:** We will systematically mine major public repositories, including the 4D Nucleome (4DN) Data Portal, the ENCODE Portal, the Cistrome Data Browser, and the Gene Expression Omnibus (GEO).
*   **Data Types and Selection:** We will assemble a diverse collection of datasets from approximately 200 human and mouse cell lines, covering a wide range of developmental stages, tissues, and disease states. This includes:
    1.  **3D Genomics:** High-resolution in situ Hi-C and Micro-C data to serve as the ground truth for 3D architecture.
    2.  **1D Epigenomics:** ChIP-seq data for a core set of histone modifications (activating: H3K4me1, H3K4me3, H3K27ac; repressive: H3K9me3, H3K27me3; elongation: H3K36me3), architectural proteins (CTCF, RAD21, SMC3), and key transcription factors.
    3.  **Chromatin Accessibility:** ATAC-seq or DNase-seq data.
    4.  **Transcription:** RNA-seq and/or GRO-seq data to quantify gene expression.
*   **Processing Pipeline:** To ensure consistency, all raw data will be reprocessed through a unified, containerized (Docker/Singularity) pipeline. Hi-C data will be processed using HiC-Pro and normalized using iterative correction and equilibration (ICE). ChIP-seq and ATAC-seq data will be processed using ENCODE standard pipelines. All data will be mapped to the latest reference genome builds (hg38/mm10). The final output will be a multi-modal data tensor where each genomic bin (e.g., 10 kb resolution) is annotated with a feature vector comprising its full epigenomic and transcriptional state.

**Aim 2: Development and Training of a Hybrid AI-Physics Simulation Engine.**
This Aim constitutes the core technical innovation of the project, where we will build a new class of predictive model.
*   **Polymer Physics Framework:** We will implement a coarse-grained bead-spring polymer model using highly efficient simulation engines like OpenMM or LAMMPS, which can leverage GPU acceleration. Each bead will represent a genomic region (e.g., 10 kb), and the model will include potentials for chain connectivity, excluded volume, and specific, non-covalent interactions.
*   **Machine Learning Architecture:** We will develop a deep neural network (DNN) to learn the mapping from the 1D epigenomic feature vector of a genomic bin to the physical parameters of the corresponding bead in the polymer model. The output of the DNN will define the bead's 'type' or 'color', which in turn dictates its interaction energies with other bead types. This allows for a rich, data-driven parameterization of the physical model. For example, the model can learn that beads with high H3K27ac levels strongly attract each other.
*   **Differentiable Training Loop:** A key challenge is to train the DNN based on the output of the polymer simulation. We will employ a novel training strategy that makes the entire pipeline differentiable. The polymer simulation will be run for a set number of steps to generate a simulated contact map. A loss function (e.g., stratum-adjusted correlation coefficient) will quantify the difference between the simulated and experimental Hi-C maps. We will use techniques from differentiable physics or reinforcement learning (e.g., policy gradients) to compute the gradient of this loss with respect to the DNN's parameters, allowing for end-to-end training via backpropagation. This computationally intensive task requires significant HPC resources and the combined expertise of our physics and computer science team members.

**Aim 3: Model Validation, Perturbation Analysis, and Disease Application.**
In this Aim, we will rigorously test our model's predictions and apply it to generate novel biological hypotheses.
*   **Cross-Cell-Type Validation:** As described in Hypothesis 1.1, we will perform k-fold cross-validation, training the model on a subset of cell types and testing its ability to predict the 3D structure of held-out cell types from their 1D epigenome alone. This will assess the model's generalizability.
*   **In Silico Perturbation Experiments:** We will use public Hi-C data from experiments involving the acute depletion of proteins like CTCF, WAPL, or cohesin. We will simulate these perturbations by modifying the model's parameters accordingly (e.g., removing CTCF-mediated loop anchor interactions) and compare the predicted changes in contact maps to the experimental data. This provides a stringent test of the model's mechanistic basis.
*   **Cancer Genome Modeling:** We will apply the fully trained model to cancer cell lines with well-characterized epigenomic alterations (e.g., from the Cancer Cell Line Encyclopedia). We will generate de novo predictions of 3D genome rewiring and correlate these predicted structural changes (e.g., new enhancer-promoter loops) with observed differential gene expression from matched RNA-seq data, thereby generating testable hypotheses about mechanisms of oncogene activation.

**Timeline and Milestones:**
*   **Year 1:** Complete data acquisition and harmonization pipeline (M6). Develop and benchmark the polymer simulation and ML framework (M12). First working group in-person meeting (M3).
*   **Year 2:** Complete training of the first-generation model on the full dataset (M18). Perform cross-cell-type validation and initial in silico perturbation experiments (M24). Release beta version of the open-source software (M24).
*   **Year 3:** Conduct comprehensive cancer genome modeling studies (M30). Finalize and publish the simulation engine and the primary scientific findings (M36). Host a community training workshop (M36).

Expected Outcomes And Impact

This project is poised to deliver transformative outcomes that will significantly advance the fields of molecular and cellular biology, genomics, and computational biology. The impact will span from fundamental scientific understanding to potential translational applications, while also building a lasting community resource and training the next generation of interdisciplinary scientists.

**Intellectual Merit and Contributions to the Field:**
The primary outcome of this research will be a paradigm shift in how we understand and study genome organization. We will move the field from a descriptive phase, cataloging chromatin structures, to a predictive and mechanistic one.
1.  **A Unified, Predictive Model of Genome Folding:** We will deliver the first generalizable, physics-based simulation engine that can accurately predict the 3D genome architecture of a mammalian cell solely from its 1D epigenomic profile. This represents a 'holy grail' for the field and will serve as a powerful hypothesis-generating tool for the entire research community.
2.  **The 'Epigenetic-to-Physical' Rulebook:** Our hybrid AI-physics approach will produce a quantitative mapping—a 'dictionary'—that translates the combinatorial language of histone modifications and protein binding into the physical forces that shape the genome. This will provide unprecedented mechanistic insight into how the cell leverages the epigenome to control its physical state and, consequently, its function.
3.  **Novel Insights into Gene Regulation:** By simulating genome folding in diverse cellular contexts, including development and disease, our model will uncover novel principles of long-range gene regulation. It will allow us to systematically probe how changes in chromatin state lead to the formation or dissolution of enhancer-promoter loops, providing a physical basis for understanding gene expression dynamics.

**Broader Impacts and Applications:**
The impact of our work will extend far beyond basic science.
1.  **Interpreting Disease-Associated Genetic Variation:** A major challenge in human genetics is interpreting the function of the ~98% of disease-associated variants that lie in non-coding regions. Our simulation engine will provide a revolutionary tool to address this. By simulating the effects of a non-coding variant on the local epigenome, we can predict its impact on 3D looping and gene regulation, providing a mechanistic link from genotype to phenotype for diseases like cancer, autoimmune disorders, and neurodevelopmental disorders.
2.  **Advancing Synthetic Biology and Genome Engineering:** The ability to predictively model genome folding opens the door to forward engineering, or 'genome programming.' Our framework could guide the design of synthetic epigenomes using tools like dCas9-fused epigenetic editors to create bespoke 3D structures, thereby controlling gene expression programs for therapeutic or biotechnological applications.
3.  **A Lasting Community Resource:** We are committed to Open Science principles. The entire simulation framework, including all code, trained models, and processed data, will be made publicly available through a user-friendly, well-documented open-source platform (e.g., on GitHub and Zenodo). We will provide containerized versions (Docker/Singularity) to ensure reproducibility and ease of use. This will democratize access to sophisticated 4D nucleome modeling.

**Dissemination and Training:**
We will pursue a multi-pronged dissemination strategy. Key scientific findings will be published in high-impact, peer-reviewed journals (e.g., Nature, Cell, Science). The computational methods and software will be published in specialized journals (e.g., Nature Methods, Nature Computational Science). We will present our work at major international conferences (e.g., Gordon Research Conferences, Keystone Symposia, ISMB). Crucially, this project is designed as a training vehicle. Graduate students and postdoctoral fellows will be at the heart of the collaboration, gaining unique, transdisciplinary skills at the interface of biology, physics, and computer science. We will host an annual virtual workshop, open to the community, to train other researchers on how to use our tools and approach.

**Long-Term Vision and Sustainability:**
Our long-term vision is to establish a collaborative, computational hub for the 4D nucleome. The framework developed in this project will be extensible, designed to incorporate new data types (e.g., single-cell Hi-C, live-cell imaging) and more sophisticated modeling approaches as they become available. The working group formed through this NCEMS award will build lasting collaborative relationships, positioning us to secure future funding (e.g., NIH U-series grants) to sustain and expand this community resource long after the initial funding period.

Budget And Resources

The proposed research represents a community-scale synthesis project that is beyond the capabilities of any single research laboratory or existing collaboration. The sheer scale of the data integration, the computational intensity of the hybrid AI-physics models, and the requirement for deep, synergistic expertise from disparate scientific fields necessitate the support and resources provided by the NCEMS program. Our budget is designed to support the personnel, computational infrastructure, and collaborative activities essential for the project's success.

**Personnel (Total: $XXX,XXX):**
The intellectual core of this project is the collaborative effort of our team. We request funding to support the trainees who will execute the research and the coordination required to manage this multi-investigator effort.
*   **Postdoctoral Fellows (2 FTE):** We request support for two postdoctoral fellows for the three-year project duration. One fellow will have a background in computational physics and machine learning, focusing on the development of the simulation engine. The second fellow will have expertise in genomics and bioinformatics, leading the data harmonization and biological validation efforts. This cross-pollination is central to our training goals.
*   **Graduate Students (2 FTE):** Support for two graduate students who will work on specific sub-projects, such as applying the model to specific disease systems or developing new analytical modules for the software. This provides a critical training opportunity for the next generation of data-savvy scientists.
*   **Project Manager / Data Scientist (0.5 FTE):** We request support for a part-time staff scientist to coordinate the working group's activities, manage the massive integrated dataset, oversee the development of the open-source platform, and ensure adherence to project timelines and deliverables. This role is crucial for the logistical success of a multi-lab collaboration.

**Computational Resources (Total: $XX,XXX):**
The training of our hybrid AI-physics model is exceptionally computationally demanding, requiring thousands of GPU hours. This is far beyond the typical resources of an academic lab.
*   **Cloud Computing / HPC Access:** We request significant funds for purchasing compute time on a commercial cloud platform (e.g., AWS or Google Cloud) or for access to a national high-performance computing (HPC) facility. This will be used for the end-to-end training of the deep learning model coupled with molecular dynamics simulations, as well as for storing and processing the petabyte-scale public datasets.

**Travel (Total: $XX,XXX):**
Fostering deep, continuous collaboration is a primary goal of the NCEMS program and is essential for our project. 
*   **Working Group Meetings:** We request funds to support quarterly in-person meetings for the 10 PIs and all supported trainees. These intensive, multi-day workshops will be hosted on a rotating basis at the PIs' institutions and are critical for brainstorming, troubleshooting, and ensuring the project remains integrated and on track.
*   **Conference Dissemination:** Funds are requested for trainees and PIs to present our findings at one major international conference per year (e.g., ISMB, GRC on Genome Architecture, Keystone Symposia), facilitating dissemination and feedback from the broader scientific community.

**Other Direct Costs (Total: $X,XXX):**
*   **Publication Costs:** Funds to cover open-access publication fees in high-impact journals, ensuring our findings are freely accessible.
*   **Data Sharing and Archiving:** Costs associated with long-term data hosting on platforms like Zenodo to ensure the durability and accessibility of our generated data products and models.

**Justification for NCEMS Support:**
This project is uniquely aligned with the NCEMS mission. It is a pure data synthesis project that does not generate new experimental data. The scientific question—uncovering the physical code of genome folding—is fundamental and requires integrating diverse datasets at a scale no single lab could manage. Our team was specifically assembled for this proposal, bringing together world leaders in polymer physics, machine learning, bioinformatics, and molecular biology who have not previously collaborated in this manner. The required computational resources and dedicated project management are substantial. NCEMS support is therefore not just beneficial but essential to catalyze this collaboration and enable a project of this ambition and potential impact.",,
