{
    "call": "This funding organization is dedicated to catalyzing multidisciplinary scientific teams to synthesize publicly available data to address fundamental questions related to emergence phenomena in the molecular and cellular biosciences. It supports community-scale synthesis and integration of existing molecular and cellular data to: ÔÇ∑Answer novel questions ÔÇ∑Solve long-standing puzzles in the molecular and cellular sciences ÔÇ∑Develop innovative research and analytical strategies ÔÇ∑Tap diverse new talent ÔÇ∑Train the future data-savvy workforce A community-scale synthesis project has three defining characteristics. First, it uses only existing publicly available data and often integrates distinct datasets together. It does not generate any new experimental data. Second, the project is beyond the capabilities of most individual research labs and instead requires collaboration between two or more labs. Finally, such a project requires diverse scientific perspectives and expertise to come together, often resulting in novel, deeper, and broader insights. You research should focus on well-defined scientific questions or methodological developments that advance molecular and cellular sciences through synthesis research to acquire insights from a collaborative, transdisciplinary approach. This RFP invites proposals for Working Groups that will: Address Compelling Scientific Questions: Tackle novel and significant research questions in molecular and cellular biology through data synthesis. Stimulate Cross-Disciplinary Collaboration: Bring together researchers from diverse fields. Utilize Publicly Available Data: Leverage existing data sources to develop innovative research and analytical strategies. Require NCEMS Support and Resources: The proposed synthesis project must demonstrate a clear need for NCEMS support that goes beyond the capabilities of a single lab or existing collaboration. Promote Collaborative Partnerships: Assemble teams that reflect a range of scientific expertise, geographic locations, career stages, and institutional research levels. Adhere to Open Science Principles: Commit to making findings, data, and analysis workflows publicly available in accordance with community-created policies on open, team, and reproducible science. Train the Next Generation: Provide training opportunities to trainees (graduate students and postdocs) through collaborative research efforts within the working group.",
    "proposals": [
        {
            "proposal_id": "1",
            "proposal_title": "Emergent Order Parameters of Allostery: An AI Framework for Predicting and Engineering Protein Regulation",
            "authors": [
                "Banu Ozkan",
                "Denise Okafor",
                "Vincent Voelz"
            ],
            "authors_departments": [],
            "abstract": "Allostery‚Äîthe ability of a perturbation at one site in a protein to control function at a distant site‚Äî underlies enzyme catalysis, signaling, and gene regulation. Yet we still lack a general, quantitative way to define how ‚Äúallosteric‚Äù a protein is, to compare that across systems, and to predict how substitutions will strengthen, weaken, or rewire that control. This project develops a physics-informed data science framework to obtain quantitative order parameters of allostery and use them to classify, predict, and ultimately engineer regulatory behavior. We will integrate molecular dynamics (MD) simulations, deep mutational scanning (DMS) datasets, and evolutionary information (Table 1) to extract dynamic coupling measures that report how a local perturbation propagates through the protein over time. From these data we will derive physically interpretable allosteric order parameters that quantify three key features: (i) strength of long-range communication, (ii) directionality/asymmetry of information flow, and (iii) timescale of propagation. These parameters will form the basis of an ‚Äúallostery score‚Äù that allows us to categorize the degree and mode of allostery across proteins and across species. We will then train machine learning models to predict how specific amino acid substitutions alter these order parameters ‚Äî i.e., how a mutation changes not just local stability, but global regulatory wiring. Finally, we will apply this framework to design mutations that reprogram allosteric regulation in transcription factors, and experimentally test these designs. The outcome is a generalizable, predictive map from sequence to allosteric control.",
            "full_draft": "Allostery, the process by which a perturbation at one site in a biomolecule modulates activity at a distant site, is a universal mechanism of molecular regulation that underlies enzyme catalysis, signal transduction, and gene expression. 1‚Äì4 Often referred to as the ‚Äúsecond secret of life‚Äù after the discovery of DNA structure, allostery enables proteins to transmit information and coordinate functions across long distances within their structures. This capacity allows for fine-tuned regulation of biological activity in response to ligand binding, post-translational modification, or mutations at distal residues, making allostery a fundamental principle of molecular function. Historically, allostery has been understood through structural and thermodynamic models that emphasize conformational transitions. The Monod‚ÄìWyman‚ÄìChangeux (MWC) model (the symmetry model) and the Koshland‚ÄìNemethy‚ÄìFilmer (KNF) model (the sequential model) established the notion that proteins exist in discrete conformational states, and that ligand binding shifts their equilibrium. 5,6 These frameworks elegantly explained positive and negative cooperativity in multimeric enzymes and laid the foundation for viewing allostery as a structural transition between ‚ÄúT‚Äù (tense) and ‚ÄúR‚Äù (relaxed) states. Later extensions, such as the population-shift and energy-landscape models, 7‚Äì9 recognized that allostery arises not from a single structural change but from redistributions among pre-existing conformational ensembles. Nevertheless, these thermodynamic treatments inherently simplify the protein energy landscape and often cannot capture entropic contributions to allosteric regulation. For example, when ligand binding increases internal disorder (yielding negative entropy changes that enhance catalytic turnover or binding promiscuity), traditional models struggle to describe these effects quantitatively. Modeling such phenomena requires moving beyond discrete-state thermodynamics to an ensemble-based view in which proteins explore a continuum of microstates characterized by correlated fluctuations. 10 The limitations of purely structural or thermodynamic frameworks have motivated an alternative paradigm of ‚Äúdynamic allostery‚Äù which focuses on how allosteric regulation emerges from modulations of normal modes/internal fluctuations rather than from large-scale conformational rearrangements. First 1 articulated by Dryden and Cooper 11 and later formalized by McLeish and co-workers using elastic field theory, 12 this framework posits that local perturbations alter the effective elasticity of a protein, reshaping patterns of correlated motions across the structure. Nussinov and colleagues further highlighted that such dynamic coupling among residues can transmit signals over nanometer length scales, providing a purely physical basis for long-range communication even in the absence of visible conformational shifts. 13 Experimental evidence increasingly supports this dynamic view of allostery. Techniques such as nuclear magnetic resonance (NMR) spectroscopy, single-molecule F√∂rster resonance energy transfer (smFRET), and hydrogen‚Äìdeuterium exchange mass spectrometry (HDX-MS) have revealed that ligand binding, mutations, or allosteric activation often alter residue-level fluctuations and correlated motions even when the overall crystallographic structures remain nearly indistinguishable. 14‚Äì16 These observations demonstrate that functional regulation can be encoded not in changes to mean atomic coordinates, but in shifts in the amplitude, correlation, and timescale of internal motions. In other words, dynamics that carry the physical signatures of allosteric communication. Despite these advances, both classical and modern models of allostery remain largely qualitative. Classical thermodynamic frameworks such as the Monod‚ÄìWyman‚ÄìChangeux (MWC) and Koshland‚Äì Nemethy‚ÄìFilmer (KNF) models focus on equilibrium population shifts between discrete conformations, while more recent computational and AI-based methods emphasize structural or network-level correlations. Reviews of the field 17,18 have catalogued a broad range of approaches, from graph- theoretic community analysis and elastic network models to deep-learning tools for allosteric-site prediction, but none provide a rigorous, quantitative, and generalizable metric for evaluating the degree of allostery in arbitrary proteins. Most existing frameworks identify where allostery occurs (e.g., specific pockets, residue clusters, or communication pathways) but fail to quantify how strongly or in what direction information propagates through the molecular network. Measures such as residue centrality, mutual information, or correlated motions are often heuristic, lacking explicit grounding in physical quantities that reflect energetic coupling or dynamic responses. Consequently, these methods cannot be used to compare allosteric strength or directionality across different proteins, ligand states, or evolutionary variants. Moreover, nearly all allosteric models are validated on individual case studies rather than across diverse protein families, limiting their transferability and generality. What remains missing is a unified, dynamics-based framework capable of quantifying allosteric communication in physical terms, such as fluctuation propagation, dynamic coupling strength, or information transfer efficiency, and of applying these principles systematically across the proteome. Such a framework would, for the first time, enable the quantitative evaluation of the degree of allostery in any protein, connecting molecular dynamics to function in a predictive and comparable way. How can we define, quantify, and classify allostery using measurable, dynamics-derived quantities applicable to any protein? To establish a truly general and predictive theory of allostery, we must define it in terms of physics-based order parameters that capture dynamic coupling, energy transfer, and information flow that reflect the underlying energy landscape, how these order parameters shift when allosteric ligands are bound. Parameters such as correlation functions, dynamic coupling indices, fluctuation-response relationships, and entropy-transfer metrics provide quantitative measures of how perturbations at one site affect distant residues through the protein‚Äôs dynamic network and how networks are rewired when a ligand is bound. 3,19‚Äì22 The increasing availability of large public data sets makes this an especially ripe time to synthesize disparate sources of information to move beyond descriptive models of allostery, towards predictive models. By combining molecular dynamics (MD) simulations, statistical mechanics, and evolutionary data, we aim to construct quantitative descriptors that measure the magnitude, directionality, and symmetry of allosteric communication. Embedding these physical order parameters into data-driven models and graph-based representations will, for the first time, allow us to evaluate the degree of allostery across diverse proteins and conditions. Our goals are 2 to: 1)Integrate deep mutational scanning (DMS) data and MD simulation (i.e. correlation functions) of allosteric proteins with AI to learn dynamic descriptors that can classify degree of allostery, 2)Use the AI/ML model to categorize allostery across the proteome, and 3) Finally,apply this model to proteins from organisms ranging from primitive (fruit fly) to complex (human), to understand evolution of allostery. This unified, dynamics-centered approach transcends system-specific models and offers a predictive foundation for understanding how mutations, ligands, or evolutionary changes rewire communication networks. In doing so, it provides the conceptual and computational basis for designing allosteric drugs and synthetic molecular assemblies with tunable regulatory properties. Proposed Activities. This project will develop a physics-informed data science framework to define and quantify allostery by integrating three major data streams: molecular dynamics (MD) simulations, deep mutational scanning (DMS) datasets, and evolutionary sequence information collected from proteins across diverse taxa. Our overall objective is to move from descriptive accounts of allostery to a predictive, quantitative theory‚Äî one that yields measurable order parameters through which allosteric behavior can be compared and forecast across proteins and species. Our central hypothesis is that allostery can be quantitatively defined by time-dependent correlation features of residue motion. Specifically, we hypothesize that strong and directionally asymmetric dynamic couplings between distant sites‚Äîas captured by correlation functions and fluctuation‚Äìresponse relationships‚Äîcorrespond to measurable allosteric regulation. In other words, the physical ‚Äúwiring diagram‚Äù of long-range communication in a protein can be read directly from its intrinsic dynamics, and the magnitude, directionality, and temporal propagation of that wiring can predict the degree of allostery (i.e., how function alters in the presence of an allosteric ligand). A second hypothesis is that the architecture of these allosteric communication networks is not arbitrary but evolves systematically along evolutionary lineages. From simpler to more complex organisms, we expect optimization of information transmission across the protein structure, reflected in changes in network connectivity, coupling asymmetry, and robustness to mutation. A third working hypothesis is that machine learning models trained on integrated dynamical, mutational, and evolutionary features will generalize‚Äîable to classify the degree and ‚Äúmode‚Äù of allostery in new proteins (or engineered variants) that were never explicitly measured, and to do so with quantifiable accuracy. To test these hypotheses, we will generate and extract physics-based descriptors of allosteric communication from MD simulations of representative allosteric proteins, including signaling proteins (e.g. KRAS, EGFR), transcription factors (e.g. AR, ArgR) and kinases (e.g. BTK, SRC). Through NCEMS data scouting, we have identified 28 proteins in total (Table 1), which all have DMS data available in MaveDB 23 (Data Source 1). these, 10 have MD data publicly available in either MDRepo24 (Data Source 2) or MDVerse 25 (Data Source 3). For systems without available MD data, we will generate initial structures using BioEmu 26 and perform simulations on the Folding@home distributed computing platform.27 For each system, we will analyze multiple states (apo, ligand-bound, and selected mutational variants) to compute residue‚Äìresidue time correlation functions and dynamic coupling metrics, quantifying how a localized perturbation at one residue propagates to distant sites using time- dependent linear response models combined with MD data. In parallel, we will incorporate experimental functional measurements from DMS datasets (Table 1), which report how single or double substitutions alter activity, binding, regulation, or fitness. These data allow us to quantify changes in fitness upon allosteric ligand binding. Through the integrated analysis of MD databases and BioEmu, we will efficiently compute mutation-induced changes in conformational dynamics and correlate them with dynamic descriptors at corresponding residues and along predicted communication networks. Relating MD-derived mutation effects to dynamic descriptors enables a direct test of our central hypothesis: if our dynamic descriptors accurately capture allosteric signal flow, then residues (or residue pairs) with strong, asymmetric coupling in MD should correspond 3 to positions where mutations have large distal functional effects, revealing how specific substitutions modulate allosteric regulation. We will further integrate evolutionary information by mapping conservation, coevolution patterns, and phylogenetic depth onto the same dynamic networks. The goal is to distinguish communication pathways that are conserved‚Äîand thus likely essential‚Äîfrom those that are rewired over evolutionary time. Here we test the evolutionary hypothesis: if allosteric networks are under selection for efficient long-range information transfer, then features such as coupling strength, redundancy of parallel paths, and tolerance to symmetry breaking (as quantified by long-distance dynamic coupling3) should change systematically from primitive organisms to more complex ones. By comparing orthologous proteins across taxa represented in Table 1, we will quantify how these network properties scale with organismal complexity and regulatory sophistication. We will embed all integrated features into a low-dimensional, continuous ‚Äúallosteric order- parameter‚Äù using nonlinear dimensionality reduction (diffusion maps, 28 manifold learning,29 and related approaches 30). This addresses a core gap in the field: most current approaches identify where allostery happens (e.g., a pocket or hinge helix) but cannot assign a quantitative value to how strongly, in what direction, or with what symmetry information propagates through the protein. By projecting diverse proteins (and their variants and ligand-bound states) into a common coordinate system defined by physically interpretable dynamic descriptors, we will generate a quantitative scale of allosteric strength and architecture that is comparable across protein families. This directly tests our generalization hypothesis: if allostery is governed by universal physical principles, unrelated proteins with similar dynamic coupling architectures should cluster together in this space, even if they differ in sequence, fold, or biological role. Finally, we will apply our AI-driven framework to design and experimentally validate allosteric regulation in transcription factors. Using learned dynamic coupling maps, we will identify residues where targeted mutations or ligand binding could reprogram DNA-binding affinity and allosteric control. The predictive models will generate a set of candidate perturbations predicted to strengthen or weaken regulatory coupling. The Okafor lab will test these computational predictions experimentally using mutagenesis and biophysical assays, providing a powerful feedback loop between computation and experiment that demonstrates the practical potential of our framework for the rational design of allosteric regulation. Outcomes By integrating multi-scale biophysical data with modern AI frameworks, this project will: ‚óè Deliver a quantitative and predictive definition of allostery grounded in time dependent order parameter obtained from molecular dynamics. ‚óè Produce open-source analytical tools (AllosteryMap) and curated datasets linking molecular motion, mutational effects, and evolution. ‚óè Provide generalizable AI models to classify and predict allosteric mechanisms across the proteome. ‚óè Generate new biological insights into how allostery evolves and how it can be engineered for synthetic control. 4 Rationale for a working group Addressing the question of how to define, quantify, and predict allosteric effects across the proteome requires an inherently interdisciplinary approach. The Ozkan lab has deep expertise in modeling allosteric interactions using dynamics-based descriptors, answering this question at scale necessitates integration of complementary perspectives and datasets. The Voelz lab brings essential expertise in large-scale molecular dynamics simulations, machine learning, and Bayesian inference to extract and interpret high-dimensional free energy landscapes‚Äîcritical for identifying subtle or emergent allosteric modes. The Okafor lab brings expertise in allosteric transcription factors, an ideal system for investigating allostery due to clear and generalizable functional readouts (e.g. gene expression, DNA binding) in response to ligand binding. The Okafor lab will also provide mechanistic insight through MD simulations and mutational analysis to validate and refine computational predictions. This synthesis project requires uniting evolutionary-scale mutational data (e.g., from deep mutational scanning), atomistic dynamic signatures, and machine learning frameworks into a cohesive model that can predict and classify allosteric behavior across diverse proteins and organisms. No single lab possesses the full toolkit or resources to span this range‚Äîfrom biophysics and AI to comparative evolution and synthetic design. The collaborative working group format is essential to bridge these domains, enable data integration and model validation, and ensure robust generalization of results. Rationale for NCEMS support. We would need NCEMS support to (1) help assemble datasets for synthesis, (2) prepare large-scale simulations to generate dynamical trajectory data, and (3) practical expertise/consultation about the best architectures for machine learning models that can integrate experimental data with dynamical descriptors from simulations. Requested resources Proposed Timeline Development 5 Resource Resource request Overall duration of support (up to 2 years) 2 years Staff Scientist time (up to 8 months full time equivalent over 2 years) 8 months Staff Scientist Assistant time (up to 11 months full time equivalent over 2 years) 11 months Estimated storage requirements in terabytes 200 TB Estimated CPU requirements in CPU hours 5000 CPU hours Estimated GPU requirements in GPU hours 25000 GPU hours Phase Timeline (Months) Major Activities Project Initiation and Data Integration 1‚Äì4 Identify target proteins and orthologs across species using curated data; Generate structural models using AlphaFold, Boltz2, and BioEmu to initiate dynamics simulations; Extract thermodynamic and biophysical quantities from DMS data; Begin preliminary MD simulations on wild-type and mutant variants. Dynamics Analysis and Descriptor Development 5‚Äì12 Compute dynamic coupling and asymmetry using time-dependent fluctuation- based descriptors; Integrate DMS data with simulation-derived features; Apply Bayesian inference to explore latent allosteric landscapes; Build ML models to classify and quantify allosteric effects; Publish the first descriptor paper. Experimental Validation and Allosteric Design 11‚Äì18 ‚Ä¢Use trained AI/ML models to design variants with varying degrees of allostery; Evaluate model generalizability across transcription factor families; Refine models using feedback from collaborators; Publish manuscripts on allosteric design descriptors and tools. Evolutionary and Database 18‚Äì24 ‚Ä¢Apply models to orthologous transcription factors across species; Publish manuscript on the evolution of allostery;Extend analysis to human proteome to quantify allostery, Release curated datasets and tools in public repositories.",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "2",
            "proposal_title": "From Protein Knots to Pierced Lassos: Unraveling the Evolutionary Origin of Complex Topologies",
            "authors": [],
            "authors_departments": [],
            "abstract": "Proteins can adopt intricate three-dimensional structures, including complex topologies such as knots, slipknots, pierced lassos, and even more complex structures. These entangled architectures challenge traditional views of protein folding and stability, suggesting that nature may use topological complexity as a mechanism to fine-tune function, regulation, or mechanical resilience. Despite growing structural data, the evolutionary origins and functional consequences of these nontrivial topologies remain largely unexplored. This project aims to leverage existing databases to identify the evolutionary turning points at which topological changes first arose within protein families. By integrating sequence, structural, and phylogenetic data from resources such as UniProt, PDB, AlphaFold, AlphaKnot, and AlphaLasso, we will reconstruct annotated phylogenetic trees that map the emergence and diversification of entangled proteins across evolutionary time. We will analyze whether specific sequence motifs, disulfide patterns, or intrinsically disordered regions are associated with the onset of entanglement, and whether such proteins are enriched in pathogenic or stress-adapted organisms. Furthermore, we will assess how topological complexity correlates with functional hotspots, including catalytic or allosteric regions, to reveal possible adaptive advantages. This work will establish a framework to trace, predict, and design protein topologies with distinct structural and functional characteristics. The outcomes will not only deepen our understanding of how topology shapes evolution and biomolecular behavior but also provide an open, data-driven platform for studying entanglement as a functional and evolutionary principle in protein science.",
            "full_draft": "INTRODUCTION AND GOALS Proteins exhibit an extraordinary diversity of three-dimensional structures, many of which feature complex topologies such as knots, slipknots, and pierced lassos. These nontrivial polypeptide entanglements challenge classical views of protein folding or stability, suggesting that evolution has harnessed topological complexity to fine-tune function, conformational dynamics, and regulation. Despite growing structural data, the evolutionary emergence of these entangled topologies remains largely unexplored. Understanding when and how such structural features first appeared in evolution could reveal key insights into the relationship between sequence divergence, folding constraints, and adaptive function. This proposal seeks to address the question: Can we leverage existing structural and phylogenetic databases to identify the evolutionary ‚Äúturning point‚Äù at which a topological change occurred within a protein family? By integrating structural bioinformatics, topology analysis, and phylogenetic reconstruction, this work aims to uncover how nature‚Äôs use of entanglement has shaped protein evolution and function. Potential Secondary Questions: 1. What proteomic data can help us identify properties of proteins with nontrivial topologies that are statistically different from those with trivial topology? 2. How do sequences involved in protein entanglement contribute to functional sites of the protein, such as active sites, allosteric sites, cryptic sites, or binding specificity? 3. Through the analysis of curated biological databases, what patterns of nontrivial protein topologies can be identified in pathogenic organisms, and how might these structural features contribute to pathogenicity, immune evasion, or other disease-related functions? Approach. Based on current resources, no single database directly identifies the precise phylogenetic branch point at which a topological change occurs. However, integrating existing databases, such as sequence databases (UniProt) [6], structure databases (PDB [7], AlphaFold DB [1]), and topological annotation databases (KnotProt [3], LassoProt [5], AlphaKnot/AlphaLasso [2, 4]), with functional annotation [8-14] and evolutionary annotation [15-18] allows us to reconstruct phylogenetic trees and map the emergence of knotted, slipknotted, and lassoed topologies. By combining structural annotations with evolutionary analysis, it is possible to infer likely branch points where nontrivial topologies first appear, marking distinct epochs in protein evolution that reflect the emergence of new structural complexity. The proposed project aims to explore the structural and functional implications of protein entanglements, focusing on knots, covalent lassos, and non-covalent lassos. We anticipate generating at least two primary literature articles, including annotated phylogenetic trees that track the evolutionary emergence of these nontrivial topologies. The initial analysis will focus on knotted proteins, a well-characterized set, before expanding to other entangled topologies to capture broader evolutionary patterns. Previous studies provide a foundation for this work. A significant sequence homology was identified among knotted and unknotted proteins, noting that knotted proteins often differ from unknotted ones primarily by additional loop segments [19]. Slipknotted and unknotted monovalent cation-proton antiporters were demonstrated to be evolved from a common ancestor, illustrating how entanglement can emerge during evolution [20]. Sequence-based machine learning was used to identify unknotted proteins within knotted families, providing critical information for constructing phylogenetic trees [21]. Finally, having an AI model [21] that determines whether a given sequence is knotted allows us to fill in the2 proteins is missing gaps between unknown sequences. And finally, but most importantly, a correct (manually verified) annotation and division into all known families for knotted [22] and lasso type [23] now available. Together, these studies suggest that sequence, structure, and evolutionary data can be integrated to identify the emergence and functional relevance of nontrivial protein topologies. PROPOSED ACTIVITIES We will start with 6 superfamilies of knotted proteins where the knot topology is strictly conserved (SPOUT, ITIH, AdoMet synthase, TDD, Carbonic anhydrase, UCH) and 3 superfamily where topology is not strictly conserved (ATC/OTCase, knotted solenoid fold (new deposition to the PDB), CACA knotted membrane protein) based on table 1 from [22] and 26 family of lasso type protein (here we list the biggest superfamily, more than 10000 members, where the L_1 topology is represented by at least 50% of family members: - IPR018152, IPR024134, IPR001424, IPR036423, IPR007112, IPR036749*, IPR039417**,IPR039391*, IPR003245, IPR025661, IPR013201, IPR001508 ,IPR025660, IPR018202 ,IPR019594, IPR001283, IPR013128, IPR015683*, IPR001190, IPR000668, IPR001563 ,IPR009009, IPR000169, IPR036908, IPR001320, IPR001828) based on [23] and our classification. For each target, we will identify a high-quality orthologous clade using the OMA database [15]. Clade sequences will then be systematically annotated for topological features using specialized tools, including KnotProt 2.0 [3] and LassoProt [5] for knotted and lasso topologies, as well as AlphaProt 2.0 [2] and AlphaLasso [4] for advanced structural and topological predictions. The TimeTree database will be used to estimate evolutionary divergence time [16], focusing on the aforementioned families. This approach will enable comprehensive mapping of sequence-to-topology relationships within evolutionary lineages. Several proteome-level databases, such as PhosphoSitePlus [12] for post-translational modifications, Human Protein Atlas [13] for tissue expression and subcellular localization, and MobiDb [14] for the presence of intrinsically disordered regions, will be used to characterize properties of proteins with nontrivial topologies (first secondary question). Thanks to the available UniProt [6] references, we will overlay those features on top of the phylogenetic maps to uncover evolutionary patterns associated to the presence of entanglement which distinguish nontrivial from trivial topologies in a statistically significant manner. In a similar way, we will search for evolutionary patterns connected to the presence and emergence of entanglement using functional annotations at a residue level from InterPro/PROSITE [7] (second secondary question). The Meltome Atlas dataset [10] can be used to assess the correlation between entanglement and thermal stability. We will test the relevance of entangled topologies to pathogenic organisms and disease (third secondary question) in different respects. First, the MaveDB database [11] will be used to test the possible connection, at the residue level, between the presence of entanglement and the loss of function/activity in deep mutational scanning data. Second, disease-related databases will be used to test the possible association with entanglement features such as gene essentiality across hundreds of genomically characterized cancer cell lines (DepMap) and the presence of the disease in itself (Open Targets Platform [8] and DisGeNet [9]). Third, resources such as NCBI Virus [17] and GISAID [18] will be used to identify high-quality orthologous clades in which to look for evolutionary patterns, specific to viruses, connected to the presence and emergence of non-trivial topologies. Our team has added Dr. Ryan Cheng and his student Virangi Hewage from the University of Kentucky (see SI Table). Dr. Cheng, an expert in evolutionary analysis, will strengthen the project by enabling the 3 integration of complex datasets, conducting in-depth sequence‚Äìstructure‚Äìfunction analyses, and supporting more robust, data-driven conclusions. OUTCOMES This project is expected to produce several key outcomes. First, we will generate annotated phylogenetic trees highlighting the evolutionary emergence of knotted, slipknotted, and lassoed protein topologies. Second, we will identify sequence and structural features that statistically distinguish proteins with nontrivial topologies from trivial ones, including correlations with functional sites such as active, allosteric, or cryptic sites. Third, analysis of pathogenic proteins will reveal patterns by which topological complexity contributes to stability, interaction specificity, or virulence, providing mechanistic insights into disease-related functions. The significance of these outcomes lies in advancing fundamental understanding of protein evolution, structure, and function. By integrating sequence, structural, and evolutionary data, this work will provide new frameworks for predicting the emergence of complex topologies and their functional consequences. These insights will enable rational design of proteins with specific topologies and properties, informing applications in biotechnology, synthetic biology, and drug design. Additionally, understanding the role of entangled topologies in pathogenic proteins may reveal new targets for therapeutic intervention. Overall, this project will bridge computational, structural, and evolutionary biology to uncover principles governing the formation and function of nontrivial protein topologies. Working group approach. We envision synthesizing sequence annotation, protein entanglement, and evolutionary databases; the combined expertise of this Working Group will be necessary to properly analyze the resulting data. Understanding fundamental biological questions on the evolution of nontrivial topologies will provide new tools to design novel topologies with specific properties, e.g., mechanical. This is a timely pursuit, given the growing availability of high-quality biological databases and the advancement of analytical tools capable of extracting meaningful patterns from complex sequence and structural data. NCEMS support. While our team offers broad and complementary expertise, including experience in database analysis, additional support is required to manage and interpret the large-scale and complex proteomic datasets generated in this project. A major component of this project involves modeling protein structures at evolutionary branch points, where little structural information is available. Given the variety of entangled protein topologies to be analyzed, the required structural modeling exceeds the capacity of any single subgroup. NCEMS resources are therefore essential to enable comprehensive analysis of key protein structures, integration of large-scale sequence and structure data, and the application of advanced modeling and prediction tools. Requested resources. To successfully carry out this project, we request NCEMS support for IT and personnel resources. Specifically, we require access to sequence, structure, functional and topological annotation, and phylogenomic databases, which will be generated and integrated using NCEMS resources alongside publicly available datasets. The ideal NCEMS Staff Scientist would have expertise in large language models (LLMs), proteomic databases, and protein structure prediction and analysis. 4 We will annotate phylogenetic trees and generate model structures at evolutionary branch points where topological changes arise; the latter task dominates GPU usage. We plan to deal with ~300,000 sequences overall (29 families/superfamilies with ~ 10000 sequences each). We will need ~20 TB durable storage, ~75,000 CPU core‚Äëhours for database search (~15 min per sequence), and ~650 GPU‚Äëhours for structure modeling at evolutionary branch points using A100/H100 80 GB GPUs, with up to 8 GPUs in parallel. We expect to model on average 3 branch points per family/superfamily, with 5 possible representatives per branch, and 3 seeds per representative in the structure prediction runs, leading to overall ~1350 models to be generated, each taking 0.5 GPU hours. We estimate 3 branch points per family to take into account both possible false positives and the possibility of different topological changes within the same family/superfamily in the case of the more complex knotted/lassoed topologies. Proposed Timeline. We anticipate a two-year period of NCEMS support to carry out the proposed data synthesis and analysis (Table 1). Week-to-week progress will be coordinated through regular online meetings, and the Working Group will convene for one in-person meeting at the midpoint of the project to review progress and refine strategies. Personnel exchanges within the Working Group will be used to facilitate collaboration and ensure effective progress toward project milestones. We request 8 months of Staff Scientist time (full-time equivalent) and 11 months of Staff Scientist Assistant time (full-time equivalent) distributed over the two-year period to support data analysis, structural modeling, and database integration. The project is expected to require approximately 20 terabytes of data storage to accommodate large-scale sequence, structure, and topological datasets. This timeline and level of support align with the scope and complexity of the proposed project. These resources are critical to enable large-scale data synthesis, structural modeling at evolutionary branch points, and comprehensive analysis of diverse entangled protein topologies. Table 1. Proposed Project Timeline over 24 months. Activity Month Data gathering and integration for knotted proteins 1-6 Generate annotated phylogenetic trees for knotted/lasso type proteins and structural models at evolutionary branch points 7-10 First publication on knotted proteins across evolutionary time 11-14 Follow-up investigation on additional topologies 15-20 Second publication on additional topologies 21-24",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "3",
            "proposal_title": "Deciphering systemic biological networks through AI-driven multi-omic integration",
            "authors": [],
            "authors_departments": [],
            "abstract": "Organ cross-talk refers to the bidirectional communication between different organs that enables the body to maintain physiological homeostasis, respond and adapt during dynamic states. Disturbances to these inter-organ communication networks may result in inappropriate physiological responses, thereby compromising systemic function. This proposal will leverage a unique publicly available large-scale, multi-organ, multi-omic dataset comprising 18 solid tissues from rats subjected to an endurance exercise training program. Our hypothesis is that organ-specific signaling hierarchies will determines the coordinated multiorgan responses that drive systemic adaptation. Following organ-specific multi-omic vertical integration, we will determine 1) the predictors of phenotypic adaptations, and 2) the cross talk between organs via horizontal integration. The computed networks will provide the foundation to generate a digital twin rat which will serve as a valuable tool to model organismal responses to physiological, environmental and pharmacological challenges. This comprehensive, temporally resolved analysis will offer unprecedented insights into how molecular interactions are coordinated across organs and levels of organization to determine whole-organism physiological function.",
            "full_draft": "4. Introduction and Goals ‚Äì A statement of the outstanding question in molecular and cellular biology data science being addressed, the significance of answering that question, and a concise review of the concept and the literature to place the project in context. The primary goal of this project is to uncover the biological networks that mediate communication between organs. Such inter-organ signaling remains largely unexplored at the whole-organism level. The outcomes of this research will provide a deeper understanding of how physiological processes are coordinated both within individual organs and across the entire organism and will reveal the molecular mechanisms that enable this integration. Physiological homeostasis and adaptation arise from integrated inter-organ communication, or cross-talk. These mechanisms coordinate systemic function across diverse tissues through the production of signaling factors such as hormones, cytokines, and metabolites that enable organisms to sense, respond, and adapt to internal and external cues (Borges 2024, Bento 2019). Disruptions in these communication pathways can lead to dysfunction and maladaptation; thus, understanding organ cross-talk is fundamental to elucidating the mechanisms of organismal physiology and the molecular basis of many disease states. While communication between organs has begun to be characterized at the single-organ level (e.g. Kirk 2025), the molecular architecture, directionality, and temporal regulation of these interactions across the entire organism remain poorly understood. A comprehensive analysis of organ cross-talk will require multi-omic, multi-organ, and temporally resolved data, coupled with advanced computational approaches. This proposal will leverage genome, epigenome, transcriptome, proteome, phosphoproteome, metabolome, and lipidome datasets derived from 18 tissues of rats subjected to eight weeks of endurance exercise training (Amar , 2021, 2024, Sanford 2020). Using state-of-the-art computational methodologies, we will integrate these datasets to map molecular responses across omic layers and time, revealing how molecular networks are coordinated across organ systems to drive adaptive and physiological functions. Although recent studies have attempted to construct organ cross-talk networks, their reliance on heterogeneous and non-homologous datasets limits biological interpretability, yielding computational associations with limited physiological relevance (Li, 2025). Inconsistencies in dataset structure, format, and origin hinder direct integration and obscure meaningful biological insights. To overcome these challenges, we will implement a reaction‚Äìdiffusion equation‚Äìbased graph network to identify key regulatory nodes, infer hierarchical signaling structures, and model the dynamic interplay among organs using multi-omic data collected from the same animals (MoTrPAC). The resulting networks will form the foundation for the development of a digital twin rat to enable the simulation of molecular and physiological behavior under various perturbations. This novel platform will enable the exploration of organismal responses to positive (e.g., exercise-induced) and negative (e.g., disease-related or environmental) stimuli, providing a powerful tool for understanding and predicting systemic adaptation and dysfunction. 5. Proposed Activities ‚Äì This should include a clear statement of specific data and analytical tools that will be required for the project. The proposal should also include a clear statement on how data synthesis across the datasets will occur, given the specific datasets cited in the Datasets Table (see #12 Datasets Table below), what hypotheses (if any) will be tested, and what analyses are anticipated. Our approach is built upon the reaction‚Äìdiffusion framework, which captures both local dynamics and spatial (or network-based) interactions among organs. Specifically, we model the temporal evolution of genomic or biomarker data as != ùêøùë¢ + ùëÖ$ (ùë¢), where ùë¢(ùë°)represents the state vector of genomic or !# molecular features across organs or tissue regions, ùêøis the graph Laplacian operator that encodes organ- to-organ interactions, and ùëÖ$ (ùë¢)is a neural network‚Äìbased reaction term parameterized by weights ùúÉ. The graph Laplacian is constructed from physiological or functional connectivity data allowing the model to represent inter-organ influence and communication. The reaction term ùëÖ$ (ùë¢)captures nonlinear biochemical or regulatory processes within each organ that may not be explicitly modeled by traditional differential equations. The input data consist of longitudinal (e.g. genomic) measurements obtained from multiple organs or tissues. These data will be preprocessed using principal component analysis (PCA) to identify leading modes and reduce dimensionality, resulting in a compact latent representation of key genomic features. The output of the model is the predicted temporal and spatial evolution of these genomic states, which can be compared with observed data for validation and parameter estimation. The neural network within ùëÖ$ (ùë¢)will be trained using a physics-informed loss function, ensuring that predictions are consistent with both empirical data and the reaction‚Äìdiffusion dynamics. This hybrid structure combines mechanistic modeling with data-driven learning, enabling robust generalization even with limited data and offering interpretable insights into novel biological mechanisms. The newly identified networks will be used to create a digital twin rat. This will serve as a computational surrogate that mirrors the physiological and molecular dynamics of individual rats under various experimental conditions. Using our reaction‚Äìdiffusion‚Äìbased neural network framework, we will integrate multi-organ genomic, metabolic, and physiological data to model organ-to-organ interactions through the graph Laplacian operator, while the neural network component will capture the nonlinear regulatory and biochemical processes unique to each organ. A key feature of this framework is the continuous, bidirectional exchange of information between the physical and digital twins. Data from the physical rat, e.g., real-time physiological measurements, periodic molecular profiling, and behavioral assays, will be streamed to the digital twin to iteratively update model parameters and boundary conditions. These updates ensure that the digital twin remains synchronized with the evolving biological state of its physical counterpart. Conversely, the digital twin will generate in silico predictions of disease trajectories, organ- level responses, and potential effects of pharmacological or genetic perturbations. These predictions will then guide subsequent in vivo experiments, helping to refine hypotheses, optimize intervention timing, and prioritize experimental conditions that are most informative or impactful. Over successive iterations, this feedback loop will enable adaptive refinement of both experimental design and model fidelity. In this way, the digital twin becomes not merely a passive replica but an ‚Äúactive co-experimenter‚Äù, a computational partner that continuously learns from, informs, and augments the physical experiments. Ultimately, this approach will reduce animal use, accelerate discovery, and deepen our mechanistic understanding of system-level physiological regulation and disease progression. 6. Outcomes - Proposals should include a clear statement about the expected outcomes from this research project and the significance of those outcomes with regards to how they will advance the field. We will define the molecular basis of organ cross-talk. The newly identified organ-specific emergent properties will provide a new understanding of how body function is regulated and will allow us to derive testable biological predictions to further understand the signaling hierarchies that coordinate whole body physiology. 7. Rationale for a working group approach ‚Äì Explain why a working group approach is needed to tackle this problem. Why is the project leader‚Äôs lab unable to do it on their own? As a biologist, I lack the data science/computational expertise necessary to integrate large scale datasets or derive computational models of biological function. For this project, I will rely on expertise in computational data sciences, bioinformatics and AI/ML modeling. Thus, an interdisciplinary team is needed to integrate multi-scale, multi-omic datasets in a biologically meaningful way. 8. Rationale for NCEMS support - Why should this activity be conducted with NCEMS support? Refer to specific NCEMS support options listed elsewhere in this RFP. This project fits within the goal of NCEMS to synthesize publicly available datasets to answer fundamental questions in molecular and cellular biology, i.e. to define novel organ-specific emergent properties that govern higher organismal function. 9. Requested resources - Briefly describe any needs for IT support that are important to the success of the proposed project, including data storage, compute, and software needs. Also state the NCEMS staa scientist expertise needed, and the amount of NCEMS staa scientist time required as a percentage of a full-time employee (see Section ‚ÄòNCEMS Support for Working Groups for time limits). Further state whether a staa scientist assistant is required and what expertise they should have. We anticipate performing approximately 100 independent model training and validation runs, each encompassing multiple phases of training, hyperparameter optimization, and cross-validation. Based on prior benchmarks with similar architectures and dataset scales, each run is expected to require roughly 200 GPU hours, resulting in a total estimated demand of 20,000 GPU hours (100 runs √ó 200 GPU hours per run). These computations will primarily support the training of our reaction‚Äìdiffusion‚Äìbased neural network models and the exploration of organ-specific parameter spaces under varying biological and experimental constraints. In addition to GPU utilization, we project the need for approximately 10,000 CPU hours to handle complementary computational tasks such as data preprocessing, normalization, PCA-based dimensionality reduction, graph construction for inter-organ connectivity, and large-scale sensitivity and stability analyses of trained models. We further request 10 TB of high-performance storage to accommodate model checkpoints, intermediate parameter files, simulation outputs, and curated multi- omic and physiological datasets. This storage allocation is essential to enable reproducibility, facilitate multi-run comparisons, and maintain persistent digital twin states across iterative model updates and experimental feedback cycles. Staff expertise availability and time involvement will be determined after consultation with the NCEMS team after assessing the projects‚Äô demands. 10. Proposed Timeline ‚Äì State the duration of NCEMS support needed (ranging from 6 months to 2 years), as well as the anticipated timeline, and milestones. Short (6 months) and long (2 years) duration support is of equal importance to NCEMS. It is important that the timeline of support matches the proposed project. Our goal is to generate sufficient data during YR1 with at least multi-omic integration of 1 organ to: 1) publish a proof of principle study, and 2) submit an NIH-R01 type proposal (NIA manages the MoTrPAC study). By end of YR2 we expect to have a complete map of the interactions between organs, and have identified the molecular drivers of physiological adaptation (extended manuscript #2)",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "4",
            "proposal_title": "Conserved Disorder: Learning Sequence Conservation from Functional Constraints in Intrinsically Disordered Regions",
            "authors": [],
            "authors_departments": [],
            "abstract": "Intrinsically disordered regions (IDRs) of proteins play essential roles in regulation, signaling, and molecular recognition, yet their evolutionary conservation patterns remain poorly understood. While folded domains exhibit clear conservation of structure and function, IDRs evolve rapidly and appear sequence diverse, obscuring how they encode functional specificity. Bridging the sequence function relationship in IDRs and even their reliable functional annotation remains a major challenge. Emerging evidence, however, suggests that subsets of IDRs harbor conserved sequence features such as short linear motifs (SLiMs), charge patterning and aromatic clustering that give rise to transient structural elements and encode critical molecular functions. This project aims to systematically identify and characterize the most conserved IDRs across proteomes by integrating large-scale sequence alignments, molecular dynamics (MD) simulations, and experimental functional annotations from publicly available databases. By combining resources such as UniProt, Eukaryotic Linear Motif (ELM), IDRome, and MaveDB, we will quantify the interplay between evolutionary, structural, and functional signatures of conservation in disordered regions. Our multidisciplinary Working Group unites computational and experimental biophysics to build a curated, open-access database of conserved IDRs. This synthesis effort will reveal how dynamic, flexible protein regions maintain specific and evolvable functions, an emergent property at the mesoscale interface between sequence evolution, molecular structure, and cellular regulation.",
            "full_draft": "3. Introduction and Goals Main challenges. The long-term objective of disordered protein research is to establish a quantitative sequence‚Äìensemble‚Äìfunction relationship that can explain how sequence variations shape conformational behavior and functional outcomes(1‚Äì3). Achieving this goal remains difficult for three main reasons. First, IDR sequences change rapidly through evolution, making it difficult to identify conserved elements that guide functional inference(2). Second, their flexible conformations complicate structural characterization and comparison of ensemble properties(4). Third, functional annotation is highly biased toward folded domains and lacks IDR-specific annotation, leaving few experimentally validated references for mapping sequence features to function(5). Rationale. A practical way to make progress toward this larger goal is to first study the subset of IDRs that are relatively conserved. These regions represent tractable systems where evolutionary constraints are detectable, structural tendencies are recurrent, and functional data are more often available; yet, not all the disordered sequence is conserved giving rise to alterations in the overall properties of these IDRs. Understanding how these ‚Äúconserved‚Äù IDRs encode function will reveal which sequence features and physicochemical properties are maintained under selection, providing a foundation for extending these principles to the broader landscape of more variable disordered regions. To accomplish this, we will analyze two complementary data sources. To tackle the annotation and characterization of ‚Äúrelatively conserved‚Äù sequences, we will focus on proteome-scale synthesis by integrating experimentally annotated and predicted motifs from ELM(6) and IDRome(7) with functional and regulatory data from MaveDB(8) and UniProt(9). As a second instance, to explore the role of more variable disordered regions, we will investigate viral proteomes(10), where disordered regions evolve rapidly and contain dense mutational and functional information. Together, these datasets balance evolutionary breadth and sequence depth, enabling identification of both general trends and specific mechanisms that define conserved disorder-mediated function. Key questions. This project therefore addresses the central question: what defines the most conserved IDRs across evolution, and how do their sequence and structural features give rise to conserved molecular functions? We will pursue this through three objectives. First, we will identify conserved IDRs by synthesizing large-scale evolutionary and structural data to establish quantitative definitions of conservation in disordered contexts. Second, we will investigate how conserved IDRs differ from variable ones in sequence composition, transient structure, and coevolutionary coupling with neighboring domains. Third, we will integrate post-translational modification (PTM) and deep mutational scanning (DMS) data to evaluate how functional and regulatory constraints reinforce conservation within IDRs. Significance. By focusing on conserved IDRs and cross-referencing them with diverse functional datasets, this work will establish an essential foundation for IDR functional annotation. The resulting conserved regions can serve as anchor points for future sequence alignment and comparative analysis, ultimately advancing a more general framework for connecting sequence, ensemble, and function in the disordered proteome. 4. Proposed Activities This project aims to systematically identify and characterize conserved intrinsically disordered regions (IDRs) by integrating evolutionary, structural, and experimental data. The combined framework will enable quantitative assessment of conservation within disordered sequences and link conserved features to functional outcomes. Preliminary data. Our initial analysis focused on identifying conserved motifs and experimentally constrained IDRs across diverse proteins. In a preliminary data-scouting effort supported by Dr. Daniel Nissley (NCEMS), we limited the dataset to experimentally validated functional annotations, which represent a smaller but higher-confidence subset compared to bioinformatically inferred annotations. From this analysis, we identified 103 proteins that contain experimentally validated deep mutational scanning (DMS) data (from 1149 UniProt IDs in MaveDB), short linear motifs (from 2728 unique sequences in the ELM database), and PTM annotations (from UniProt) (Fig. 1B). Using the most recent disorder predictor AIUpred(11), we calculated per-residue disorder scores and used human p53 as an illustrative example (Fig. 1A). Although DMS studies are often focused on structured, functionally important domains, a substantial number of sequences also contained highly disordered regions (Fig. 1C) with multiple PTM sites and ELM motifs (Fig. 1D). We next applied a filtering pipeline: sequences with >80 % disordered coverage and >30 disordered residues in MaveDB regions (AIUpred threshold = 0.5) were retained, yielding 23 sequences. Requiring at least one ELM motif and one PTM site within the disordered region reduced this to 8 sequences (orange dots in Fig. 1C and 1D), representing ~8 % of the 103 experimentally validated proteins. Figure 1. Framework integrating mulitple datasets. (A) Per-residue disorder scores predicted by AIUpred (blue), experimentally annotated ELM motifs (green), and PTMs (red) are shown for human p53. (B) Workflow summarizing data source integration. Numbers in parentheses indicate the number of sequences after each filtering step. (C) Percentage of disordered residues and number of disordered residues in MaveDB across 103 proteins with experimental annotations. (D) Relationship between the number of PTM sites and ELM motifs. Orange dots in (C) and (D) showed the 8 sequences after filtering. Of the eight sequences identified, most are involved in transcriptional regulation or signaling, including p53 (Uniprot ID: P04637), Lyn (P07948), HCK (P08631), CBX5 (P45973), FUBP2 (Q92945), and TAZ (Q9GZV5). The remaining proteins, Œ±-synuclein (P37840) and the genome polyprotein (Q80J95), represent distinct cases associated with synaptic vesicle trafficking regulation or viral regulatory mechanisms. Increasing evidence has highlighted transient, specific interactions within disordered transcription activation domains(12, 13), suggesting that these regions might be evolutionarily constrained. Together, these findings provide strong preliminary evidence that combining sequence databases, disorder predictors, and functional annotations can yield a curated resource of functionally validated, conserved IDRs. Broad proteome-scale synthesis. To expand beyond experimentally validated motifs, we will apply ELM prediction tools(6) to all human IDRs in IDRome(7) (28,058 IDRs). This will generate a comprehensive set of predicted motif occurrences. These predicted and known motifs will be integrated with DMS and PTM annotations to construct a proteome-scale map of IDRs under functional constraint. Based on our preliminary finding that ~8 % of proteins contain functionally annotated disordered regions, we estimate identifying roughly 1,500 sequences across the proteome. This expanded dataset will enable proteome- wide assessment of conserved disorder features and form the foundation for a publicly accessible database of conserved IDRs. Such a resource will guide experimental design and facilitate hypothesis-driven studies of IDR-mediated regulation. Sequence-level features. Because conservation within IDRs may not occur at the single-residue level but rather through averaged sequence features across short motifs, we will calculate physicochemical and compositional descriptors (e.g., charge, hydropathy, and feature patterning(14‚Äì16)) using fragment windows of 5‚Äì10 residues. This approach will allow correlation of specific sequence features with functional conservation, revealing underlying mechanisms that couple sequence patterns to biological roles in IDRs. Structural-level features. Given the vast sequence diversity of IDRs, learning direct sequence‚Äìfunction relationships remain challenging. To complement sequence-based analyses, we will perform all-atom explicit-solvent molecular dynamics simulations using state-of-the-art IDR force fields(17, 18) on the curated dataset. We will extract global (e.g., radius of gyration), pairwise (e.g., distance distributions, relaxation dynamics), and local (e.g., secondary structure propensity, solvent accessibility) structural descriptors. These metrics will be evaluated to determine whether structural organization within IDRs correlates with functional annotations and complements sequence-based conservation. Viral high-variation synthesis. In parallel, we will extend the same framework to viral proteomes from GISAID(10) (e.g. ~15000 hCoV-19 sequences), which offer a natural testbed for exploring functional disorder under high sequence variability. Viral IDRs evolve rapidly but often maintain conserved motifs, charge patterning, and biophysical properties. By analyzing disordered viral regions with available mutational and structural data, we will assess how functional constraints are preserved despite extensive sequence divergence. This viral dataset complements the human proteome analysis by emphasizing naturally variant systems, offering high-resolution insights into how evolutionary selection maintains function through dynamic and flexible sequences. Integrative framework with predictive power. To unify sequence- and structure-level analyses, we will develop a machine learning framework that integrates physicochemical descriptors, motif occurrences, and structural features from simulations. Using the curated database as training data, the model will learn patterns predictive of functional conservation across IDRs. This approach will enable rapid identification of conserved functional regions in new sequences and provide an extensible platform for data-driven prediction and hypothesis generation. Alternative strategies. If specific datasets prove incomplete for our large-scale analysis, several alternative data sources are available to ensure continuity of analysis and database development. For IDR sequence data, MobiDB(19), which provides disorder predictions for 245 million entries, offer extensive coverage across species. For DMS and functional variant data, ProteinGym provides machine learning‚Äìready benchmark sets(20) and ClinVar provides human genetic variations(21). Both complement MaveDB entries. For PTM annotations, alternative repositories such as PhosphoSitePlus(22) also provides experimentally verified data. Orthologue databases including OrthoDB(23) and Ensembl(24) will be employed as an independent line of evidence to evaluate cross-species sequence conservation within identified IDRs. These resources enable additional validation of conserved motifs and sequence features through evolutionary context, complementing the functional annotations derived from DMS and PTM. 5. Expected Outcomes This project will deliver a comprehensive framework and dataset for identifying and characterizing conserved IDRs. Specifically, we expect to: 1) Establish a curated database of ~1,500 human IDRs that integrate disorder prediction, motif occurrence, experimental perturbation data, and post-translational modifications; 2) Quantify conservation patterns within disordered regions using both sequence-derived and structure-informed features, and reveal sequence and structural determinants that underlie functional constraint in dynamic, flexible regions; 3) Demonstrate predictive generalizability across viral proteomes, highlighting conserved mechanisms in rapidly evolving systems; 4) Provide an open, extensible platform for the community to explore conserved disorder features and guide future experimental design. Together, these outcomes will define conserved functional signatures in IDRs, bridging molecular sequence variability and biological robustness with structural insights. 6. Rationale for a Working Group Approach No single laboratory can independently address the synthesis required for this project. The proposed work demands cross-disciplinary integration of molecular biophysics, bioinformatics, and simulation methodologies, along with experimental expertise to coordinate access to heterogeneous datasets. This collaborative framework leverages the complementary strengths of the Mittal, Soranno, and Zheng laboratories, spanning data-driven modeling, molecular dynamics simulations, and biophysical interpretation. Together, the team will develop shared computational infrastructure for data harmonization and analysis while training the next generation of interdisciplinary scientists at the interface of computation and experiment. 7. Rationale for NCEMS Support NCEMS support is essential for integrating large, heterogeneous datasets and ensuring open, reproducible analysis. Dr. Daniel Nissley from NCEMS has already provided key expertise in curating the preliminary database, demonstrating the value of NCEMS-trained scientists in data harmonization and workflow design. Continued support will enable development of scalable bioinformatics pipelines, metadata integration, and reproducible computational frameworks. NCEMS will also provide CyVerse-based compute and storage resources for large-scale sequence alignment and database hosting, ensuring open- access dissemination consistent with its mission to advance collaborative and transparent science. 8. Requested Resources The proposed project will run for two years, requiring dedicated computational and data management support from NCEMS. We request 8 months of full-time equivalent (FTE) effort from an NCEMS staff scientist with expertise in bioinformatics pipeline development, data harmonization, and large-scale database management. This scientist will lead the integration of sequence, structure, and functional datasets; optimize workflows for reproducibility; and coordinate storage and compute resources through CyVerse. We further request 11 months of full-time equivalent effort from a staff scientist assistant skilled in Python-based data processing, metadata annotation, and cloud workflow automation. The assistant will support curation of the conserved IDR database, parallelization of large-scale sequence alignment tasks, and integration of machine learning‚Äìbased investigation on the curated database. The project will require approximately 50 TB of storage to host integrated datasets, intermediate analysis files, and simulation trajectories. We request 1,000,000 CPU hours to support large-scale sequence alignment, disorder prediction, and statistical computation of sequence and structural features across ~50,000 IDRs. In addition, 1,000,000 GPU hours are requested for all-atom molecular dynamics simulations for representative IDRs (~1,500 sequences). These computational resources are essential for achieving scalable, high-resolution characterization of conserved disorder features. 9. Proposed Timeline During Months 1‚Äì6, we will scale up sequence database curation and establish shared computational infrastructure through NCEMS and CyVerse. This phase will focus on harmonizing sequence, PTM, and motif datasets and implementing reproducible workflows for large-scale data integration. In Months 7‚Äì 12, we will develop quantitative criteria for sequence conservation and initiate molecular dynamics simulations on representative IDRs. These analyses will test feature extraction pipelines and validate the framework connecting sequence features to experimental annotations. From Months 13‚Äì18, efforts will shift toward investigating mechanisms of structural conservation within disordered regions. We will complete the curated database, perform co-evolution analyses between conserved regions, and refine simulation-based structural descriptors. Finally, during Months 19‚Äì24, we will conduct public release and community dissemination of the conserved IDR database, prepare documentation for open-access hosting through NCEMS, and submit manuscripts describing the integrated framework and key findings.",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "5",
            "proposal_title": "Integrative Multi-Omics Synthesis to Identify Shared Molecular Mechanisms of Mammalian Hibernation",
            "authors": [],
            "authors_departments": [],
            "abstract": "Hibernation is a complex physiological strategy that allows certain mammals to withstand prolonged periods of resource scarcity and low temperatures by dramatically suppressing metabolism, while maintaining organ integrity and preserving muscle and bone mass. Despite decades of study, the shared molecular and cellular regulatory mechanisms that drive and sustain hibernation remain largely unresolved. This project aims to unify existing omics-scale datasets, mostly transcriptomic, from numerous hibernating mammals to identify conserved pathways and molecular signatures underlying this extreme phenotype. Our project will leverage publicly available datasets and apply cutting-edge computational tools that are developed in the context of this project to analyze transcriptional variation underlying hibernation in a phylogenetic context.",
            "full_draft": "4. Introduction and Goals Hibernation is a physiological strategy that allows animals to endure extended periods of limited resources and low temperatures. Within mammals, hibernation is observed in a subset of rodent, bat, lemur, and bear species. Across these diverse species, hallmarks of hibernation include profound reductions in metabolic rate, reductions in body temperature, reduced feeding or complete fasting, and inactivity (1). Still, interspecific nuance in hibernation physiology is rampant across mammalian hibernators. For example, hibernating ground squirrels reduce body temperatures to near environmental levels and experience intermittent ‚Äúarousal‚Äù periods in which metabolic rate and body temperatures temporarily rebound to normal levels (2). In contrast, bears experience less extreme decreases in body temperature and metabolic rate, yet sustain this suppressed state for the duration of hibernation without physiological arousals. Our understanding of hibernation physiology across mammals thus argues for both the presence of a shared suite of hibernation mechanisms, with additional species-specific variation. Identifying genes and signaling pathways underlying hibernation phenotypes in many or all hibernators will highlight mechanisms of fundamental importance for metabolic modulation in mammals. Numerous studies have investigated hibernation mechanisms in specific species and tissues to date, providing insight into the genes, signaling pathways, and biological processes driving hibernation phenotypes in specific lineages (3-11). However, few studies have used a comparative approach to test whether hibernation across mammals is driven by shared versus lineage-specific transcriptional mechanisms (but see 12,13). Moreover, neither of those approaches have considered the phylogenetic context in which the species are placed, thereby giving equal weight to more closely or distantly related species and not accurately capturing the evolution of transcriptional change while analyzing hibernation-related transcriptional differences and similarities. By integrating decades of existing datasets on hibernation gene expression spanning multiple tissues from diverse mammalian hibernators using a phylogenetic framework, we will identify shared and unique molecular mechanisms underlying hibernation. While identifying shared 1 mechanisms is the ultimate goal of this proposal, identifying lineage-specific mechanisms will reveal unique solutions to the same evolutionary problem of seasonal resource availability. Both findings will advance our understanding of the diversity of molecular mechanisms driving hibernation, thus providing context for understanding how these lineages can achieve metabolic feats beyond the physiological capacity of humans. Collectively, this aim will address the following questions: How do diverse mammals converge on similar physiological outcomes during hibernation through distinct yet overlapping molecular mechanisms? By comprehensively integrating existing datasets across taxa‚Äîincluding bears, ground squirrels, bats, and lemurs‚Äîthis synthesis will address the central question: What conserved molecular processes enable reversible metabolic suppression and tissue protection across hibernators? And test the following hypothesis: hibernating mammals share a set of genes that are differentially regulated during hibernation and thus represent ‚Äúcore‚Äù genes underlying hibernation phenotypes. One of the challenges is that the datasets are sparse and are not necessarily taken from the same tissue among species. Therefore, approaches are needed that analyze changes in a phylogenetic framework and account for missing data. We propose to develop the necessary tools to address our central question. 5. Proposed Activities Existing RNA-sequencing (RNA-seq) datasets comparing hibernation to active-state gene expression in a variety of hibernating mammals are publicly available (Datasets Table #1-12, 14-16, 20). These data are comprised of RNA-seq from numerous tissues (adipose, liver, muscle, bone, intestine, brain, kidney, and blood) collected from species from major hibernating mammal groups (squirrels, hamsters, bears, bats, and lemurs). We will also include an additional 17 non-hibernating species from (14) (Datasets Table #13, 17-19, 21) and add other datasets if additional RNA-seq datasets from multiple tissues become publicly available during the project. We will also leverage existing high quality reference genomes (Datasets Table #22) and whole- genome alignments (Datasets Table #23) for the proposed analysis. To identify shared and lineage-specific gene expression shifts during hibernation, raw RNA-seq reads will be downloaded from the NCBI Short Read Archive and quality-checked with FastQC (15); low quality reads will be removed before proceeding. For each species, RNA-seq reads will be mapped to the corresponding reference genome (Datasets Table #22) using STAR (16) and raw read counts will be quantified using featureCounts (17). Using these existing data, we will investigate the evolution of hibernation using two complementary approaches: First, we will use a phylogenetically informed approach to test for convergent shifts in gene expression in hibernating species while focusing exclusively on expression in active (i.e., non- hibernating) physiological states. This will test the hypothesis that there are baseline differences in ‚Äúnormal‚Äù state expression patterns between hibernating and non-hibernating species. This analysis will reveal previously undetected nuances in active season physiology and gene expression in hibernating species and provide a better baseline understanding for the degree of similarity between active hibernators and non-hibernators, while accounting for tissue. There are several developed methods that can be used for this analysis, including EVE (18) and CAGEE (19). These models often treat gene expression evolution as an Ornstein-Uhlenbeck (OU) process, which is typically described as modeling a trait evolving under stabilizing selection toward some optimum value (20). By allowing for a shift in the optimum trait value along the 2 phylogeny, these OU models can be used to model directional selection. Note that the OU model is generally found to be an adequate model of gene expression evolution (21). While these analyses are running, we will develop novel approaches to investigate longitudinal shifts (i.e., active versus hibernation) in gene expression in hibernators using a phylogenetic context. We will implement a bivariate model of trait evolution that simultaneously considers the evolution of gene expression in both the hibernating and active states. Two versions of the model will be fit: 1) the optimum is constrained to be the same for a gene in both states (no differential expression), and 2) the optimum of a gene is allowed to vary (differential expression). For non- hibernating species, we will treat gene expression evolution as a univariate trait mapping to the active state. We will identify the better model fit via AICc, as is standard in phylogenetic comparative methods. By utilizing data across many species, this model is intended to identify convergent shifts in gene expression between the active and hibernating states. We expect that the multi-optimum bivariate OU model will perform better than the single-optimum bivariate OU model only when there are convergent shifts in gene expression between states across the hibernators. This framework will also provide other parameters of interest, such as the strength of natural selection on gene expression. Model performance will be determined via simulation tests. For each approach, lists of convergently expressed genes will be compared between species using ortholog information generated using OrthoFinder (22). We will also use existing (Data Table #23) and generate genome alignments for all focal study species (Data Table #22) using Cactus (23) for further evaluation of gene orthology and as a valuable resource for future hibernation research. Shared and lineage-specific differentially expressed genes will then be characterized via Gene Ontology and KEGG pathway overrepresentation analyses using clusterProfiler in R (24). Ultimately, the interpretation of the genes will rely on a mix of database integration and the expertise of our working group team. Figure 1. Hibernation capacity is distributed across the mammalian phylogeny. Species in red exhibit hibernation adaptations and have publicly available RNA-seq data comparing active and hibernating states. A selection of non- hibernating mammals are shown in black. RNA-seq datasets representing ‚Äúnormal‚Äù and other relevant treatments (i.e., fasting, high-fat diet) are available for mouse and human and will also be included in this study. 3 6. Outcomes The insights derived in this project will illuminate evolutionarily conserved adaptations with potential translational relevance to human health, such as ischemia tolerance, metabolic control, and organ preservation. This project will test whether a core set of genes underlie shifts in metabolism and other hibernation phenotypes across multiple tissues in diverse mammalian hibernators. This will provide a valuable perspective on the extent to which mammalian hibernation is driven by the same underlying mechanisms and will likely identify a suite of genes essential to metabolic innovation in mammals. Findings from this project will inform our understanding of the relationship between human metabolism and that of other mammals, providing crucial context for ongoing efforts to translate our understanding of mammalian hibernation for the treatment and prevention of numerous human diseases. For example, this project may reveal novel gene regulatory programs underlying the onset and reversal of insulin resistance in hibernating species; these findings could then be extended by future work to develop therapeutic approaches to improve insulin sensitivity in diabetic and pre-diabetic human patients. As the proposed work spans multiple disciplines ranging from phylogenetics to functional genomics to cell biology and physiology, the diverse expertise of this working group will be critical for effectively interpreting and maximizing the translational potential of our findings. Our anticipated outputs include (i) primary literature article(s), (ii) model of trait evolution that simultaneously considers the evolution of gene expression in two states (e.g. hibernating and active states), (iii) a perspective of implementing phylogenetically-informed methods to analyze longitudinal datasets among species and (iv) a curated dataset of hibernation-related studies that will make it easier to add and implement future studies. 7. Rationale for a working group approach This working group with diverse expertise is needed to tackle the complex problem of understanding the molecular and cellular mechanisms underlying hibernation because this problem spans multiple specialized fields and data types. Integrating and unifying diverse omics- scale datasets across multiple species is very challenging due to differences in data types, experimental designs, and species biology. Moreover, hibernation involves intricate physiological processes regulated at multiple molecular levels‚Äîtranscriptomic, proteomic, metabolomic, and epigenomic‚Äîwhich require expertise and technology beyond the scope of one laboratory. Therefore, we have assembled a team with expertise ranging from cell physiology to computational phylogenetic approaches. Addressing hibernation benefits from a systems biology perspective that integrates inputs from molecular biology, physiology, genomics, bioinformatics, and computational modeling specialists. The team will harmonize, analyze, and interpret these complex datasets jointly. Challenges in multi-omics, such as high-dimensional data analysis, missing data, and variable quality control standards, require our diverse expertise to both develop workflows and interpret results meaningfully. In summary, the complexity of hibernation biology and the scale and diversity of data involved necessitate a multidisciplinary working group to bring together expertise, computational resources, and mathematical models to effectively identify conserved regulatory mechanisms underlying this extreme phenotype. 8. Rationale for NCEMS support This project leverages expertise from multiple disciplines and individuals at different career stages. All aspects of the NCEMS support would further the aims of this proposal by having the necessary personnel, Staff Scientific (33% FTE) and Staff Scientist Assistant (50% FTE), 4 available for the data wrangling, harmonization, metadata creation and curation, preliminary analyses, advanced modeling and statistical methodologies, and technical training and mentoring essential for the execution of this project. NCEMS support is essential for conducting this research because integrating RNA-sequencing data from multiple species from active and hibernating states in a phylogenetic framework requires extensive data synthesis, multidisciplinary expertise, and advanced cyberinfrastructure that exceed the resources of an individual lab. The access NCEMS provides to staff scientists and assistants with specialized skills in data wrangling, advanced analytic methods, among others, is ideally suited to answering our big question. Moreover, the center‚Äôs cyberinfrastructure, project management assistance, and commitment to open science best practices creates a unique environment where we could work together to disentangle the shared mechanisms underlying hibernation biology that could not be achieved in any of our labs alone. 9. Requested resources We are requesting data storage (~1 Tb), compute for mapping data to genomes and downstream analyses which requires standard memory compute nodes with multiple cores, all software is open source or will be developed and implemented as part of this project. We also request Staff Scientific (33% FTE) and Staff Scientist Assistant (50% FTE) with experience analyzing genomic data, ideally with some knowledge of phylogenetic methods. 10. Proposed Timeline ‚Äì 2 years Timeline Main Activities & Milestones Key Outcomes Months 1‚Äì3 RNA-seq QC and mapping; Ortholog mapping & GO annotation across species >90% gene orthologs and GO annotations completed Months 4‚Äì6 Analyses of convergent active state expression across mammals using EVE or similar approach Genes with shared or hibernation-specific expression patterns across species, tissues Months 5‚Äì12 Development and validation of bivariate models to analyze expression in active and hibernation states Visit to NCEMS / Working group personnel exchange Latent factors and phylogenetic signals discovered Months 13‚Äì15 Visualization of results; interpret models; biological interpretation Visualization of key features and regulators; prepare perspective for publication Months 16‚Äì18 Revise models as needed; functional characterization of genes using GO, KEGG enrichment with ClusterProfiler Signaling pathways of interest in hibernating species Months 19‚Äì21 Synthesize findings; relate to physiology Publish curated dataset Months 22‚Äì24 Prepare publications Final outputs readied for dissemination",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "6",
            "proposal_title": "Revealing the Ontology of Allostery through Community-Scale Synthesis",
            "authors": [],
            "authors_departments": [],
            "abstract": "Allostery is a fundamental property of proteins wherein perturbations at one site can influence another distant site. Understanding the sequence, structural, and dynamic determinants of allostery is critical to engineering new biosensors, designing allosteric modulators, and predicting the functional impact of mutations. For many proteins, allostery allows control of protein activity in response to external stimuli, such as ligand binding, protein or nucleic acid interactions, post-translational modifications, light, and redox state. There are proteins, though, that do not clearly exhibit such regulatory mechanisms, and are thus rarely included in publicly available allosteric databases. Consequently, it remains unclear what features distinguish regulated systems and regulatory sites from their non-regulated counterparts. To this end, we will focus our work on a specific class of proteins‚Äînamely transporter proteins‚Äîand assemble a database that is, on the one hand, very specific‚Äîrestricted to all such proteins, and broad because it includes proteins for which there is a lack of evidence for allosteric regulation. This augmented allosteric database will provide a basis for identifying the degree of allostery in proteins. Through these databases, and associated computational interrogations, we will reveal the determinants of allostery in transporter proteins, a large and diverse family of membrane proteins comprised of allosteric and non-allosteric members.",
            "full_draft": "Introduction and Goals. Our goal is to understand the evolutionary origin of allostery in transporter proteins. To achieve this goal, we will construct a curated database of proteins with allosteric versus non-allosteric functions that contain experimental annotations, mutational scans, NMR data, molecular simulations, evolutionary sequence data, and other information indicating key allosteric amino acid sites. While prior allostery databases have curated examples of regulated proteins,1,2 we will identify protein families that contain both allosteric and non-allosteric family members, enabling comparison. To focus our work, we will first restrict these questions to a manageable and abundant family of proteins ‚Äî viz., transporter proteins ‚Äî which includes both allosteric and non-allosteric homologs. These cases allow comparison that may reveal distinguishing features for control. Importantly, transporters are a well-studied family with many instances of regulation in independent family members.3 Transporters can be complex multi-domain systems where allosteric determinants are not trivial to devise.4 For 1 example, we will study the P-type ATPase superfamily of transmembrane transporters. This family includes metal binding machinery that is driven by allosteric communication, hydrolysis and phosphorylation leading to transmembrane rearrangements and changes in metal transport. Metal promiscuity or specificity is controlled by a combination of sequence determinants and allosteric networks and are ideal systems to tackle our questions. By synthesizing existing datasets around the annotation of transporter families and allostery, we will create a robust dataset ‚Äî which is evidently nonempty as it includes the aforementioned P-Type transporters ‚Äî for study and characterization of allosteric features and their evolutionary principles that might be transferable across other members of the family or other families of allosteric proteins. This dataset will allow us to resolve several fundamental questions of the ontology and fitness of allostery. During the proposed two-year effort we will aim to ask (through collaboration) and answer (through data synthesis and publications) the following two short-term aims: (Aim 1) Identify collections of amino acid sites underlying allosteric mechanisms by comparing regulated and non-regulated transporter homologs. (Aim 2) Determine the evolutionary origins of allostery in transporter families. Through examination of homologous sequences, we will establish if allostery evolved recurrently, or diverged from specific ancestors. The success of these aims will position us to address the long-term objectives of our collaboration: Identify the set of biophysical properties that co-evolve with allostery. Are any properties prerequisites to the evolution of allostery? Or does allostery potentiate the evolution of any other properties? Determine whether allostery is an intrinsic, emergent property of proteins or whether it evolves under specific conditions to enable flexible modes of regulation. Apply the principles learned to design and experimentally characterize synthetic allosteric systems. As noted below, based on the preliminary results seeded by NCEMS, we anticipate applying for additional funding in year two of this award to sponsor these future activities. Relevant to our synthesis questions: Some tools have been previously developed that propose allosteric binding pockets. AllositePro (dev. by the Molecular Design 2 Laboratory at Shanghai Jiaotong University) combines Fpocket with coarse-grained MD calculations (only available through a web server).5 Passer (developed by the Tao research group at Southern Methodist University) also uses Fpocket with three different ML methods (Code is available as well as APIs).6 ProDomino is another recent machine learning tool for predicting sites tolerant to domain insertion, which often have allosteric potential.7 Additionally, Ohm is an algorithm that predicts the contribution of residues to allosteric signaling given a protein structure. 8 In our groups, we have developed tools for co-evolutionary sequence analysis and machine learning classification of functional sequences,9‚Äì11 analysis of conformational energies, frustration, and propensities,5,12 design of synthetic allostery, 4,13‚Äì15 and methods to extract allosteric residue couplings from molecular dynamics data.16,17 After synthesizing an information-rich dataset, we plan to apply these tools to identify defining features of allostery from sequence, structure, and dynamics, and explain the evolutionary origins of allosteric mechanisms. Proposed Activities. Our central goal is to understand the sequence and biophysical features that distinguish proteins with existing allosteric regulation from their non-regulated counterparts. The project will consist of two parts in cocreation: (1) dataset creation and (2) dataset synthesis. (1) Dataset creation. We will work with the NCEMS team to curate a dataset of transporter proteins labeled according to whether or not there is evidence that they are allosterically regulated. During proposal creation, we consulted with an NCEMS staff scientist to conduct a preliminary scouting of transporter families that have annotations on allosteric vs. non-allosteric designations, and deep mutational scanning data available that would strengthen our hypothesis and future analysis. This helped confirm the feasibility of our proposed work and the potential for data synthesis. Data required: Sequence datasets, protein structures, deep mutational scanning data, literature synthesis, as well as annotations related to allosteric features like input sites, target sites, and, in some cases, allosteric pathways. Publicly available data sources: An appropriately annotated allosteric database, using a combination of the Allosteric Database (mdl.shsmu.edu.cn/ASD/),1 and the Transport Protein Data Base (tcdb.org).3 As it may be difficult to resolve between allosteric and 3 non-allosteric family members cleanly, we will consider protein families in which some members contain a domain insertion and others do not, analogous to Wolf et al.7 We can also leverage mutational data in MAVEdb (https://www.mavedb.org/).18 As we combine and annotate the existing data, the new database will form what we will call the augmented allosterome database, which will be one of our products to be made available through CyVerse/NCEMS. We will also need data from InterPro,19 UniProt,20 and the Protein Data Bank21 (PDB). Limitations or problems with existing data: The central challenge of existing databases is that they lack negative examples of non-regulated proteins, hindering comparative approaches. Moreover, some databases are only available upon request, or the datasets are no longer publically available (websites are deprecated). The intersection of all the features required for proper analysis, like sequence, structure, and experimental data, might lead to a small or sparse dataset. It may also be challenging to generate clean annotations of allosteric versus non-allosteric family members for broad families like the transporters. To create a concrete definition of allostery, we will use domain insertion as a proxy for the introduction of new allosteric regulation. More specifically, we will consider a dataset of transporter proteins where some family members have domain insertions (implying inter-domain regulation) and other members do not (implying that the regulation is missing). This follows a similar logic as a recent machine learning campaign for predicting surfaces for engineering new regulation.7 Our initial collaboration during NCEMS data scouting suggests this approach will be productive. (2) Dataset synthesis. Once the database is assembled, we will work on the two scientific Aims. We will conduct an analysis of allosteric and non-allosteric transporter proteins to compare sequence and biophysical features distinguishing them (Aim 1). We will also create evolutionary landscapes to examine the emergence of allostery across the transporter family (Aim 2). Towards these aims, the team will accelerate discovery through the following specific activities: The Hernandez lab will perform Adaptive Steered Molecular Dynamics (ASMD) on the binding/deformation of identified allosteric proteins, and develop neural networks to learn allosteric property prediction from the extended database. The Reynolds lab and Morcos lab will perform statistical analysis (SCA, DCA) across sequence alignments of allosteric and non-allosteric protein family members to distinguish sequence features associated with allostery. This will be done at the single amino acid level by analyzing the impact of mutations on allosteric activity and predicted 4 fitness using machine learning techniques, such as latent generative landscapes (LGL), to classify and reveal evolutionary origins of transporter allosteric function. The Potoyan lab will develop an MD/AI framework to predict allosteric hotspots, provide mechanistic interpretability, and quantitatively rank the impact of point mutation changes. By integrating the outcomes of the participating groups, together, we will develop a predictive, explainable framework that can distinguish between allosteric and non-allosteric proteins and predict mutational changes. Software tools: Direct Coupling Analysis,9 Statistical Coupling Analysis,11 Latent Generative Landscapes,4,10 Alphafold,22 PyTorch, OpenMM, HMMER23 , Gromacs, NAMD, VMD, and the Cyverse platform. Outcomes. Despite decades of study, it is unclear what properties distinguish allosteric from non-allosteric systems. We seek to define (1) physical, material properties and (2) sequence signatures that separate allosteric from non-allosteric family members. Critically, the sequences and physical properties of proteins are interdependent; sequence encodes physical properties, and these properties in turn influence the evolution of sequence. An approach such as ours, which synthesizes information on protein dynamics and evolution, is uniquely positioned to further our understanding of this complex relationship. Our work will address the fundamental question of how allostery evolves. These findings will enable the annotation of new allosteric systems, the interpretation of mutational effects on allostery (including mutations associated with disease), the prediction of druggable allosteric sites, and facilitate the design of synthetic allosteric systems. We will also uncover the dynamical information ‚Äîbe it from molecular dynamics or spatiotemporal experiments‚Äî and specific descriptors that are needed to better resolve the nonequilibrium function of allosteric protein. Data products: An augmented allosterome database comprising (1) sequence alignments, (2) protein structures and trajectories, and (3) existing deep mutational scanning data for a set of roughly 50 protein families that contain both allosteric and non-allosteric family members. For families in the database, we will compute measures of conformational frustration (through molecular dynamics)12 , statistical coupling analysis (DCA9 , SCA11), and latent generative landscapes10 (a strategy for using variational autoencoders to embed sequences in latent space). These features will be compared between allosteric and non-allosteric family members to identify distinguishing features. Publication: We anticipate that the products from our working group will include a publication sharing the dataset itself, and then one or two additional papers communicating the results of Aims 1 and 2. 5 Rationale for our working group. This work requires expertise in machine learning to condense large amounts of data and generate interpretable insights on allosteric control. It also requires expertise in protein biophysics, chemistry, and simulation to model and characterize allosteric mechanisms, as well as molecular evolution to investigate phylogenetic and selective forces driving allostery. Additionally, it requires experimental expertise to formulate further questions and hypotheses for future investigation. The team has the right mixture of expertise to achieve these goals. In this working group we brought together: Materials Chemistry, Chemical Physics, Machine Learning (ML), Computational Biophysics, Experimental Biophysics, Chemical Evolution, Biochemistry, Computational Biology, Molecular Evolution Rationale for NCEMS support. We imagine the NCEMS team helping in two phases. First, they could help to assemble a curated database consisting of annotated transporter entries that contain structural, dynamic, and mutational data. We will also leverage the NCEMS expertise in using novel AI techniques to mine the scientific literature for allosteric systems not deposited in databases in an effective and easily machine-readable way. The database should maintain a good balance of highly allosteric, weakly allosteric, and non-allosteric entries, and be open to community augmentation. Second, they could assist in large-scale data analysis, testing the statistical association between allostery and various computed features (including conformational frustration and sequence coevolution metrics). The ideal NCEMS Staff Scientist should have: Knowledge and curiosity in protein databases, experimental techniques, bioinformatics, data annotation and AI techniques for data mining. Training and expertise in creating computational workflows using federal computational resources. 6 Requested resources Resource Resource request Overall duration of support (up to 2 years) 2 years Staff Scientist time (up to 8 months full-time equivalent over 2 years) 8 months (steps 1,2,5, and supervision) Staff Scientist Assistant time (up to 11 months full-time equivalent over 2 years) 11 months (steps 3, 5, and perhaps 6) Estimated storage requirements in terabytes 2 terabytes Estimated CPU requirements in CPU hours 3,000,000 hours Estimated GPU requirements in GPU hours 40,000 hours Justification for compute resources Storage Requirements. The 2 TB storage request includes 1 TB for all-atom molecular dynamics simulation (MD) models and trajectories, and 1 TB for saving ML/AI models, preparing protein structure datasets (PDBs), and compiling protein multiple sequence alignment datasets (MSAs). CPU Requirements. We request 3,000,000 CPU hours to run all-atom MD simulations and perform statistical coupling analysis (SCA). Typically, 500-5000 CPU hours are needed to run 1 ns simulations of a protein system. For each model, at least 20 ns is necessary to perform the equilibration and productive simulations, and we plan to simulate 50-100 protein models. For running SCA, 10 CPU hours are needed per alignment. We expect to run SCA on 50-100 alignments, potentially with multiple replicates to enable sampling over run parameters. These combined requirements informed our total estimate. GPU Requirements. The requested 40,000 GPU hours will be used to (1) train AI models and (2) run all-atom MD simulation models. Training AI models typically requires 1000 GPU hours on an NVIDIA A100 to run 50-200 epochs. We plan to compare about 10-20 different AI models, including constructing latent generative landscapes (LGLs) for generative protein sequence design. Together, we estimate this will require approximately 20,000 GPU hours. For MD simulation, 2-10 GPU hours are needed to run a 1 ns simulation for most protein systems. For each protein model, a minimum of total 20 ns is needed to perform the equilibration and productive simulations, and we plan to simulate 100-200 protein models. This will require an additional 20,000 GPU hours in total. 7 Proposed Timeline 1. (Months 1-3): Dataset identification: families with both allosteric vs. non-allosteric members 2. (Months 3-6): Data curation: how many of these families have additional data like mutational scanning, source and sink sites, types of input ligands, and annotated phenotypic responses 3. (Months 6-12): Data Analysis: evolutionary analysis, energetic calculations, machine learning, and classification. 4. (Months 12-16): Performance assessment: is it possible to predict allosteric vs. non-allosteric proteins, can we correctly identify key allosteric residues, can we propose changes that potentially could induce or inhibit allostery on both classes of proteins? 5. 6. 7. (Months 16-19): Model generation and final database development. (Months 19-24): Manuscript(s) writing and tool deliverables (Months 22-24): Further hypotheses and proposed future work.",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "7",
            "proposal_title": "SynEvo: Discovering Evolutionary Principles of Molecular Organization through Cross-Species Data Synthesis",
            "authors": [],
            "authors_departments": [],
            "abstract": "Living systems exhibit emergent organization that bridges molecular and evolutionary scales. Genetic variation, regulatory control, and metabolic dynamics interact across levels of complexity to produce coordinated cellular behavior, yet most datasets capture only one layer at a time. Despite vast genomic, transcriptomic, proteomic, and metabolomic resources across species and environments, no synthesis framework connects these data to reveal how evolutionary forces shape the mesoscale‚Äîthe intermediate level where genes, proteins, and metabolites assemble into adaptive networks. The SynEvo Working Group will develop a machine-learning‚Äìdriven synthesis framework that unites population-genetic and multi-omics data to uncover conserved and divergent axes of molecular organization. Using well-characterized human and mosquito datasets, we will identify how genetic variation, regulatory control, and metabolic flux interact to produce conserved mesoscale modules reflecting evolutionary constraint and innovation. These contrasting systems serve as testbeds for uncovering evolutionary design principles of molecular coordination‚Äîhow selection and constraint shape regulatory‚Äìmetabolic organization across life. SynEvo integrates interpretable machine learning with genome-scale metabolic (GEM) and MiReN (Minimal Regulatory Network; MILP-based regulatory inference) approaches, producing an open SynEvo Atlas hosted within NCEMS infrastructure‚Äîa reusable resource for evolutionary systems biology. By revealing both species- specific adaptations and conserved mesoscale design principles, SynEvo will advance conceptual and computational frontiers, providing fundamental insight into how life organizes itself across molecular, physiological, and evolutionary dimensions, with applications to human health, malaria transmission, and environmental resilience.",
            "full_draft": "Introduction and Goals A central challenge in modern biology is connecting population-level genetic variation‚Äî acting on alleles and loci‚Äîto emergent physiological behavior arising from coordinated molecular systems. Population genetics characterizes selection and drift, while systems biology explains network organization and dynamics; the mesoscale interface linking these remains underexplored. SynEvo tackles this gap by combining interpretable machine learning with mechanistic models to quantify how evolution sculpts regulatory‚Äì metabolic organization. By integrating population-genetic and systems-level perspectives, SynEvo seeks to uncover general evolutionary rules that govern how regulatory and metabolic networks organize, adapt, and maintain robustness across species. Existing Datasets We will leverage major human resources‚ÄîGTEx, All of Us, and TOPMed‚Äîwhich collectively provide multi-tissue expression and deep population variation at scale [1‚Äì4], complemented by PRIDE and MetaboLights for proteomes and metabolomes [5,6]. For mosquitoes, Ag1000G and VectorBase offer extensive population-genomic and transcriptomic data for Anopheles gambiae and related vectors [7,8]. In addition, we will build upon existing Anopheles gambiae genome-scale metabolic reconstructions (e.g., iAG343 and subsequent community-curated models) [9], refining them using Ag1000G and VectorBase annotations to ensure metabolic comparability with global human metabolic model Human1 [10]. Pathway and reaction-network scaffolds from KEGG, Reactome, and BiGG support biochemical interpretability [11‚Äì13]. Cross-species feature alignment will rely on orthology frameworks (OrthoFinder, eggNOG) [14,15]. Prior Analyses Comparative genomics and constraint-based models have revealed lineage-specific adaptations and conserved biochemical organization [16‚Äì18], while machine-learning analyses have uncovered structure in isolated omics layers [19‚Äì21]. However, no framework jointly embeds population-genetic variation with multi-omics across species and grounds the resulting latent axes in mechanistic biochemistry and regulation. Project Significance SynEvo addresses a central question in evolutionary systems biology: what general organizational principles allow molecular networks to remain robust and adaptive across millions of years of divergence? SynEvo‚Äôs distinctive advance is a hybrid, interpretable synthesis: data-driven embeddings constrained by GEMs‚Äîincluding Human1 for human metabolism [10]‚Äîand MIREN-style regulatory logic [22] to ensure learned mesoscale modules correspond to physiologically plausible flux and control states. Unlike existing integrators such as MultiPLIER, OmicsIntegrator2, or DeepOmics [24‚Äì 25], SynEvo explicitly couples population-genetic variation with mechanistically constrained embeddings, producing interpretable, evolution-aware latent spaces. This combination of population context and biochemical constraint has not been implemented in current machine-learning-based synthesis efforts. SynEvo‚Äôs insights will yield testable, cross-taxa hypotheses about evolutionary design constraints that extend beyond any one species. Biologically, the downstream applications are high impact: understanding how evolution preserves or rewires molecular organization informs predictive evolutionary theory, human health, and vector adaptation, with implications for resilience under environmental change [1‚Äì11]. Hypotheses 1. Evolutionary Organization Hypothesis. Evolution acts through conserved mesoscale regulatory‚Äìmetabolic axes that coordinate gene regulation and metabolism to maintain core cellular functions across species, reflecting universal organizational constraints on molecular networks. 2. Mechanistic Integration Hypothesis. Embedding GEM and MIREN constraints within interpretable machine-learning architectures will reveal latent modules corresponding to physiologically meaningful flux and control states [10,22]. 3. Cross-Species Alignment Hypothesis. Manifold alignment of mesoscale embeddings between humans and mosquitoes will identify both conserved and lineage-specific modules, quantifiable by alignment metrics (target ‚â•0.7 correlation) and shared pathway enrichment [14,15]. Validation. Each hypothesis will be evaluated using quantitative and biological validation metrics. Statistical support will include cross-validation accuracy, module reproducibility across resampling, and alignment scores across organisms. For population genetic analysis, evolutionary simulators can be used to create high-quality labeled data, which will also be used for validation. Metabolic model validation will benchmark inferred mesoscale modules against flux balance analysis (FBA) and flux sampling simulations from both Human1 [10] and Anopheles GEMs (iAG343-derived) [9]. Module-level flux states will be compared with transcriptomic and metabolomic measurements from GTEx and MetaboLights (human) and from published Anopheles metabolic studies. Cross-species metabolic coherence will be assessed by comparing subsystem-level flux ratios (e.g., glycolysis, amino acid, and lipid metabolism) and pathway enrichment overlap (target ‚â•0.6 correlation in subsystem activity). Deviations will be interpreted as lineage-specific rewiring or environmental adaptation signatures. Biological validation will assess pathway enrichment against curated resources (KEGG, Reactome, BiGG) and correspondence with flux distributions from Human1 and regulatory edges from MIREN [22] simulations. A hypothesis will be considered supported when at least 70 % of inferred modules show significant cross-species overlap (FDR < 0.05) and pathway concordance. We will pursue three interlocking goals: 1. Construct interoperable data resources harmonizing genomic, transcriptomic, proteomic, and metabolomic measurements across species using standardized ontologies, orthology-based mappings, and cross-referenced identifiers [9‚Äì13]. 2. Develop interpretable machine-learning models‚Äîgraph neural networks [27], variational autoencoders [28], and constraint-based hybrids‚Äîthat jointly embed genetic and molecular variation to uncover conserved latent dimensions [17‚Äì19]. 3. Identify emergent mesoscale modules‚Äîregulatory or metabolic subnetworks that are conserved, co-adaptive, or uniquely derived‚Äîvalidated through enrichment analyses, reproducibility tests, and cross-species alignment metrics [9‚Äì13,29]. Proposed Activities SynEvo Integrator (Data Harmonization). We will deploy an open, version-controlled pipeline for multi-omics and population data, implementing standardized normalization, metadata harmonization, and cross-species alignment via OrthoFinder and eggNOG [13, 15]. Pathway scaffolds (KEGG/Reactome) and reaction networks (BiGG) will anchor interpretability [11‚Äì13]. To ensure feasibility, we will prioritize well-curated public subsets early (e.g., GTEx tissues with robust proteomics) to achieve proof-of-concept within months 1‚Äì6; broader integration follows as harmonization matures [1, 5, 9]. (Imputation for sparse mosquito variants? will be used only for exploratory synthesis; definitive cross-species conclusions will rely on directly measured modalities.) Mesoscale Inference (Hybrid Interpretable Models). We will learn low-dimensional joint embeddings via graph neural networks and variational autoencoders [17‚Äì19, 27, 28] that connect population variation to molecular states while enforcing GEM constraints (flux feasibility) and MIREN-style regulatory control (transcriptional logic) [10, 22]. Model audits will quantify reconstruction accuracy, generalization, and biological coherence (flux‚Äìexpression correlation, enrichment, perturbation-informed plausibility via Human1 simulations) [11‚Äì13, 22]. Predicted flux distributions and module activity will be benchmarked against both Human1 and Anopheles GEM simulations, evaluating consistency with known metabolic pathways (e.g., glycolysis, TCA cycle, lipid metabolism) and available metabolomic perturbation data. Success criteria: latent axes map to known metabolic subsystems/regulons; modules are reproducible across resamples; and predicted flux/regulatory couplings agree with mechanistic expectations. Cross-species Synthesis (SynEvo Atlas). Cross-species manifold alignment provides a direct route to identifying shared mesoscale design motifs‚Äîpatterns of regulation and metabolism that define the fundamental architecture of cellular organization. We will perform manifold alignment [21] across human and mosquito embeddings, guided by orthology and pathway correspondence, and score conservation/divergence using modularity, robustness, and alignment metrics (‚â• 0.7 target) with pathway overlap [11‚Äì15]. The SynEvo Atlas (NCEMS-hosted) will expose conserved cores and lineage-specific innovations, providing interactive network views, downloadable embeddings, and workflows. If full alignment proves infeasible within the period, we will release SynEvo Atlas v1 with within-species atlas layers and partial cross-species mappings‚Äîensuring tangible deliverables. Risk mitigation and contingency planning. If data sparsity or alignment performance limits cross-species synthesis, we will prioritize human-focused analyses in Year 1 while developing generalized transfer- learning modules adaptable to additional species as data coverage improves. This ensures all milestones yield publishable and reusable outputs regardless of dataset completeness. Outcomes Scientific contributions and open products. SynEvo will release (i) the SynEvo Integrator (harmonization workflows), (ii) hybrid ML + mechanistic models (GEM/MIREN-constrained embeddings), and (iii) the SynEvo Atlas (interactive mesoscale maps). All will be FAIR-compliant, version- controlled, archived with DOIs, and licensed for reuse [5, 6, 11‚Äì13]. Significance to field. SynEvo delivers the first cross-species, population-aware atlas of mesoscale regulation and metabolism, bridging population genetics with systems-level organization. By coupling interpretable embeddings to mechanistic constraints, SynEvo reveals evolutionary principles of molecular organization with clear hypotheses, metrics, and reproducible analyses. By grounding mesoscale modules in experimentally validated metabolic reconstructions for both humans and mosquitoes, SynEvo ensures that inferred principles reflect physiologically meaningful flux‚Äìcontrol architectures rather than abstract embeddings. These analyses will formalize general mesoscale design rules linking evolutionary variation to molecular organization, establishing a foundation for predictive, cross-taxa models of adaptation. This extends the synthesis arc from bacterial cell-state mapping to eukaryotes and vectors, enabling comparative tests of constraint vs. adaptation in network architecture. Specific broader impacts include advancing understanding of the genetic bases of insecticide resistance and malaria transmission, while uncovering general rules of regulatory‚Äìmetabolic organization that illuminate how life achieves robustness and adaptability across species. Future work and dissemination. SynEvo is designed to scale to additional taxa (Drosophila, C. elegans, plants) and modalities (single-cell, microbiome, epigenomics), with GEM/MIREN extensions supporting predictive simulations of environment-dependent reprogramming [11‚Äì13]. This expansion will test whether the mesoscale principles uncovered in humans and mosquitoes represent universal evolutionary constraints on molecular systems. SynEvo will also promote workforce development through open training materials, hackathon-style workshops, and mentorship of graduate and undergraduate participants. These activities will extend NCEMS‚Äôs educational mission and broaden engagement in open, data-driven evolutionary systems biology. Rationale for NCEMS Support SynEvo epitomizes NCEMS‚Äôs mission to enable synthesis science by uniting existing datasets to uncover emergent principles unobservable within individual studies. This proposal does not involve new experimental data generation but leverages NCEMS‚Äôs infrastructure for large-scale data harmonization, machine-learning computation, and open dissemination. SynEvo therefore exemplifies the synthesis mission of NCEMS‚Äî not only leveraging computational infrastructure but also producing conceptual frameworks and open resources that enable discovery of universal principles across data types and species. The interdisciplinary team combines complementary expertise: Saha provides leadership in ML-informed ‚Äôomics analysis and genome-scale metabolic and regulatory modeling [28‚Äì35], with demonstrated advances in antibiotic-resistance prediction [28], cancer metabolism and GEM+ML integration [29,31,35], enzyme-constrained modeling in Treponema pallidum [32], CRISPRi-guided regulatory interrogation in Chlamydia trachomatis [33], thermodynamics-aware metabolic analysis in Staphylococcus aureus [34], and integrated computational‚Äìexperimental GEM studies in S. aureus [35]; Mathieson contributes expertise in machine learning for populationgenetic inference, including fully connected networks [36], convolutional neural networks [37,38], GANs [39,40], and interpretability [41]. . Fumagalli brings expertise in machine learning [38], natural selection inference [42], and mosquito genetics [40]. Saha has access to and dedicated GPU/CPU nodes at Holland Computing Center (fastest resources in the state of Nebraska) at University of Nebraska-Lincoln. Mathieson also has access and funding for PARCC (Penn Advanced Research Computing Center), a new supercomputer at the University of Pennsylvania with high-end GPU capabilities. Together, they will coordinate with NCEMS staff scientists for workflow deployment, visualization design, and dataset curation. This collaboration ensures both technical feasibility and community accessibility. NCEMS‚Äôs computational resources‚Äîhigh-throughput storage, scalable CPUs/GPUs, and FAIR-compliant workflow systems‚Äîare uniquely suited to SynEvo‚Äôs scope. Beyond computational support, NCEMS provides the collaborative framework needed for interdisciplinary integration, ensuring that SynEvo‚Äôs outcomes will align with and enhance the broader NCEMS synthesis ecosystem. All analyses involving human data will use only de-identified public or controlled-access repositories (GTEx, All of Us, TOPMed), following repository and NIH data-use agreements. Early workflow testing will emphasize publicly available subsets to ensure deliverables within the two-year project period. SynEvo will contribute durable NCEMS assets‚Äîharmonization workflows, ontology mappings, and model templates‚Äîthat other working groups can reuse, directly expanding NCEMS‚Äôs synthesis infrastructure. Requested Resources and Proposed Timeline We request two years of NCEMS Working Group support. Approximately 200,000 CPU and 30,000 GPU hours will be required for dataset harmonization, embedding learning, and cross-species alignment. Ten terabytes of temporary storage will accommodate intermediate data products. We request 25% FTE for an NCEMS staff scientist and 45% FTE for a staff assistant to assist with data ingestion, workflow deployment, and reproducibility verification. One in-person synthesis meeting after the first-year milestone will coordinate integration among collaborators. Proposed Timeline Throughout all phases, the team will document methods and lessons learned to inform future NCEMS working groups pursuing cross-species synthesis. Timeframe Activities & Milestones Months 1‚Äì Deploy SynEvo Integrator within NCEMS; harmonize curated 6 human/mosquito subsets; establish ontology and orthology mappings; initial feature-alignment analyses. Train GEM/MIREN-hybrid models; benchmark GEM-based module flux Months 7‚Äì 12 predictions for Human1 and Anopheles models; validate subsystem coherence using existing metabolomic datasets; validate within-species mesoscale modules; release synthesis report and manuscript. Months 13‚Äì18 Conduct manifold alignment; identify conserved/divergent modules; develop Atlas interface. Finalize and release SynEvo Atlas and toolkit; ensure FAIR metadata Months 19‚Äì24 standards and DOI registration for datasets and models; submit manuscript; archive workflows in NCEMS and CyVerse; publish tutorials/documentation.",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "8",
            "proposal_title": "A Unified Framework for Analyses of Proteome Turnover Dynamics",
            "authors": [],
            "authors_departments": [],
            "abstract": "Proteins vary widely in how long they persist within cells, from minutes to years, and this variation plays a central role in regulating biological function and maintaining cellular homeostasis. Decades of research have revealed several mechanisms that influence protein half-lives across the kingdoms of life. However, the differential impact of these mechanisms on protein half-lives within and between species remains incompletely understood. This knowledge gap can be addressed by comprehensive analyses of global proteome stability datasets enabled by recent advances in mass spectrometry-based proteomics. However, these datasets are currently fragmented across heterogeneous studies that are not easily amenable to meta-analysis. This Working Group will bring together complementary expertise in proteomics, computational biology, systems biochemistry, and evolution to systematically harmonize and integrate publicly available datasets on protein turnover across four representative species across two kingdoms of life - E. coli, S. cerevisiae, M. musculus, and H. sapiens. By reanalyzing existing data using consistent approaches and unifying metadata standards, we will generate a comprehensive, comparable, and publicly accessible resource, the Protein Stability Database (PSD). This database will enable us to identify the molecular and evolutionary principles that govern protein stability and assess how turnover patterns vary across different cell types and organisms. Beyond addressing these core questions, our Working Group aims to establish best practices and a shared vocabulary for measuring and reporting protein turnover, enabling seamless incorporation of future datasets and integration with other NCEMS data resources.",
            "full_draft": "Introduction and Goals Protein turnover, the continual balance between synthesis and degradation, defines the lifetime of every protein in the cell and is a fundamental determinant of proteome homeostasis. The half-life of a protein reflects the integrated effects of its sequence, structure, post- translational modifications, and interactions with cellular protein quality-control pathways (1-4). Despite its central importance, our understanding of what governs protein turnover kinetics remains incomplete. Although many of the underlying mechanisms regulating protein stability are well characterized and broadly conserved, the extent to which these mechanisms shape the observed diversity of protein half-lives across species remains unclear. For example, it remains unknown whether long-lived bacterial proteins are also long-lived in eukaryotes, or how bacterial orthologs of ubiquitin-tagged proteins in eukaryotes achieve regulated stability despite lacking the ubiquitin‚Äì proteasome system. Likewise, while sequence-encoded degradation signals such as degrons and N-end rule motifs are conserved across life, their specific recognition and implementation differ widely. Our goal is to integrate existing proteome turnover datasets to identify shared and species-specific determinants of protein stability across eukaryotic and prokaryotic species. Figure 1. Project overview Recent advances in bottom-up proteomics have enabled proteome-wide measurements of protein half- lives through dynamic labeling techniques such as pulsed SILAC and dynamic isotopic labeling (5-7). These approaches have revealed that turnover rates vary by several orders of magnitude, even within a single cell type. However, despite the proliferation of such datasets, most studies have been conducted independently, focused on only one or a few cell types or closely related organisms, analyzed using different kinetic models, and published in heterogeneous formats. Consequently, the field lacks an integrated, quantitative framework that connects turnover rates to molecular features and evolutionary context. The fragmented nature of existing datasets has hindered meta-analyses and limited our understanding of how protein stability is organized across species. More broadly, there are no unified resources for examining how diverse sequence and structural properties, such as intrinsic disorder, folding stability, and domain organization, collectively determine protein lifetimes in vivo. The harmonized datasets produced through this project will enable systematic cross-species and mechanistic comparisons of protein turnover. A major unresolved question in the field of proteostasis is the degree to which protein half-lives are conserved across species and cell types. Comparative studies within eukaryotes have revealed that some protein classes, such as ribosomal proteins and metabolic enzymes, maintain remarkably consistent turnover rates across evolution, whereas others, including regulatory and stress-response proteins, show considerable variation (18). This variability likely reflects adaptation to organismal traits such as metabolic rate, lifespan, and optimal growth temperature. However, to date, no cross-kingdom comparisons have been conducted, and it remains unknown how turnover patterns differ between the major domains of life. In particular, it is unclear how the distinct proteostasis architectures of prokaryotes and eukaryotes (e.g., protease networks vs. ubiquitin‚Äìproteasome and autophagy systems) differentially shape proteome stabilities at a global level. Systematically comparing turnover dynamics across species will clarify how conserved degradation principles are adapted to different cellular and evolutionary contexts and may identify shared and species-specific rules governing protein half-lives. Understanding the determinants of protein half-lives requires integrating diverse layers of biological information. Sequence-encoded features, such as degrons, amino-terminal residues, charge, hydrophobicity, and predicted disorder, are known to influence degradation susceptibility; however, these effects are modulated by higher-order properties, including domain organization, folding energetics, and the presence of intrinsically disordered regions (8, 9). Structural proteomics techniques such as SPROX, TPP, LiP and CETSA now provide global measures of thermodynamic stability that can be directly compared to turnover rates (10-12), while advances in structure prediction (AlphaFold, ESMFold) make it possible to extract biophysical descriptors for nearly every protein (13, 14). At the same time, proteostasis networks, including chaperones, E3 ubiquitin ligases, and autophagy pathways, play central roles in determining whether a misfolded or damaged protein is refolded or degraded (15-17). However, these data layers have not been combined systematically, and the relative contributions of diverse sequence and structure predictors of protein longevity across kingdoms of life remain to be quantified in a unified framework. Quantifying cross-species trends in protein half-lives requires a systematic, cross-species comparison grounded in a consistent methodology, which no existing resource currently provides. At present, turnover data are fragmented across studies that differ in scope, methodology, and analytical treatment. For example, protein half-life measurements, structural stability data, and thermal stability datasets are typically stored in separate repositories and analyzed using distinct pipelines and reference proteomes, complicating direct comparison. Moreover, turnover rates have been analyzed based on a variety of kinetic models and normalization procedures, resulting in incompatible parameters across datasets. Even when comparable data exist, meaningful biological variations in relative protein half-lives may be obscured by systematic shifts in absolute half-lives resulting from differences in global factors, such as metabolic rate and growth dynamics. Addressing these challenges requires harmonizing datasets, refitting kinetic models using unified procedures, and normalizing turnover estimates to account for species-specific physiological scaling factors. We propose to address these gaps by establishing a comprehensive, harmonized database of protein half-lives and associated molecular features across four well-studied organisms: E. coli, S. cerevisiae, M. musculus, and H. sapiens. These species were chosen because they span the major evolutionary transitions from prokaryotic to eukaryotic proteostasis systems and collectively account for the majority of available quantitative turnover data. By limiting the initial analysis to this set, we can ensure both data depth and biological interpretability while developing scalable workflows that can later be extended to additional species and experimental contexts. Our Working Group will harmonize existing datasets, re-analyze them using consistent kinetic models, and integrate them with structural, biophysical, and proteostasis-related annotations to uncover the molecular and evolutionary determinants of protein stability. This synthesis will address two overarching questions: (1) To what extent are protein half-lives conserved across species? and (2) What shared and species-specific molecular and biophysical properties determine protein half-lives across organisms? Answering these questions will not only advance our mechanistic understanding of proteome dynamics but also create the first community- accessible, metadata-rich resource for studying protein turnover at scale. Ultimately, we envision that this framework will enable new hypotheses about how protein longevity shapes cellular adaptation, aging, and evolution, and will provide a foundation for integrating turnover measurements with other NCEMS data resources on protein folding, interactions, and post-translational regulation. Proposed Activities 1. Gathering and re-analysis of protein half-life data We will systematically assemble and harmonize all publicly available quantitative protein half-life datasets for the five selected species using a unified computational workflow. Dynamic-labeling studies employing pulsed SILAC and related approaches will be identified from repositories such as PRIDE and curated with detailed metadata. An initial NCEMS data-scouting analysis has identified several key turnover datasets from bacteria (19, 20) (Source ID 1), S. cerevisiae (21, 22) (Source ID 2), and mammalian systems (23-26) (Source ID 3-4). In this initial phase, datasets will be reanalyzed where raw intensity or labeling data are available. Turnover rates will be refitted using single- and multi-exponential decay models implemented in Python, following established approaches from previous proteome dynamics studies. In doing so, we will explicitly account for two key parameters that may affect the accuracy and comparability of turnover measurements across datasets. First, we will correct for variations in amino acid precursor enrichment kinetics due to differences in labeling strategies, ensuring that turnover rates derived from different experimental designs can be meaningfully compared (27, 28). Second, our turnover fitting models will accommodate deviations from first-order kinetics by allowing for the presence of multiple degradation phases (29). Such deviations may reflect the presence of distinct protein pools that differ in stability, or coexisting protein isoforms and post-translational variants with heterogeneous turnover behavior. Confidence intervals and quality metrics will be derived through non-linear least squares and Bayesian sampling methods. Rather than developing new algorithms, our goal is to uniformly apply well-established models and quality control metrics to ensure comparability across datasets and species. Each dataset will be standardized to include controlled vocabulary fields describing the organism, cell type, growth rate, temperature, labeling duration, and kinetic model used to derive half-lives. The harmonized datasets will be stored in interoperable formats to facilitate reuse and integration with complementary molecular features. The final outcome will be a comprehensive, metadata-rich resource of high-quality, normalized turnover measurements across the five focal organisms, publicly released through NCEMS cyberinfrastructure to ensure open access and reproducibility. 2. Conservation of protein half-lives across species We will then analyze the evolutionary conservation of protein stability. Orthologous proteins will be mapped among E. coli, S. cerevisiae, mouse, and human using OrthoDB and UniProt annotations. Cross-species comparisons will quantify how half-lives vary among orthologs and functional classes, revealing which aspects of proteome stability are evolutionarily conserved and which are tuned to lineage-specific physiology. We will also explore how organismal traits, such as optimal growth temperature, metabolic rate, and lifespan, correlate with global turnover patterns, providing insights into the selective pressures that shape proteome stability. These comparative analyses will generate a quantitative framework for understanding how evolution and environment jointly influence the dynamics of protein maintenance. 3. Uncovering predictors of conserved and species-specific protein half-lives Finally, we will use the unified dataset described above and our insights on evolutionary conservation of protein stabilities to conduct multivariate analyses to identify molecular and biophysical determinants of protein stability. First, we will integrate protein-intrinsic properties, such as degron motifs, terminal residues, charge distribution, hydrophobicity, predicted disorder, and amino acid composition, with higher-order features derived from omics, structural, and thermodynamic data. These include domain organization, intrinsically disordered regions, contact order, and model confidence metrics (pLDDT) from multiple structural prediction algorithms such as AlphaFold, OmegaFold, and ESMFold. Folding energetics will be incorporated from proteome-wide stability measurements obtained using SPROX, TPP, and CETSA experiments. Extrinsic cellular factors, including chaperone interactions, ubiquitination, E3- ligase targeting, and autophagy substrate status, will be analyzed in parallel to assess the role of proteostasis networks. Regression and machine-learning approaches will be applied to quantify the relative contributions of these features and to identify conserved versus lineage-specific predictors. In parallel, we will evaluate more advanced architectures, such as using deep neural nets with a cross- attention mechanism to learn half-life across sample types from protein embeddings, which are transformer-based lower-dimension representations of protein sequences and structures from large protein language models such as ESM2, ProtT5. We will next perform multi-target prediction models that will aim to simultaneously predict protein half-life, folding stability, and other proteostatic parameters directly from sequence and structural features. Training will be performed on CyVerse GPU nodes using PyTorch/PyTorch Lightning framework. Performance will be evaluated using a variety of metrics, e.g., reconstruction loss for finding fast-turnover regulatory proteins, and comparison of prediction accuracy against baseline models such as existing degron databases. Together, these analyses will take advantage of the harmonized data set to provide the most comprehensive examination to date of the molecular features that govern protein half-lives across biological systems. From there, we will explore explainability/interpretability analysis, including using gradient or attention-based methods or ablation methods. Expected Outcomes and Broader Impacts This project will lead to the establishment of the Protein Stability Database (PSD), the first harmonized, metadata-rich database of protein half-lives across multiple species. The specific short-term applications of PSD include the identification of conserved stability determinants and the establishment of quantitative models linking sequence and structure to degradation kinetics. Beyond these immediate research goals, the PSD will serve as a valuable resource for evaluating kinetic models, training machine learning frameworks in protein turnover prediction, and developing quantitative theories of proteostasis. We note that PSD is envisioned as a static, versioned database that will not require NCEMS support for web interfaces, update pipelines, or ongoing maintenance. The database will be released in an archival format and hosted through NCEMS-supported infrastructure and institutional repositories. Our long-term vision for this effort is not to develop a continuously updated resource, but rather to establish a uniform set of best practices for conducting and reporting protein half-life measurements. By defining a shared vocabulary and standardized metadata schema, this effort will facilitate the seamless incorporation of future datasets generated by the protein turnover community, ensuring that subsequent data can be integrated using consistent and interoperable formats without requiring additional NCEMS resources. By adhering to FAIR data principles, the PSD will promote transparency, reproducibility, and cross- disciplinary reuse. All analysis pipelines, code, and data will be released through CyVerse and GitHub to ensure transparency and community accessibility. The project will also generate manuscripts describing both the database resource and the cross-species comparative analysis, contributing to the NCEMS mission of fostering synthesis and discovery through shared data integration. Rationale for a Working Group and NCEMS support This project requires interdisciplinary expertise in proteomics, computational biology, and evolutionary systems analysis that extends beyond any single laboratory. The team brings complementary strengths: Ghaemmaghami and Lau provide expertise in dynamic labeling, quantitative mass spectrometry, and data harmonization; Vogel contributes expertise in large-scale modeling of protein turnover and proteostasis networks as well as expertise in the evolution of protein structure and function; and NCEMS provides the ideal collaborative and computational environment for cross-disciplinary synthesis. Dedicated staff support and NCEMS cyberinfrastructure are essential for metadata curation, ontology development, and large- scale computation. The Working Group model will enable close coordination among institutions, facilitate the creation of community standards, and ensure that all outputs are openly disseminated for future use. By unifying fragmented datasets and creating a scalable comparative framework, this project will directly advance the NCEMS goal of uncovering emergent principles in molecular and cellular systems. Requested Resources All computation will be performed on standard CyVerse CPU and GPU nodes. Preliminarily we anticipate requiring up to 5,000 V100/A100 GPU hours, which will cover architecture comparison, transformer model training, protein embedding constructions, architecture comparisons, and interpretability/retraining; we also anticipate up to 5,000 CPU hours for proteomics data processing, statistical modeling, and database construction tasks; and approximately 15 TB of storage (potentially up to 50 TB depending on the scope of mass spectrometry data re-processing). Personnel support includes one Staff Scientist (33% FTE for 12 months) responsible for data integration, ontology design, and predictive modeling, and one Staff Scientist Assistant (50% FTE for 12 months) for metadata curation and dataset reprocessing. Additional support includes one in-person synthesis meeting for all PIs (Year 1) and open-access publication fees for two manuscripts ($5,000 total) Milestones and Timeline Over the two-year project period, the Working Group will begin with a kick-off meeting to catalog datasets and establish metadata standards (Months 1‚Äì3), followed by implementation of the half-life re-fitting pipeline and re-analysis of E. coli and yeast datasets (Months 4‚Äì6). The mid-year phase will focus on integrating mouse and human data, developing the ontology, and modeling turnover predictors (Months 7‚Äì9), ortholog mapping, and presentation of preliminary findings at NCEMS (Months 10‚Äì12). The second year will emphasize cross-species modeling and release of a beta version of the Protein Stability Database (PSD) (Months 13‚Äì15), manuscript drafting and model validation (Months 16‚Äì18), public release of the analysis pipeline and finalized ontology (Months 19‚Äì21), and a synthesis meeting culminating in final manuscripts and a white paper summarizing project outcomes (Months 22‚Äì24).",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "9",
            "proposal_title": "Predictive Synthesis Framework for Elucidating Biophysical Cell State from Spatial Transcriptomics and Proteomics",
            "authors": [],
            "authors_departments": [],
            "abstract": "Cell states reflect not only molecular abundance but also where molecules reside. Public spatial omics (Spatch) now provide high-plex RNA (Xenium/CosMx) and protein (CODEX) maps, yet cross-modal synthesis at subcellular scales remain fragmented. We will develop an open, end-to-end synthesis framework that (i) registers adjacent-section spatial RNA-Protein images at subcellular resolution using diffeomorphic alignment (STalign), (ii) harmonizes RNA features into protein-scale rasters (SEraster) to create a one-to-one correspondence of patches to be compared (iii) quantify membrane/nuclear proximity, polarity, local density, and other features (iv) trains RNA-to-protein predictors and learn a unified biophysical fingerprint of cell state. Using the paired Xenium/CosMx-CODEX cancer atlas as a validation set and curated public cohorts as application sets, we will benchmark against abundance-only baselines. Framework performance will be assessed by overall registration quality, verified patch correspondence, and cross-modal predictive accuracy, with appropriate positive and negative controls to rule out leakage and spurious structure. We will test the hypotheses that RNA-protein spatial relationships yield predictive signals for cell state beyond abundance and that low- dimensional biophysical fingerprints precede and predict cell fate decisions. All code, trained models, and harmonized datasets generated from this work will be released under permissive licenses with containerized, reproducible workflows and scheduled public snapshots. This community-scale synthesis will convert dispersed spatial imaging datasets into predictive, interpretable models of cell state, while leveraging NCEMS (CyVerse compute, data harmonization, and project management) for scalable analysis and dissemination.",
            "full_draft": "Introduction and Goals: Cells with similar bulk expression can behave very differently because subcellular location‚Äîat membranes, along the nuclear periphery, or in polarized domains‚Äî shapes function.1,2 In immunity, cancer, and development, this subcellular organization often governs signaling and decision points.3‚Äì7 Yet, most spatial omics analyses still treat RNA and protein as separate, abundance-centric snapshots, leaving the predictive role of subcellular organization undermeasured. Progress has been limited due to: (i) scarcity of public datasets that pair spatial transcriptomics with spatial proteomics at subcellular scales; (ii) the technical difficulty of aligning adjacent sections; (iii) standard labels and benchmarks that prioritize abundance over localization; and (iv) the lack of community workflows to compare modalities on matched subcellular patches. A recent benchmarking study introduced Spatch, a large public resource with paired spatial transcriptomics and proteomics from adjacent sections in three human cancers.8 This creates a timely opportunity, but without shared, reproducible methods to compare RNA and protein in the same subcellular context, analyses will remain abundance-centric and incompatible. Our long-term goal is to establish a predictive synthesis framework that treats subcellular organization as a core variable alongside abundance, elucidating biophysical cell state from spatial transcriptomics and proteomics. The objective is to create and test learned biophysical fingerprints‚Äîcompact, interpretable features that integrate RNA and protein localization‚Äîto forecast cell state and fate. We will test two hypotheses: (H1) subcellular RNA and protein organization adds predictive signal for cell state beyond abundance; and (H2) learned biophysical fingerprints precede and predict cell fate and associated protein localization. By learning interpretable fingerprints that link RNA and protein localization to cell state and fate, this work will provide a general framework for connecting molecular maps to emergent behavior and advance the understanding of how cellular systems are organized and regulated. Figure 1. A. Framework flow: Paired spatial RNA(Xenium/CosMx) and protein(CODEX) images are registered (STalign) and aggregated into shared subcellular patches (SEraster), and summarized as biophysically meaningful features. B. Cell state and fate: Hypothesis 1 (left), our multimodal framework will outperform abundance-only models, recovering known types and revealing macrophage sub-states (resting vs activated). Hypothesis 2 (right), analyzing lineage data, the fingerprint separates fates (e.g., neuron vs liver) pre-commitment and predicts branch-specific protein localization from progenitor RNA profiles. Proposed Activities: Activity 1. Develop a unified subcellular synthesis framework. We will construct an open- source pipeline to integrate, annotate, and synthesize paired spatial RNA and protein datasets. It will operate on the paired spatial transcriptomics (Xenium, CosMx) and spatial proteomics (CODEX) data from adjacent serial sections of three human cancers in Spatch. Our pipeline will be configurable and readily extensible to diverse paired spatial RNA and protein datasets across various tissues as they become available. The pipeline proceeds in three distinct steps: (i) register adjacent RNA and protein sections into a common coordinate frame, (ii) harmonize them onto a shared pixel grid to create one-to-one subcellular patches, and (iii) quantify biophysically meaningful features on each matched patch. Subcellular registration (STalign). Adjacent sections may undergo rotations, stretching, and other non-rigid distortions during imaging, making rigid or affine transformations insufficient for image alignment. To address these challenges, we previously developed STalign, which uses large deformation diffeomorphic metric mapping (LDDMM) to align thin 2D sections so that shared tissue structures coincide in a common coordinate framework.9,10 We will apply STalign to register Xenium with CODEX, as well as CosMx with CODEX, to enable comparison of transcriptomic and proteomic expression magnitudes at matched spatial locations. Harmonize to shared patches (SEraster). To evaluate correspondence at matched spatial locations, we will apply our previously developed method SEraster to aggregate molecular expression onto patches on a shared pixel grid, yielding a one-to-one correspondence of patches across modalities to be compared. Quantify biophysical features. For each patch across the registered datasets, we will compute (i) total expression magnitude; (ii) nuclear and cytoplasmic expression integrating DAPI-nuclear staining; (iii) membrane-associated expression; (iv) colocalization; (v) supervised image features such as SIFT scores; and (vi) deep-learning-based extracted features from domain-specific vision foundation models such as SubCell.11 This spatial registration and rasterization will establish the essential framework for subsequent analysis and hypothesis testing. Evaluation. We will quantify registration accuracy by (i) median/RMSD error between manually annotated shared landmarks and (ii) concordance of matched-location molecular signals (Pearson/Spearman or mutual information), plus per-cell centroid error where segmentations exist. Deformation quality will be checked via Jacobian regularity and inverse-consistency. Patch correspondence will be verified by identical shared-grid indices and mask overlap (tissue, nuclei, membrane; Dice/IoU). Controls include (i) RNA-to-RNA alignment (Xenium to CosMx) to provide an upper bound in the absence of cross-modality differences, and (ii) spatial shuffles/label permutations as negative controls to ensure apparent structure requires true spatial correspondence. The Spatch resource supplies paired Xenium/CosMx (RNA) and adjacent- section CODEX (protein) data and documents CODEX to Xenium/CosMx registration, providing an appropriate basis for cross-modal evaluation. Activity 2. Test Hypothesis 1 ‚Äî Cell state: subcellular RNA‚Äìprotein organization adds predictive signal beyond abundance. RNA and protein levels often correlate poorly due to post-transcriptional control, yet the predictive contribution of subcellular localization is rarely quantified.12‚Äì15 With the Spatch dataset and registered sections, shared subcellular patches, and quantified features from Activity 1, we will (i) learn a multimodal fingerprint (embedding) of cell state and (ii) build spatial RNA-to-protein predictors. Primary Question: biophysical fingerprint (embedding) of cell state. We will train with a contrastive, multimodal embedding that maps RNA and protein from the same cell/region to nearby points, yielding a single numeric fingerprint of cell state. To interpret these fingerprints, we will use a two-step validation and discovery approach. (i) Validation: we will project Spatch‚Äôs cell-state labels into the learned embedding and compare against abundance-only embeddings to confirm recovery of known groups and proper RNA-protein pairing. (ii) Discovery: within a single annotated type, we will test whether adding spatial features reveals biophysical sub-states missed by abundance-only clustering and characterize each sub-state by its RNA-protein spatial relationships (e.g, colocalization, mutual exclusion, boundary-restricted patterns), with representative patches for interpretation. For example, for T cells, validation requires that CD4+ and CD8+ classes from distinct neighborhoods and that paired RNA/protein from the same region are nearest neighbors across modalities. For discovery within macrophages, the embedding may separate a resting sub-state with diffuse protein distribution from an active/polarized sub-state with membrane-enriched proteins. We will summarize these patterns with their defining RNA-protein spatial features and patches. Secondary Question: Predictive model. We will train a second model to predict protein- localization maps from spatial RNA features. We will then benchmark it against two baselines: (i) RNA-abundance-only model using per-cell RNA counts aggregated across space (Xenium/CosMx), and (ii) non-spatial model using SEraster patch RNA totals with spatial cues removed. All models will be evaluated against the same CODEX protein localization maps on the shared SEraster grid. We hypothesize that incorporating spatial RNA features will improve protein-location predictions by capturing trafficking and other biophysical processes absent from non-spatial measurements. Activity 3. Test Hypothesis 2 ‚Äî Cell fate: biophysical fingerprints can predict cell fate decisions. Building on Activity 1 (registered sections, shared subcellular patches, quantified features) and Activity 2 (validated fingerprint and RNA to protein predictor), we will test whether subcellular organization provides early predictive signal of cell fate. Current fate models rely on transcriptomic snapshots, yet transcriptionally similar cells can diverge, implying that determinative information may be carried by spatial molecular organization. We will compute fingerprints to test whether spatial signatures (i) separate lineages before abundance changes and (ii) let progenitor profiles forecast fate-specific protein patterns. Primary Question: Numeric fingerprint model. We will apply the validated fingerprint model to trajectory datasets. We will test whether cells destined for different fates (e.g., progenitor versus differentiated) separate in fingerprint space before detectable changes in bulk gene expression. We will then summarize the dimensions driving early separation as specific RNA-protein spatial relationships (e.g., membrane enrichment, boundary-restricted patterns). Secondary Question: Predictive model. We will apply the validated RNA-to-protein predictor to generate per-cell protein-localization maps. We will track how these predicted protein patterns change along inferred lineages and branches. We will test whether progenitor RNA profiles predict the protein-localization signatures of downstream differentiated states. We will benchmark against predictors that ignore spatial structure and against RNA-abundance-only baselines. Together, these analyses will generate testable hypotheses about how subcellular spatial architecture influences cell fate. Outcomes: Technological advancement. A validated computational and biophysical framework will be produced, together with an open-source toolchain for spatial registration, cross-modal integration, and hypothesis generation. These resources will enable researchers to apply the framework to large, heterogeneous public datasets and to reproduce analyses across labs. Scientific advancement. The work will advance the field toward predictive understanding of cellular function by integrating fragmented datasets to identify biophysical mechanisms that govern cellular organization and cell-state transitions. It will address a recognized gap in linking molecular patterns to measurable phenotypes. Data resources. Harmonized Validation and Application datasets, with associated metadata curated by NCEMS staff, will be released as standardized community resources to support future computational and biophysical studies. Publications. One primary literature article is expected to report the framework and the biological insights from its application, including the Hypothesis Roadmap. Rationale for a working group approach: This project spans four expert domains: computational machine learning, single-molecule biophysics, statistical physics, and cell biology, beyond the scope of any single PI‚Äôs lab. Dr. Fan will lead the generative AI framework with biophysical constraints. Dr. Chen will provide the single-molecule models and measurements to inform those constraints. Dr. Presse will contribute statistical-physics methods for uncertainty quantification and correlation validation. This team delivers a framework that is computationally robust, biophysically grounded, statistically validated, and biologically interpretable. Rationale for NCEMS support: This project is a data-synthesis effort aligned with the NCEMS mission, but it is not feasible without infrastructure that our PI team does not possess. Feasibility depends on three NCEMS-supported capabilities: (i) expert data harmonization to integrate two large, heterogeneous sources (the Spatch validation set and an Application set assembled from fragmented public data), a task requiring Staff Scientist effort; (ii) scalable computation to train a generative AI model on terabytes of imaging data, enabled through NCEMS-coordinated CyVerse resources; and (iii) dedicated project management to coordinate a multi-institutional, virtual team and keep milestones on track. With this combination of harmonization, computation, and coordination, the work is executable; without it, the project remains out of reach. Requested resources: IT support. We request access to NCEMS-coordinated CyVerse computation and storage, including scalable capacity for the curated validation and application dataset for Aim 1 (estimated 20 TB) and high-performance, GPU-enabled resources to train and validate the generative AI framework for Aim 2 and 3 (estimated 106 CPU hours and GPU hours). Staff scientist support. We request 8-month support from a staff scientist with expertise in bioinformatics, data harmonization, and management of large public repositories. This role will lead data curation, harmonization, and pipeline management for Aim 1. We request a staff scientist (12-month) with expertise in cloud computing and ML/AI pipeline deployment to support PIs in deploying, debugging, and benchmarking the AI model on CyVerse for Aims 2 and 3. Proposed Timeline: Time Specific Activity Milestone (M) Months 1-3 (Q1) | Project kickoff & NCEMS training | Develop data governance plan | Finalize \"Application Set\" datasets M1: Project initiated Months 4-9 (Q2-Q3) | NCEMS staff harmonizes datasets | PIs develop & benchmark v1.0 AI framework | M2: Datasets harmonized M3: AI v1.0 benchmarked Months 10-15 (Q4-Q5) | Validate AI model on \"Spatch\" data (Aim 1) | Begin biophysical analysis (Aim 2) | Hold in-person working group meeting | M4: AI model validated M5: In-person meeting held Months 16-21 (Q6-Q7) | Apply validated AI to \"Application Set\" (Aim 2) | Analyze data & generate \"Hypothesis Roadmap\" (Aim 3) | M6: \"Hypothesis Roadmap\" generated Months 22-24 (Q8) | Release open-source toolchain (Aim 3) | Submit framework paper for publication | M7: Toolchain released M8: Manuscript submitted",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "10",
            "proposal_title": "Integrating AI and Molecular Dynamics Simulations for Quantitative Prediction of Enzyme Functions",
            "authors": [],
            "authors_departments": [],
            "abstract": "Recent AI advances have transformed biomolecular research, enabling accurate prediction of biomolecu- lar functions from sequence and structural data. However, the role of dynamic motions and environmental responses in shaping biomolecular function remains underexplored in AI-driven approaches. In this pro- posal, our working group aims to bridge this gap by integrating temporal and physicochemical insights from molecular dynamics (MD) with AI techniques to enhance predictive accuracy in protein function, focusing on enzymes. We will work closely with NCEMS in two phases: (I) Comprehensive data curation of enzyme function annotations, collection of multi-scale simulation trajectories, and biophysical deter- minants causally linked to enzyme activity. (II) Synthesizing simulations and biophysical determinants through a multi-modal AI model to enhance enzyme function predictions. The project integrates comple- mentary expertise across multiple labs and will leverage NCEMS computational resources to generate large-scale data. By synergizing AI and MD, we will advance biomolecular modeling beyond the static sequence-structure paradigm to include temporal resolution, guiding rational design of therapeutic and functional biomolecules.",
            "full_draft": "Introduction and Goals Background and Significance Recent advances in artificial intelligence (AI), particularly in protein lan- guage models (PLMs)[1] and structure-augmented PLMs[2,3], have enabled highly accurate predictions of protein functions from sequence and structural data. However, these models did not consider protein dy- namics, which are crucial for many functional proteins, including enzymes[4], folding-switching proteins[5] , and intrinsically disordered proteins[6]. Molecular dynamics (MD) simulations capture temporal and mech- anistic information governed by physicochemical force fields, and recent forcefield development[7‚Äì9] has significantly enhanced simulation accuracy across various resolutions, from all-atom to coarse-grained (CG) models. These advances present an unprecedented opportunity to integrate dynamic simulation data into AI foundation models to enhance biomolecular functional predictions. Synergistic MD4AI Framework We recently developed MD4PLM, an AI-MD framework that inte- grates temporal information and structural ensembles from multi-resolution MD simulations into dynamics- informed embeddings for protein function predictions (Fig. 1). The model highlights several innovations: ‚ù∂ Spatio-temporal GNN: (1) Dynamic Graph Construction: MD trajectories are modeled as time-indexed residue-level graphs, where spatial edges from Voronoi tessellation and sequential edges preserve both con- tact and backbone structure. (2) Spatial Encoding via Dynamic Conformational Encoder: Within each frame, residue features are updated through geometric-aware message passing and edge refinement. (3) Temporal Encoding with Causal Attention: Causal self-attention integrates past conformations to cap- ture multi-timescale dynamics while preserving physical causality. (4) Trajectory Aggregation: Frame- wise embeddings are pooled via learnable weights to yield residue-level representations. ‚ù∑ Aggregation of Conformation Embeddings: Structural embeddings from sampled conformations are aggregated via mean pooling into a dynamics-informed representation and fused with baseline sequence or structure em- beddings, providing an augmented input enriched with conformational diversity for downstream function 1 predictions. ‚ù∏ Integrating Multi-scale Simulations via Gated Fusion: Trajectories from all-atom, coarse- grained (CG), and AI-driven MD (AI-MD) are independently encoded to generate view-specific dynamic embeddings. A gated fusion module, inspired by multi-modal learning techniques[10], is employed to syn- thesize complementary information and functional impacts across different MD trajectories. By incorporating MD-derived dynamics, MD4PLM augments static sequence and structure representa- tions with conformational diversity and temporal information, enabling more accurate prediction of protein functions shaped by both sequence constraints and dynamic structural transitions. In preliminary studies, we applied MD4PLM to predict annotated Gene Ontology (GO) terms and Enzyme Commission (EC) numbers, the most widely used and generalizable benchmarks for protein function prediction[3,11‚Äì13] . Curated Input Data Data curation is an indispensable element of our work. To inte- grate protein dynamics into function predic- tion, we curated both functional annotations and simulation trajectories across multiple resolutions. Functional Annotation: We collected high- confidence labels from UniProt/SwissProt[14] , including GO terms (molecular function (MF), biological process (BP), and cellu- lar component (CC)) and EC numbers for 1,861 proteins with experimental structures from PDB[15]. To ensure statistical robust- ness, we retained only GO terms and EC classes with sufficient label support (‚â•50 and ‚â•10, respectively). Sequence redun- dancy was reduced to 95% identity using Figure 1: Illustration of the MD4PLM model. Protein sequences are processed through the pre-trained ESM-2 model [1] to generate sequence embeddings, which are integrated with multi-scale MD simulations ‚Äì All-atom, CG, and AI-MD ‚Äì to capture protein conformational dynamics. Dynamic graphs (a) are constructed via Voronoi tessellation and backbone connectivity (b) followed by geometric modeling. (c) Temporal dependencies are incorporated to propagate short-term interactions into long-term functional states. The fused embeddings are then used for downstream function prediction, including GO term and EC number annotations. MMseqs2[16], and the data was split into training, validation, and test sets at (8:1:1), stratified by function classes. Multi-resolution Simulation Data: We curated and performed MD simulations using three complementary strategies: (1) All-atom MD: Trajectories from ATLAS[17] (100 ns, CHARMM36m forcefield[7]) and md- CATH[18] (500 ns, CHARMM22*[19] forcefield) were collected, capturing localized fine-grained atomistic interactions. (2) CG MD: Simulations were carried out using the MARTINI forcefield[9,20,21] (100 ns), capturing large conformational transitions. (3) AI-MD: Generative model MDGen[22] was used to gener- ate simulation trajectories (100 ns), accelerating rare-event sampling. In total, we collected trajectories for all 1,861 functionally annotated proteins. As shown in Fig. 2, three simulations capture complementary conformational spaces. Preliminary Results As summarized in Table 1, MD4PLM con- sistently outperforms state-of-the-art sequence-based[1,26], structure- based[2,11], and hybrid sequence-structure models[3,27] across mul- tiple protein functional prediction tasks. Specifically, (1) Incorpo- rating MD simulated ensembles into existing models (GearNet[11] , GVP[2], and LM-GVP[27]) improves prediction accuracy over static sequence‚Äìstructure baselines. (2) MD4PLM-All-atom, trained only with all-atom MD trajectories, shows that explicitly modeling tempo- ral dependencies leads to additional performance gains. (3) Each MD model contributes distinct advantages: all-atom MD improves GO- BP (Fmax=48.04), CG MD gives the best EC prediction (Fmax=38.40), 2 Figure 2: Time-lagged independent component analysis (tICA) [23‚Äì25] visualization of a protein‚Äôs conformational landscape from MD simulations in three resolutions: all-atom simulations (blue), CG simulation (green), and AI-MD simulation (red). and AI-MD performs best on GO-MF (Fmax=50.46). (4) Integrating all three dynamic views via gated fu- sion leads to the strongest overall performance (Fmax=49.02, AUPRC=47.37), with consistent gains across EC and GO categories. The largest improvement is observed in GO-CC (Fmax=61.75), where diverse con- formational dynamics are critical. Together, these results demonstrate that multi-resolution dynamics offer complementary, non-redundant dynamics information that enhances protein function prediction, with mod- els trained on CG and AI-MD trajectories achieving accuracy comparable to all-atom simulations. Goals In this NCEMS proposal, we will analyze and refine MD4PLM model for biomolecular function prediction, and upgrade it to integrate multi-scale simulation data with biophysical determinants. Although MD4PLM is broadly applicable to any biomolecular functions, we will focus on enzyme function predic- tions, given the strong applications of enzyme activity and its close link to protein dynamics[4,28]. We will curate and synthesize data across multiple simulation resolutions‚Äìall-atom, CG, and AI-MD‚Äìand integrate functionally important biophysical determinants. We aim to address the following key questions: ‚ù∂ Curation of dynamic-relevant enzyme functions: (1) Which enzyme functions are linked to protein dy- namics? (2) How can simulation-derived dynamics most effectively improve function prediction? ‚ù∑ Enhancing predictions with biophysical determinants: (1) Which determinants can be incorporated in AI training to improve enzyme function predictions? (2) How can they be optimally integrated into MD4PLM? ‚ù∏ Optimizing the use of limited MD data: (1) What are the optimal simulation lengths and sampling fre- quencies given finite computational resources? (2) How to design an AI model to best leverage available simulation data and enhanced sampling techniques? Proposed Activities Our central hypothesis is that enzyme function prediction can be improved by incorporating protein dy- namics from multi-scale MD simulations and biophysical determinants that causally influence enzymatic activity. To test this, we will collaborate with NCEMS on a two-phase project synthesizing simulation and biophysical data to benchmark our MD4PLM model and evaluate the selection of biophysical determinants to enhance predictive accuracy. Table 1: Performance comparison of different protein representation models. Gray-shared regions represent predictions with models trained with MD-supplied data. Bolded numbers represent the best predictive performance among all the models. Phase I: Curation of Enzyme Data and Biophysics Determi- nants We will work with NCEMS staff scientists to curate a com- prehensive enzyme database with well-annotated functions, includ- ing, but not limited to, EC num- bers, UniProt IDs, PDB struc- tures, key mutations, thermostabil- ity, turnover numbers, and catalytic efficiency. Data will be curated from public databases (see the sub- mitted Datasets Table). We will extract biophysical fea- tures from both the experimen- tal[29] or predicted[30‚Äì34] structures. We will focus on two resolutions: (1) Residue-level features: electrostatic surface potential computed by Adaptive Poisson-Boltzmann Solver (APBS)[35], frustration index computed by Frustratometer[36‚Äì39], solvent Accessible Surface Area (SASA) computed by MDtraj[40], and crystallographic B-factors extracted from PDB structures[29]. (2) Model EC | GO-BP || GO-MF || GO-CC || Average Fmax AUPRC Fmax AUPRC Fmax AUPRC Fmax AUPRC Fmax AUPRC Sequence-Based Models. Molecule-level features: statistical potentials for protein stability, such as RWPlus[41], DOPE[42], and GOAP[43]. Biophysics-based energies from CG[44,45] and all-atom[7,46] force fields. We will then collect multi-scale simulations for the identified enzymes: (1) All-atom simulations from existing databases, such as ATLAS[17], mdCATH[18], and public datasets from Pittsburgh Supercomputing Center (PSC) Anton projects. (2) CG simulations generated using MARTINI models[9,20,21]. (3) AI-MD simulations generated using MDGen[22] or other generative modeling tools[47]. From these trajectories, we will derive additional simulation-based biophysical determinants, such as residual forces, water/ions radial distribution functions (RDFs), temperature, and salt concentrations. All the processed trajectories and determinants will be deposited in open repositories (e.g., MDRepo[48]) for transparency and reproducibility. Phase II: Synthesizing Multi-scale MD Simulations and Biophysical Determinants to Predict Enzyme Functions We will apply MD4PLM to predict enzyme functions annotated from Phase I. Specifically, we will utilize multimodal learning techniques recently developed in PI Chen‚Äôs lab[49‚Äì68] to integrate spatial and temporal information from multi-scale MD simulations, along with extracted biophysical determi- nants, into unified embeddings for enzyme function predictions. Importantly, we will analyze cases where MD4PLM fails, which will guide us in identifying key biophysical determinants influencing enzyme func- tions, selecting simulation data that fully cover the values of these biophysical determinants, and further improving the architecture design of MD4PLM. Meanwhile, we will benchmark MD4PLM performance against state-of-the-art (SOTA) sequence-based and structure-based protein function prediction models, which will help us identify enzyme functions most improved by incorporating simulated protein dynamics. By the end of Phase II, we aim to establish a deeper understanding of enzyme functions where a dynamics-aware predictive model can be beneficial, and develop an upgraded MD4PLM model for enzyme function prediction. Outcomes Phase I will deliver a comprehensive, multi-resolution MD simulation data for enzymes with well- annotated functions and a curated dataset of biophysical determinants causally related to enzyme activ- ities. Phase II will deliver a benchmark of MD4PLM against SOTA enzyme function predictors, identify key biophysical determinants influencing enzyme functions, and highlight functions that benefit most from simulated dynamics. This phase will culminate in a collaborative research publication detailing our inte- grative data analyses and a review article summarizing the current SOTA predictive models, evaluating the benefits of integrating sequence, structure, and dynamics in enzyme function predictions. All datasets will be made publicly available in collaboration with NCEMS via open repositories (e.g., MDRepo, Zenodo, GitHub) under open science principles. Rationale for a Working Group Approach Our working group brings together a unique team with complementary expertise spanning MD simulations and biophysical analyses (Lin), AI model development (Chen and Shen), and bioinformatics and experi- mental enzyme studies (Lagard). No single lab possesses the full expertise and computational resources needed for the proposed goals. Additionally, the extensive MD simulations and data storage outlined in Re- quested Resources necessitate a collaborative team effort. Pitfalls and Alternative Approaches Our preliminary results (Figs. 1 and Table 1) support the feasibility of our approach. A potential challenge is the limited conformational sampling in MD simulations, a common limitation across MD-based studies. To mitigate this, we will focus our simulations using CG and AI-MD models and benchmark results against all-atom simulations for selected proteins. If discrepancies remain between predicted and experimental functional properties, we will apply enhanced sampling techniques in MD simulations, such as replica exchange[69] and umbrella sampling[70]. We will further validate simulations with multiple force fields[7,8,71] for selected proteins to ensure accurate representation of simulated structural ensembles. 4 Rationale for NCEMS Support The proposed project is highly interdisci- plinary and computationally intensive, re- quiring coordination and resources beyond individual labs. NCEMS support is essen- tial to bring together all four teams and assis- tance from the staff scientists. Specifically, we request support of NCEMS staff scien- tists to: (1) curate enzyme property data from existing databases (see Datasets Ta- ble); (2) perform MD simulations; (3) ex- tract biophysical features from structures and simulated structural ensemble. Finally, additional compu- tational resources from NCEMS are requested for MD simulations. Requested Resources NCEMS staff scientist (8 months support over 2 years): The staff scientist will curate enzyme function Figure 3: Computational benchmark MD simulations of five proteins of different sizes using MARTINI CG (A) and AI-driven MD (B) simulations. data, collect all-atom MD simulation datasets, perform MD simulations, and extract biophysical determi- nants from MD trajectories. Staff scientist assistant: 50% FTE per year): The assistant will support data curation and assist in per- forming MD simulations based on input packages provided by the working group. Computational resources: We will collect all-atom simulations from public repositories and perform CG and AI-driven MD simulations. Based on our computational benchmarks (Fig. 3), a typical 700-aa pro- tein from the existing data achieves‚àº209ns/hour using the MARTINI CG force field and‚àº2,283ns/hour for AI-driven MD on a single GPU. We will simulate 10,000 enzyme proteins selected from the public databases (see the uploaded Datasets Table for details), and requested 156,681 GPU hours (or 10 million CPU hours based on the credit exchange and calculator provided by NSF-ACCESS), ensuring the comple- tion of 3 ¬µs of simulation per protein, and 30 TB of data storage (3 GB per protein). Program manager and event coordinator: As needed. ‚ù∫ Publication costs: $5,000 to support open access publication. Personnel exchange costs: Funds for three short-term graduate student exchanges between labs and visits to NCEMS. Proposed Timeline: 12 months Phase I, 12 months Phase II 0-2 months: Data curation and simulation setup: We will start Phase I with a joint kick-off meeting with the NCEMS project manager to finalize the plan. We will curate data to select enzymes with well-annotated functions, collect or predict structures using deep learning tools, and prepare CG- and AI-driven MD sim- ulation packages. NCEMS commitment: staff scientist (1 month); assistant (50% FTE). 3-12 months: MD simulation and feature extraction: We will collect all-atom simulations and perform CG and AI-driven MD simulations for curated enzymes and extract biophysical features from the trajectories. NCEMS commitment: staff scientist (3 months); assistant (50% FTE). 13-18 months: Biophysical determinants analysis: We will continue collecting simulation data, analyze ex- isting simulation data using the MD4PLM model to identify key biophysical determinants causally related to enzyme function, and integrate them for improved prediction accuracy. NCEMS commitment: staff sci- entist (3 months), assistant (50% FTE). 19-24 months: Benchmark SOTA model and dissemination: We will benchmark enzyme function predic- tions between MD4PLM and all SOTA models. We will prepare a research article and a review article as described in Section Outcomes. NCEMS commitment: staff scientist: (1 month); assistant: (50% FTE).",
            "proposal_status": "",
            "ranking": 0
        },
        {
            "proposal_id": "11",
            "proposal_title": "Discovery and functional analysis of bacterial organelles through mining and refinement of omics data",
            "authors": [],
            "authors_departments": [],
            "abstract": "Once considered exclusive to eukaryotes, subcellular organelles are now recognized as common features of prokaryotic cells. Bacterial organelles exhibit remarkable diversity and specialization, falling into three structural classes: protein-bounded compart- ments, lipid-bounded compartments, and biomolecular condensates. While recent model sys- tem studies have uncovered molecular mechanisms controlling the formation and function of several bacterial organelles, their activities in ecological contexts, environmental responsive- ness, and inter-organelle interactions remain poorly characterized. Here, we propose a com- munity synthesis approach to define the landscape of bacterial organelles in the human gut mi- crobiome using publicly available multi-omics data. We have assembled a team of experts on the genetics and biochemistry of bacterial microcompartments, encapsulins, ferrosomes, and biomolecular condensates. With input from NCEMS staff we will enumerate the occurrence of bacterial organelles from metagenomic datasets and examine meta-transcriptomic and meta- proteomic data to characterize their regulation and expression. This framework will allow us to test key hypotheses, such as whether organelles with overlapping functions can co-exist within the same organisms as well as various members of a microbial community and how organelle expression patterns change in response to environmental conditions. We will also examine co- ordinated expression between organelles, which will provide insights into potential inter-orga- nelle interactions. These studies will form the basis for publications on the distribution, co-oc- currence, and functional roles of bacterial organelles. The generated data will guide future hy- potheses, shed light on the evolutionary trajectory of cellular compartmentalization, and enable the rational design of biotechnological applications based on the unique features of bacterial organelles.",
            "full_draft": "Introduction and Goals. For decades, the presence of subcellular organelles was seen as the hallmark of eukaryotic cells - a biological invention that enabled the evolution of complex cells and multicellular organisms. In contrast, bacterial cells were often described as \"bags of en- zymes\" devoid of recognizable compartments. However, we now know that organelle formation is common in many prokaryotic cells and central to their metabolic health and survival in challenging environments. Most eukaryotic cells possess a standard set of organelles such as the nucleus, mitochondria, and endoplasmic reticulum. Bacterial organelles, however, are far more diverse and specialized. They fall into three broadly defined classes based on their structural compart- mentalization. First are protein- bounded compartments - a form of compartmentalization unique to bacteria. These include Bacterial Microcompartments (BMCs) Figure 1. The three classes of bacterial organelles display distinct genetic \"fingerprints\". (A) Protein-bounded organelles, such as bacterial microcompartments, are marked by operons encoding shell proteins and cargo enzymes. (B) Lipid-bounded organelles, like ferrosomes, feature conserved genes (e.g., fezB) typically arranged in operons. (C) Biomolec- ular condensates lack specific genetic markers, instead characterized by features such as intrinsically disordered regions, and components en- coded at separate loci. and encapsulins, both featuring a protein shell encapsulating an enzymatic core (1, 2). Second are lipid-bounded compartments, such as magnetosomes in magnetotactic bacteria and the recently described ferrosomes, both of which consist of metal crystals encased within a lipid bilayer membrane (3, 4). The third category consists of biomolecular condensates - dynamic assemblies of diverse macromolecules that form in cells through phase transitions, and whose diversity and roles in bacteria are only beginning to be appreciated (5, 6). Despite this substantial revision in our understanding of cellular life, knowledge of bacterial organelles remains rudimentary compared to our sophisticated understanding of eukaryotic or- ganelles. Several features of bacteria have impeded study of their organelles. First, bacterial cells are significantly smaller than eukaryotic cells, requiring advanced light and electron mi- croscopic techniques for observation and identification. Second, bacterial organelles often oc- cur in uncultured or less commonly studied organisms, necessitating the development of spe- cialized culturing regimes and genetic tools. Finally, most bacteria live in complex microbial communities. Studies of model bacteria growing under lab conditions may therefore obscure organelle regulation and function. Due to these obstacles, organelle function in complex micro- bial communities, their response to environmental cues, and potential interactions between compartments remain poorly understood. A bioinformatics approach, followed by synthetic biology, offers a powerful means to ad- dress these challenges. The prevalence of BMCs, encapsulins, and ferrosomes across the bacterial kingdom has been characterized (1, 3, 7). However, no studies have utilized meta- omics data to identify and elucidate the regulation of these organelles. Moreover, a systematic assessment of condensate-forming proteins in bacteria is still lacking. To address these gaps, we propose leveraging publicly available multi-omics datasets to systematically map the preva- lence, diversity, and regulatory dynamics of bacterial organelles. We will focus on the human gut microbiome, which, according to our data scouting, offers extensive publicly available multi-omics datasets. We will begin by leveraging recent advances in understanding the mo- lecular rules governing organelle formation and function to identify and categorize BMCs, en- capsulins, ferrosomes, and biomolecular condensates from metagenomic data. Next, we will examine associated metatranscriptomic and metaproteomic datasets to assess the regulation and expression of each organelle type in the gut microbiome, focusing on how expression pat- terns change in response to environmental conditions. We will also investigate coordinated ex- pression among organelles to explore potential inter-organelle interactions. These analyses will facilitate hypothesis development for future studies. Beyond simple ac- counting, we envision combining the assembled databases with gene neighborhood analyses to further refine the regulatory and functional landscape of bacterial organelles. Focused anal- yses of genetic and structural features can elucidate formation mechanisms and trace the evo- lutionary history of compartmentalization across the tree of life. This approach can be applied to other omics datasets to explore the ecological diversity of bacterial organelles. Finally, these combined efforts will provide data to more systematically exploit bacterial organelles in syn- thetic biology and biotechnology. Proposed activities 1. Identify and categorize BMCs, encapsulins, ferrosomes, and biomolecular condensates using curated metagenomic datasets. The human gut microbiome is perhaps the best-studied microbial community with a number of high quality and publicly available datasets ready to be used in our project. Therefore, we will utilize GMrepo, a manually curated database of human gut metagenomes with extensive sample metadata and classification based on human pheno- types. Our analysis will focus on samples from healthy individuals and those associated with diarrhea (PRJEB14038), Clostridium infections (PRJNA648321), cystic fibrosis (PRJNA314903), and irritable bowel syndrome (PRJNA70521). These dysregulated states rep- resent environmental perturbations that have a multitude of impacts on the composition of the gut microbial community. We will systematically identify the presence and diversity of BMCs, encapsulins, ferrosomes, and biomolecular condensates across different human gut microbi- ome samples. We will test the hypothesis that organelles with overlapping functions, such as encapsulins and ferrosomes, display distinct patterns of occurrence and environmental respon- siveness within a specific ecological context. Specifically, we will determine which bacterial species harbor these organelles, whether individual hosts contain both organelle types or only one, and how organelle distributions vary with phenotypes and environmental conditions. With the exception of biomolecular condensates, the general workflow for identifying bacte- rial organelles across metagenomic datasets begins with the generation of profile Hidden Mar- kov Models (HMMs) for proteins specific to each organelle type (described below), sourced ei- ther from databases like Pfam or built from representative sequences using tools such as HMMER. HMM profiles are available on the InterPro website, accessible under the specific Pfam entries: ebi.ac.uk/interpro/entry/pfam/PFXXXXX. The profiles available there also contain information on the threshold used to reliably gather sequences without false positives but those values might need to be double checked for accuracy with our datasets. Metagenomic data are translated into protein sequences, which are then scanned using organelle-specific HMM profiles to identify the presence of various organelles. BMCs: Identified by the presence of hallmark shell proteins with Pfam identifiers PF00936 and PF03319, organized in operons of 5-20 genes with typically several copies of the two types of shell proteins (1). Encapsulins: Similar to BMCs, the most reliable method to identify them is by their shell proteins, with HMMs available for three distinct classes, PF04454 for type 1, PF19307 for type 2 and PF08967 for type 4. Unlike the BMC type shell protein, the encapsulins are closely re- lated to virus capsid proteins so the potential for false positives is much higher (7). Ferrosomes: Ferrosomes can be detected by the presence of FezB homologs; while there is no specific pfam available, the HMM for FezB can be built from the sequences availa- ble in the Supplementary Data 2 of (3). FezA proteins can also be identified as small mem- brane proteins containing the characteristic GXXXG motif but that would require different anal- ysis methods (3, 4). For biomolecular condensates, we will identify the presence of condensate-forming proteins in the human gut microbiome using the machine learning model Phaseek (8), which predicts and ranks proteins based on their likelihood to form condensates. 2. Identify the regulation and expression of bacterial organelles in the gut microbiome using meta-transcriptomic and meta-proteomic data. Previous activities allow us to develop HMMs and other tools to identify bacterial organelles in curated metagenomic databases. Here, we will use these tools and leverage multi-omics techniques to matched sample sets to gain com- prehensive insights into both the compositional structure and the functional expression of the gut microbiome. Specifically, we will utilize the Inflammatory Bowel Disease (IBD) Multi-omics Database (MGYS00006120), which provides longitudinal profiles of 90 subjects over the course of one year. By integrating these diverse datasets, we will track microbial community dynamics and the functional activity of bacterial organelles over time. Given that IBD is associated with ecological disruption and reduced microbial diversity, it offers an ideal context to investigate how environ- mental shifts regulate organelle expression and activity in complex microbiomes. We will test the hypothesis that organelle expression patterns change in response to environmental condi- tions and that certain organelles exhibit coordinated expression, suggesting potential inter-or- ganelle interactions. 2A. Identify the regulation and expression of BMCs, encapsulins, and ferrosomes. We will process and analyze these multi-omics datasets using MGnify, following the workflow specified in Wang et al. (9). This toolset includes methods to handle the gene calling of DNA sequence data for metagenomes as well as handling of proteomics raw data. The methods are freely available under https://github.com/PRIDE-reanalysis/MetaPUF. 2B. Identify the regulation and expression of bacterial condensates. Predicted conden- sate-forming proteins will be analyzed alongside metatranscriptomic data (MGnify). We will construct co-expression networks using WGCNA (10) and perform gene neighborhood analy- sis to infer potential regulatory modules. Meta-proteomic data, where available, will be used to validate protein expression. To further elucidate interaction networks, we will integrate multi- omics data and apply domain and disorder analyses using Pfam, InterProScan, and IUPred2A as appropriate. Outcomes. This project will deliver both conceptual advances and shared resources to trans- form how bacterial subcellular organization can be studied and modeled within molecular sys- tems science. Searchable, integrated database of bacterial organelles: We will assemble a comprehen- sive database cataloging BMCs, encapsulins, ferrosomes, and biomolecular condensates identified from curated metagenomic datasets. This resource will integrate multi-omic, struc- tural, and functional annotation to enable cross-species comparisons, facilitating discovery of new organelle classes and comparison of their ecological and functional roles. This database will be publicly accessible and actively maintained for at least three years. Hypothesis-driven insights into organellar function and distribution: By systematically ana- lyzing organelles across samples associated with distinct human phenotypes, such as diar- rhea, cystic fibrosis, Clostridium infections, and IBD, we will test the hypothesis that organelles with overlapping functions (for example, encapsulins and ferrosomes) exhibit distinct patterns of environmental occurrence and taxonomic distribution. We anticipate generating publications that detail the distribution, co-occurrence, and functional roles of organelles, both within and across organelle classes. Predictive framework for structural motifs and compartment discovery: Using computational tools including HMMER, Pfam, and Phaseek, we will predict protein structural motifs and iden- tify novel organelle candidates. These methods will extend beyond annotation and deliver mechanistic hypotheses regarding compartment assembly, organelle function, and adaptation strategies. Synthesis of existing datasets will reveal conserved and divergent strategies of compartmentalization across bacterial phyla. We will publish our predictive framework and its application in peer-reviewed journals. Regulation and expression analysis across multi-omics data: By integrating data from the IBD Multi-omics Database and using matched metagenomic, metatranscriptomic, and metap- roteomic samples, we will test the hypothesis that organelles are selectively expressed and regulated according to environmental stresses. We will track dynamic changes in organelle ex- pression over time, construct co-expression networks (e.g., using WGCNA) for condensates, and publish detailed analyses of regulatory patterns. Broader impacts: Training and outreach activities will engage early-career researchers and computational biologists, strengthening community capacity in microbial systems data science. Collectively, these outcomes will establish a new data-driven foundation for bacterial com- partment biology, aligning with NCEMS‚Äôs goals of integrating molecular data, developing pre- dictive models, and fostering cross-disciplinary collaboration. By uniting structural, functional, and evolutionary information, this work will redefine the boundaries of bacterial cell organiza- tion and create lasting infrastructure to accelerate discovery in microbial systems biology. Rationale for a working group approach. Our team brings together complementary exper- tise across different classes of bacterial compartments and experimental methodologies. By integrating these diverse perspectives, we can achieve a comprehensive and representative understanding of the current landscape of bacterial organelles - insights that would not be pos- sible by any individual member alone. Dr. Markus Sutter, a co-lead on our team, has extensive experience in cataloging the diversity and distribution of BMCs (1). Drs. Arash Komeili and Hualiang Pi have been instrumental in identifying ferrosome-associated (Fez) proteins essen- tial for ferrosome formation (3, 4). Drs. Daniel Trettel and Y Hoang contribute specialized ex- pertise in the emerging area of bacterial condensates (6, 11). Together, our collaborative ap- proach fosters cross-disciplinary innovation and positions us to make significant contributions to the understanding of bacterial organelles. Rationale for NCEMS support. NCEMS support is essential for the success of this project due to their specialized expertise in multi-omics data organization, integration, and advanced analytics. The NCEMS team will facilitate the harmonization and pairing of diverse data types, such as metagenomic, metatranscriptomic, and metaproteomic datasets, at the individual sam- ple level, ensuring robust cross-omics analysis. NCEMS‚Äô experience in handling large-scale, complex datasets will be invaluable for quality control, metadata standardization, and down- stream statistical analysis in our study. By leveraging NCEMS resources and computational infrastructure, we can accelerate data processing, enhance analytical rigor, and maximize the impact of our findings, positioning the project for future scale-up and broader collaboration. Requested resources Resource Request/Details Storage 20 TB (2‚Äì5 GB per isolate, raw data) Duration of support Staff scientist, staff assistant, storage, CPU CPU Staff Scientist 8 months, early in project 50,000 CPU hours (~100 CPU hours/genome assem- bly) Staff Scientist Assistant 6 months (Year 1), 5 months (Year 2) GPU None Proposed Timeline Timeframe Description Feb‚ÄìMar 2026 Team assembly & training: form the project team, schedule regular meetings, and conduct open science and bioinformatics training sessions. Apr‚ÄìJul 2026 Build HMMs for ferrosomes; use the diarrhea dataset (PRJEB14038) to conduct a pilot study to identify BMCs, encapsulins, ferrosomes, and condensates; apply the pipeline to other datasets in GMrepo; Jul-Aug 2026 Refine identification methods by determining generalizable markers for searching novel bacterial compartments. Sep‚ÄìDec 2026 Process the IBD multi-omics database (data cleaning, de novo annotation, harmo- nization,‚Ä¶) Jan‚ÄìMar 2027 Perform parallel analyses for each organelle class using the IBD multi-omics data. May 2027‚ÄìJan 2028 Develop and deploy searchable databases for specific bacterial organelles; write multiple manuscripts for peer-reviewed journals.",
            "proposal_status": "",
            "ranking": 0
        }
    ]
}